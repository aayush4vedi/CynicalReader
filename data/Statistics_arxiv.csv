Topic,Title,Content
Statistics,kernsmoothirt r package kernel smoothing item response theory,item response theory irt model class statistical model used describe response behavior individual set item certain number option adopted researcher social science particularly analysis performance attitudinal data psychology education medicine marketing field aim measure latent construct irt analysis use parametric model rely assumption often satisfied case nonparametric approach might preferable nevertheless many software application allowing use address gap paper present r package kernsmoothirt implement kernel smoothing estimation option characteristic curve add several plotting analytical tool evaluate whole testquestionnaire item subject order show package capability two real datasets used one employing multiplechoice response scaled response
Statistics,dbkgrad r package mortality rate graduation fixed adaptive discrete beta kernel technique,kernel smoothing represents useful approach graduation mortality rate though exist several option performing kernel smoothing statistical software package contribution date focused application technique graduation context also although shown use variable adaptive smoothing parameter based information provided exposed risk death provides additional benefit specific computational tool approach essentially absent furthermore little attention given providing method available software kind subsequent analysis respect graduated mortality rate facilitate analysis field r package dbkgrad introduced among available kernel approach considers recent discrete beta kernel estimator fixed adaptive variant approach boundary bias automatically reduced age pragmatically considered discrete variable bandwidth fixed adaptive allowed manually given user selected crossvalidation pointwise confidence interval considered age also provided application mortality rate sicily region italy year also presented exemplify use package
Statistics,expectation maximization framework yulesimon preferential attachment model,paper develop expectation maximization em algorithm estimate parameter yulesimon distribution yulesimon distribution exhibit rich get richer effect whereby type rule tends dominate distribution ubiquitous industrial setting em algorithm presented provides frequentist bayesian estimate lambda parameter placing estimation method within em framework able derive standard error resulting estimate additionally prove convergence yulesimon em algorithm study rate convergence explicit closed form solution rate convergence algorithm given
Statistics,quantile regression qualifying match probabilistic load forecasting,present simple quantile regressionbased forecasting method applied probabilistic load forecasting framework global energy forecasting competition hourly load data log transformed split longterm trend component remainder term key forecasting element quantile regression approach remainder term take account weekly annual seasonalities interaction temperature information used stabilize forecast longterm trend component public holiday information ignored still forecasting method placed second open data track fourth definite data track forecasting method remarkable given simplicity model method also outperforms vanilla benchmark consistently
Statistics,forecasting wind power modeling periodic nonlinear effect conditional heteroscedasticity,article present approach enables joint wind speed wind power forecast wind park combine multivariate seasonal time varying threshold autoregressive moving average tvarma model power threshold generalized autoregressive conditional heteroscedastic powertgarch model modeling framework incorporates diurnal annual periodicity modeling periodic bsplines conditional heteroscedasticity complex autoregressive structure nonlinear impact contrast usually timeconsuming estimation approach likelihood estimation apply highdimensional shrinkage technique utilize iteratively reweighted least absolute shrinkage selection operator lasso technique allows conditional heteroscedasticity provides fast computing time guarantee parsimonious regularized specification even though parameter space may vast able show approach provides accurate forecast wind power turbinespecific level forecasting horizon h short mediumterm forecast
Statistics,simulator engine streamline simulation,simulator r package streamlines process performing simulation creating common infrastructure easily used reused across project methodological statistician routinely write simulation compare method preexisting one developing idea temptation write quick dirty simulation try idea approach rapid prototyping useful sometimes backfire bug introduced using simulator allows one remove dirty without sacrificing quick coding quick statistician focus exclusively aspect simulation specific particular paper written code written simulator succinct highly readable easily shared others modular nature simulation written simulator promotes code reusability save time facilitates reproducibility syntax simulator lead simulation code easily humanreadable benefit using simulator include ability step simulation change one aspect without rerun entire simulation scratch straightforward integration parallel computing simulation ability rapidly generate plot table report minimal effort
Statistics,bayesian hierarchical modelling inferring genetic interaction yeast,identifying genetic interaction given microorganism yeast difficult quantitative fitness analysis qfa highthroughput experimental computational methodology quantifying fitness microbial culture qfa used compare fitness observation different genotype thereby infer genetic interaction strength current naive frequentist statistical approach used qfa model betweengenotype variation difference genotype variation different condition thesis bayesian approach introduced evaluate hierarchical model better reflect structure design qfa experiment first twostage approach presented hierarchical logistic model fitted microbial culture growth curve hierarchical interaction model fitted fitness summary inferred genotype next onestage bayesian approach presented joint hierarchical model require univariate summary fitness used pas information model new hierarchical approach compared using dataset examining effect telomere defect yeast better describing experimental structure new evidence found gene complex interact telomere cap various extension model including model data transformation batch effect intrinsically stochastic growth model also considered
Statistics,exponential increase test power ztest chisquare test auxiliary information,main goal article study auxiliary information used improve power two famous statistical test z test chisquare test information nature probability set partition expectation function even required exact information given estimate based larger sample example definition auxiliary information found statistical literature recalled article notion auxiliary information discussed general point view two statistical test modified auxiliary information taken account one show particular power test increased exponentially statistical example treated show concreteness method
Statistics,parameterization copula covariance decay stochastic process application,work study problem constructing stochastic process predetermined covariance decay parameterizing marginals given family copula present several example illustrate theory including important gaussian euclidean family copula associate theory common applied time series model present general methodology estimate given parameter interest identifiable process covariance decay exemplify proposed methodology present simple monte carlo application parameter estimation time series methodology also applied u stock market index
Statistics,generalized fellegisunter framework multiple record linkage application homicide record system,present probabilistic method linking multiple datafiles task trivial absence unique identifier individual recorded common scenario linking census data coverage measurement survey census coverage evaluation general multiple recordsystems need integrated posterior analysis method generalizes fellegisunter theory linking record two datafiles modern implementation multiple record linkage goal classify record ktuples coming k datafiles according different matching pattern method incorporates transitivity agreement computation data used model matching probability use mixture model fit matching probability via maximum likelihood using em algorithm present method decide record ktuples membership subset matching pattern prove optimality apply method integration three colombian homicide record system perform simulation study order explore performance method measurement error different scenario proposed method work well open direction future research
Statistics,multilayer tensor factorization application recommender system,recommender system widely adopted electronic commerce entertainment industry individualized prediction recommendation benefit consumer improve business intelligence article propose innovative method namely recommendation engine multilayers rem tensor recommender system proposed method utilizes structure tensor response integrate information multiple mode creates additional layer nested latent factor accommodate betweensubjects dependency one major advantage proposed method able address coldstart issue absence information new customer new product new context specifically provides effective recommendation subgroup information achieve scalable computation develop new algorithm proposed method incorporates maximum block improvement strategy cyclic blockwisecoordinatedescent algorithm theory investigate algorithmic property global local convergence along asymptotic consistency estimated parameter finally proposed method applied simulation iri marketing data million observation product sale numerical study demonstrate proposed method outperforms existing competitor literature
Statistics,control chart monitoring variance time series,paper derive control chart variance gaussian process using likelihood ratio approach generalized likelihood ratio approach sequential probability ratio method generalized sequential probability ratio procedure shiryaevroberts procedure generalized shiryaevroberts ap proach recursive presentation calculation control statistic given autoregressive process order extensive simulation study scheme compared existing control chart variance order ass performance scheme average run length average delay used
Statistics,two challenge stealthy hypervisors detection time cheating data fluctuation,hardware virtualization technology play significant role cyber security one hand technology enhance security level designing trusted operating system hand technology taken modern malware rather hard detect none existing method able efficiently detect hypervisor face countermeasure time cheating temporary self uninstalling memory hiding etc new hypervisor detection method described paper detect hypervisor countermeasure even count several nested one novel approach rely new statistical analysis time discrepancy examination set instruction unconditionally intercepted hypervisor reliability achieved comprehensive analysis collected data despite fluctuation offered method comprehensively assessed intel amd cpu
Statistics,power analysis smart design sample size estimation determining best dynamic treatment regime,sequential multiple assignment randomized trial smart design become increasingly popular field precision medicine providing mean comparing sequence treatment tailored individual patient ie dynamic treatment regime dtr construction evidencebased dtrs promise replacement adhoc onesizefitsall decision pervasive patient care however substantial statistical challenge sizing smart design due complex correlation structure dtrs embedded design since primary goal smart construction optimal dtr investigator interested sizing smart based ability screen dtrs inferior optimal dtr given amount done using existing method paper fill gap developing rigorous power analysis framework leverage multiple comparison best methodology method employ monte carlo simulation order compute minimum number individual enroll arbitrary smart evaluate method extensive simulation study illustrate method retrospectively computing power extending treatment effectiveness naltrexone smart study
Statistics,ssmousetrack analysing computerized tracking data via bayesian statespace model r,recent technological advance provided new setting enhance individualbased data collection computerizedtracking data became common many behavioral social research adopting instantaneous tracking device computermouse wii joystick data provide new insight analysing dynamic unfolding response process ssmousetrack r package modeling analysing computerizedtracking data mean bayesian statespace approach package provides set function prepare data fit model ass result via simple diagnostic check paper describes package illustrates used model analyse computerizedtracking data case study also included show use package empirical case study
Statistics,investigating sprawl using aic recursive partitioning tree machine learning approach assessing association poverty commute time,sprawl according glaeser kahn century phenomenon people dependent cityliving due automobile therefore live outside public transportation sphere city usually seen pleasant accompanied improved quality life addressed problem remains sprawl cause loss job afford luxurious alternative inferior substitute glaeser kahn therefore question hope suggest sprawl occurred u poverty one consequence
Statistics,interest interactive tool exploring result simulation study,simulation study allow u explore property statistical method provide powerful tool multiplicity aim among others evaluating comparing new existing statistical method assessing violation modelling assumption helping understanding statistical concept supporting design clinical trial increased availability powerful computational tool usable software contributed rise simulation study current literature however simulation study involve increasingly complex design making difficult provide relevant result clearly dissemination result play focal role simulation study drive applied analyst use method shown perform well setting guide researcher develop new method promising direction provide insight le established method crucial digest relevant result simulation study therefore developed interest interactive tool exploring result simulation study tool developed using shiny framework r available web app standalone package requires uploading tidy format dataset result simulation study r stata sa spss commaseparated format variety performance measure estimated automatically along monte carlo standard error result performance summary displayed tabular graphical fashion wide variety available plot consequently reader focus simulation parameter estimands interest conclusion interest facilitate investigation result simulation study supplement reporting result allowing researcher share detailed result simulation reader explore freely
Statistics,item response theory based ensemble machine learning,article propose novel probabilistic framework improve accuracy weighted majority voting algorithm order assign higher weight classifier correctly classify hardtoclassify instance introduce item response theory irt framework evaluate sample difficulty classifier ability simultaneously three model created different assumption suitable different case making inference keep balance accuracy complexity experiment base model constructed single tree via bootstrap explain model illustrate irt ensemble model construct classifying boundary also compare performance widely used method show model performs well datasets
Statistics,grouped sparse paired comparison bradleyterry model,wide class paired comparison especially sport game subject divided several group intragroup comparison dense intergroup comparison sparse typical example include nfl regular season motivated situation propose group sparsity paired comparison show consistency asymptotical normality maximum likelihood estimate bradleyterry model number parameter go infinity paper simulation carried illustrate group sparsity asymptotical result
Statistics,generalized spatial spatiotemporal autoregressive conditional heteroscedasticity,paper introduce new spatial model incorporates heteroscedastic variance depending neighboring location proposed process regarded spatial equivalent temporal autoregressive conditional heteroscedasticity arch model show additionally introduced spatial arch model used spatiotemporal setting contrast temporal arch model distribution known given full information set prior period distribution straightforward spatial spatiotemporal setting however possible estimate parameter model using maximumlikelihood approach via monte carlo simulation demonstrate performance estimator specific spatial weighting matrix moreover combine known spatial autoregressive model spatial arch model assuming heteroscedastic error eventually proposed autoregressive process illustrated using empirical example specifically model lung cancer mortality u county compare introduced model two benchmark approach
Statistics,algorithm multivariate data standardization third moment,algorithm transforming multivariate data form normalized first second third moment presented
Statistics,forecasting historical data process knowledge misspecification comparison,faced task forecasting dynamic system practitioner often available historical data knowledge system combination intuition dictate perfect knowledge system theory yield perfect forecasting often knowledge system partially known known parameter known incorrectly contrast forecasting using previous data without process knowledge might result accurate prediction simple system fail highly nonlinear chaotic system paper author demonstrate even chaotic system forecasting historical data preferable using process knowledge knowledge exhibit certain form misspecification extensive simulation study range misspecification forecasting scenario examined goal gaining improved understanding circumstance forecasting historical data preferred using process knowledge
Statistics,classification issue within ensemblebased complex system simulation task,contemporary task complex system simulation often related issue uncertainty management come lack information knowledge simulated system well restriction model set used one powerful tool uncertainty management ensemblebased simulation us variation input output data model parameter available version model improve simulation performance furthermore system model complex system simulation especially case hiring ensemblebased approach considered complex system result identification complex model structure parameter provide additional source uncertainty managed within presented work developing conceptual technological approach manage ensemblebased simulation taking account changing state simulated system system model within ensemblebased approach state system considered subject classification consequent inference better strategy ensemble evolution simulation time ensemble aggregation ensemble evolution enables implementation dynamic reactive solution automatically conform changing state system ensemble aggregation considered within scope averaging regression way selection classification way complement classification mentioned earlier approach technological basis approach includes ensemblebased simulation technique using domainspecific software combined within composite application data science approach analysis available datasets simulation data observation situation assessment etc machine learning algorithm class identification ensemble management knowledge acquisition
Statistics,responsematrixcentred approach presenting crosssection measurement,current canonical approach publishing crosssection data unfold reconstructed distribution detector effect like efficiency smearing undone mathematically yielding distribution true event property illposed problem even small statistical variation reconstructed data lead large change unfolded spectrum work present alternative complementary approach responsematrixcentred forwardfolding approach offer convenient way forwardfold model expectation truth space reconstructed quantity compared data directly similar usually done full detector simulation within experimental collaboration detector response efficiency smearing parametrised matrix effect detector measurement given model simulated simply multiplying binned truth expectation value response matrix systematic uncertainty detector response handled providing set matrix according prior distribution detector property marginalising background event included likelihood calculation giving background event bin truth space facilitate straightforward use response matrix new software framework developed response matrix utility remu remu python package distributed via python package index us widely available standard scientific python library depend custom experimentspecific software offer method needed build response matrix monte carlo data set use response matrix forwardfold truthlevel model prediction compare prediction real data using bayesian frequentist statistical inference
Statistics,highdimensional variable selection via lowdimensional adaptive learning,stochastic search method socalled adaptive subspace adasub method proposed variable selection highdimensional linear regression model method aim finding best model respect certain model selection criterion based idea adaptively solving lowdimensional subproblems order provide solution original highdimensional problem usual type model selection criterion used akaike information criterion aic bayesian information criterion bic extended bic ebic last particularly suitable highdimensional case limiting property new algorithm analysed shown certain condition adasub converges best model according considered criterion simulation study performance adasub investigated comparison alternative method effectiveness proposed method illustrated via various simulated datasets highdimensional real data example
Statistics,role symmetry bayesian solution differential equation,interpretation numerical method finite difference method differential equation point estimator suggests formal uncertainty quantification also performed context competing statistical paradigm considered bayesian probabilistic numerical method pnms obtained bayesian statistical principle deployed bayesian pnm appealing property closed composition uncertainty due different source discretisation numerical method jointly modelled rigorously propagated despite recent attention exact bayesian pnm numerical solution ordinary differential equation ode proposed raise fundamental question whether exact bayesian method general nonlinear ode even exist purpose paper provide positive answer limited class ode end work foundational level novel bayesian pnm proposed proofofconcept proposal synthesis classical lie group method exploit underlying symmetry gradient field nonparametric regression transformed solution space ode procedure presented detail first second order ode relies certain strong technical condition existence solvable lie algebra satisfied numerical illustration provided
Statistics,pólyagamma sampler generalized logistic regression,paper introduce novel bayesian data augmentation approach estimating parameter generalised logistic regression model propose polyagamma sampler algorithm allows u sample exact posterior distribution rather relying approximation simulation study illustrates flexibility accuracy proposed approach capture heavy light tail binary response data different dimension methodology applied two different real datasets demonstrate polyagamma sampler provides precise estimate empirical likelihood method outperforming approximate approach
Statistics,exact bayesian inference discretely observed markov jump process using finite rate matrix,present new methodology bayesian inference rate parameter discretely observed continuoustime markov jump process countably infinite state space usual method choice inference particle markov chain monte carlo particle mcmc struggle observation noise small consider challenging regime exact observation provide two new methodology inference case minimal extended state space algorithm mesa nearly minimal extended state space algorithm nmesa extending markov chain monte carlo state space mesa nmesa use exponentiation finite rate matrix perform exact bayesian inference markov jump process even though state space countably infinite numerical experiment show improvement particle mcmc factor three several order magnitude
Statistics,density nested archimedean copula,nested archimedean copula recently gained interest since generalize wellknown class archimedean copula allow partial asymmetry sampling algorithm strategy well investigated nested archimedean copula however likelihood based inference important density present work fill gap general formula derivative node inner generator appearing nested archimedean copula developed lead tractable formula density nested archimedean copula arbitrary dimension number nesting level large various example including famous archimedean family transformation given furthermore numerically efficient way evaluate logdensity presented
Statistics,parameter estimation discretely observed fractional ornsteinuhlenbeck process yuima r package,paper proposes consistent asymptotically gaussian estimator drift diffusion coefficient hurst exponent discretely observed fractional ornsteinuhlenbeck process estimation drift result obtained case h paper also provides readytouse software r statistical environment based yuima package
Statistics,parameter estimation censored sample using expectationmaximization algorithm,paper deal parameter estimation data randomly right censored maximum likelihood estimate censored sample obtained using expectationmaximization em monte carlo em mcem algorithm introduce concept em mcem algorithm develop parameter estimation method variety distribution normal laplace rayleigh distribution proposed method illustrated three example
Statistics,challenge opportunity statistic statistical education looking back looking forward,anniversary asa provides opportunity look back past peer future led forebear found association commonality still see insight might glean experience observation use anniversary chance reflect headed term statistical education amidst growth data science statistic science learning data fostering multivariable thinking building datarelated skill developing simulationbased problem solving help ensure statistician fully engaged data science analysis abundance data available u
Statistics,bayesian modeling mcmc computation linear logistic regression presenceonly data,presenceonly data referred situation given censoring mechanism binary response observed respect outcome usually called textit presence work present bayesian approach problem presenceonly data based two level scheme probability law casecontrol design combined handle double source uncertainty one due censoring one due sampling propose new formalization logistic model presenceonly data allows insight inferential issue related model concentrate case linear logistic regression order make inference parameter interest present markov chain monte carlo algorithm data augmentation require priori knowledge population prevalence simulation study concerning simulated datasets related different scenario presented comparing proposal optimal benchmark
Statistics,updated guideline updated curriculum gaise college report introductory statistic modern student,since american statistical association asa endorsement guideline assessment instruction statistic education gaise college report change statistic field statistic education major impact teaching learning statistic live world statistic science learning data fastestgrowing science technology engineering math stem undergraduate degree united state according asa many job demand understanding explore make sense data light new report change demand discipline group volunteer revised gaise college report updated report endorsed board director american statistical association july help shed additional light revision process subsequent change report review report share insight committee thought assumption
Statistics,online updating statistical inference big data setting,present statistical method big data arising online analytical processing large amount data arrive stream require fast analysis without storageaccess historical data particular develop iterative estimating algorithm statistical inference linear model estimating equation update new data arrive algorithm computationally efficient minimally storageintensive allow possible rank deficiency subset design matrix due rareevent covariates within linear model setting proposed onlineupdating framework lead predictive residual test used ass goodnessoffit hypothesized model also propose new onlineupdating estimator estimating equation setting theoretical property goodnessoffit test proposed estimator examined detail simulation study real data application estimator compare favorably competing approach estimating equation setting
Statistics,spmoran ver r package moran eigenvectorbased scalable spatial additive mixed modeling,study demonstrates use spmoran r package estimating moran eigenvectorbased scalable spatial additive mixed model related spatial model concrete package implement standard spatial regression model extension including spatially nonspatially varying coefficient model model group effect spatial unconditional quantile regression model low rank spatial econometric model model estimated computationally efficiently see also http githubcomdmurakaspmoran another example using boston house price dataset
Statistics,julian ernst besag march august biographical memoir,julian besag outstanding statistical scientist distinguished pioneering work statistical theory analysis spatial process especially conditional lattice system work seminal statistical development last several decade ranging image analysis markov chain monte carlo method clarified role autologistic autonormal model instance markov random field paved way use diverse application later work included investigation efficacy nearest neighbour model accommodate spatial dependence analysis data agricultural field trial image restoration noisy data texture generation using lattice model
Statistics,castor contextual iot time series data model management scale,demonstrate castor cloudbased system contextual iot time series data model management scale castor designed assist data scientist exploring retrieving relevant time series contextual information required predictive modelling task b seamlessly storing deploying predictive model cloud production environment c monitoring performance predictive model production semi automatically retraining case performance deterioration main feature castor efficient pipeline ingesting iot time series data real time scalable hybrid data management service time series contextual data versatile semantic model contextual information easily adopted different application domain abstract framework developing storing predictive model r python deployment service automatically train andor score predictive model upon userdefined condition demonstrate castor realworld smart grid use case discus adopted application domain smart building telecommunication retail manufacturing
Statistics,convergence rate quasi stationary distribution shiryaevroberts diffusion,classical shiryaev robert martingale diffusion considered interval given absorbing boundary shown rate convergence diffusion quasistationary cumulative distribution function cdf q x stationary cdf h x atoinfty worse log uniformly result established explicitly constructing new tight lower upperbounds q x using certain latest monotonicity property modified bessel k function involved exact closedform formula q x recently obtained polunchenko
Statistics,continuously updated data analysis system,data science important know building paper describes idealized final product data science project called continuously updated dataanalysis system cudas cudas concept synthesizes idea range successful data science project nate silver fivethirtyeight cudas built context state economy state climate demonstrate build two cudas system first provides continuouslyupdated rating soccer player based newly developed augmented adjusted plusminus statistic second creates large dataset synthetic ecosystem used agentbased modeling infectious disease
Statistics,expanding scope statistical computing training statistician software engineer,traditionally statistical computing course taught syntax particular programming language specific statistical computation method since publication nolan temple lang seen greater emphasis data wrangling reproducible research visualization shift better prepares student career working complex datasets producing analysis multiple audience argue statistician often called upon develop statistical software analysis r package implementing new analysis method machine learning system integrated commercial product demand different skill describe graduate course developed meet need focusing four theme programming practice software design important algorithm data structure essential tool method code review revision semesterlong software project student practice skill software engineering course allows student expand understanding computing applied statistical problem building expertise kind software development increasingly province working statistician see model future evolution computing curriculum statistic data science
Statistics,implementing version control git learning objective statistic course,version control system record change file set file time change tracked specific version file recalled later essential element reproducible workflow deserves due consideration among learning objective statistic course paper describes experience implementation decision four contributing faculty teaching different course variety institution faculty set version control learning objective successfully integrated teaching git one statistic course various approach described paper span different implementation strategy suit student background course type software choice assessment practice presenting wide range approach teaching git paper aim serve resource statistic instructor teaching course level within undergraduate graduate curriculum
Statistics,explicit estimation derivative data differential equation gaussian process regression,work employ bayesian inference framework solve problem estimating solution particularly derivative satisfy known differential equation given noisy scarce observation solution data address key issue accuracy robustness derivative estimation use gaussian process jointly model solution derivative differential equation regarding linear differential equation linear constraint gaussian process regression constraint method gprc developed improve accuracy prediction derivative nonlinear differential equation propose picarditerationlike approximation linearization around gaussian process obtained data gprc still iteratively applicable besides product expert method applied ensure initial boundary condition considered enhance prediction accuracy derivative present several numerical result illustrate advantage new method comparison standard datadriven gaussian process regression
Statistics,survey bayesian statistical approach big data,modern era characterised era information big data motivated huge literature new method extracting information insight data natural question approach differ available prior advent big data present review published study present bayesian statistical approach specifically big data discus reported perceived benefit approach conclude addressing question whether focusing improving computational algorithm infrastructure enough face challenge big data
Statistics,cognitive constructivism epistemic significance sharp statistical hypothesis natural science,book present case defense constructivist epistemological framework use compatible statistical theory inference tool basic metaphor decision theory maximization gambler expected fortune according subjective utility prior belief learned experience metaphor proven useful leading development bayesian statistic since xxth century revival rooted work de finetti savage others basic metaphor presented text foundation cognitive constructivism eigensolution verification objective epistemic status fbst full bayesian significance test cornerstone set statistical toll conceived ass epistemic value eigensolutions according four essential attribute namely sharpness stability separability composability believe alternative perspective complementary one ofered decision theory provide powerful insight make pertinent contribution context scientific research
Statistics,sweave documentation implementing markov chain monte carlo estimating confidence,file sweave documentation example provided flegal j jones g l implementing markov chain monte carlo estimating confidence handbook markov chain monte carlo edited brook gelman jones g meng x published chapman hallcrc press
Statistics,hotelling test highly correlated data,paper motivated analysis gene expression set especially finding differentially expressed gene set two phenotype gene expression level highly correlated likely approximately normal distribution therefore seems reasonable use twosample hotelling test data discover unexpected property test making different majority test previously used data appears hotelling test always reach maximal power marginal distribution differentially expressed highly correlated data maximal power attained half marginal distribution essentially different case correlation coefficient greater test powerful one marginal distribution shifted omparing case marginal distribution equally shifted moreover correlation coefficient increase power hotelling test increase well
Statistics,two sample problem exact distribution numerical solution simulation,work presented article suggests solution two sample problem keywords two sample problem welchaspin solution fisherbehrens problem nuisance parameter similarity linnik phenomenon
Statistics,development initial validation scale measure instructor attitude toward conceptbased teaching introductory statistic health behavioral science,despite decade reform effort student continue experience difficulty understanding applying statistical concept predominant focus reform content pedagogy technology assessment little attention instructor characteristic however strong theoretical empirical evidence instructor attitude impact quality teaching learning objective study develop initially validate scale measure instructor attitude toward reformoriented conceptbased teaching introductory statistic health behavioral science tertiary level scale referred fat faculty attitude toward statistic data obtained instructor usa international analyzed using factor analysis multidimensional scaling hierarchical cluster analysis overall scale consists five subscales total item overall alpha construct validity established specifically overall scale subscales except perceived difficulty plausibly differentiated lowreform highreform practice instructor statistically significant difference attitude observed respect age gender employment status membership status professional organization ethnicity highest academic qualification degree concentration scale considered reliable valid measure instructor attitude toward reformoriented conceptbased constructivist teaching introductory statistic health behavioral science tertiary level five dimension influence instructor attitude additional study required confirm structural psychometric property
Statistics,lp nested symmetric distribution,tractable generalization gaussian distribution play important role analysis highdimensional data one general superclass normal distribution class nu spherical distribution whose random variable represented product x rcdot u uniformly distribution random variable u level set positively homogeneous function nu arbitrary positive radial random variable r prominent subclass nu spherical distribution spherically symmetric distribution nu x generalized class lp spherically symmetric distribution nu x xp class contain gaussian special case general however nu spherical distribution computationally intractable since instance normalization constant fast sampling algorithm unknown arbitrary nu paper introduce new subclass nu spherical distribution choosing nu nested cascade lp norm class still computationally tractable includes aforementioned subclass special case derive general expression lp nested symmetric distribution well uniform distribution lp nested unit sphere including explicit expression normalization constant state several general property lp nested symmetric distribution investigate marginals maximum likelihood fitting discus tight link well known machine learning method independent component analysis ica independent subspace analysis isa mixed norm regularizers finally derive fast exact sampling algorithm arbitrary lp nested symmetric distribution introduce nested radial factorization algorithm nrf form nonlinear ica
Statistics,brief history fail safe number applied research,rosenthal failsafenumber fsn probably one best known statistic context metaanalysis aimed estimate number unpublished study metaanalyses required bring metaanalytic mean effect size statistically insignificant level already scargle schonemann scargle fundamental critique claimed stability basic rationale fsn approach objection focusing basic assumption fsn treat number study unbiased averaging null expressed throughout history fsn different author elashoff iyengar greenhouse see also scargle particular elashoff objection appears important first critique pointing directly central problem fsn r r claim number study hidden drawer would achieve mean effect size zero combined study reviewed surely allowed hidden study negative average hidden study would necessary obtain zero mean effect size p thus user metaanalysis could aware right beginning something wrong statistical reasoning fsn particular applied research perspective therefore interest whether fundamental objection fsn reflected standard handbook metaanalysis well course even importantly metaanalytic study
Statistics,conversation james hannan,jim hannan professor lived interesting life one whose fundamental research repeated game fully appreciated late career service meteorologist army world war ii jim played poker made weather forecast curious later research included strategy repeated play apply selecting best forecaster james hannan born holyoke massachusetts september attended st jerome high school january received phb st michael college colchester vermont jim enlisted u army air force train serve meteorologist took army airbases china close war following discharge army jim studied mathematics harvard graduated m june prepare doctoral work statistic university north carolina fall jim went university michigan summer routine admission physical revealed spot lung possibility tuberculosis caused jim stay ann arbor fall veteran administration hospital framingham massachusetts condition followed closely discharged hospital spring started study chapel hill fall began research compound decision theory herbert robbins feeling need teaching experience jim left chapel hill two year short thesis take three year appointment instructor catholic university washington dc told renewal coming jim felt pressure finish degree
Statistics,conversation martin bradbury wilk,martin bradbury wilk born december montr e al qu e bec canada completed beng degree chemical engineering mcgill university worked research engineer atomic energy project national research council canada went iowa state college completed msc phd degree statistic respectively oneyear postdoc john tukey became assistant director statistical technique research group princeton university served professor director research statistic rutgers university parallel also career bell laboratory murray hill new jersey turn member technical staff head statistical model method research department statistical director management science research wrote number influential paper statistical methodology period notably testing procedure normality shapiro wilk statistic probability plotting technique multivariate data martin moved higher management level american telephone telegraph company occupied various position culminating assistant vicepresident director corporate planning returned canada became first professional statistician serve chief statistician accomplishment statistic canada numerous contributed resurgence institution international standing played crucial role reinstatement cabinetcancelled census
Statistics,degree equivalence key comparison,interlaboratory key comparison data analysis procedure comparison proposed recommended cipm therein degree equivalence measurement standard laboratory participated comparison one two laboratory introduced corresponding clear plausible measurement model given author offered possible measurement model given comparison suitable model selected rigorous analyzing step expectation value degree equivalence systematic laboratoryeffects model selected right one report model based one true value existence assumption however year new version vocabulary international metrology vim issued true value given measurement standard perceived multi true value following given statistic distribution applying perception true value measurement standard combination step measurement model developed degree equivalence analyzed result show although new definition systematic laboratoryeffects model still reasonable one given key comparison
Statistics,squaring circle cubing sphere circular spherical copula,exist circular spherical copula rd exist circularly symmetric distribution unit disk spherically symmetric distribution unit ball rd whose onedimensional marginal distribution uniform answer yes circular spherical copula unique determined explicitly oneparameter family elliptical bivariate copula obtained unique circular copula oblique coordinate transformation copula obtained nonlinear transformation uniform distribution unit ball rd also described determined explicitly
Statistics,conversation george g roussas,george g roussas born city marmara central greece june received ba high honor mathematics university athens phd statistic university california berkeley served assistant professor mathematics california state university san jose faculty member department statistic university wisconsin madison starting assistant professor becoming professor professor applied mathematics director laboratory applied mathematics university patras greece elected dean school physical mathematical science university patras chancellor university served three year vice presidentacademic affair new university crete greece visiting professor intercollege division statistic university california davis appointed professor associate dean chair graduate group statistic university served two administrative capacity elected member international statistical institute since fellow royal statistical society since fellow institute mathematical statistic since fellow american statistical association since served member council hellenic mathematical society president balkan union mathematician
Statistics,baby morse theory data analysis,methodology proposed inferring topology underlying point cloud data approach employ basic element morse theory capable producing point estimate various topological quantity eg genus also ass sampling uncertainty probabilistic fashion several example point cloud data three dimension utilized demonstrate method yield interval estimate topology data surface embedded
Statistics,revealing suboptimality condition strategic decision,conceptual view fitness fitness measurement strategic decision information system technological system innovation becoming important recent year paper determines dynamic fitness landscape lead termination decision maker research reaching global maximum strategic decision dynamic specified according management decision making model supported simulation result article determines simulation result mean fitness value probability optimality correlation two concept may remarkable according revealing optimal value innovative researchbased decision making approach beside suboptimal result traditional decision making approach
Statistics,best city psychology research worldwide map visualizing city ratio observed expected number highlycited paper,present scientometric result worldwide center excellence psychology based web science data domainspecific excellence identified city highly cited paper published data refer psychology article published documented social science citation index citation frequency may visualized city article output least statistical z test used evaluation degree observed number topcited paper city differs number expected basis randomness selection paper map visualizing city ratio significant difference observed expected number highlycited paper point excellence center city east west coast united state well great britain germany netherlands ireland belgium sweden finland australia taiwan furthermore positive nonsignificant difference favor high citation rate documented city united state great britain netherlands scandinavian germanspeaking country belgium france spain israel south korea china scientometric result show convincingly highlycited psychological research article come angloamerican country nonenglish european country number englishlanguage publication increased last decade
Statistics,modern portfolio theory using sastextregistered,investment approach financial instrument varied often produce unpredictable result many investor earlier day investment banking suffered catastrophical loss due poor strategy lack understanding financial market development investment banking many innovative investment strategy proposed make portfolio return higher overall market one famous theory portfolio creation management modern portfolio theory proposed harry markowitz paper shall apply theory creating portfolio stock well managing
Statistics,concise resolution two envelope paradox,paper demonstrate new perspective two envelope problem hope show convincing clarity paradox result inherent problem pertaining interpretation bayesian probability specifically subjective probability inconsistent reality mislead reasoning based bayesian decision theory
Statistics,proof asymptotics wavelet variance long memory process using taylor expansion,long memory process selfsimilarity scaleinvariant property low frequency prove log scaledependent wavelet variance long memory process asymptotically proportional scale using taylor expansion wavelet variance
Statistics,divergence formula regularization method constraint,derive divergence formula group regularization method constraint formula useful regularization parameter selection provides unbiased estimate number degree freedom begin deriving formula smoothing spline extend setting penalized spline ridge regression functional linear regression
Statistics,sparse solution overdetermined linear system column orthogonal,paper consider problem obtaining best k sparse solution axy subject constraint column orthogonal naive approach obtaining solution problem exponential complexity exist regularization method lasso obtain approximate solution paper show obtain exact solution problem much le computational effort compared brute force search column orthogonal
Statistics,individual neutrality collective decision making,derive simple mathematical theory show two decisionmaking entity work better together least one occasionally willing stay neutral provides mathematical justification ageold cliche among marriage counselor
Statistics,fast nongaussian bayesian matching pursuit method sparse reconstruction,fast matching pursuit method using bayesian approach introduced sparse signal recovery method referred ngpfbmp performs bayesian estimate sparse signal even signal prior nongaussian unknown agnostic signal statistic utilizes priori statistic additive noise sparsity rate signal shown easily estimated data available ngpfbmp utilizes greedy approach orderrecursive update metric find dominant sparse support determine approximate minimum mean square error mmse estimate sparse signal simulation result demonstrate power robustness proposed estimator
Statistics,experimental design partially observed markov decision process,paper deal question effectively conduct experiment partially observed markov decision process provide data informative parameter interest method markov decision process especially dynamic programming introduced used algorithm maximize relevant fisher information algorithm applied two pomdp example method developed also applied stochastic dynamical system suitable discretization consequently show control policy look like morrislecar neuron model simulation result presented discus parameter dependence within method dealt use prior develop tool update control policy online demonstrated another stochastic dynamical system describing growth dynamic dna template pcr model
Statistics,benford law theoretical explanation base,paper present possible theoretical explanation benford law develop recursive relation probability using simple intuitive idea first use numerical solution recursion verify solution converge benford law finally solve recursion analytically yeild benford law base
Statistics,perceptive statistical variability indicator,concept variability uncertainty epistemic alleatory came experience coexist different connotation therefore article attempt express relation analytic mean firstly setting sight difference common characteristic inspired definition average number equally probable event based entropy concept probability theory article introduced two related perceptive statistical measure indicate variability basic probability distribution first equivalent number hypothetical distribution one sure impossible outcome indicates variability second appropriate equivalent number hypothetical distribution equal probability indicates invariability article interprets common property variability uncertainty theoretical distribution oceanwide wind wave directional property using long term observation compiled global wave statistic
Statistics,conversation stephen e fienberg,following conversation based part transcript interview funded pfizer global researchconnecticut american statistical association department statistic university connecticutstorrs part conversation distinguished statistician memory professor harry posten
Statistics,generalized labeled multibernoulli approximation multiobject density,multiobject inference multiobject probability density capture uncertainty number state object well statistical dependence object exact computation multiobject density generally intractable tractable implementation usually require statistical independence assumption object paper propose tractable multiobject density approximation capture statistical dependence object particular derive tractable generalized labeled multibernoulli glmb density match cardinality distribution first moment labeled multiobject distribution interest also shown proposed approximation minimizes kullbackleibler divergence special tractable class glmb density based proposed glmb approximation demonstrate tractable multiobject tracking algorithm generic measurement model simulation result multiobject trackbeforedetect example using radar measurement low signaltonoise ratio snr scenario verify applicability proposed approach
Statistics,curriculum guideline learning objective survey five statistic program,asa guideline undergraduate statistic major aimed provide guidance program undergraduate degree statistic content skill statistic major learning new guideline forthcoming important help program develop assessment cycle evaluation know student learning want learn improve program time first step process translate broader guideline institutionspecific measurable learning outcome paper provides example five program guideline hope serve illustrative example program moving forward new guideline
Statistics,compass statistical researcher,hiked many mile alongside several professor traversed statistical path regime switching trail changed direction following class foundation discipline play game research limbo student academic one thing among prof bernardi teaching never clear draw route research map need know destination must also understand arrived
Statistics,robust hypothesis testing α divergence,robust minimax test two composite hypothesis determined neighborhood two nominal distribution respect set distance called alpha divergence distance proposed sion minimax theorem adopted characterize saddle value condition least favorable distribution robust decision rule robust likelihood ratio test derived nominal probability distribution satisfy symmetry condition design procedure shown simplified considerably parameter controlling degree robustness bounded bound shown resulting solution set equation simulation performed evaluate exemplify theoretical derivation
Statistics,teaching learning data visualization idea assignment,article discus make statistical graphic prominent element undergraduate statistic curriculum focus several different type assignment exemplify incorporate graphic course pedagogically meaningful way assignment include student deconstruct reconstruct plot copy masterful graph create oneminute visual revelation convert table picture develop interactive visualization eg virtual earth plotting canvas addition describing goal detail assignment also discus broader topic graphic key concept think warrant inclusion statistic curriculum advocate attention need paid fundamental field statistic level introductory undergraduate graduate level course rapid rise tool visualize data eg google trend gapminder manyeyes tableau increased use graphic medium understanding principle good statistical graphic ability create informative visualization ever important aspect statistic education
Statistics,replication communication population dynamic scientific discovery,many published research result false controversy continues role replication publication policy improving reliability research addressing problem frustrated lack formal framework jointly represents hypothesis formation replication publication bias variation research quality develop mathematical model scientific discovery combine element model provides dynamic model research well formal framework reasoning normative structure science show replication may serve ratchet gradually separate true hypothesis false factor make initial finding unreliable also make replication unreliable important factor improving reliability research rate false positive base rate true hypothesis offer suggestion addressing result also bring clarity verbal debate communication research surprisingly publication bias always obstacle instead may positive impact suppression negative novel finding often beneficial also find communication negative replication may aid true discovery even attempt replicate diminished power model speaks constructively ongoing debate design conduct science focusing analysis discussion precise internally consistent model well highlighting importance population dynamic
Statistics,exponentiated extended weibullpower series class distribution,paper introduce new class distribution compounding exponentiated extended weibull family power series family distribution contains several lifetime model complementary extended weibullpower series generalized exponentialpower series generalized linear failure ratepower series exponentiated weibullpower series generalized modified weibullpower series generalized gompertzpower series exponentiated extended weibull distribution special case obtain several property new class distribution shannon entropy mean residual life hazard rate function quantiles moment maximum likelihood estimation procedure via emalgorithm presented
Statistics,failure us jaynes principle transformation group,bertand paradox fundamental problem probability cast doubt applicability indifference principle showing may yield contradictory result depending meaning assigned randomness jaynes claimed symmetry requirement principle transformation group solve paradox selecting unique solution problem show case every variant obtained principle indifference also obtained jaynes principle transformation group symmetry mathematically implemented different way depending procedure random selection one us describe simple experiment support result symmetry argument solution different jaynes jaynes method thus best seen tool obtain probability distribution principle indifference inconvenient resolve ambiguity inherent use principle still depends explicitly defining selection procedure
Statistics,twoenvelope problem informed choice,host game present two indistinguishable envelope agent one envelope randomly selected allocated agent agent informed monetary content one envelope twice dilemma condition would beneficial switch allocated envelope complementary one objective envelopeswitching strategy determine benefit switching allocated envelope content expected content complementary envelope agent upon revealing content allocated envelope must consider event likely taken place result host activity preceding approach stark contrast considering agent reasoning particular outcome seek derive strategy based relative content presented envelope however former reasoning seek identify initial amount could result observed amount facilitates identification appropriate switching strategy knowledge content allocation process essential agent derive successful switching strategy distribution function host sampled initial amount assigned first envelope every play game agent afforded opportunity sighting content randomly allocated envelope determine expected benefit switching
Statistics,removing gaussian noise optimization weight nonlocal mean,new image denoising algorithm deal additive gaussian white noise model given like nonlocal mean method filter based weighted average observation neighborhood weight depending similarity local patch contrast nonlocal mean filter instead using fixed gaussian kernel propose choose weight minimizing tight upper bound mean square error approach make possible define weight adapted function hand mimicking weight oracle filter regularity condition target image show obtained estimator converges usual optimal rate proposed algorithm parameter free sense automatically calculates bandwidth smoothing kernel fast implementation straightforward performance new filter illustrated numerical simulation
Statistics,hear forget understand modified mooremethod mathematical statistic course,moore introduced method graduate mathematics instruction consisted primarily individual student work challenging proof jones cohen described adaptation le explicit competition suitable undergraduate student liberal art college paper detail adaptation modified mooremethod teach mathematical statistic describes way approach help engage student foster teaching statistic group student worked set difficult problem theoretical applied every two week class time devoted coaching session instructor group meeting time class presentation r used estimate solution empirically analytic result intractable well provide environment undertake simulation study aim deepening understanding complementing analytic solution group presented comprehensive solution complement oral presentation development parallel technique empirical analytic problem solving explicit goal course also attempted communicate way statistic used tackle interesting problem group problem solving component use technology allowed student attempt much challenging question could otherwise solve
Statistics,revealing beauty behind sleeping beauty problem,large number essay address sleeping beauty problem undermines validity bayesian inference ba van fraassen reflection principle study straightforward analysis problem based probability theory presented key difference previous work apart random experiment imposed problem description different one also considered order negate confusion involved conditional probability result analysis indicate inconsistency take place whereas bayesian inference reflection principle valid
Statistics,tree oriented data analysis,complex data object arise many area modern science including evolutionary biology nueroscience dynamic gene expression medical imaging object oriented data analysis ooda statistical analysis datasets complex object data analysis tree data object exciting research area interesting question challenging problem thesis focus tree oriented statistical methodology algorithm solving related mathematical optimization problem research motivated goal analyzing data set image human brain artery approach take use novel representation brain artery system point phylogenetic treespace treespace property unique global geodesic lead notion geometric center called frechet mean sample data point frechet function sum squared distance point data point frechet mean minimizer frechet function thesis use property frechet function develop algorithmic system computing frechet mean property frechet function also used show sticky law large number describes surprising stability topological tree structure sample frechet mean population frechet mean also introduce nonparametric regression brain artery tree structure response variable age based weighted frechet mean
Statistics,quantile mixture,note give explicit expression quantile mixture two random variable carefully examine possible case discrete continuous variable possibly unbounded support result useful finding bound valueatrisk risky portfolio partial information available bernard vanduffel
Statistics,true cluster,constructivist philosophy hasok chang active scientific realism used argue idea truth cluster analysis depends context clustering aim different characteristic clustering required different situation researcher explicit requirement idea true cluster research based clustering becomes scientific uniqueness transparent open communication idea natural kind human construct highlight human experience reality outside observer control seems make certain distinction category inevitable various desirable characteristic clustering various approach define contextdependent truth listed discus impact idea comparison clustering method choice clustering method related decision practice
Statistics,paradox spectral representation stationary random process,note aim show paradox spectral representation stationary random process
Statistics,generalized probability statistical theory,review article present different formal framework description generalized probability statistical theory discus particular case probability appearing classical quantum mechanic possible generalization approach n kolmogorov r cox noncommutative model approach generalized probability based convex set
Statistics,using google scholar predict self citation case study health economics,metric designed quantify influence academic increasingly used easily estimable perhaps popular h index metric however potentially impacted excessive self citation work explores issue using group researcher working well defined sub field economics namely health economics employ self citation identification software identifies characteristic best predict self citation provides evidence regarding scale self citation field degree self citation impact inference relative influence individual health economist using data health economist suggests self citation associated geographical region longevity health economist early career researcher researcher mainland europe australasia self citing frequently
Statistics,brief history long memory hurst mandelbrot road arfima,long memory play important role many field determining behaviour predictability system instance climate hydrology finance network dna sequencing particular important test process exhibiting long memory since impact accuracy confidence one may predict future event basis small amount historical data major force development study long memory late benoit b mandelbrot discus original motivation development long memory mandelbrot influence fascinating field also elucidate sometimes contrasting approach long memory different scientific community
Statistics,using board game mathematica teach fundamental finite stationary markov chain,markov chain important example course stochastic process simple board game used illustrate fundamental concept example looping board game like monopoly consists recurrent state game player win reaching final square like chute ladder consists transient state except last one availability computer algebra package game analyzed example mean time transient state stationary probability recurrent state easily computed article analyzes simple board game mathematica indicates extended complex situation
Statistics,data science statistic curriculum preparing student think data,growing number student completing undergraduate degree statistic entering workforce data analyst position expected understand utilize database data warehouse scrape data internet source program solution complex problem multiple language think algorithmically well statistically data science topic traditionally major component undergraduate program statistic consequently curricular shift needed address additional learning outcome goal paper motivate importance data science proficiency provide example resource instructor implement data science statistic curriculum provide case study seven institution varied approach teaching data science demonstrate curricular innovation address new need also included example assignment designed course foster engagement undergraduate data data science
Statistics,conversation howell tong,following conversation partly based interview took place hong kong university science technology july
Statistics,precinct size matter large precinct bias u presidential election,examination precinct level data u presidential election reveals correlation large precinct increased fraction republican vote large precinct bias analyzed respect voter heterogeneity voter inconvenience precinct size increase analysis show voter inconvenience significant factor election outcome certain state may significantly disadvantage democratic candidate
Statistics,visualizing probabilistic proof,author revisits blue bus problem famous thoughtexperiment law involving probabilistic proof present simple bayesian solution different version blue bus hypothetical addition author express solution standard visual format ie term probability natural frequency
Statistics,mere renovation little late need rethink undergraduate curriculum ground,last halfdozen year seen american statistician publish wellargued provocative call change thinking statistic teach among brown ka nolan templelang legler et al within past year asa issued new comprehensive set guideline undergraduate program asa accepting applauding background current article argues need rethink curriculum ground offer five principle two caveat intended help u along path toward new synthesis principle caveat rest sense three parallel evolution convergence trend role mathematics computation context within statistic education ongoing change together article cited seminal provocation leo breiman call deep rethinking teach undergraduate particular following brown ka put priority two goal make fundamental concept accessible minimize prerequisite research
Statistics,locating statistic world finding,paper attempt situate statistic relation qualitative research method mean finding compare contrast aspect qualitative research method statistical inquiry attempt answer question whether element qualitative research method included statistic teaching
Statistics,crisis evidence probability statistic discover cause,probability model useful explaining uncertainty know never used say already know probability statistical model useless discerning cause classical statistical procedure frequentist bayesian implementation falsely imply speak cause hypothesis test bayes factor ever used even assuming know cause partial cause set observation reporting via relative risk exagerates certainty future often lot overcertainty made much worse parametetric predictive method used unfortunately predictive method rarely used even cause must still assumption meaning certainty scientific pronouncement high
Statistics,xlmhg test enrichment technical report,minimum hypergeometric test mhg powerful nonparametric hypothesis test detect enrichment ranked binary list provide detailed review definition well algorithm used implementation enable efficient computation exact pvalue introduce generalization mhg termed xlmhg provides additional control type enrichment tested describe precise algorithmic modification necessary compute test statistic pvalue xlmhg algorithm building block gopca recently proposed method exploratory analysis gene expression data using prior knowledge
Statistics,conversation robert c elston,robert c elston born february london england went cambridge university study natural science obtained ba diploma agriculture dip ag came u age study animal breeding cornell university received phd postdoctoral fellow biostatistics university north carolina unc chapel hill studied mathematical statistic rose academic rank department biostatistics unc becoming full professor professor head department biometry genetics louisiana state university medical center new orleans moved case western reserve university professor epidemiology biostatistics served chairman directed phd student mentored postdoctoral fellow one regard founder pedigree research genetic epidemiology estimated progeny among many honor nih research career development award leadership award international society human genetics william allan award american society human genetics nih merit award marvin zelen leadership award harvard university fellow american statistical association institute mathematical statistic well fellow ohio academy science leader research genetic epidemiology year published research article biostatistics genetic epidemiology application also coauthored edited book biostatistics population genetics method analysis genetic data
Statistics,conversation jerry friedman,jerome h friedman born yreka california usa december received high school education yreka high school spent two year chico state college transferring university california berkeley completed undergraduate degree physic phd highenergy particle physic postdoctoral research physicist lawrence berkeley laboratory moved stanford linear accelerator center slac head computation research group retaining position appointed half time professor department statistic stanford university remaining half time slac appointment held visiting appointment csiro sydney cern department statistic berkeley active career commercial consultant jerry became professor emeritus department statistic apart publication highenergy physic early career jerry published research article book statistic computer science including coauthoring pioneering book classification regression tree element statistical learning many publication hundred thousand citation eg cart book much software incorporated commercial product including least one popular search engine many method algorithm essential inclusion modern statistical data mining package honor include following rietz lecture wald lecture election american academy art science u national academy science fellow american statistical association paper year jasa technometrics statistician year asa chicago chapter acm data mining lifetime innovation award emanuel carol parzen award statistical innovation noether senior lecturer american statistical association ieee computer society data mining research contribution award
Statistics,framework infusing authentic data experience within statistic course,working complex data one important update asa curriculum guideline undergraduate program statistical science infusing authentic data experience within course allow student opportunity learn practice data skill prepare dataset analysis modest scope seniorlevel culminating experience authentic data experience provide opportunity demonstrate connection data skill statistical skill result practice data skill undergraduate statistician
Statistics,mandelbrot fractional renewal model nonergodic missing link change point long range dependence,problem noise u century often framed fourier spectral language famous solution tended stationary long range dependent lrd model mandelbrot fractional gaussian noise view increasing importance physic nonergodic fractional renewal model present preliminary result research history mandelbrot little known work area speculate lack awareness work physic statistic community may affected development complexity science discus difference hurst effect noise lrd concept often treated equivalent
Statistics,point process multitarget tracking,finiteset statistic fisst approach multitarget tracking introduced current extended form date elementary alternative fisst proposed based finite point process rather rf accompanied singlesensor multisensor version claimed generalization phd filter ifilter journal advance information fusion jaif elsewhere author went claim fisst pgflfunctional derivative approach actually due corollary puremathematics paper moyal described point process pgflfunctional derivative approach multitarget tracking supposedly based paper shown nonrfs point process phenomenologically erroneous foundation multitarget tracking nearly every equation concept discussion derivation methodology jaif paper originally appeared fisst publication without attributed fisst possibly due moyal point process approach described jaif differs fisst regard terminology notation thus sense appears obscured phenomenologically erroneous improperly attributed copy fisst also shown derivation singlesensor multisensory ifilter appear major error subsequent recasting multisensor ifilter traffic mapping filter
Statistics,dynamic data statistic classroom,call using real data classroom long meant using datasets culled cleaned wrangled prior student working observation however important part teaching statistic include actually retrieving data internet nowadays many different source data continually updated organization hosting data website r tool download dynamic data improved way make accessing data possible even introductory statistic class provide five full analysis dynamic data well additional nine source dynamic data brought classroom goal work demonstrate using dynamic data short learning curve even introductory student faculty unfamiliar landscape example provided unlikely create expert data scraper help motivate student faculty toward engaged use online data source
Statistics,reduced perplexity simplified perspective assessing probabilistic forecast,simple intuitive approach assessment probabilistic inference introduced shannon information metric translated probability domain translation show negative logarithmic score geometric mean equivalent measure accuracy probabilistic inference thus quantitative reduction perplexity inverse geometric mean probability good inference algorithm reduce uncertainty qualitative reduction due increased clarity original set probabilistic forecast central tendency geometric mean insight provided showing renyi tsallis entropy function translated probability domain weighted generalized mean distribution generalized mean probabilistic forecast form spectrum performance metric referred risk profile arithmetic mean used measure decisiveness mean used measure robustness
Statistics,designing modular software case study introductory statistic,modular programming development paradigm emphasizes selfcontained flexible independent piece functionality practice allows new feature seamlessly added desired unwanted feature removed thus simplifying userfacing view software recent rise webbased software application presented new challenge designing extensible modular software system paper outline framework designing system focus reproducibility result present case study shinybased web application called intro allows user perform basic data analysis statistical routine finally highlight challenge encountered address combining modular programming concept reactive programming used shiny
Statistics,revisiting nested group testing procedure new result comparison robustness,group testing origin identification syphilis u army world war ii much theoretical framework group testing developed starting late continued work recently advent new laboratory genetic technology increasing interest group testing design cost saving purpose paper compare different nested design including dorfman sterrett optimal nested procedure obtained dynamic programming elucidate comparison develop closedform expression optimal sterrett procedure provide concise review prior literature commonly used procedure consider design prevalence disease known well investigate robustness procedure incorrectly assumed article provides technical presentation interest researcher well pedagogical perspective supplementary material article available online
Statistics,demmartingales functionnal hill process small parameter,association random variable demimartingales recent field handling asymptotic behavior sum dependent random variable apply technique establish asymptotic law demimartingale next apply result find asymptotic behavior functional hill process small parameter within extreme value theory evt field result would hard find whithout demimartingales technique
Statistics,review mixed effect model extension ecology r,review book mixed effect model extension ecology r zuur ieno walker saveliev smith springer asked review book american statistician wrote review invitation revoked review
Statistics,asymptotics ajtaikomlóstusnády statistic,day widespread analysis wasserstein distance theoretical empirical measure one first investigation topic given paper written ajtai komlos tusnady interestingly neighboring question posed paper settled already without original one paper going delineate limit behavior original statistic help computer simulation time kept eye theoretical grasping problem based computer simulation opinion limit distribution gaussian
Statistics,claude bouchu intendant de bourgogne au siècle atil inventé le mot statistique,objective paper examine assertion word statistic would used first time century report written claude bouchu administrator bourgogne historical bibliographical analysis carried judge credibility thesis physical inspection report make possible bring final answer
Statistics,final solution monty hall problem three prisoner problem,recently proposed linguistic interpretation quantum mechanic called quantum classical measurement theory quantum language characterized kind metaphysical linguistic turn copenhagen interpretation turn physic language extend quantum theory classical system also yield quantum mechanical world view ie philosophy quantum mechanic word quantum philosophy believe quantum language powerful language describe science purpose paper describe montyhall problem three prisoner problem quantum language course believe proposal final solution two problem thus paper answer question philosopher continued stick problem reader find problem never elementary solved without deep understanding probability dualism key word philosophy probability fisher maximum likelihood method bayes method principle equal priori probability
Statistics,j b haldane contribution bayes factor hypothesis test,article brings attention historical development gave rise bayes factor testing point null hypothesis composite alternative line current thinking find conceptual innovation assign prior mass general law due series three article dorothy wrinch sir harold jeffreys however historical investigation also suggests j b haldane made important contribution development bayes factor proposing use mixture prior comprising point mass continuous probability density jeffreys aware haldane work may inspired pursue concrete statistical implementation conceptual idea thus appears haldane may played much bigger role statistical development bayes factor hitherto assumed
Statistics,bayesian prediction physical model application optimization synthesis pharmaceutical product using chemical kinetics,quality control industrial process increasingly making use prior scientific knowledge often encoded physical model require numerical approximation statistical prediction subsequent optimization key ensuring process output meet specification target however numerical expense approximating model pose computational challenge identification combination process factor confidence quality response recent work bayesian computation statistical approximation emulation expensive computational model exploited develop novel strategy optimizing posterior probability process meeting specification ensuing methodology motivated demonstrated chemical synthesis process manufacture pharmaceutical product within initial set substance evolve according chemical reaction certain process condition series new substance one substance target pharmaceutical product two unwanted byproduct aim determine combination process condition amount initial substance maximize probability obtaining sufficient target pharmaceutical product whilst ensuring unwanted byproduct exceed given level relationship factor amount substance interest theoretically described solution system ordinary differential equation incorporating temperature dependence using data small experiment shown methodology approximate multivariate posterior predictive distribution pharmaceutical target byproduct therefore identify suitable operating value material replicate analysis found
Statistics,interactive graphic functional data analysis,although established graphic accompany common functional data analysis generating graphic dataset analysis cumbersome time consuming often barrier visualization inhibit useful exploratory data analysis prevent development intuition method application particular dataset refundshiny package developed address issue several common functional data analysis conducting analysis plotshiny function used generate interactive visualization environment contains several distinct graphic many updated response user input visualization reduce burden exploratory analysis serve useful tool communication result nonstatisticians
Statistics,feasibility study least square method fitting nongaussian noise data,study investigate feasibility least square method fitting nongaussian noise data add different level two typical nongaussian noise levy stretched gaussian noise exact value selected function including linear equation polynomial exponential equation maximum absolute mean square error calculated different case levy stretched gaussian distribution many application fractional fractal calculus observed nongaussian noise le accurately fitted gaussian noise stretched gaussian case appear perform better levy noise case stressed leastsquares method inapplicable nongaussian noise case noise level larger
Statistics,dutch book sleeping beauty evidential decision theorist,context sleeping beauty problem argued socalled halfers avoid dutch book argument adopting evidential decision theory introduce dutch book variant sleeping beauty problem argue evidential decision theorist fall prey whether halfers thirders argument crucially requires action provide evidence agent would decision point exactly information also decision point different symmetric information
Statistics,rational choice guide u correct em de se belief,significant controversy remains constitute correct selflocating belief scenario sleeping beauty problem proponent halfer thirder side attempt settle issue one natural approach consists creating decision variant problem determining action various candidate belief prescribe assessing whether action reasonable step back dutch book argument special case approach sleeping beauty game also constructed make similar point building recent article james rshaw em de se belief rational choice em synthese show general wary argument unintuitive action may result reason unrelated belief hand show restrict attention em additive game thirder necessarily maximize em ex ante expected payout halfer case assuming causal decision theory conclude necessarily settle issue speculate might
Statistics,semiparametric estimation normal variancemean mixture model,paper study problem statistical inference parameter semiparametric variancemean mixture class mixture recently become rather popular statistical financial modelling design semiparametric estimation procedure first estimate mean underlying normal distribution recovers nonparametrically density corresponding mixing distribution illustrate performance procedure simulated real data
Statistics,data visualization day one bringing big idea intro stats early often,world awash data ability think compute data become important skill student many field reason inclusion level statistical computing many introductorylevel course grown common recent year existing literature documented multiple success story teaching statistic r bolstered capability r markdown article present inclass data visualization activity intended expose student r r markdown first week introductory statistic class activity begin brief lecture exploratory data analysis r student placed small group tasked exploring new dataset produce three visualization describe particular insight immediately obvious data upon completion student produced series univariate multivariate visualization real dataset practiced describing
Statistics,coherent combination probabilistic output group decision making algebraic approach,current decision support system address domain heterogeneous nature becoming progressively larger system often require input expert judgement variety different field intensive computational power produce score necessary rank available policy recently integrating decision support system introduced enable formal bayesian multiagent decision analysis distributed consequently efficient system different panel expert oversee disjoint correlated vector variable expert group need deliver certain summary variable jurisdiction properly derive overall score available policy present algebraic approach make methodology feasible wide range modelling context enables u identify summary needed combination judgement also able demonstrate coherence sense formalize still guaranteed panel share partial specification model panel member illustrate algebraic approach applying specific class bayesian network demonstrate use derive closed form formula computation joint moment variable determine score different policy
Statistics,estimation inverse weibull distribution typei hybrid censoring,hybrid censoring mixture type type ii censoring scheme paper present statistical inference inverse weibull distribution data typei hybrid censored first consider maximum likelihood estimator unknown parameter observed maximum likelihood estimator obtained closed form obtain bayes estimator corresponding highest posterior density credible interval unknown parameter assumption independent gamma prior using importance sampling procedure also compute approximate bayes estimator using lindley approximation technique performed simulation study real data analysis order compare proposed bayes estimator maximum likelihood estimator
Statistics,estimation p x weibull distribution based hybrid censored sample,hybrid censoring scheme mixture typei typeii censoring scheme based hybrid censored sample paper deal ference r p x x two independent weibull distribution different scale parameter shape pa rameter maximum likelihood estimator mle approximate mle amle r obtained asymptotic distribution maxi mum likelihood estimator r obtained based asymptotic distribu tion confidence interval r derived two bootstrap confidence interval also proposed consider bayesian estimate r propose corresponding credible interval r monte carlo simulation performed compare different proposed method analysis real data set also presented illustrative purpose
Statistics,cognitive transfer outcome simulationbased introductory statistic curriculum,cognitive transfer ability apply learned skill knowledge new application context investigation evaluates cognitive transfer outcome tertiarylevel introductory statistic course using catalst curriculum exclusively used simulationbased method develop foundation statistical inference common assessment instrument administered end course measured learning outcome student catalst student showed evidence near far transfer outcome scoring high higher assessed learning objective compared peer enrolled similar course emphasized parametric inferential method eg ttest
Statistics,alone least u homonymy large scale social group,article brings forward estimation proportion homonym large scale group based distribution first name last name subset group estimation based generalization birthday paradox problem main result society france united state identity collision based first last name frequent large majority population least one homonym smaller setting much le frequent even small group thousand people least one couple homonym individual homonym
Statistics,conjecture optimal nested generalized group testing algorithm,consider finite population n item item probability pi defective goal identify item mean group testing generalized group testing problem hereafter ggtp case displaystyle n p cite proved pairwise testing algorithm optimal nested algorithm respect expected number test n displaystyle p rrange hereafter optimal boundary value note present result help define generalized pairwise testing algorithm hereafter gpta ggtp present two conjecture pi ldots n belong rrange gpta optimal procedure among nested procedure applied pi nondecreasing order pi ldots n belong rrange gpta optimal nested procedure ie minimises expected total number test respect possible testing order class nested procedure although conjecture logically reasonable able empirically verify first one particular level n also provide short survey ggtp
Statistics,estimands analysis adverse event presence varying followup time within benefit assessment therapy,analysis adverse event aes key component assessment drug safety profile inappropriate analysis method may result misleading conclusion therapy safety consequently benefitrisk ratio statistical analysis aes complicated fact followup time vary patient included clinical trial paper take focus analysis ae data presence varying followup time within benefit assessment therapeutic intervention instead approaching issue directly solely analysis point view first discus estimated context safety data leading concept estimands although current discussion estimands mainly related efficacy evaluation concept applicable safety endpoint well within framework estimands present statistical method analysing aes focus time occurrence first ae specific type give recommendation estimator used estimands described furthermore state practical implication analysis aes clinical trial give overview example across different indication also provide review current practice health technology assessment hta agency respect evaluation safety data finally describe problem metaanalyses ae data sketch possible solution
Statistics,hyperspectral data analysis r hsdar package,hyperspectral remote sensing promising tool variety application including ecology geology analytical chemistry medical research article present new hsdar package r statistical software performs variety analysis step taken typical hyperspectral remote sensing approach package introduces new class efficiently storing large hyperspectral datasets hyperspectral cube within r package includes several important hyperspectral analysis tool continuum removal normalized ratio index integrates two widely used radiation transfer model addition package provides method directly use functionality caret package machine learning task two case study demonstrate package range functionality first plant leaf chlorophyll content estimated second cancer human larynx detected hyperspectral data
Statistics,bayes bayes longer question,paper seek provide thorough account ubiquitous nature bayesian paradigm modern statistic data science artificial intelligence maligned one hand philosophically hated idea subjective probability used prior specification hand intractability computation needed bayesian estimation inference bayesian school thought permeates pervades virtually area science applied science engineering social science even liberal art often unsuspected way thanks part availability powerful computing resource also literally unavoidable inherent presence quintessential building block bayesian paradigm walk life bayesian way handling statistical learning estimation inference mainstream also becoming central approach learning data paper explores relevant element help reader appreciate pervading power presence bayesian paradigm statistic artificial intelligence data science emphasis gospel according reverend thomas bayes turned truly good news case amazing saving grace seek learn statistically data help reader gain deeper tangible practical insight bayesian machinery point computational tool designed r statistical software environment help explore bayesian statistical learning
Statistics,absolutely zero evidence,statistical analysis often used evaluate evidence scientific hypothesis various statistic eg pvalues likelihood ratio bayes factor interpreted measure evidence strength consider evidence measurement point view representational measurement theory argue familiar evidence statistic conform legitimate measurement scale type consider notion absolute scale evidence measurement sense defined focusing particularly notion absolute evidence turn something one might expected
Statistics,extreme shock model alternative perspective,extreme shock model introduced gut h usler study system random time subject shock random magnitude system break shock overcomes given resistance level paper propose alternative approach extreme shock model using reinforced urn process consequence able look problem bayesian nonparametric perspective providing predictive distribution system default
Statistics,test nonequivalence among absolutely nonsingular tensor geometric invariant,absolutely nonsingular tensor characterized determinant polynomial nonquivalence among absolutely nonsingular tensor respect class linear transformation chage tensor rank studied shown theoretically affine geometric invariant constant surface determinant polynomial useful discriminate nonequivalence among absolutely nonsingular tensor also numerical caluculations presented invariant shown useful indeed caluculation invarinats design also commented showed algebraic problem tensor data analysis attacked affine geometric method
Statistics,flexible observed factor model separate dynamic factor volatility correlation matrix,article considers regression model observed factor observed factor flexible stochastic volatility structure separate dynamic volatility correlation matrix correlation matrix factor timevarying evolution described inverse wishart process model specifies evolution observed volatility flexibly particularly attractive dimension observation high markov chain monte carlo algorithm developed estimate model straightforward use algorithm obtain predictive distribution future observation carry model selection model illustrated compared wisharttype factor multivariate stochastic volatility model using various empirical data including monthly stock return portfolio weighted return evidence suggests model better predictive performance paper also allows idiosyncratic error follow individual stochastic volatility process order deal volatile data daily weekly stock return
Statistics,note biasedness unbiasedness twosample kolmogorovsmirnov test,paper deal twosample kolmogorovsmirnov test biasedness test unbiased general case different sample size found biased distribution value significance level alpha moreover discovered exists number observation significance level alpha test unbiased level alpha
Statistics,time stop teaching frequentism nonstatisticians,cease teaching frequentist statistic undergraduate switch bayes reduce amount confusion overcertainty rife among user statistic
Statistics,renyi entropy convergence max domain attraction,paper prove renyi entropy linearly normalized partial maximum independent identically distributed random variable convergent corresponding limit renyi entropy linearly normalized partial maximum converges nondegenerate random variable
Statistics,null hypothesis significance test mixup two different theory basis widespread confusion numerous misinterpretation,null hypothesis statistical significance test nhst widely used quantitative research empirical science including scientometrics nevertheless since introduction nearly century ago significance test controversial many researcher aware numerous criticism raised nhst practiced nhst characterized null ritual overused often misapplied misinterpreted nhst fact patchwork two fundamentally different classical statistical testing model often blended wishful quasibayesian interpretation undoubtedly major reason nhst often misunderstood nhst also intrinsic logical problem epistemic range information provided test much limited researcher recognize article introduce scientometric community theoretical origin nhst mostly absent standard statistical textbook discus prevalent problem relating practice nhst trace problem back mixup two different theoretical origin finally illustrate misunderstanding example scientometric literature bring forward modest recommendation sound practice quantitative data analysis
Statistics,r markdown integrating reproducible analysis tool introductory statistic,nolan temple lang argue ability express statistical computation essential skill key related capacity ability conduct present data analysis way another person understand replicate copyandpaste workflow artifact antiquated userinterface design make reproducibility statistical analysis difficult especially data become increasingly complex statistical method become increasingly sophisticated r markdown new technology make creating fullyreproducible statistical analysis simple painless provides solution suitable cutting edge research also use introductory statistic course present evidence r markdown used effectively introductory statistic course discus role rapidlychanging world statistical computation
Statistics,method comparing chess opening,quantitative method described comparing chess opening test opening baseline opening run chess engine controlled condition compared evaluate effectiveness test opening result intuitively appealing case agree expert opinion specific contribution work development objective measure may used evaluation refutation chess opening process left thought experiment subjective conjecture thereby large variety opinion great deal debate
Statistics,conversation donald b rubin,donald bruce rubin john l loeb professor statistic harvard university made fundamental contribution statistical method missing data causal inference survey sampling bayesian inference computing application wide range discipline including psychology education policy law economics epidemiology public health social biomedical science
Statistics,validating sample average approximation solution negatively dependent batch,sampleaverage approximation saa practical mean finding approximate solution stochastic programming problem involving extremely large infinite number scenario saa also used find estimate lower bound optimal objective value true problem coupled upper bound provides confidence interval true optimal objective value valuable information quality approximate solution specifically lower bound estimated solving multiple saa problem obtained using particular sampling method averaging obtained objective value stateoftheart method lowerbound estimation generate batch scenario saa problem independently paper describe sampling method produce negatively dependent batch thus reducing variance sampleaveraged lower bound estimator increasing usefulness defining confidence interval optimal objective value provide condition new sampling method reduce variance lower bound estimator present computational result verify scheme reduce variance significantly comparison traditional latin hypercube approach
Statistics,conditional quantile estimation optimal quantization,paper use quantization construct nonparametric estimator conditional quantiles scalar response given ddimensional vector covariates x first focus population level show optimal quantization x consists discretizing x projecting appropriate grid n point allows approximate conditional quantiles given x show approximation arbitrarily good n go infinity provide rate convergence approximation error turn sample case define estimator conditional quantiles based quantization idea prove estimator consistent fixed n population counterpart result illustrated numerical example dominance estimator local constantlinear one nearest neighbor one demonstrated extensive simulation companion paper charlier et al
Statistics,sociotype new conceptual construct human social network application mental health quality life,present work discus pertinence sociotype construct theoretically empirically oriented term based conceptual chain genotypephenotypesociotype suggests evolutionary preference human specie determined average social relationship core pattern sociotype explored herein networking relationship young people university student filling questionnaire social interaction spite preliminary study interesting result obtained gender conversation time mental health sociability level satisfaction personal relationship sociotype hypothesis could timely enterprise mental health quality life policy
Statistics,le problem probability discussed bernoulli montmort waldegrave,part v second edition pierre r e mond de montmort essay danalyse sur le jeux de hazard published contains correspondence probability problem montmort nicolaus bernoulli correspondence begin last published letter dated november montmort nicolaus bernoulli discussion strategy play card game le bit news montmort friend waldegrave paris going take care printing book earlier correspondence bernoulli montmort apparent waldegrave also analyzed le come mixed strategy solution also suggested working problem pool often called waldegrave problem universit tsbibliothek basel contains additional fortytwo letter bernoulli montmort written well two letter bernoulli waldegrave letter french provide translation key passage trio continued discus probability problem particularly le still discussion essay danalyse went print describe probability content body correspondence put historical context also provide proper identification waldegrave based manuscript archive nationales de france paris
Statistics,conversation nancy flournoy,nancy flournoy born long beach california may graduating polytechnic school pasadena earned b m biostatistics ucla bachelor master degree worked statistician regional medical program ucla receiving master degree spend three year southwest laboratory education research development seal beach california flournoy joined seattle team pioneering bone marrow transplantation moved transplant team newly formed fred hutchinson cancer research center director clinical statistic supervised group responsible design analysis simultaneous clinical trial support clinical division supervised development interdisciplinary shared data software system recruited leonard b hearne create database management system married cancer center also university washington received doctorate biomathematics became first female director program statistic national science foundation nsf received service award nsf national institute statistical science facilitating interdisciplinary research flournoy joined department mathematics statistic american university moved department chair university missouri became curator distinguished professor
Statistics,conversation richard olshen,richard olshen born portland oregon may richard spent early year chevy chase maryland lived life california received ab statistic university california berkeley phd statistic yale university writing dissertation direction jimmie savage frank anscombe served research staff statistician lecturer yale richard accepted faculty appointment stanford university held tenured faculty position university michigan university california san diego stanford university since stanford professor health research policy biostatistics chief division biostatistics since professor courtesy electrical engineering statistic various time visiting faculty position columbia harvard mit stanford hebrew university richard research interest statistic mathematics application medicine biology much work concerned binary treestructured algorithm classification regression survival analysis clustering classification survival analysis used success computeraided diagnosis prognosis especially cardiology oncology toxicology coauthored book classification regression tree leo brieman jerome friedman charles stone give motivation algorithm various example mathematical theory come known cart algorithm approach treestructured clustering applied problem digital radiography stanford ee professor robert gray hiv genetics latter work including study single nucleotide polymorphism helped shed light presence hypertension certain subpopulation woman
Statistics,propagation uncertainty risk analysis safety integrity level composition,many risk analysis result given mean value often input data also mean value however required accuracy result often interval value e g derivation safety integrity level sil paper reason accuracy input data risk analysis particular certainty result demanded also backside coin sil composition discussed result show common method risk analysis faulty sil allocation kind sil calculus seems infeasible without additional requirement composed component justification common practice parameter scaling wellconstructed semiquantitative risk analysis also provided
Statistics,combating antistatistical thinking using simulationbased method throughout undergraduate curriculum,use simulationbased method introducing inference growing popularity stat course due part increasing evidence method ability improve student statistical thinking impact come simulationbased method clearly presenting overarching logic inference b strengthening tie statistic probability mathematical concept c encouraging focus entire research process facilitating student thinking advanced statistical concept e allowing time explore talk real research messy data f acting firmer foundation build statistical intuition thus argue simulationbased inference entry point undergraduate statistic program student simulationbased inference used throughout undergraduate statistic course order achieve goal fully recognize benefit simulationbased inference undergraduate statistic program need break free historical force tying undergraduate statistic curriculum mathematics consider radical innovative new pedagogical approach course fully implement assessmentdriven content innovation embrace computation throughout curriculum
Statistics,study property estimation problem modified extension exponential distribution,present paper considers modified extension exponential distribution three parameter study main property new distribution special emphasis median mode moment function characteristic related reliability study modified extension exponential distribution mexed obtained bayes estimator scale shape parameter using lindley approximation lapproximation squared error loss function approximation technique possible compute interval estimate parameter therefore also propose gibbs sampling method generate sample posterior distribution basis generated posterior sample computed bayes estimate unknown parameter constructed highest posterior density credible interval monte carlo simulation study carried compare performance bayes estimator corresponding classical estimator term simulated risk real data set considered illustrative purpose study
Statistics,third way probability statistic beyond testing estimation importance relevance skill,third way implementing probability model practicing answer question put term observables eliminates frequentist hypothesis testing bayes factor also eliminates parameter estimation third way logical probability approach make statement pr x observables interest taking value given probative data x past observation present model possibly deduced significance false idea probability model show causality place importance relevance model built keeping information relevant important decision maker statistician model stated publicly verifiable fashion prediction model must undergo verification process trust put
Statistics,exploration statistic research approach expose undergraduate authentic data analysis,exploration statistic research workshop oneweek nsffunded summer program introduces undergraduate student current research problem applied statistic goal workshop expose student exciting modern applied statistical research practice ultimate aim interesting seeking training statistic undergraduate graduate level program explicitly designed engage student connection authentic domain problem statistical idea approach needed address problem important aspect statistical thinking difficult teach sometimes lacking methodological course program past nine year ran workshop six time similar program science two time describe program summarize feedback participant identify key feature success abstract feature provide set recommendation faculty incorporate important element regular course
Statistics,extended dynamic generalized linear model twoparameter exponential family,develop bayesian framework estimation prediction dynamic model observation twoparameter exponential family different link function introduced model mean precision exponential family allowing introduction covariates time series component explore conjugacy analytical approximation class partial specified model keep computation fast algorithm west harrison migon extended cope twoparameter exponential family model methodological novelty illustrated two application real data first considers unemployment rate brazil second macroeconomic variable united kingdom
Statistics,conversation professor tadeusz caliński,tadeusz cali n ski born pozna n poland despite absence formal secondary eduction pole second world war entered university pozna n initially studying agronomy later year mathematics taught statistic biometry experimental design agricultural university pozna n period founded developed pozna n interuniversity school mathematical statistic biometry become one important school type poland beyond supervised phd student many currently professor variety university professor emeritus among many award professor cali n ski received order polonia restituta outstanding achievement field education science polish statistical society awarded jerzy sp l awaneyman medal contribution development research statistic poland professor cali n ski addition doctoral degree honoris causa agricultural university pozna n warsaw university life science research interest include mathematical statistic biometry application agriculture natural science biology genetics published article scientific journal well sanpei kageyama two important book randomization approach design analysis experiment extremely active successful initiating contributing fruitful international research cooperation polish statistician biometricians colleague various country particularly netherlands france italy great britain germany japan portugal conversation addition cover history biometry experimental design poland early influence british statistician
Statistics,conversation alan gelfand,alan e gelfand born april bronx new york attended public grade school undergraduate work called city college new york ccny cuny excelling mathematics surprised saddened mother going way across country stanford graduate school completed dissertation direction professor herbert solomon making academic grandson herman rubin harold hotelling alan accepted faculty position university connecticut uconn promoted tenured associate professor full professor year later became interested decision theory empirical bayes eventually led publication gelfand smith j amer statist assoc paper introduced gibbs sampler statistician revolutionized bayesian computing alan interest turned strongly spatial statistic leading fundamental contribution spatiallyvarying coefficient model coregionalization spatial boundary analysis wombling spent year faculty uconn retiring become james b duke professor statistic decision science duke university serving chair duke continued work spatial methodology increasing impact environmental science date published paper book also supervised phd dissertation postdoc interview done prior conference family academic descendant colleague celebrate birthday contribution statistic took place april duke university
Statistics,multidimensional stream signature representation,signature path essential object theory rough path signature representation data stream recover standard statistic eg moment data stream classification random walk indicates advantage using signature stream feature set machine learning
Statistics,statistician data scientist,according recent report european commission world generates every minute million billion data byte equivalent dvd company build decisionmaking process exploiting data increase productivity treatment valorization massive data consequence employment graduate student statistic additional skill student trained statistic need acquire become data scientist evolve training future graduate adapt rapid change area without neglecting traditional job fundamental lasting foundation training considering notion big data questioning emergence new science data science present current development training engineer mathematical modeling insa toulouse
Statistics,outline bayesian decision theory,paper give outline bayesian decision theory
Statistics,impugning randomness convincingly,john organized state lottery wife main prize may feel event winning nt particularly random would argue fair court law traditional probability theory even notion random event algorithmic information theory applicable realworld scenario like lottery one attempt rectify
Statistics,picking winner daily fantasy sport using integer programming,consider problem selecting portfolio entry fixed cardinality contest topheavy payoff structure ie winning go topranked entry framework general used model variety problem movie studio selecting movie produce venture capital firm picking startup company invest individual selecting lineup daily fantasy sport contest example focus model portfolio selection task combinatorial optimization problem submodular objective function given probability least one entry winning show probability approximated using pairwise marginal probability entry winning certain structure joint distribution consider model entry jointly gaussian random variable present closed form approximation objective function building consider scenario entry given sum constrained resource present integer programming formulation construct entry formulation us principle based theoretical analysis construct entry maximize expected score entry subject lower bound variance upper bound correlation previously constructed entry demonstrate effectiveness integer programming approach apply daily fantasy sport contest topheavy payoff structure find approach performs well practice using integer programming approach able rank topten multiple time hockey baseball contest thousand competing entry approach easily extended problem constrained resource topheavy payoff structure
Statistics,square importance sum square making general linear model simple,statistic one valuable discipline science based proof alone produce result approach statistic acceptable language proof science yet statistic difficult understand large percentage evaluating even research reason difficulty may statistic operates counter way people think well widespread phobia numeracy adding difficulty undergraduate textbook tend make statistical test seem unorganized conglomeration unrelated procedure lead failure student understand parametric procedure studying introductory course ultimately thing stem common source statistic precisely material complex presentation must simple article endeavor
Statistics,understanding convolutional neural network,convoulutional neural network cnns exhibit extraordinary performance variety machine learning task however mathematical property behavior quite poorly understood work form framework analyzing operation perform goal project present key result theory provide intuition cnns work
Statistics,peter hall work highdimensional data classification,article summarise peter hall contribution highdimensional data including geometric representation variable selection method based ranking also discus work classification problem concluding personal reflection interaction
Statistics,bringing order chaos brickyard,allegory published titled chaos brickyard spoke decline quality research intervening time greater awareness issue action improve research endeavor emerged still problem persist paper intended clarify challenge particularly respect quantitative research suggest way improve quality published research paper highlight feasible refinement analytical technique made provides guide fundamental principle related data analysis research
Statistics,identifiability testability grt individual difference,silbert thomas showed failure decisional separability general identifiable fully parameterized time gaussian grt model recent extension time grt model grtwind developed solve problem conceptually similar problem simultaneous identifiability mean marginal variance grt model central ability grtwind solve problem assumption universal perception consists shared perceptual distribution modified attentional global scaling parameter soto et al universal perception valid grtwind solves issue paper show grtwind universal perception subjectspecific failure decisional separability mathematically thereby empirically equivalent model decisional separability failure universal perception provide formal proof fact mean marginal variance general simultaneously identifiable time grt model including grtwind result taken delineate precisely assumption universal perception must consist based result related recent mathematical development grt framework propose addition requiring fixed subset parameter determine location scale given grt model subset parameter must set grt model fix orthogonality modeled perceptual dimension central conceptual underpinning grt framework conclude discussion perceptual primacy relationship universal perception
Statistics,consider avoiding significance level,suggested shortcoming null hypothesis significance testing nhst viewed perspective bayesian statistic turn benign traditional threshold p value substituted sufficiently smaller value illustrate posterior probability stating given data render rejected nhst p value uniform prior shown much smaller value n even exceeds n contrast p value posterior probability exceed n neither n yet interesting posterior probability becomes quite independent n p value hence practically satisfying alpha postulate set cornfield condition p value measure evidence view low prospect researcher soon convert use bayesian statistic form thus suggest researcher elect conservative option resorting nhst encouraged avoid much possible using p value threshold rejecting analysis presented may used discus afresh level threshold p value seems reasonable practical substitute
Statistics,sterrett procedure generalized group testing problem,group testing useful method broad application medicine engineering even airport security control consider finite population n item item probability pi defective goal identify item mean group testing generalized group testing problem optimum procedure respect expected total number test unknown even case pi equal cite proved ordered partition respect pi optimal dorfman procedure procedure obtained optimum solution ie found optimal partition dynamic programming paper investigate sterrett procedure procedure provide close form expression expected total number test allows u find optimum arrangement item particular group also show ordered partition optimal procedure even slightly modified dorfman procedure procedure prime discovery implies finding optimal procedure appears hard computational problem however using optimal ordered partition procedure show procedure prime uniformly better procedure based numerical comparison procedure uniformly significantly better procedure prime
Statistics,conditional visualization statistical model introduction condvis package r,condvis package interactive visualization section data space showing fitted model section observed data near section primary goal interpretation complex model showing observed data support fitted model video accompaniment paper available http wwwyoutubecomwatch preprint version article appear journal statistical software
Statistics,scale curvature effect principal geodesic analysis,growing interest using close connection differential geometry statistic model smooth manifoldvalued data particular much work done recently generalize principal component analysis pca method dimension reduction linear space riemannian manifold one generalization known principal geodesic analysis pga paper novel fashion obtains taylor expansion scaling parameter introduced domain objective function pga shown technique lead better closedform approximation pga also reveals effect scale curvature distribution data solution pga difference firstorder tangent space approximation approach able applied pga also generalization pca generally intrinsic statistic riemannian manifold
Statistics,devastating example halfer rule,update de dicto belief face de se evidence sleeping beauty problem divide philosopher two camp halfers thirders disagreement among halfers position generalize example full generalization always given one notable exception halfer rule agent update uncentered belief based uncentered part evidence brief article provide simple example halfer rule prescribes credence argue reasonably held anyone particular credence constitute egregious violation reflection principle discus consequence halfing general
Statistics,causal influence linear response model,intuition causation fundamental almost every research study life science refers concept however widely accepted formal definition causal influence observables still missing framework linear langevin network without feedback linear response model developed measure causal influence based decomposition information flow time discus main property compare information measure like transfer entropy finally outline difficulty extension general definition causal influence complex system
Statistics,p value,model consistently treated approximation procedure consistent treat model true context p value one measure approximation small p value indicating poor approximation approximation region defined distinguished confidence region
Statistics,stop test opinion bias statistical test,statistician quarrel hypothesis testing debate usually focus method correct one fundamental question whether test hypothesis tends forgotten lack debate root desire idea believe defend cognitive experiment showing choose idea become prey large number bias several bias grouped together single description opinion bias opinion bias nothing desire believe something defend also despite feeling believing solid logical philosophical ground paper show combine fact even logic never prove idea right wrong problem brain cause pick idea hypothesis testing terminology recipe disaster testing place thinking hypothesis
Statistics,bayesvarsel bayesian testing variable selection model averaging linear model using r,paper introduces r package bayesvarsel implement objective bayesian methodology hypothesis testing variable selection linear model package computes posterior probability competing hypothesesmodels provides suite tool specifically proposed literature properly summarize result additionally ourpack armed function compute several type model averaging estimation prediction weight given posterior probability bayesvarsel contains exact algorithm perform fast computation problem small moderate size heuristic sampling method solve large problem software intended appeal broad spectrum user interface carefully designed highly intuititive inspired wellknown lm function issue prior input carefully addressed default usage fully automatic user bayesvarsel implement criteriabased prior proposed bayarri et al advanced user possibility using several popular prior literature package available comprehensive r archive network cran illustrate use bayesvarsel several data example
Statistics,highdimensional nonparametric monotone function estimation using bart,estimation regression relationship large set potential predictor x x p flexible nature nonparametric approach bart bayesian additive regression tree allows much richer set possibility restrictive parametric approach however may often occur subject matter consideration suggest relationship monotone one predictor situation propose monotone bart constrained version bart us monotonicity information improve function estimation without need using parametric form imposing monotonicity appropriate result function estimate smoother interpretable ii better outofsample predictive performance iii le uncertainty iv le sensitivity prior choice key aspect unconstrained bart model carry directly monotone bart imposition monotonicity constraint necessitates fundamental rethinking model implemented particular original bart algorithm markov chain monte carlo algorithm relied conditional conjugacy longer available highdimensional constrained space
Statistics,building communication skill theoretical statistic course,traditional theoretical statistic course develops theoretical underpinnings discipline usually following probability course undergoing nearcontinuous revision statistic community particular recent version course incorporated computation take look different aspect revision building student communication skill course written verbal form allow student demonstrate ability explain statistical concept two separate project discussed engaged class size spring first project computational aspect performed using r statistical theory component writing component based historical german tank problem second project involved class presentation written report summarizing critiquing andor explaining article selected american statistician
Statistics,graph downsampling technique based graph fourier transform,paper provide graph fourier transform based approach downsample signal graph bandlimited signal graph test provided identify whether signal reconstruction possible given downsampled signal moreover signal bandlimited provide quality measure comparing different downsampling scheme using quality measure propose greedy downsampling algorithm prevailing approach consider undirected graph exploit topological property graph order downsample grid proposed method exploit spectral property graph signal applicable directed graph undirected graph graph negative edgeweights provide several experiment demonstrating downsampling scheme compare quality measure measure like normalized cut
Statistics,puma criterion mode criterion,show recently proposed enhanced puma estimator array processing minimizes criterion function wellestablished mode estimator puma principalsingularvector utilization modal analysis mode method direction estimation
Statistics,hypertools python toolbox visualizing manipulating highdimensional data,data visualization reveal trend pattern otherwise obvious raw data summary statistic visualizing lowdimensional data relatively straightforward example plotting change variable time x coordinate graph always obvious visualize highdimensional datasets similarly intuitive way present hypetools python toolbox visualizing manipulating large highdimensional datasets primary approach use dimensionality reduction technique pearson tipping bishop embed highdimensional datasets lowerdimensional space plot data using simple yet powerful api many option data manipulation eg hyperalignment haxby et al clustering normalizing etc plot styling toolbox designed around notion data trajectory point cloud position object moving space visualized trajectory hypertools us dimensionality reduction algorithm create similar trajectory time series highdimensional observation trajectory may plotted interactive static plot visualized animation dimensionality reduction alignment algorithm also reveal structure static datasets eg collection observation attribute present several example showcasing using toolbox explore data trajectory lowdimensional embeddings reveal deep insight datasets across wide variety domain
Statistics,using study study help statistic student ass research finding,one learning goal introductory statistic course develop ability make sense research finding published paper atlantic magazine regularly publishes feature called study study summarizes multiple article published particular domain describe classroom activity develop capacity using study study activity student read capsule summary twelve research paper related restaurant dining published april selected paper report topic seating arrangement server posture plate color size use background music relate revenue ambiance perceived food quality student assigned one twelve paper read critique part small group group critique shared class instructor pilot study conducted academic year amherst college student noted key detail included published summary generally skeptical published conclusion student often provided additional summarization information journal article better describe result independently assessing comparing original study conclusion capsule summary study study student practice developing judgment assessing validity statistical result
Statistics,enriching student conceptual understanding confidence interval interactive triviabased classroom activity,confidence interval provide way determine plausible value population parameter omnipresent research article involving statistical analysis appropriately key statistical literacy learning objective ability interpret understand confidence interval wide range setting instructor devote considerable amount time effort ensure student master topic introductory course beyond yet study continue find confidence interval commonly misinterpreted even expert trouble calibrating individual confidence level article present tenminute trivia gamebased activity address misconception exposing student confidence interval personal perspective describe activity integrated statistic course onetime activity repetition interval throughout course discus result using activity class present possible extension
Statistics,nonparametric spherical regression using diffeomorphic mapping,spherical regression explores relationship variable spherical domain develop nonparametric model us diffeomorphic map sphere restriction mapping diffeomorphisms natural several setting model estimated penalized maximumlikelihood framework using gradientbased optimization towards goal specify firstorder roughness penalty using jacobian diffeomorphisms compare prediction performance proposed model stateoftheart method using simulated real data involving cloud deformation wind direction vectorcardiograms model found outperform others capturing relationship spherical variable
Statistics,burst frequency offset analysis implication descent rate endofflight,malaysian airline flight veered course unexpectedly scheduled trip kuala lumpur beijing march tracked via military radar malacca strait disappearing radar subsequently believed turned south towards southern indian ocean crashing approximately hour later article discus specifically analysis burst frequency offset bfo metadata satcom message shown bfos corresponding last two satcom message plane march suggest flight rapidly descending accelerating downwards message exchange ground station ceased
Statistics,social big data analytics consumer choice two sided online platform perspective,dissertation examines three distinct big data analytics problem related social aspect consumer choice main goal line research help two sided platform firm target marketing policy given great heterogeneity among customer three essay combined structural modeling machine learning approach first understand customer response intrinsic extrinsic factor using unique data set scraped web explore method optimize two sided platform firm reaction accordingly first essay examines social learning mobile app store context controlling intrinsic value hedonic utilitarian mobile apps price advertising number option available second essay investigates bidder anticipated winner loser regret context ebay online auction platform using large data set ebay empirical bayesian estimation method quantify bidder anticipation regret various product category investigate role experience explaining bidder regret learning behavior third essay investigates effect gamification incentive mechanism online platform user generated content use ensemble method lda mixed normal kmean clustering method segment user competitor collaborator achiever explorer uninterested user finding help gamification platform target user simulation counterfactual analysis suggests two sided platform increase number user contribution making earning badge difficult
Statistics,jbs haldane could done better,review contribution jbs haldane development bayes factor hypothesis test etz wagenmakers focus haldane proposition mixture prior genetic example haldane note inverse probability mathematical proceeding cambridge philosophical society haldane never followed idea difficult gauge motivation intention argue contrary haldane stated intention replacing flat prior reasonable assumption actually chose example unreasonable flat prior considering information available haldane derive superior prior compare haldane flat prior haldane main intent article seems explore different parameter region binomial conjugate beta furthermore agree etz wagenmakers haldane serendipitously adopted mixture prior comprising point mass smooth distribution genetic example
Statistics,estimating probability function observed noise convex,consider realvalued function observed stochastic noise finite set design point within euclidean space wish determine whether exists convex function go true function value design point develop asymptotically consistent bayesian sequential sampling procedure estimate posterior probability true iteration posterior probability estimated using monte carlo simulation offer three variance reduction method change measure acceptancerejection conditional monte carlo numerical experiment suggest conditional monte carlo method preferred
Statistics,new concept measurement error regularity effect characteristic,several literature author give new thinking measurement theory system based error nonclassification philosophy completely overthrow existing measurement concept system precision trueness accuracy paper focusing issue error regularity effect characteristic author thematic interpretation prove error regularity actually come different cognitive perspective also unable used classifying error error effect characteristic actually depend artificial condition rule repeated measurement still unable used classifying error thus perspective error regularity effect characteristic existing error classification philosophy still incorrect uncertainty concept system must interpreted error nonclassification philosophy naturally becomes way measurement theory
Statistics,best fractional derivative fit data,aim work show based concrete data observation choice fractional derivative modelling problem relevant accuracy method using least square fitting technique determine order fractional differential equation better describes experimental data different type fractional derivative
Statistics,nonparametric estimation conditional distribution regression boundary point,nonparametric regression standard statistical tool increased importance big data era boundary point pose additional difficulty local polynomial regression used alleviate local linear regression example easy implement performs quite well interior well boundary point estimating conditional distribution function andor quantile function given regressor point immediate via standard kernel method problem ensue local linear method used particular distribution function estimator guaranteed monotone increasing quantile curve cross paper hand simple method correcting local linear distribution estimator monotonicity proposed good performance demonstrated via simulation real data example
Statistics,mathematically sensible explanation concept statistical population,statistic education concept population widely felt hard grasp result vague explanation textbook textbook author therefore chose mention paper offer new explanation proposing new theoretical framework population sampling aim achieve high mathematical sensibleness explanation term population given clear definition relationship simple random sampling iid random variable examined mathematically
Statistics,new theory measurement error uncertainty,traditional measurement theory interprets variance dispersion measured value actually contrary general mathematical concept variance constant paper fully demonstrate variance measurement theory actually evaluation probability interval error instead dispersion measured value point key point mistake traditional interpretation fully interpret series change conceptual logic processing method brought new concept
Statistics,remark variance bound,shown formula variance combined series yield surprisingly simple proof well known variance bound
Statistics,information v uncertainty foundation science environmental modeling,information accounting provides better foundation hypothesis testing uncertainty quantification quantitative account science derived perspective alleviates need epistemic bridge principle solves problem ad hoc falsification criterion deal verisimilitude facilitating general approach processlevel diagnostics argument wellknown inconsistency bayesian classical statistical hypothesis test due fact probability theory insufficient logic science information theory extension probability theory required provide complete logic base quantitative theory empirical learning organizing question case becomes whether theory model le true much uncertainty associated particular model instead whether information available experimental data might allow u improve model becomes formal hypothesis test provides theory model diagnostics suggests new approach building dynamical system model
Statistics,multiple object tracking unknown background labeled random finite set,paper proposes online multiple object tracking algorithm operate unknown background majority multiple object tracking application model parameter background process clutter detection unknown vary time hence ability algorithm adaptively learn parameter essential practice work detail generalized labeled multi bernouli glmb filter tractable provably bayes optimal multiobject tracker tailored learn clutter detection parameter fly tracking provided background model parameter fluctuate rapidly compared data rate proposed algorithm adapt unknown background yielding better tracking performance
Statistics,fiducial string,fiducial argument fisher described biggest blunder recent review hannig et al demonstrates current increasing interest brilliant idea short note analysis example introduced seidenfeld fiducial distribution restricted string keywords phrase bayesian fiducial inference restriction parameter uncertainty quantification epistemic probability statistic manifold
Statistics,network flow approach visualising role covariates random forest,propose novel application parallel coordinate plot sankey diagram represent hierarchy interacting covariate effect random forest visualisation summarises frequency path tree random forest visualisation role covariates random forest include ranked bar dot chart depicting scalar metric contribution individual covariates predictive accuracy random forest line graph depicting various summary effect varying particular covariate prediction random forest heatmaps metric strength interaction pair covariates parallel coordinate plot response class depicting distribution value covariates among observation representative predicted belong class together visualisation facilitate substantial insight role covariates random forest communicate frequency hierarchy covariates effect across random forest order covariates occur hierarchy visualisation address gap demonstrate visualisation using random forest fitted publicly available data provide software implementation form r package
Statistics,application bayesian network estimation individual psychological characteristic,accurate qualitative comprehensive assessment human potential one important challenge company collective apply bayesian network developing accurate overall estimation psychological characteristic individual based psychological test result identify much individual posse certain trait example trait could stress resistance readiness take risk ability concentrate certain complicated work common way studying psychological characteristic individual testing additionally overall estimation usually based personal experience subjective perception psychologist group psychologist investigated psychological personality trait
Statistics,statistic educational challenge century,teach teach honest answer question painful painful teach lag decade behind practice reduce gap prepare data science workforce trained nextgeneration statistician challenging open problem requires many wellthoughtout experiment finding secret sauce goal article lay basic principle guideline rather creating pseudocurriculum based cherrypicked topic expedite process finding objective solution
Statistics,redundancy scheme engineering coherent system via signaturebased approach,paper proposes signaturebased approach solving redundancy allocation problem component lifetime heterogeneous also dependent two common scheme allocation active standby redundancy considered component lifetime independent proposed approach lead simple manipulation various illustrative example also analysed method implemented practical complex engineering system
Statistics,distance correlation new tool detecting association measuring correlation data set,difficulty detecting association measuring correlation establishing cause effect fascinated mankind since time immemorial democritus greek philosopher underscored well importance difficulty proving causality wrote would rather discover one cause gain kingdom persia address difficulty relating cause effect statistician developed many inferential technique perhaps wellknown method stem karl pearson coefficient correlation pearson introduced late century based idea francis galton describe lecture recentlydevised distance correlation coefficient describe advantage pearson classical measure correlation examine application distance correlation coefficient data drawn large astrophysical database desired classify galaxy according various type lecture analyze data arising ongoing national discussion relationship statebystate homicide rate stringency state law governing firearm ownership lecture also describe remarkable singular integral lie core theory distance correlation coefficient see singular integral admits generalization truncated maclaurin expansion cosine function theory spherical function symmetric cone
Statistics,scientific progress despite irreproducibility seeming paradox,appears paradoxical science producing outstanding new result theory rapid rate time researcher identifying serious problem practice science cause many report irreproducible invalid certainly practice science need improved scientist pursuing goal however perspective argue seeming paradox new always part way science work likely remain first introduce paradox review wide range challenge appear make scientific success difficult next describe factor make science workin past present presumably also future suggest remedy present practice science need applied selectively slow progress illustrate example conclude argument communication science need emphasize problem enormous success benefit science brought bringing element modern society
Statistics,replacing p value frequentist posterior probability possible parameter value must uniform baserate prior probability definition random sampling model,possible parameter value random sampling model shown definition uniform baserate prior probability allows frequentist posterior probability distribution calculated possible parameter value conditional solely actual study observation likelihood probability distribution random selection modelled symmetrical continuous function frequentist posterior probability something equal extreme null hypothesis equal pvalue otherwise p value would approximation idealistic probability replication based assumption perfect study methodological reproducibility used upper bound realistic probability replication may affected various confounding factor bayesian distribution combined frequentist distribution idealistic frequentist posterior probability replication may easier pvalue nonstatisticians understand interpret
Statistics,virtue automated qsar new kid block,quantitative structureactivity relationship qsar proved invaluable tool medicinal chemistry data availability unprecedented level various database collaborated resurgence interest qsar context rapid generation quality predictive model highly desirable hit identification lead optimization showcase application automated qsar approach randomly selects multiple trainingtest set utilizes machinelearning algorithm generate predictive model result demonstrate autoqsar produce model improved similar quality generated practitioner field fraction time despite potential concept benefit community autoqsar opportunity largely undervalued
Statistics,conducting highly principled data science statistician job joy,highly principled data science insists methodology scientifically justified statistically principled computationally efficient astrostatistics collaboration together reminiscence illustrates increased role statistician play ensure trio advance science data along way
Statistics,statistic student identification inferential model element within context invention,statistical thinking partially depends upon iterative process essential feature problem setting identified mapped onto abstract model archetype translated back context original problem setting wild pfannkuch assessment introductory statistic often relies task present student data context expects choose describe appropriate model study explores postsecondary student response alternative task prompt student clearly identify sample population statistic parameter using context invention data include free text narrative response random sample student sample introductory statistic student result suggest student response often portrayed sample population accurately portrayal statistic parameter le reliable associated description wide variety concept response frequently attributed variable kind statistic study design detail parameter implication instruction research discussed including call emphasis modeling paradigm introductory statistic
Statistics,optimal policy group testing incomplete identification,consider large infinite population item item independent others defective probability p good probability goal identify n good item quickly possible following group testing policy policy considered test item together group test outcome group size ni negative accept item group good otherwise discard group move next group continue exact n good item found goal find optimal testing configuration ie group size policy expected waiting time obtain n good item minimal recently gusev found optimal group testing configuration assumption constant group size ninfty note optimal solution policy finite n provided keywords dynamic programming optimal design partition problem shurconvexity
Statistics,data science three ring circus big tent,part collection discussion piece david donoho paper year data science appearing volume issue journal computational graphical statistic
Statistics,data science v statistic two culture,data science business learning data traditionally business statistic data science however often understood broader taskdriven computationallyoriented version statistic term data science broader idea conveys origin statistic reaction narrower view data analysis expanding upon view number statistician paper encourages bigtent view data analysis examine evolving approach modern data analysis relate existing discipline statistic eg exploratory analysis machine learning reproducibility computation communication role theory finally discus trend mean future statistic highlighting promising direction communication education research
Statistics,using random variable predict experimental outcome,shall show paper experiment bernoulli trial success probability p curious feature possible correctly predict outcome probability p
Statistics,curriculum guideline undergraduate program data science,park city math institute pcmi summer undergraduate faculty program met purpose composing guideline undergraduate program data science group consisted undergraduate faculty variety institution u primarily discipline mathematics statistic computer science guideline meant provide structure institution planning revising major data science
Statistics,combining empirical likelihood robust estimation method linear regression model,ordinary least square ols maximum likelihood ml robust method widely used method estimate parameter linear regression model well known method perform well distributional assumption error term however distributional assumption error may appropriate data set case nonparametric method may considered carry regression analysis empirical likelihood el method one nonparametric method el method maximizes function multiplication unknown probability corresponding observation constraint inherited normal equation ols estimation method however well known ols method poor performance outlier data paper consider el method robustifyed constraint robustification constraint done using robust estimation method regression provide small simulation study real data example demonstrate capability robust el method handle unusual observation data simulation real data result reveal robust constraint needed heavy tailedness andor outlier possible data
Statistics,statistical analysis effect current potential proposed rule game tennis,aid mathematical modelling basic tool random walk absorbing barrier derive subsequent formula study effect different version possible rule different rule probability winning game probability break point occurrence mathematical expectation number rally point mathematical expectation number break point game expressed check rule atp statistic men player conclusion suggest slight essential modification rule tennis game namely second service case first service fault allowed first three point rally would partially preserve tradition server advantage modern game time would reduce predictability game significantly increasing way excitement spectator
Statistics,football player bear resemblance messi statistical analysis,many pundit fan ask question football player bear resemblance lionel messi chelsea eden hazard paulo dybala heir messi national team argentina alike player messi someone completely else general research evaluation player performance originated context baseball usa currently great importance almost every team sport planet specifically football club manager use data player similarity looking replacement player presumably similar one also research presented direction certainly interesting football pundit football fan therefore aim study answer question title use statistical analysis based data ongoing league season retrieved whoscored w database w provides detailed data parameter goal scored number assist shot goal pass dribble foul player top european league rank respect overall performance study parameter criterion relevant attacking player used set player candidate alike messi w top list selected data normalization application proper metric function similar player lionel messi found
Statistics,risk factor associated mortality game throne longitudinal cohort study,objective ass mortality identify risk factor associated mortality game throne got design setting longitudinal cohort study fictional kingdom westeros essos participant character appearing got since airing first episode screen time greater equal minute main outcome measure allcause mortality multivariate cox proportional hazard model used ass risk factor associated mortality represented hazard ratio episode unit time result character followed median time episode total character died external invasive injury common cause death attributing total death age decade significant risk factor death hr ci although statistically nonsignificant allegiance house targaryen hr ci associated higher risk mortality per episode house stark character residing south le likely die character residing north hr ci advisor showed lower risk mortality member house statistical significance hr ci conclusion high mortality rate among character got residing north member house dangerous got allegiance house stark trended safer house targaryen
Statistics,differentiating pseudo determinant,class derivative defined pseudo determinant det hermitian matrix class shown nonempty unique canonical member mathbf nabla det det moorepenrose pseudo inverse classic identity gradient determinant thus reproduced example provided including maximum likelihood problem rankdeficient covariance matrix degenerate multivariate gaussian distribution
Statistics,element kopula eventological copula theory,new probability theory eventology theory concept kopula eventological copula introduced theorem characterization set event kopula proved serf eventological preimage wellknown sclar theorem copula kopulas doublet triplet event given well nsets event
Statistics,project based approach statistic data science,increasingly datadriven world facility statistic important ever student institution without statistician often fall mathematics faculty teach statistic course paper present model mathematician asked teach statistic follow model entail connecting faculty numerous department campus develop list topic building repository realworld datasets faculty creating project student interface datasets write lab report aimed consumer statistic discipline end result student well prepared interdisciplinary research accustomed coping idiosyncrasy real data sharpened technical writing speaking skill
Statistics,assessing association precourse metric student preparation student performance introductory statistic result early data simulationbased inference v nonsimulation based inference,recent simulationbased inference sbi movement algebrabased introductory statistic course stat provided preliminary evidence improved student conceptual understanding retention however little known whether positive effect preferentially distributed across type student entering course consider two metric stat student preparation precourse performance concept inventory math act score may may associated end course student performance conceptual inventory student across preparation level tended show improvement stat improvement observed across student preparation level early version sbi course furthermore student gain tended similar regardless whether student entered course preparation le recent data sample student using current version sbi course showed similar result though direct comparison nonsbi student possible overall analysis provides additional evidence sbi curriculum effective improving student conceptual understanding statistical idea postcourse regardless student preparation work needed better understand nuance student improvement based student demographic prior coursework well instructor institutional variable
Statistics,shiny update old experiment game,game powerful tool learning statistical methodology effective game design involves fine balance caricature realism simultaneously illustrate salient concept controlled setting serve testament realworld applicability striking balance particularly challenging response surface design domain realworld scenario often play long time scale theory revised model inferential technique improved knowledge updated present game borrowing liberally one first played forty year ago attempt achieve balance reinforcing cascade topic modern nonparametric response surface sequential design optimization game embeds blackbox simulation within shiny app whose interface designed simulate realistic informationavailability setting offering stimulating competitive environment wherein student try new methodology ultimately appreciate power limitation interface rule timing course material evaluation described along case study involving cohort student virginia tech
Statistics,emanuel parzen memorial model two kernel championed,manny parzen passed away february article written partly memorial appreciation manny made important contribution several area two influenced contribution kernel density estimation reproducing kernel hilbert space two kernel title fond memory manny phd advisor begin memorial followed discussion manny influence density estimation rkhs method picture gallery trip come next followed technical part article goal show risk model built using rkhs penalized likelihood method subject personal sample density used attribute model
Statistics,sure two approach statistical inference,suppose told taking statin reduce risk heart attack stroke next ten year woman better emotional intelligence men may wonder accurate confident assertion woman emotional intelligence bearing mind conclusion based sample data aim present two statistical approach question like approach often called null hypothesis testing prefer phrase baseline hypothesis standard approach many area inquiry fraught problem approach viewed generalisation idea confidence interval application bayes theorem unlike approach approach provides tentative estimate probability hypothesis interest approach explain first principle building common sense statistical concept like average randomness derive answer rationale behind answer achieved using computer simulation method resampling bootstrapping using spreadsheet available web avoid use probability distribution normal etc minimalist reasonably rigorous analysis particularly useful discipline like statistic widely used people specialist intended audience includes statistician user statistical method statistical expert
Statistics,scientific discovery modelcentric framework reproducibility innovation epistemic diversity,consistent confirmation obtained independently lend credibility scientific result refer result satisfying consistency reproducible assume reproducibility desirable property scientific discovery yet seemingly science also progress despite irreproducible result indicating relationship reproducibility desirable property scientific discovery well understood property include early discovery truth persistence truth discovered time spent truth longterm scientific inquiry build mathematical model scientific discovery present viable framework study desirable property including reproducibility framework assume scientist adopt modelcentric approach discover true model generating data stochastic process scientific discovery analyze property process using markov chain theory monte carlo method agentbased modeling show scientific process may converge truth even scientific result reproducible irreproducible result necessarily imply untrue result proportion different research strategy represented scientific population scientist choice methodology complexity truth strength signal contribute counterintuitive finding important insight include innovative research speed discovery scientific truth facilitating exploration model space epistemic diversity optimizes across desirable property scientific discovery
Statistics,theory cricket target score predictability,propose model recalculating target score rain affected match based empirical data development current stage cricket different method introduced recalculate target score interpreted game currently international cricket council icc us duckworthlewis method past strongly considered changing vjd method introduce simple approach calculate target score interrupted game considering area run rate curve calculate target analysed decade worth empirical data using various statistical method case duckworth lewis method also two parameter model over wicket combination also found one day international cricket odi wicket play crucial role whereas cricket effect run rate game degree using empirical mathematical argument show run scoring distribution independent inning
Statistics,resolving lord paradox,explanation lord paradox using ordinary least square regression model given paradox regression parameter interpreted predictive causal stricter condition aware law average use derivation supermodel given submodel residual modelled potential predictor solution
Statistics,viewing simpson paradox,well known simpson paradox puzzling surprising many especially empirical researcher user statistic however surprise far mathematical detail concerned lot written paradox beyond grasp user short article explaining phenomenon easy way grasp using simple algebra geometry mathematical condition paradox occur made explicit simple geometrical illustration used describe consider reversal association two binary variable say x third binary variable say z show always possible define z algebraically nonextreme dependence x therefore occurrence paradox depends identifying practical meaning given context interest subject domain expert finally discus paradox predictive context since literature argued paradox resolved using causal reasoning
Statistics,bnsp r package fitting bayesian semiparametric regression model variable selection,r package bnsp provides unified framework semiparametric locationscale regression stochastic search variable selection statistical methodology package built upon utilizes basis function expansion represent semiparametric covariate effect mean variance function spikeslab prior perform selection regularization estimated effect addition main function performs posterior sampling package includes function assessing convergence sampler summarizing model fit visualizing covariate effect obtaining prediction new response mean given featurecovariate vector
Statistics,data learning big data,technology generating huge growing availability observa tions diverse nature big data placing data learning central scientific discipline includes collection storage preprocessing visualization essentially statistical analysis enormous batch data paper discus role statistic regarding issue raised big data new paradigm also propose name data learning describe activity allow obtain relevant knowledge new source information
Statistics,teaching computational reproducibility neuroimaging,describe projectbased introduction reproducible collaborative neuroimaging analysis traditional teaching neuroimaging usually consists series lecture emphasize big picture rather foundation technique based lecture often paired practical workshop student run imaging analysis using graphical interface specific neuroimaging software package experience suggests combination leaf student superficial understanding underlying idea informal inefficient inaccurate approach analysis address problem based course around substantial openended group project allowed u teach computational tool ensure computationally reproducible work unix command line structured code version control automated testing code review b clear understanding statistical technique used basic analysis single run mri scanner emphasis put group project showed importance standard computational tool accuracy efficiency collaboration project broadly successful engaging student working reproducibly real scientific question propose course model foundation future program neuroimaging believe also serve model teaching efficient reproducible research field computational science
Statistics,constructive algebraic proof student theorem,student theorem important result statistic state normal population sample variance independent sample mean chisquare distribution existing proof theorem either overly rely advanced tool moment generating function fail explicitly construct orthogonal matrix used proof paper provides elegant explicit construction matrix making algebraic proof complete constructive algebraic proof proposed thus suitable included textbook
Statistics,nonparametric confidence region veronesewhitney mean antimeans planar kendall shape space,paper brief revision vwmeans extrinsic mean real complex projective space relative veronesewhitney embeddings give two example sample vw mean computation planar kendall shape space derive large sample pivotal nonparametric bootstrap confidence region vwantimeans using vwanticovariance matrix sample counterpart
Statistics,discussion using stacking average bayesian predictive distribution yao et al,begin summarizing key idea paper discussion talk graphical modeling perspective posterior contraction rate alternative method aggregation moreover also discus possible application stacking method problem particular aggregating sub posterior distribution distributed computing
Statistics,conversation jon wellner,jon august wellner born portland oregon august received bachelor degree university idaho phd degree university washington assistant professor associate professor university rochester returned university washington remained uw faculty member since time course long distinguished career jon made seminal contribution variety area including empirical process semiparametric theory shapeconstrained inference coauthored number extremely influential book honored le cam lecturer ims french statistical society fellow ims asa aaa elected member international statistical institute served coeditor annals statistic editor statistical science president ims made knight order netherlands lion free time jon enjoys mountain climbing backcountry skiing cascade british columbia
Statistics,mathematics freechoice paradigm,chen risen pointed logical flaw affecting conclusion number past experiment used freechoice paradigm measure choiceinduced attitude change went design implement freechoice experiment used novel type control group order avoid logical pitfall paper describe method freechoice experiment correctly conducted even without control group
Statistics,integral mean,harmonic geometric arithmetic heronian contraharmonic mean studied many mathematician h even studied mean geometrical point view established inequality using circle radius e beckenback r bellman introduced several inequality corresponding mean paper introduce concept mean function integral mean give bound mean function integral mean
Statistics,data scraping ingestation modeling bringing data carscom intro stats class,new tool made much easier student develop skill work interesting data set begin extract meaning data fully appreciate statistical analysis cycle student benefit repeated experience collecting ingesting wrangling analyzing data communicating result bring opportunity classroom describe classroom activity originally developed danny kaplan macalester college student expand upon statistical problem solving handscraping data carscom ingesting data r carrying analysis relationship price mileage model year selected type car
Statistics,game time statistical contest classroom,describe contest variable selection part statistic course graduate student particular possibility create contest offered additional challenge advanced student since working data becoming important teaching statistic greatly encourage instructor try
Statistics,perspective literature role expert judgment scientific statistical research practice,article produced result symposium statistical inference introduction literature function expertise judgment choice practice statistic scientific research particular expert judgment play critical role conducting frequentist hypothesis test bayesian model especially selection appropriate prior distribution model parameter subtlety interpreting result also discussed finally external recommendation collected effectively encourage proper use judgment statistic paper synthesizes literature purpose creating single reference inciting productive discussion improve future statistic science
Statistics,interrater software analysis interrater reliability permutating pair multiple user,interrater quantifies reliability multiple raters evaluate group subject calculates group quantity fleiss kappa improves existing software keeping information user quantifying user agreed rest group accomplished permutation user pair software written python run linux code deposited zenodo github software used evaluation interrater reliability systematic review medical diagnosis algorithm education application others
Statistics,benchmarking cluster analysis white paper,achieve scientific progress term building cumulative body knowledge careful attention benchmarking utmost importance mean proposal new method data preprocessing new dataanalytic technique new method output postprocessing extensively carefully compared existing alternative existing method subjected neutral comparison study date benchmarking recommendation benchmarking frequently seen context supervised learning unfortunately dearth guideline benchmarking unsupervised setting area clustering important subdomain address problem discussion given theoretical conceptual underpinnings benchmarking field cluster analysis mean simulated well empirical data subsequently practicality address benchmarking question clustering dealt foundational recommendation made
Statistics,canadian crime rate penalty box,period canadian violent crime rate remained strongly correlated national hockey league nhl penalty canadian property crime rate similarly correlated stolen base attempt major league baseball mlb course correlation imply causation prove association simply presented observation curious reader might tempted conduct additional research ask question order enhance conversation transition away state confusion clarify situation prevent false attribution possibly solve problem economist call identification
Statistics,see clearly reinterpreting statistical significance,null hypothesis significance testing remains popular despite decade concern misuse misinterpretation believe much problem due language significance testing little meaning word significance despite limitation nullhypothesis test argue remain useful many context guide whether certain effect seen clearly context eg whether clearly see correlation betweengroup difference positive negative therefore suggest researcher describe conclusion nullhypothesis test term statistical clarity rather statistical significance simple semantic change could substantially enhance clarity statistical communication
Statistics,complementary lipschitz continuity result distribution intersection union independent random set finite discrete space,prove intersection union independent random set finite space achieve form lipschitz continuity precisely given distribution random set xi function mapping random set distribution distribution intersection independence assumption xi lipschitz continuous unit lipschitz constant space random set distribution endowed metric defined lk norm distance inclusion functionals also known commonality moreover function mapping random set distribution distribution union independence assumption xi lipschitz continuous unit lipschitz constant space random set distribution endowed metric defined lk norm distance hitting functionals also known plausibility using epistemic random set interpretation belief function also discus ability distance yield conflict measure proof paper derived framework dempstershafer belief function let alone discussion conflict measure straightforward transcribe proof general non necessarily epistemic random set terminology
Statistics,helix modelling mardiaholmes model framework extension mardiaholmes model,noisy twodimensional data approximately uniformly distributed near circumference ellipse mardia holmes developed model fit ellipse paper adapt methodology analysis helix data three dimension helix axis known mardiaholmes model circular case fitted projecting helix data onto plane normal helix axis axis unknown iterative algorithm developed estimate axis methodology illustrated using simulated protein alphahelices also give multivariate version mardiaholmes model applicable fitting ellipsoid particular cylinder
Statistics,generalized form subjective probability,paper motivated question give concept probability adequate realworld meaning explain certain type phenomenon found instance ellsberg paradox attempt answer question constructing alternative theory one proposed earlier paper basis various important criticism raised earlier theory conceptual principle corresponding definition probability laid explained detail particular required fully specify probability distribution definition distribution function variable concerned also assessment internal andor external strength function relative distribution function interest way defining probability applied various example problem including perhaps notably longrunning controversy concerning distinction bayesian fiducial inference characteristic definition probability carefully evaluated term issue set address
Statistics,asccr frame learning essential collaboration skill,statistic data science especially collaborative discipline typically require practitioner interact many different people group consequently interdisciplinary collaboration skill part personal professional skill essential success applied statistician data scientist skill learnable teachable learning improving collaboration skill provides way enhance one practice statistic data science help individual learn skill organization teach developed framework covering five essential component statistical collaboration attitude structure content communication relationship call asccr frame framework incorporated formal training program classroom job also used individual selfstudy show framework applied specifically statistician data scientist improve collaboration skill interdisciplinary impact believe asccr frame help organize stimulate research teaching interdisciplinary collaboration call individual organization begin generating evidence regarding effectiveness
Statistics,surrogate modeling stochastic function application computational electromagnetic dosimetry,metamodeling complex numerical system recently attracted interest mathematical programming community despite progress high performance computing simulation remain costly matter fact assessment exposure radio frequency electromagnetic field computationally prohibitive since one simulation require hour moreover many engineering problem carrying deterministic numerical operation without considering uncertainty lead unreliable design paper focus surrogate modeling particular type computational model called stochastic simulator contrast deterministic simulator yield unique output set input parameter stochastic simulator inherently contain source randomness output given point probability density function characterizing stochastic simulator even time consuming paper represents stochastic simulator stochastic process describes metamodeling approach based karhunenlo eve spectral decomposition
Statistics,openness reproducibility insight modelcentric approach,paper investigates conceptual relationship openness reproducibility using modelcentric approach heavily informed probability theory statistic first clarify concept reliability auditability replicability reproducibility denotes potential scientific objective advance conceptual analysis delineate relationship open scientific practice objective using notion idealized experiment identify component experiment need reported need repeated achieve relevant objective modelcentric framework propose aim contribute precision clarity discussion surrounding socalled reproducibility crisis
Statistics,batch self organizing map distributional data using adaptive distance,paper deal batch self organizing map algorithm dbsom data described distributionalvalued variable kind variable characterized take value onedimensional probability frequency distribution numeric support objective function optimized algorithm depends choice distance measure according nature date wasserstein distance proposed one suitable metric compare distribution widely used several context analysis distributional data conventional batch som algorithm consider variable equally important training som however well known variable le relevant others task order take account different contribution variable propose adaptive version dbsom algorithm tackle problem additional step relevance weight automatically learned distributionalvalued variable moreover since wasserstein distance allows decomposition two component one related mean one related size shape distribution also relevance weight automatically learned measurement component emphasize importance different estimated parameter distribution example real synthetic datasets distributional data illustrate usefulness proposed dbsom algorithm
Statistics,quantifying privacy nuclear warhead authentication protocol,international verification nuclear warhead practical problem protection secret warhead information paramount importance propose measure would enable weapon owner evaluate privacy proposed protocol technologyneutral fashion show problem reducible natural corrective learning natural learning computed without assumption inspector corrective learning account inspector prior knowledge natural learning provides warhead owner useful lower bound information leaked proposed protocol using numerical example demonstrate proposed measure correlate better accuracy maximum posteriori probability estimate alternative measure
Statistics,property new sineskewed cardioid distribution,new sine skewed cardioid ssc distribution introduced characterized ahsanullah study asymptotic property tail determining extreme value domain characteristic function moment likelihood estimator two parameter asymptotic normality moment estimator random generation data textit ssc distribution finally proceed simulation study show performance random generation method quality moment estimation parameter
Statistics,rapid prototyping model healthcare alternative payment model replicating federally qualified health center advanced primary care practice demonstration,innovation healthcare payment service delivery utilizes high cost high risk pilot paired traditional program evaluation decisionmakers unable reliably forecast impact pilot intervention complex system complicating feasibility assessment proposed healthcare model developed validated discrete event simulation de model primary care patient diabetes allow rapid prototyping assessment model pilot implementation replicated four outcome center medicare medicaid service federally qualified health center advanced primary care practice pilot de model simulates synthetic population healthcare experience including symptom onset appointment scheduling screening treatment well impact physician training network detailed event module developed peerreviewed literature synthetic patient attribute modify probability distribution event output direct episode care attribute turn modified patient experience model replicates direction effect physician training selected outcome strength effect increase number training simulated effect strength replicates pilot result eye exam nephropathy screening overestimate result ldl screening model improve decisionmakers ability ass feasibility pilot success reproducible literaturebased system model model identifies intervention healthcare system component outcome sensitive aspect monitored controlled pilot implementation work needed improve replication ldl screening elaborate submodels related intervention component
Statistics,pragmatic hypothesis evolution science,paper introduces pragmatic hypothesis relates concept spiral scientific evolution previous work determined characterization logically consistent statistical hypothesis test showed modal operator obtained test represented hexagon opposition however despite importance precise hypothesis science accepted logically consistent test show dilemma overcome use pragmatic version precise hypothesis pragmatic version allow level imprecision hypothesis small relative experimental condition introduction pragmatic hypothesis allows evolution scientific theory based statistical hypothesis testing interpreted using narratological structure hexagonal spiral defined pierre gallais
Statistics,application robust estimator shewhart scharts,maintaining quality manufactured product desired level known increase customer satisfaction profitability shewhart control chart widely used statistical process control spc technique monitor quality product control process variability based assumption independent normally distributed data set sample mean standard deviation statistic known efficient conventional estimator determine process location scale respectively hand guarantee realworld process data would normally distributed outlier may exist andor sampled population may contaminated case efficiency conventional estimator significantly reduced power shewhart chart may undesirably low eg occasional outlier rational subgroup phase dataset may drastically affect sample mean standard deviation resulting serious delay detection inferior product phase ii procedure efficient analysis required use robust estimator contamination consequently determined robust estimator efficient diffuse localized symmetricasymmetric contamination higher power detecting disturbance compared conventional method
Statistics,key factor drop attend lecture,addition learning check testing result performed lecture extended factor find key dropping factor among number success learning check testing number attendance followup program class etc found key factor strongly related student risk following badly failed student score range final examination tend absent regular class fail learning check testing even attended reluctant attend followup program class successful student score range final examination attend class get good score every learning check testing failed student badly score range final examination reveal side feature appeared score range score range therefore crucial attend lecture order drop student failed learning check testing half testing time almost absolutely failed final examination could cause drop also student successful learning check testing two third testing time took better score final examination
Statistics,robust adaptive control chart,statistical process control procedure applied require relatively strict condition use assumption violated method become inefficient leading increased incidence false signal therefore robust version control chart sought le sensitive respect breach normality independence measurement robust control chart however usually increase delay detection assignable cause negative effect extent removed aid adaptive approach
Statistics,variability interpretation dutch probability phrase risk miscommunication,verbal probability phrase often used express estimated risk study focus numerical interpretation dutch probability frequency phrase including several complementary phrase test symmetry interpretation many phrase studied phrase presented context ordinary situation survey distributed among statistician nonstatisticians dutch native language response participant showed large variability interpretation dutch phrase neutral context seemed structural influence furthermore result demonstrated asymmetry interpretation dutch complementary phrase large variability interpretation found among statistician nonstatisticians among male female however structural difference found group concluding large variability interpretation verbal probability phrase even within subpopulation therefore verbal probability expression may risk miscommunication
Statistics,characterization sine skewed von mi distribution,von mi distribution one important distribution statistic deal circular data paper consider basic property characterization sine skewed von mi distribution
Statistics,application bandlimited extrapolation forecasting weather financial time series,paper describes practical application causal extrapolation sequence purpose forecasting method proof applied simulation measure range data accurately extrapolated real world data australian stock exchange australian bureau meteorology tested compared simple linear extrapolation data majority tested scenario casual extrapolation proved effective forecaster
Statistics,note fibonacci sequence random variable,focus paper random sequence form x x x n x x dot referred fibonacci random sequence fr initial random variable x x assumed absolutely continuous joint probability density function pdf f x x fr completely determined x x member fibonacci sequence digamma equiv examine distributional limit property random sequence x n
Statistics,comparison plotting system output beginner analyst,r programming language built ecosystem package allow analyst accomplish task example least two clear workflow creating data visualization r using base graphic package referred base r addon package based grammar graphic perform empirical study quality scientific graphic produced beginning r user experiment learner taking data science course coursera platform randomized complete identical plotting exercise either base r system learner asked evaluate peer term visual characteristic key scientific cognition observed graphic created two system rated similarly many characteristic however graphic generally judged visually pleasing case faceted scientific plot easier understand result suggest graphic system useful hand beginning user natural faceting system may easier use beginning user displaying complex relationship
Statistics,synthesis highresolution load profile minimal data,estimation new energy supply system important highresolution energy load profile profile general either present costly obtain therefore present method synthesizes load profile minimal given data maximal resolution general initial data setting includes month integral load profile day resulting time series feature important property represent real energy profile
Statistics,three issue impeding communication statistical methodology incomplete data,identify three issue permeating literature statistical methodology incomplete data written nonspecialist statistician investigator first mathematical defect notation yob ymis used partition data observed missing component second issue concerning notation p ryobs ymis p ryobs used communicating definition missing random mar third framing ignorability emulating completedata method exactly rather treating question ignorability merit issue present literature long time simple remedy purpose paper raise awareness issue explain remedied
Statistics,modeling health expenditure japan healthy life year lost methodology,healthy life year lost methodology hlyl introduced model estimate health expenditure japan hlyl theory estimation method presented book springer series demographic method population analysis vol titled exploring health state population dynamic modeling method demography health issue population aging mortality data analysis special application appear chapter book healthmortality approach estimating healthy life year lost compared global burden disease study application world usa japan estimation healthy life expectancy italy simple model based mortality rate skiadas arezzo present main part methodology detail illustration develop extend life table important estimate healthy life year lost along fitting health expenditure related case application result quite promising important support decision maker health agency powerful tool improve health expenditure allocation future prediction
Statistics,statistical testing linear probability space,imagine could calculate posttest probability ie bayes theorem simple addition possible stop thinking probability ranging naturally occurring linear probability space data transformed logarithm odds ratio odds space probability replaced w weight probability would like argue multiple benefit performing statistical testing linear probability space statistical testing accurate linear probability space space effect size called impact difference mean two treatment bayes theorem simply wposttestwpretestitest significance p value replaced certainty c w p value method transform data linear probability space described
Statistics,datadriven discovery coordinate governing equation,discovery governing equation scientific data potential transform datarich field lack wellcharacterized quantitative description advance sparse regression currently enabling tractable identification structure parameter nonlinear dynamical system data resulting model fewest term necessary describe dynamic balancing model complexity descriptive ability thus promoting interpretability generalizability provides algorithmic approach occam razor model discovery however approach fundamentally relies effective coordinate system dynamic simple representation work design custom autoencoder discover coordinate transformation reduced space dynamic may sparsely represented thus simultaneously learn governing equation associated coordinate system demonstrate approach several example highdimensional dynamical system lowdimensional behavior resulting modeling framework combine strength deep neural network flexible representation sparse identification nonlinear dynamic sindy parsimonious model first method kind place discovery coordinate model equal footing
Statistics,contribution plot decomposition graphical display rv coefficient application genetic brain imaging biomarkers alzheimer disease,alzheimer disease ad chronic neurodegenerative disease cause memory loss decline cognitive ability ad sixth leading cause death united state affecting estimated million american ass association multiple genetic variant multiple measurement structural change brain recent study ad used multivariate measure linear dependence rv coefficient author decomposed rv coefficient contribution individual variant displayed contribution graphically investigate property contribution plot term underlying linear model discus estimation component plot correlation signal may sparse contribution plot applied simulated data genomic brain imaging data alzheimer disease neuroimaging initiative
Statistics,statistical witchhunts science justice pvalue crisis,provide accessible insight current replication crisis statistical science revisiting old metaphor court trial hypothesis test inter alia define diagnose harmful statistical witchhunting justice science extends replication crisis hunt pvalues currently underway
Statistics,governance social medium data different focus government internet company,government internet company regulate user data social medium attracts public attention study tried answer two question kind country send request facebook user data kind country get request reply facebook aim figure country economic political social factor affect government request user data facebook response rate request result show country higher gdp per caput higher level human freedom lower level rule law send request user data facebook tends reply government request country higher level human freedom lower level political stability conclusion government facebook show different focus governance social medium data
Statistics,method estimation threeparameter reflected weibull distribution,paper propose method estimation parameter threeparameter reflected weibull distribution moment estimator maximum likelihood estimator location scale parameter free maximum likelihood estimator location scale parameter free maximum likelihood estimator based data transformation avoids problem unbounded likelihood estimator mont carlo simulation show location scale parameter free maximum likelihood estimator performs better method moment maximum likelihood estimator term bias root mean squared error finally two example based real data set presented illustrate method
Statistics,fault diagnosis using clustering statistical test use hypothesis testing,predictive maintenance conditionbased monitoring system seen significant prominence recent year minimize impact machine downtime production cost predictive maintenance involves using concept data mining statistic machine learning build model capable performing early fault detection diagnosing fault predicting time failure fault diagnosis one core area actual failure mode machine identified fluctuating environment manufacturing clustering technique proved reliable compared supervised learning method one fundamental challenge clustering developing test hypothesis choosing appropriate statistical test hypothesis testing statistical analysis use underlying assumption data realworld data incapable satisfying assumption paper dedicated overcoming following challenge developing test hypothesis fault diagnosis application using clustering technique performing permanova test hypothesis testing
Statistics,first course data science,data science discipline provides principle methodology guideline analysis data tool value insight driven huge workforce demand many academic institution started offer degree data science many graduate undergraduate level curriculum may differ different institution varying level faculty expertise different discipline math computer science business etc developing curriculum university massachusetts dartmouth started offering degree program data science fall undergraduate graduate level quite article published deal graduate data science course much le dealing undergraduate one discussion focus undergraduate course structure function specifically first course data science design course center around concept called data science life cycle view task step practice data science forming process consisting state indicate come life different task data science depend interact others birth data product reach conclusion naturally different piece data science life cycle form individual part course detail piece filled concept technique skill popular industry consequently design course principled practical significant feature course philosophy line activity theory course based use tool transform real data order answer strongly motivated question related data
Statistics,response critique reproducibility research misinterpretation pvalues,proposed p value supplemented estimate false positive risk fpr fpr defined probability claim real effect basis p value single unbiased experiment mistaken result occurred chance bayesian quantity mean infinitude way calculate choice way estimate fpr therefore arbitrary maintain reasonable way advantage mathematically simpler proposal easier understand method might make easily accepted user always every statistician agrees paper response critique paper arandjelovic
Statistics,statistical method research done science rather mathematics,paper study statistical method example us random regression model intercept slope clusterspecific regression line modeled bivariate random effect maximizing model restricted likelihood often give boundary value random effect correlation variance argue problem problem discipline little understanding contemporary model method map data inferential summary lack understanding even model simple nearexclusive reliance mathematics mean understanding math alone longer sufficient argue discipline break open blackbox method mimicking five step molecular biologist commonly use break open nature black box design simple model system formulate hypothesis using system test experiment system iterate needed reformulate test hypothesis finally test result vivo system demonstrate identifying condition randomregressions restricted likelihood likely maximized boundary value resistance approach seems arise view lack certainty intellectual heft mathematics perhaps simulation experiment literature rarely measure new method operating characteristic small range situation argue work make useful contribution including molecular biology finding sometimes design used five step contribution much practical value mathematical result therefore merit publication much mathematical result discipline esteem highly
Statistics,many perspective deborah mayo statistical inference severe testing get beyond statistic war,new book philosopher deborah mayo relevant data science topical reason take various controversial position regarding hypothesis testing statistical practice also entry point thinking philosophy statistic present article slightly expanded version series informal review comment mayo book hope discussion introduce people mayo idea along perspective topic address
Statistics,illustration risk borrowing information via shared likelihood,concrete stylized example illustrates inference may degraded rather improved incorporating supplementary data via joint likelihood example likelihood assumed correctly specified prior parameter interest necessary joint modeling approach suffer misspecification prior nuisance parameter
Statistics,score function bayesian cluster analysis,propose score function bayesian clustering function parameter free capture interplay within cluster variance cluster entropy clustering used choose number cluster wellestablished clustering method hierarchical clustering k mean algorithm
Statistics,rediscovering little known fact ttest algebraic geometric distributional graphical consideration,discus role null hypothesis play construction test statistic used make decision hypothesis construct test statistic point null hypothesis binomial proportion common recommendation act null hypothesis true argue surface onesample ttest point null hypothesis gaussian population mean appear follow recommendation show simple algebraic manipulation usual tstatistic lead equivalent test procedure consistent recommendation provide geometric intuition regarding equivalence consider extension testing nested hypothesis gaussian linear model discus application graphical residual diagnostics form test statistic make practical difference argue issue discussed advanced undergraduate graduate course
Statistics,proof first digit law laplace transform,first digit law also known benford law significant digit law empirical phenomenon leading digit number real world source favor small one form log law keep elusive one hundred year obscure whether law due logical consequence number system mysterious mechanism nature provide simple elegant proof law application laplace transform important tool mathematical method physic reveal first digit law originated basic property number system thus attributed basic mathematical knowledge wide application
Statistics,karl pearson logic science renouncing causal understanding bride inverted spinozism,karl pearson leading figure xx century statistic coworkers crafted core theory method language frequentist classical statistic prevalent inductive logic contemporary science however working statistic kpearson interest life namely order philosophy physic biological heredity key concept philosophical epistemological system antispinozism form transcendental idealism carried subsequent work logic scientific discovery article main goal analyze kpearson early philosophical theological idea investigate idea came influence contemporary science either directly indirectly use variant theory method dialect statistic corresponding variant statistical inference procedure specific belief calculus
Statistics,maximum likelihood degree chemical reaction equilibrium,complexity maximum likelihood estimation measured maximum likelihood degree ml degree paper study maximum likelihood problem associated chemical network composed one single chemical reaction equilibrium assumption
Statistics,new approach chain sampling inspection plan,develop decision rule regarding acceptance rejection production lot based sample data purpose acceptance sampling inspection plan dependent sampling procedure cumulate result several preceding production lot testing expensive destructive chaining past lot reduce size required sample essential acceptance rejection production lot article new approach chaining past lot result proposed named modified chain group acceptance sampling inspection plan requires smaller sample size commonly used sampling inspection plan group acceptance sampling inspection plan single acceptance sampling inspection plan comparison study done proposed group acceptance sampling inspection plan well single acceptance sampling inspection plan example given illustrate proposed plan good manner
Statistics,data science biomedicine,highlight role data science biomedicine manuscript go general particular presenting global definition data science showing trend discipline together term cloud computing big data addition since data science mostly related area like economy business describe importance biomedicine biomedical data science bd present challenge dealing data coming range biological medical research focusing methodology advance biomedical science discovery interdisciplinary context
Statistics,justifying norm inductive inference,bayesian inference limited scope applied idealized context none hypothesis consideration true committed always using likelihood measure evidential favoring even inappropriate purpose paper study inductive inference general setting finding truth necessarily goal measure evidential favoring necessarily likelihood use accuracy argument argue probabilism develop new kind argument argue two general updating rule reasonable different context one updating rule standard bayesian updating bissiri et al general bayesian updating douven ibebased updating vassend quasibayesian updating special case updating rule novel
Statistics,introduction bent jorgensen idea,briefly expose key aspect theory use dispersion model bent jorgensen played crucial role driving force inspiration source starting general notion dispersion model built using minimalistic mathematical assumption specialize two class family distribution different statistical flavor exponential dispersion proper dispersion model construction dispersion model involves solution integral equation general untractable difficulty disappear mathematical structure assumed reduces calculation moment generating function riemannstieltjes integral exponential dispersion proper dispersion model respectively new technique constructing dispersion model based characteristic function introduced turning integral equation tractable convolution equation yielding example dispersion model neither proper dispersion exponential dispersion model corollary cardinality regular nonregular dispersion model large selected application discussed including exponential family nonlinear model generalized linear model particular case several model clustered dependent data based latent levy process
Statistics,causal screening dynamical system,many classical algorithm output graphical representation causal structure testing conditional independence among set random variable dynamical system local independence used analogously testable implication underlying datagenerating process suggest inexpensive method causal screening provide output sound causal interpretation assumption ancestral faithfulness popular model class linear hawkes process used provide example dynamical causal model argue sparse causal graph output often close complete give example framework apply challenging biological system
Statistics,review groupbased method teaching statistic higher education,teaching statistic higher education uk still largely lecturebased despite recommendation given american statistical association gaise report emphasis placed active learning strategy student take responsibility learning one possible model collaborative learning student learn group carefully crafted problem long suggested strategy teaching statistic article review two specific approach fall collaborative learning model problem teambased learning consider evidence changing model teaching statistic well give practical suggestion could implemented typical statistic class higher education
Statistics,step step mathematical derivation tutorial kalman filter,present step step mathematical derivation kalman filter using two different approach first consider orthogonal projection method mean vectorspace optimization second derive kalman filter using bayesian optimal filtering provide detailed proof method equation expanded detail
Statistics,bayesian statistic course undergraduate bayesian thinking computing research,propose semesterlong bayesian statistic course undergraduate student calculus probability background cultivate student bayesian thinking bayesian method applied real data problem leverage modern bayesian computing technique implementing bayesian method also deepen student understanding method collaborative case study enrich student learning provide experience solve openended applied problem course emphasis undergraduate research accessible academic journal article read discussed critiqued class increased confidence familiarity student take challenge reading implementing sometimes extending method journal article course project
Statistics,baymedr r package calculation bayes factor equivalence noninferiority superiority design,clinical trial often seek determine equivalence noninferiority superiority experimental condition eg new drug compared control condition eg placebo already existing drug use frequentist statistical method analyze data type design ubiquitous importantly however frequentist inference several limitation bayesian inference remedy shortcoming allows intuitive interpretation article outline frequentist conceptualization equivalence noninferiority superiority design discus disadvantage subsequently explain bayes factor used compare relative plausibility competing hypothesis present baymedr r package provides userfriendly tool computation bayes factor equivalence noninferiority superiority design detailed instruction use baymedr provided example illustrates already existing result reanalyzed baymedr
Statistics,thinkaloud interview tool exploring student statistical reasoning,research shown introductory statistic student hold many misconception many also present among practicing scientist statistic becoming important many new field understanding student learn statistic important ever effectively taught describe education research method intended reveal student think provide practical way measuring understanding combination thinkaloud interview concept inventory thinkaloud interview give unprecedented insight student thinking concept inventory administered entire class measure learning demonstrate method insight gained thinkaloud interview introductory student plus largescale concept inventory data question given pre posttests hundred introductory statistic student two institution thinkaloud interview revealed previously underreported misconception sampling distribution causation helping u refine conceptual question measure prevalence large scale insight final question may help educator develop improved lesson suggesting direction future statistic education research providing practical tool researcher improve understanding student learning
Statistics,scenario culture,scenario analysis risk assessment tool aim evaluate impact small number distinct plausible future scenario paper provide overview important aspect scenario analysis including appropriate design scenario uncertainty encouraging creativity issue discussed context climate energy legal scenario
Statistics,visualisation brain statistic rpackages ggseg,increased emphasis visualizing neuroimaging result intuitive way common statistical tool dissemination bar chart lack spatial dimension inherent neuroimaging data present two package statistical software r ggseg integrate spatial component ggseg package visualize predefined brain segmentation polygon mesh respectively package integrated wellestablished rpackages allowing great flexibility tutorial present main data function ggseg package brain atlas visualization main highlighted function able display brain segmentation plot r accompanying ggsegextrapackage includes wider collection atlas intended communitybased effort develop compatible atlas ggseg overall ggsegpackages facilitate parcellationbased visualization r improve ease dissemination result increase efficiency workflow
Statistics,resolving induction problem state complete confidence via induction sun rise forever,induction form reasoning particular example general rule however establishing truth general proposition problematic always possible conflicting observation occur problem known induction problem sunrise problem quintessential example induction problem first introduced laplace however laplace solution zero probability assigned proposition sun rise forever regardless number observation made therefore often stated complete confidence regarding general proposition never attained via induction study attempted overcome skepticism using recently developed theoretically consistent procedure finding demonstrate induction one rationally gain complete confidence proposition based scientific theory
Statistics,error control numerical posterior distribution bayesian uq analysis semilinear evolution pde,elaborate result obtained cite controlling numerical posterior error bayesian uq problem considering forward map arising solution semilinear evolution partial differential equation result cite demand error estimate numerical solution fm contribution numerical method computing afterthefact ie posteriori error estimate semilinear evolution pdes show potential applicability cite important wide range family pdes numerical example given illustrate efficiency proposed method obtaining numerical posterior distribution unknown parameter nearly identical corresponding theoretical posterior keeping bayes factor close
Statistics,black box algorithm appropriate principled predictionproblem ontology,new extraordinarily productive way reasoning algorithm emerged though type reasoning come dominate area data science underdiscussed impact underappreciated example primary way reason black box algorithm paper analyze current use ie common task framework limitation find large class predictionproblems inappropriate type reasoning find common task framework provide foundation deployment algorithm real world situation building core feature identify class problem new form reasoning used deployment purposefully develop novel framework technical nontechnical people discus identify key feature prediction problem whether suitable new kind reasoning
Statistics,case study promoting informal inferential reasoning learning sampling distribution high school student,drawing inference data important skill student understand everyday life sampling distribution central topic statistical inference necessary learned student however little known teach topic high school student especially indonesian context therefore present study provides teaching experiment support student informal inferential reasoning understanding sampling distribution well student perception toward teaching experiment subject present study three one private school yogyakarta majoring mathematics natural science method data collection direct observation sampling distribution learning process interview documentation present study found informal inferential reasoning problembased learning using contextual problem real data could support student understand sampling distribution also gave positive response learning experience
Statistics,online statistic teaching learning,statistic course level teaching learning online pose challenge different aspect particular online challenge include effectively interactively conduct exploratory data analysis incorporate statistical programming include individual team project present mathematical derivation efficiently effectively article draw author experience seven different online statistic course address aforementioned challenge one course online exploratory data analysis course taught bowling green state university second course upper level bayesian statistic course taught vassar college shared among liberal art college hybrid model alo describes fivecourse mooc specialization coursera offered duke university
Statistics,mean really mean,arithmetic average collection observed value homogeneous collection quantity often taken representative observation several argument supporting choice moment inertia familiar mean note bring forth kolmogorovnagumo point view arithmetic average special case sequence function special kind quadratic geometric mean case median fails belong class function kolmogorovnagumo interpretation defensible definitive one arithmetic average essence boil fact average merely abstraction meaning within mathematical setup
Statistics,bernoulli trial skewed propensity certification validation,impetus writing paper well publicized medium report software failure cause two recent mishap boeing max aircraft problem considered though specific one sense endeavor address general matter condition item drug material specimen complex system certified use based large number bernoulli trial successful broadly paper attempt answer old honorable philosophical question namely empirical testing validate law nature message answer depends one start namely one prior distribution unknown prior distribution endow observed data paper expository begin historical overview end new idea proposal addressing question posed sequel also articulates popper notion propensity role providing proper framework bayesian inference bernoulli trial well need engage posterior distribution subjectively specified without recourse usual bayesian prior posterior iteration
Statistics,probability theory come,paper top historical perspective several phase development probability prehistoric origin modern day evolution one key methodology artificial intelligence data science machine learning written honor barry arnold birthday many contribution statistical theory methodology despite fact much barry work technical descriptive document mark achievement viewed line barry dissertation adviser stanford received phd statistic philosopher science dug deep foundation root probability breadth perspective barry inherent paper based lecture material compiled first author various published source long period time material give limited list reference cast character many contribution part historical heritage u interested probability statistic many topic spawned
Statistics,bayes theorem conditional independence,article provide substantial discussion statistical concept conditional independence routinely mentioned elementary statistic mathematical statistic textbook assumption conditional independence extended version bayes theorem proposed illustration hypothetical realworld example disease diagnosis
Statistics,r autograder prairielearn,describe use extend prarielearn framework taking advantage builtin support external autograders using custom docker container match course requirement perfectly moreover relying flexibility interface customize docker container specific extension unit testing described creates contextdependent difference student answer reference solution providing comprehensive response test time
Statistics,ramsey contribution probability legal theory,review cheryl misak frank ramsey sheer excess power oxford university press
Statistics,skater dilemma,athletic race cycling type speed skating race athlete complete relatively long distance high speed presence direct opponent win race athlete motivated hide behind others suppress energy consumption final moment race situation seems produce social dilemma player want hide behind others whereas group player attempt may lose player overtake support speed skater involved social dilemma analyzed video footage data mass start skating race find skater hid behind others avoid air resistance long time final lap tended win furthermore finish rank skater mass start race independent record skater timetrial race measured absence direct opponent result suggest strategically cope skater dilemma may key determinant winning longdistance highspeed race direct opponent
Statistics,linear regression special relativity,study investigated problem posed using ordinary least square ols estimate parameter simple linear regression specific context special relativity independent variable restricted open interval c c found ols estimate slope coefficient invariant lorentz velocity transformation accordingly alternative estimator parameter linear regression special relativity proposed estimator considered generalization ols estimator special relativity c approach infinity proposed estimator variance converges ols estimator variance respectively variance proposed estimator larger ols estimator implies hypothesis testing using ols estimator variance may result liberal test special relativity
Statistics,fourier analysis benford random variable,paper several major purpose central purpose describe benford analysis positive random variable summarize result investigation base dependence benford random variable principal tool used derive result fourier series fourier transforms second major purpose paper present introductory exposition tool motivation writing paper twofold first think theory benford random variable benford analysis positive random variable interesting deserve better known second think benford analysis provides really excellent illustration utility fourier series transforms reveals certain interconnection series transforms obvious usual way subject introduced
Statistics,bringing visual inference classroom,classroom traditionally visualize inferential concept using static graphic interactive apps example long history using apps visualize sampling distribution recent development statistical graphic created opportunity bring additional visualization classroom hone student understanding specifically lineup protocol visual inference provides framework student see difference signal noise embedding plot observed data field null noise plot lineup proven valuable visualizing randomizationpermutation test diagnosing model even conducting valid inference distributional assumption break paper provides overview lineup protocol visual inference used hone understanding key statistical topic throughout statistic curriculum
Statistics,popper falsification corroboration statistical perspective,role probability appears unchallenged key measure uncertainty used among thing practical induction empirical science yet popper emphatic rejection inductive probability logical probability hypothesis furthermore degree corroboration probability instead proposed deductive method testing many way dialectic tension many parallel statistic bayesians logicoinductive side v nonbayesians frequentists side simplistically popper seems frequentist side recent synthesis nonbayesian side might direct popperian view nuanced destination logical probability seems perfectly suited measure partial evidence support use reject past year statistician also developed related concept called likelihood played central role statistical modelling inference remarkably fisherian concept uncertainty largely unknown least severely underappreciated nonstatistical literature measure corroboration likelihood satisfies popperian requirement probability aim introduce likelihood recent extension via discussion two wellknown logical fallacy order highlight lack recognition may led unnecessary confusion discourse falsification corroboration hypothesis highlight year development likelihood concept year mark anniversary likelihood paper wish long life increased appreciation nonstatistical literature
Statistics,bayesian redesign first probabilitystatistics course,traditional calculusbased introduction statistical inference consists semester probability followed semester frequentist inference cobb challenge statistical education community rethink undergraduate statistic curriculum particular suggests focus two goal making fundamental concept accessible minimizing prerequisite research using five underlying principle cobb describe new calculusbased introduction statistic based simulationbased bayesian computation
Statistics,effect vacant lot greening impact land use business vibrancy,examine ongoing philadelphia landcare plc vacant lot greening initiative evaluate association built environment intervention change crime incidence develop propensity score matching analysis estimate effect vacant lot greening different type crime accounting substantial difference greened ungreened lot term surrounding demographic economic land use business vibrancy characteristic within matched pair greened v ungreened vacant lot estimate larger significant beneficial effect greening reducing violent nonviolent total crime compared comparison greened v ungreened lot without matching also investigate impact land use zoning business vibrancy find effect vacant lot greening total crime substantially affected particular type surrounding land use zoning presence certain business type
Statistics,dynamic ternary statistical experiment equilibrium state,study scenario dynamic ternary statistical experiment modeled employing difference equation important feature balance condition existence steadystate equilibrium give classification scenario model evolution significantly different depending domain value model basic parameter
Statistics,incertitude et mesures,educational guide focused statistical treatment measurement uncertainty condition application current practice detailed precised mean value central limit theorem linear regression last two chapter devoted introduction bayesian inference series application case machine failure date elimination background noise linear adjustment elimination outlier
Statistics,reproducible research retrospective,rapid advance computing technology past decade spurred two extraordinary phenomenon science largescale highthroughput data collection coupled creation implementation complex statistical algorithm data analysis together two phenomenon brought tremendous advance scientific discovery also raised two serious concern one relatively new one quite familiar complexity modern data analysis raise question reproducibility analysis meaning ability independent analyst recreate result claimed original author using original data analysis technique seemingly straightforward concept reproducibility analysis typically thwarted lack availability data computer code used analysis much general concern replicability scientific finding concern frequency scientific claim confirmed completely independent investigation concept reproduciblity replicability related worth noting focused quite different goal address different aspect scientific progress review discus origin reproducible research characterize current status reproduciblity public health research connect reproduciblity current concern replicability scientific finding finally describe path forward improving reproducibility replicability public health research future
Statistics,asymmetry approach study chemotherapy treatment device failure time data using modified power function distribution modified estimator,order improve already existing model used extensively bio science applied science research new class weighted power function distribution wpfd proposed various property different modification applicable real life provided mathematical derivation new distribution including moment incomplete moment conditional moment inverse moment mean residual function vitality function order statistic mill ratio information function shannon entropy bonferroni lorenz curve quantile function also characterized wpfd based doubly truncated mean aim study increase application power function distribution main feature proposed distribution induction parameter compare generalization distribution complexed many parameter used r programming estimate parameter new class wpfd using maximum likelihood method mlm percentile estimator pe modified estimator analyzing data conclude proposed model wpfd performs better data set compared different competitor model
Statistics,longitudinal data analysis using matrix completion,clinical practice biomedical research measurement often collected sparsely irregularly time data acquisition expensive inconvenient example include measurement spine bone mineral density cancer growth mammography biopsy progression defect vision assessment gait patient neurological disorder since data collection often costly inconvenient estimation progression sparse observation great interest practitioner statistical standpoint data often analyzed context mixedeffect model time treated random fixed effect alternatively researcher analyze gaussian process functional data observation assumed drawn certain distribution process model flexible rely probabilistic assumption require careful implementation study propose alternative elementary framework analyzing longitudinal data relying matrix completion method yield point estimate progression curve iterative application svd framework cover multivariate longitudinal data regression easily extended setting apply method understand trend progression motor impairment child cerebral palsy model approximates individual progression curve explains variability lowrank representation progression trend enables discovering subtypes cerebral palsy exhibit different progression trend
Statistics,extremal process elliptical domain attraction spectral representation,extremal process proposed literature modeling spatial extreme within copula framework based extreme value limit elliptical distribution davison padoan ribatet major drawback maxstable model lack spectral representation instance direct simulation infeasible main contribution note propose spectral construction extremal process interestingly extremal gaussian process introduced schlather appears special case highlight role extremal process maximum attractor process finitedimensional elliptical distribution result naturally also hold within multivariate domain
Statistics,modeling population structure hierarchical dirichlet process,propose bayesian nonparametric model infer population admixture extending hierarchical dirichlet process allow correlation locus due linkage disequilibrium given multilocus genotype data sample individual model allows inferring classifying individual unadmixed admixed inferring number subpopulation ancestral admixed population population origin chromosomal region model assume specific mutation process applied commonly used genetic marker present mcmc algorithm perform posterior inference model discus method summarise mcmc output analysis population admixture demonstrate performance proposed model simulation real application using genetic data edar gene considered ancestryinformative due wellknown variation allele frequency well phenotypic effect across ancestry structure analysis dataset lead identification rare haplotype european
Statistics,using complex survey estimate median functional variable application electricity load curve,mean profile widely used indicator electricity consumption habit customer currently electricit e de france edf class load profile estimated using pointwise mean function unfortunately well known mean highly sensitive presence outlier one consumer unusually highlevels consumption paper propose alternative mean profile median profile robust dealing large datasets functional data load curve example survey sampling approach useful estimating median profile avoiding storing whole data propose estimator median trajectory using several sampling strategy estimator comparison illustrated mean test population develop stratification based linearized variable substantially improves accuracy estimator compared simple random sampling without replacement suggest also improved estimator take account auxiliary information potential area future research also highlighted
Statistics,designed sampling large database controlled trial,increasing prevalence rich source data availability electronic medical record database electronic registry open tremendous opportunity enhancing medical research example controlled trial ubiquitously used investigate effect medical treatment perhaps dependent set patient covariates traditional approach relied primarily randomized patient sampling allocation treatment control group however covariate data large cohort group patient already collected available database one potentially design treatmentcontrol sample allocation provides far better estimate covariatedependent effect treatment paper develop new approach us optimal design experiment doe concept accomplish objective approach selects patient treatment control sample upfront based covariate value manner optimizes information content data optimal sample selection develop simple guideline optimization algorithm provides solution substantially better random sampling moreover approach cause sampling bias estimated effect reason doe principle bias estimated effect test method simulation study based testbed data set containing information effect statin lowdensity lipoprotein ldl cholesterol
Statistics,probabilistic population projection country generalized hivaids epidemic,united nation un issued official probabilistic population projection country july done simulating future level total fertility life expectancy bayesian hierarchical model combining result using standard cohortcomponent projection method country generalized hivaids epidemic treated differently others projection used highly multistate spectrumepp model complex model designed shortterm projection quantity relevant policy epidemic propose simpler approach compatible existing un probabilistic projection methodology country change life expectancy projected probabilistically using simple time series regression model current life expectancy hiv prevalence art coverage converted age sexspecific mortality rate using new family model life table designed country hivaids epidemic reproduces characteristic hump middle adult mortality input standard cohortcomponent method country method performed well outofsample crossvalidation experiment give similar population projection spectrumepp short run simpler avoiding multistate modeling
Statistics,modeling replicating statistical topology evidence cmb nonhomogeneity,banner big data detection classification structure extremely large high dimensional data set one central statistical challenge time among intriguing approach challenge tda topological data analysis one primary aim providing nonmetric topologically informative preanalyses data set make later quantitative analysis feasible tda rest strong mathematical foundation topology application faced challenge due inability handle issue statistical reliability robustness importantly inability make scientific claim verifiable level statistical confidence propose methodology parametric representation estimation replication persistence diagram main diagnostic tool tda power methodology lie fact even one persistence diagram available analysis typical case big data application replication generated allow conventional statistical hypothesis testing methodology conceptually simple computationally practical provides broadly effective statistical procedure persistence diagram tda analysis demonstrate basic idea toy example power approach novel revealing analysis cmb nonhomogeneity
Statistics,class objective prior scoring rule,objective prior distribution represent important tool allows one advantage using bayesian framework even information parameter model available usual objective approach work chosen statistical model majority case resulting prior improper pose limitation practical implementation even complexity model moderate paper propose take novel look construction objective prior distribution connection chosen sampling distribution model removed explore notion defining objective prior distribution allow one degree flexibility particular exhibiting desirable feature proper centered specific value would interest nested model comparison basic tool use proper scoring rule main result class objective prior distribution employed scenario usual model based prior fail mixture model model selection via bayes factor addition show proposed class prior result minimising information contains providing solid interpretation method
Statistics,redefining statistical significance improve reproducibility could make replication crisis worse,recent proposal redefine statistical significance benjamin et al nature human behaviour claim false positive rate would immediately improve factor greater two replication rate would double simply changing conventional cutoff statistical significance p p analyze veracity claim focusing especially benjamin et al neglect effect phacking assessing impact proposal analysis show phacking accounted perceived benefit lower threshold disappear prompting two main conclusion claimed improvement false positive rate replication rate benjamin et al exaggerated misleading ii plausible scenario lower cutoff make replication crisis worse
Statistics,novel calibration framework survival analysis binary covariate measured sparse time point,goal clinical cohort study often include evaluation association timedependent binary treatment exposure survival outcome recently several impactful study targeted association aspirintaking survival following colorectal cancer diagnosis due surgery aspirintaking value zero baseline may change value one time point estimating association complicated intermittent measurement aspirintaking naive commonlyused method lead substantial bias present class calibration model distribution time status change binary covariate estimate obtained model incorporated proportional hazard partial likelihood natural way develop nonparametric semiparametric parametric calibration model derive asymptotic theory method implement aspirin colorectal cancer study methodology allows include additional baseline variable calibration model status change time binary covariate develop riskset calibration approach useful setting association binary covariate survival strong
Statistics,statistical nonsignificance,significance test probably extended form inference empirical research significance often interpreted providing greater informational content nonsignificance article show however rejection point null often carry little information failure reject may highly informative particularly true empirical context data set large rarely reason put substantial prior probability point null result challenge usual practice conferring point null rejection higher level scientific significance nonrejections consequence advocate visible reporting discussion nonsignificant result empirical practice
Statistics,beyond homophily incorporating actor variable actororiented network model,consider specification effect numerical actor attribute statistical model directed social network fundamental mechanism homophily assortativity actor higher likelihood tied others similar value variable study mechanism may also play role attribute value two actor influence likelihood tie discus three additional mechanism aspiration send tie others high value conformity sense sending tie others whose value close may considered social norm sociability higher value tend send tie generally mechanism may operate jointly effect confounded present specification representing effect simultaneously fourparameter quadratic function value sender receiver greater flexibility obtained fiveparameter extension argue empirical researcher often overlook possibility homophily may confounded mechanism actor attribute important effect directed network specification may provide improvement illustration given dependence advice tie academic grade network mba student analyzed stochastic actororiented model
Statistics,nonparametric estimation spot covariance matrix highfrequency data,estimating spot covariance important issue study especially increasing availability highfrequency financial data study estimation spot covariance using kernel method highfrequency data particular consider first kernel weighted version realized covariance estimator price process governed continuous multivariate semimartingale next extend threshold kernel estimator spot covariance underlying price process discontinuous multivariate semimartingale finite activity jump derive asymptotic distribution estimator fixed shrinking bandwidth estimator setting jump rate convergence estimator diffusion process without jump simulation study examines finite sample property estimator addition study application estimator context covariance forecasting discover forecasting model estimator outperforms benchmark model literature
Statistics,probability causal inference robust internal validity,internal validity observational study often subject debate study define counterfactuals unobserved sample intend quantify relationship null hypothesis statistical testing nhst propose probability causal inference robust internal validity ie piv robustness index causal inference formally piv probability rejecting null hypothesis based observed sample counterfactuals provided null hypothesis already rejected based observed sample either frequentist bayesian framework one bound piv inference based bounded belief counterfactuals often needed unconfoundedness assumption dubious piv equivalent statistical power nhst thought based observed sample counterfactuals summarize process evaluating internal validity piv eightstep procedure illustrate empirical example ie hong raudenbush
Statistics,leveraging auxiliary information marginal distribution nonignorable model item unit nonresponse,often government agency survey organization know population count percentage variable survey may available auxiliary source example administrative database high quality survey present illustrate modelbased framework leveraging auxiliary marginal information handling unit item nonresponse show one use margin specify different missingness mechanism type nonresponse use framework impute missing value voter turnout subset data u current population survey cps examine sensitivity result different assumption unit item nonresponse
Statistics,statistical framework measuring temporal stability human mobility pattern,despite growing popularity human mobility study collect gps location data problem determining minimum required length gps monitoring addressed current statistical literature paper tackle problem laying theoretical framework assessing temporal stability human mobility based gps location data define several measure temporal dynamic human spatiotemporal trajectory based average velocity process activity distribution spatial observation window demonstrate use method data comprise gps location individual course month empirical result suggest gps monitoring performed period time significantly longer previously suggested furthermore argue gps study design take account demographic group keywords density estimation global positioning system gps human mobility spatiotemporal trajectory temporal dynamic
Statistics,spatial method application environmental climate data,environmental climate process often distributed large spacetime domain complexity amount available data make modelling analysis challenging task statistical modelling environment climate data several different motivation including interpretation characterisation data result statistical analysis often used integral part larger environmental study spatial statistic active modern statistical field concerned quantitative analysis spatial data dependency uncertainty spatiotemporal statistic extends spatial statistic addition time two three spatial dimension focus introductory paper provide overview spatial method application environmental climate data paper also give overview several important topic including large data set nonstationary covariance structure discussed bayesian hierarchical model provide flexible way constructing model hierarchical model may seem good solution challenge parameter estimation finally application spatiotemporal model landclim data land cover climate interaction nw europe holocene discussed
Statistics,parametric mode regression bounded response,propose new parametric framework regression analysis conditional mode bounded response focal point interest covariate effect estimation prediction based maximum likelihood method two new class regression model demonstrated also develop graphical numerical diagnostic tool detect various source model misspecification prediction based different central tendency measure inferred using various regression model compared using synthetic data simulation finally conduct regression analysis data alzheimer disease neuroimaging initiative demonstrate practical implementation proposed method supplementary material contain technical detail additional simulation data analysis result available online
Statistics,statistical consequence fat tail real world preasymptotics epistemology application,book investigates misapplication conventional statistical technique fat tailed distribution look remedy possible switching thin tailed fat tailed distribution requires changing color dress traditional asymptotics deal mainly either ninfty real world law medium number vary widely across specific distribution law large number generalized central limit mechanism operate highly idiosyncratic way outside standard gaussian levystable basin convergence example sample mean rarely line population mean effect naive empiricism sometimes estimated via parametric method empirical distribution rarely empirical parameter uncertainty compounding effect statistical metric dimension reduction principal component fails inequality estimator gini quantile contribution additive produce wrong result many bias found psychology become entirely rational sophisticated probability distribution failure financial economics econometrics behavioral economics attributed using wrong distribution book first volume technical incerto weave narrative around published journal article
Statistics,decomposition total effect notion natural counterfactual interaction effect,mediation analysis serf crucial tool obtain causal inference based directed acyclic graph widely employed area biomedical science social science epidemiology psychology decomposition total effect provides deep insight fully understand casual contribution path interaction term since fourway decomposition method proposed identify mediated interaction effect counterfactual framework idea extended sophisticated scenario nonsequential multiple mediator however method exhibit limitation causal structure contains direct causal edge mediator inappropriate modeling dependence nonidentifiability develop notion natural counterfactual interaction effect find decomposition total effect consistently realized proposed notion furthermore natural counterfactual interaction effect overcomes drawback posse clear significant interpretation may largely improve capacity researcher analyze highly complex causal structure
Statistics,bayesian approach spherical factor analysis binary data,factor model widely used across diverse area application purpose include dimensionality reduction covariance estimation feature engineering traditional factor model seen instance linear embedding method project multivariate observation onto lower dimensional euclidean latent space paper discus new class geometric embedding model multivariate binary data embedding space correspond spherical manifold potentially unknown dimension resulting model include traditional factor model special case provide additional flexibility furthermore unlike technique geometric embedding model easy interpret uncertainty associated latent feature properly quantified advantage illustrated using simulation study real data voting record u senate
Statistics,uniform bias study bahadur representation local polynomial estimator conditional quantile function,paper investigates bias weak bahadur representation local polynomial estimator conditional quantile function derivative bias bahadur remainder term studied uniformly respect quantile level covariates smoothing parameter order local polynomial estimator higher differentiability order conditional quantile function application result deal global optimal consistency rate local polynomial quantile estimator performance random bandwidth estimation conditional quantile density function latter allows obtain simple estimator conditional quantile function private value first price sealed bid auction independent private value paradigm risk neutrality
Statistics,parametric order constraint multinomial processing tree model extension knapp batchelder,multinomial processing tree mpt model tool disentangling contribution latent cognitive process given experimental paradigm present note analyzes mpt model subject order constraint subset parameter constraint consider frequently arise case response category ordered sense confidencerating data likert scale data graded guessing tendency response bias created via baserate payoff manipulation analysis contingency table order constraint many case show construct mpt model without order constraint statistically equivalent mpt model order constraint new closure result extends mathematical analysis mpt class offer approach orderrestricted inference extends approach discussed knapp batchelder usefulness method illustrated mean analysis orderconstrained version twohighthreshold model confidence rating
Statistics,classical bayesian componentwise predictor noncompact correlated arh process,special class standard gaussian autoregressive hilbertian process order one gaussian arh process bounded linear autocorrelation operator satisfy usual hilbertschmidt assumption considered compensate slow decay diagonal coefficient autocorrelation operator faster decay velocity eigenvalue trace autocovariance operator innovation process assumed usual eigenvectors autocovariance operator arh process considered projection since assumed known diagonal componentwise classical bayesian estimation autocorrelation operator studied prediction asymptotic efficiency equivalence estimator proved well associated componentwise arh plugin predictor simulation study undertaken illustrate theoretical result derived
Statistics,markov property graphical model cycle latent variable,investigate probabilistic graphical model allow cycle latent variable introduce directed graph hyperedges hedge generalizing combining marginalized directed acyclic graph mdags model latent dependent variable directed mixed graph dmgs model cycle define analyse several different markov property relate graphical structure hedg probability distribution corresponding product space set node example factorization property structural equation property orderedlocalglobal markov property marginal version various markov property hedge general equivalent cycle hyperedges present contrast simpler case directed acyclic graphical dag model also known bayesian network show markov property hedge thus corresponding graphical markov model logically related
Statistics,joint limiting law highdimensional independence test,testing independence significant interest many important area largescale inference using extremevalue form statistic test sparse alternative using quadratic form statistic test dense alternative two important testing procedure highdimensional independence however quadratic form statistic suffer low power sparse alternative extremevalue form statistic suffer low power dense alternative small disturbance may size distortion due slow convergence realworld application important derive powerful testing procedure general alternative based intermediate limiting distribution derive modelfree joint limiting law extremevalue form quadratic form statistic surprisingly prove asymptotically independent given asymptotic independency propose modelfree testing procedure boost power general alternative also retain correct asymptotic size highdimensional setting derive closedform limiting null distribution obtain explicit rate uniform convergence prove consistent statistical power general alternative demonstrate performance proposed test statistic simulation study work provides helpful insight highdimensional independence test fill important gap
Statistics,intrinsic posterior regret gammaminimax estimation exponential family distribution,practice desired estimate invariant reparameterization invariance property estimator help formulate unified solution underlying estimation problem robust bayesian analysis frequent criticism optimal estimator invariant smooth reparameterizations paper considers problem posterior regret gammaminimax prgm estimation natural parameter exponential family distribution intrinsic loss function show class jeffrey conjugate prior jcp distribution prgm estimator invariant smooth onetoone reparameterizations apply result several distribution different class jcp well usual conjugate prior distribution observe many case invariant prgm estimator class jcp distribution obtained modification prgm estimator usual class conjugate prior moreover class prior convex dependant hyperparameter belonging connected set show prgm estimator intrinsic loss function could bayes respect prior distribution original prior class theoretical result supplemented several example illustration
Statistics,information content partially rankordered set sample,partially rankordered set pro sampling generalization ranked set sampling ranker required fully rank sampling unit set hence flexibility perform necessary judgemental ranking process pro sampling wide range application different field ranging environmental ecological study medical research shown superior ranked set sampling simple random sampling estimating population mean paper study fisher information content uncertainty structure pro sample compare simple random sample sr ranked set sample r counterpart size underlying population study uncertainty structure term shannon entropy renyi entropy kullbackleibler kl discrimination measure several example including fi pro sample locationscale family distribution well regression model discussed
Statistics,quantifying information transfer mediation along causal pathway complex system,measure information transfer become popular approach analyze interaction complex system earth human brain measured time series recent work focused causal definition information transfer excluding effect common driver indirect influence former clearly constitutes spurious causality aim present article develop measure quantifying different notion strength information transfer along indirect causal path based first reconstructing multivariate causal network emph tigramite approach another class novel measure quantifies extent different intermediate process causal path contribute interaction mechanism determine pathway causal information transfer rigorous mathematical framework allows clear informationtheoretic interpretation also related underlying dynamic proven certain class process generally however estimate information transfer remain hard interpret nonlinearly intertwined complex system experiment mathematical model available measuring pathway information transfer within causal dependency structure allows least abstraction dynamic measure illustrated climatological example disentangle pathway atmospheric flow europe
Statistics,recent progress logconcave density estimation,recent year logconcave density estimation via maximum likelihood estimation emerged fascinating alternative traditional nonparametric smoothing technique kernel density estimation require choice one bandwidth purpose article describe property class logconcave density mathbb r make attractive statistical perspective outline latest methodological theoretical computational advance area
Statistics,tutorial deriving efficient influence curve large model,paper aim provide tutorial upper level undergraduate graduate student statistic biostatistics epidemiology deriving influence function nonparametric semiparametric model author build previously known efficiency theory provide useful identity formulaic technique relying basic integration selfcontained tutorial used setting one might encounter practice paper provides many example derivation wellknown influence function well new parameter interest influence function remains central object constructing efficient estimator large model onestep estimator targeted maximum likelihood estimator touch upon estimator reader familiar estimator might find tutorial particular use
Statistics,local partial autocorrelation function application,classical regular partial autocorrelation function powerful tool stationary time series modelling analysis however increasingly recognized many time series stationary use classical global autocorrelations give misleading answer article introduces two estimator local partial autocorrelation function establishes asymptotic property article illustrates use new estimator simulated real time series example clearly demonstrate strong practical benefit local estimator time series exhibit nonstationarities
Statistics,note closedform mles kcomponent loadsharing system,recently kim kvam singh sharma kumar proposed different loadsharing model developed parametric inference model however parametric estimate calculated using iterative numerical method note provide general closedform mles two loadsharing model provided
Statistics,farima model brittle,farima model longrangedependence lrd widely used many area deriving precise characterisation spectrum autocovariance function variance time function show family atypical among lrd process extremely close fractional gaussian noise precise sense furthermore show closeness property robust additive noise argue use farima generally fractionally differenced time series reassessed context particular convergence rate rescaling important noise expected
Statistics,specification uncertainty hurt progress scientometrics,caveat using statistical significance test research assessment journal informetrics available schneider focus opthof leydesdorff example misuse statistic social science however conclusion theoretical since dependent use one statistic another agree schneider insofar proposes develop statistical instrument effect size schneider however argues metatheoretical ground specification uncertainty opinion presence statistic would legitimate decisionmaking disagree uncertainty also used opening debate scientometric result error bar suppressed metatheoretical reason trusted
Statistics,foundation descriptive inferential statistic,lecture note written aim provide accessible though technically solid introduction logic systematical analysis statistical data undergraduate postgraduate student particular social science economics financial service may also serve general reference application quantitative empirical research method attempt encourage adoption interdisciplinary perspective quantitative problem arising practice note cover four broad topic descriptive statistical processing raw data ii elementary probability theory iii operationalisation onedimensional latent statistical variable according likert widely used scaling approach iv null hypothesis significance testing within frequentist approach probability theory concerning distributional difference variable subgroup target population b statistical association two variable relevance effect size making inference emphasised lecture note fully hyperlinked thus providing direct route original scientific paper well interesting biographical information also list many command running statistical function data analysis routine software package r spss excel openoffice immediate involvement actual data analysis practice strongly recommended
Statistics,estimation spatial maxstable model using threshold exceedance,parametric inference spatial maxstable process difficult since related likelihood unavailable composite likelihood approach based bivariate distribution block maximum recently proposed literature however modeling block maximum wasteful approach provided information available moreover approach based block typically annual maximum unable take account fact maximum occur simultaneously time series say daily data available estimation procedure based exceedance high threshold could mitigate problem paper focus two approach composing likelihood based pair exceedance first one come tail approximation bivariate distribution proposed ledford tawn pair observation exceed fixed threshold second one us bivariate extension rootzen tajvidi generalized pareto distribution allows model exceedance least one component threshold two approach compared simulation study according different degree spatial dependency result show strength spatial dependency threshold choice play fundamental role determining best estimating procedure
Statistics,factor paradox common factor correlated variance accounted common factor,case factor model account covariance observed variable considered quite realistic condition model error well sampling error usually occur empirical data shown principal component representing covariance accounted factor model nonzero correlation common factor factor model nonzero correlation component representing variance accounted factor model common factor also found simulation study based result concluded common factor correlated variance component representing model error well sampling error consequence even researcher decide represent small trivial variance mean common factor excluded variance still part model
Statistics,agentbased epidemiological model incarceration,build agentbased model incarceration based si model infectious disease propagation central hypothesis observed racial disparity incarceration rate black white american explained result differential sentencing two demographic group demonstrate incarceration spread social influence network even relatively small difference sentencing result large disparity incarceration rate controlling effect transmissibility susceptibility influence network structure model reproduces observed large disparity incarceration rate given difference sentence length white black drug offender united state without extensive parameter tuning establish suitability si model applied incarceration observed structural pattern recidivism emergent property model fact model show remarkably close correspondence california incarceration data without requiring parameter tuning work advance effort combine theory method epidemiology criminology
Statistics,inhomogeneous kfunction germgrain model,paper propose generalization germgrain model inhomogeneous kfunction point process apply sample image peripheral blood smear obtained patient sickle cell disease order decide whether sample belongs thin thick morphological region
Statistics,scalable framework nba player team comparison using player tracking data,release nba player tracking data greatly enhances granularity dimensionality basketball statistic used evaluate compare player performance however high dimensionality new data source troublesome demand computational resource reduces ability easily interpret finding therefore must find way reduce dimensionality data retaining ability differentiate compare player performance paper principal component analysis pca used identify four principal component account variation player tracking data regular season intuitive interpretation new dimension developed examining statistic influence new high variance low dimensional space easily compare statistical profile across principal component dimension evaluate characteristic make certain player team similar unique simple measure similarity two player team statistical profile based four principal component score also constructed statistical diversity index sdi allows quick intuitive comparison using entirety player tracking data new statistic emerge framework scalable incorporate existing new data source reconstructing principal component dimension sdi improved comparison using principal component score sdi several use case presented improved personnel management
Statistics,statistical engineering idea whose time come,several author including american statistician asa noted challenge facing statistician attacking large complex unstructured problem opposed welldefined textbook problem clearly standard paradigm selecting one correct statistical method problem sufficient new paradigm needed statistical engineering proposed discipline provide viable paradigm attack problem used conjunction sound statistical science course order develop true discipline statistical engineering need welldeveloped theory formal definition successful case study article document disseminates current state underlying theory statistical engineering purpose provide vehicle applied statistician enhance practice statistic academic interested continue development underlying theory statistical engineering
Statistics,sensory evaluation commercial coffee brand colombia,colombian coffee farmer traditionally focused effort activity including seeding planting drying strategic issue successfully compete industry branding marketing consumer research neglected research apply type sensory analysis based several statistical technique used investigate key feature ten different brand colombian coffee panel composed judge investigated nine different attribute related flavour fragrance sweetness acidity among others last section present conclusion reached regarding customer preference brand profile
Statistics,agentbased simulation learning dissemination projectbased learning context considering human aspect,work present agentbased simulation ab active learning process electrical engineering course order generate input data simulation active learning methodology developed especially parttime degree course called projectbased learning agile pbla proposed implemented regional university blumenau furb brazil analysis survey response obtained five consecutive semester using partial least square path modeling plspm possible generate data parameter use input hybrid kind agentbased simulation known pls agent simulation scenario suggests learning occur faster student higher level humanist aspect selfesteem selfrealization cooperation
Statistics,investigation different level poverty corresponding variance student academic prosperity,underprivileged student especially primary school shown le access educational material often resulting general dissatisfaction school system lower academic performance saatcioglu rury relationship family socioeconomic status student interest academic endeavor level classroom engagement participation extracurricular program analyzed socioeconomic status categorized poverty level poverty level percent poverty percent poverty higher united state census bureau student interest engagement persistence measured scalar quantity three variable never sometimes often participation student extracurricular activity also compared based category socioeconomic status running multivariate analysis variance found statistically significant variance student academic prosperity poverty level
Statistics,markov chain approach determine optimal performance period bad definition credit scorecard,performance period determination bad definition credit scorecard mix fortune typical data modeler lack literature matter led proliferation approach technique solve problem however commonly accepted approach involves subjective interpretation performance period bad definition well chicken egg problem complication result poorly developed credit scorecard minimal benefit bank paper recommending simple effective approach resolve issue
Statistics,singular value decompositionbased factorization parsimonious component model demographic quantity correlated age predicting complete demographic age schedule parameter,background formal demography long history building simple model age schedule demographic quantity eg mortality fertility rate widely used demographic method manipulate whole age schedule using parameter objective singular value decomposition svd factorizes matrix three matrix useful property including ability reconstruct original matrix using many fewer simple matrix work demonstrates property exploited build parsimonious model whole age schedule demographic quantity parameterized term arbitrary covariates method svd presented explained detail attention developing intuitive understanding svd used construct general component model demographic age schedule model demonstrated agespecific mortality fertility rate finally model used predict agespecific mortality using hiv indicator summary measure agespecific mortality predict agespecific fertility using total fertility rate tfr result component model agespecific mortality fertility rate succeeds reproducing data two input acting two input various covariates able accurately predict full age schedule conclusion svd potentially useful way summarize smooth model agespecific demographic quantity component model general method relating covariates whole age schedule comment focus work svd component model application illustrative purpose
Statistics,hyak mortality monitoring system innovative sampling estimation method proof concept simulation,traditionally health statistic derived civil andor vital registration civil registration lowincome country varies partial coverage essentially nothing consequently state art public health information lowincome country effort combine triangulate data different source produce complete picture across time space data amalgamation data source amenable approach include sample survey sample registration system health demographic surveillance system administrative record census record health facility record others propose new statistical framework gathering health population data hyak leverage benefit sampling longitudinal prospective surveillance create cheap accurate sustainable monitoring platform hyak three fundamental component data amalgamation sampling surveillance component organizes two data collection system work together data hdss frequent intense linked prospective followup b data sample survey conducted large area surrounding health demographic surveillance system site using informed sampling capture many event possible cause death verbal autopsy characterize distribution death cause population level s measurement socioeconomic status order characterize poverty wealth conduct simulation study informed sampling component hyak based agincourt hdss site south africa compared traditional cluster sampling hyak informed sampling capture death combined estimation model includes spatial smoothing produce estimate mortality lower variance small bias
Statistics,insilicova method automate cause death assignment verbal autopsy,verbal autopsy va widely used provide causespecific mortality estimate developing world setting vital registration function well va assign cause death using information describing event leading death provided care giver typically physician read va interview assign cause using expert knowledge physician coding often slow individual physician bring bias coding process result noncomparable cause assignment problem significantly limit utility physiciancoded va solution use algorithmic approach formalizes causeassignment process ensures assigned cause comparable requires many fewer personhours cause assignment conducted quickly without disrupting normal work physician peter byass interva method widely used algorithmic approach va coding aligned standard va questionnaire statistical model underpinning interva improved uncertainty need quantified link populationlevel csmfs individuallevel cause assignment need statistically rigorous addressing theoretical concern provides opportunity create new software using modern language run multiple platform widely shared building overall framework pioneered interva work creates statistical model automated va cause assignment
Statistics,beyond subjective objective statistic,argue word objectivity subjectivity statistic discourse used mostly unhelpful way propose replace broader collection attribute objectivity replaced transparency consensus impartiality correspondence observable reality subjectivity replaced awareness multiple perspective context dependence advantage reformulations replacement term oppose instead debating whether given statistical method subjective objective normatively debating relative merit subjectivity objectivity statistical practice recognize desirable attribute transparency acknowledgment multiple perspective complementary goal demonstrate implication proposal recent applied example pharmacology election polling socioeconomic stratification
Statistics,modelling hospital length stay using convolutive mixture distribution,length hospital stay los important indicator hospital activity management health care skewness distribution los pose problem statistical modelling fails adequately follow usual traditional distribution lognormal distribution aim work model variable los using convolution two distribution technique well known signal processing community specificity model variable interest considered resulting sum two random variable different distribution one variable feature patientrelated factor term need recover admission condition model hospital management process discharging process two estimation procedure proposed one classical maximum likelihood relates expectation maximisation algorithm present result obtained applying model set real data group hospital victoria australia
Statistics,implicit regression detecting constant inverse relationship bivariate random error,wooten introduced nonresponse analysis founding theory implicit regression implicit regression treat variable implicitly codependent variable explicit function dependent independent variable standard regression motivation paper introduce method implicit regression determine constant nature variable interactive term address inverse relationship among measured variable random error present direction
Statistics,bfda matlab toolbox bayesian functional data analysis,provide matlab toolbox bfda implement bayesian hierarchical model smooth multiple functional data assumption underlying gaussian process distribution gaussian process prior mean function inversewishart process prior covariance function modelbased approach borrow strength functional data increase smoothing accuracy well estimate meancovariance function simultaneously option approximating bayesian inference process using cubic bspline basis function integrated bfda allows efficiently dealing highdimensional functional data example using bfda various scenario conducting followup functional regression provided advantage bfda include simultaneously smooth multiple functional data estimate meancovariance function nonparametric way flexibly deal sparse highdimensional functional data stationary nonstationary covariance function without requirement common observation grid provides accurately smoothed functional data followup analysis
Statistics,fairly random impact winning toss probability winning,competitive sport every little thing matter yet many sport leave large lever reach team hand fate cricket world second popular sport measure one lever toss subject much recent attention using large novel dataset cricket match estimate impact winning toss probability winning data suggest winning toss increase chance winning small sim significant margin advantage varies heftily systematically closely matched competing team playing condition tautologically winning toss condition toss grant greater advantage eg day night match larger impact probability winning
Statistics,statistical method topological data analysis complex highdimensional data,utilization statistical method application within new field study known topological data analysis tremendous potential broadening exploration understanding complex highdimensional data space paper provides introductory overview mathematical underpinnings topological data analysis workflow convert sample data topological summary statistic statistical method developed performing inference topological summary statistic intention nontechnical overview motivate statistician interested learning subject
Statistics,response nist expert urge caution use courtroom evidence presentation method,press release national institute standard technology nist could potentially impede progress toward improving analysis forensic evidence presentation forensic analysis result court united state around world nist expert urge caution use courtroom evidence presentation method released october picked physorg news service argues except exceptional case result forensic analysis reported likelihood ratio press release journal article nist researcher steven p lund harri iyer based identifies legitimate point concern make strawman argument reach unjustified conclusion throw baby bathwater
Statistics,quantifying contribution training data algorithm logic performance automated causeassignment algorithm verbal autopsy,verbal autopsy va consists survey relative close contact person recently died va survey commonly used infer likely cause death individual death happen outside hospital healthcare facility several statistical algorithmic method available assign cause death using va survey method require input information joint distribution symptom cause note examine generalizability symptomcause information comparing different automated coding method using various combination input evaluation data va algorithm performance affected specific sci logic given algorithm using variety performance metric existing va algorithm demonstrate general adequacy information joint distribution symptom cause affect performance least much algorithm logic
Statistics,bayesian modeldata synthesis application global glacioisostatic adjustment,introduce framework updating large scale geospatial process using modeldata synthesis method based bayesian hierarchical modelling two major challenge come updating largescale gaussian process modelling nonstationarity address first adopt spde approach us sparse gaussian markov random field gmrf approximation reduce computational cost implement bayesian inference using inla method nonstationary global process propose two general model accommodate commonlyseen geospatial problem finally show example updating estimate global glacial isostatic adjustment gia using gps measurement
Statistics,allocation cold standby series parallel system dependent component,context industrial engineering coldstandby redundancy allocation strategy usually adopted improve reliability coherent system paper investigates optimal allocation strategy cold standby series parallel system comprised dependent component leftright tail weakly stochastic arrangement increasing lifetime case heterogeneous independent matched cold standby proved better redundancy put node weaker better component series parallel system case homogeneous independent cold standby shown redundancy put standby weaker better component enhance reliability series parallel system result developed generalize extend corresponding one literature case series parallel system dependent component numerical example also presented provide guidance practical use theoretical finding
Statistics,monte carlo simulation robustness functional location estimator based several functional depth,functional data analysis growing field study recent decade one fundamental task functional data analysis estimating sample location notion called statistical depth extended multivariate data functional data provide centeroutward order observation within sample functional curve making use intuitive nature depth method depthbased trimmed mean curve lower depth value excluded used robust location estimator sample project first introduced several stateoftheart depth approach functional data depth half region depth functional majority depth band depth modified band depth functional spatial depth described robust location estimator based functional depth studied performance estimator based different functional depth approach via simulation test finally test result showed estimator based functional spatial depth modified band depth exhibited superior performance
Statistics,using github classroom teach statistic,git github common tool keeping track multiple version data analytic content allow one person simultaneously work project github classroom aim provide way student work submit assignment via git github giving teacher opportunity teach version control tool part course fall semester implemented github classroom two educational setting introductory computational statistic lab advanced computational statistic course found many educational benefit implementing github classroom easily providing coding feedback assignment making student confident ability collaborate use version control tool future data science work encourage ease transition using github classroom provide free publicly available resource student begin using gitgithub teacher use github classroom course
Statistics,practical consideration data collection management mobile health microrandomized trial,growing interest leveraging prevalence mobile technology improve health delivering momentary contextualized intervention individual smartphones justintime adaptive intervention jitai adjusts individual changing state andor context provide right treatment right time right place microrandomized trial mrts allow collection data aid construction optimized jitai sequentially randomizing participant different treatment option many decision point throughout study often data collected passively using mobile phone ass causal effect treatment nearterm outcome care must taken designing data collection system ensure appropriately high quality make several recommendation collecting managing data mrt provide advice selecting feature collect choosing agent implement randomization identifying source missing data overcoming novel challenge recommendation informed experience heartsteps mrt designed test effect intervention aimed increasing physical activity sedentary adult also provide checklist used designing data collection system scientist focus question interest le cleaning data
Statistics,learning spatiallycorrelated temporal dictionary calcium imaging,calcium imaging become fundamental neural imaging technique aiming recover individual activity hundred neuron cortical region current method mostly matrix factorization aimed detecting neuron fieldofview inferring corresponding timetraces paper reverse modeling instead aim minimize spatial inference focusing finding set temporal trace present data reframe problem dictionary learning setting dictionary contains timetraces sparse coefficient spatial map adapt dictionary learning calcium imaging introducing constraint norm correlation timetraces incorporating hierarchical spatial filtering model correlate timetrace usage fieldofview demonstrate synthetic real data solution advantage regarding initialization implicitly inferring number neuron simultaneously detecting different neuronal type
Statistics,amazon forest fire birth weight porto velho,birth weight data livebirths public hospital porto velho amazon used multiple statistical model ass effect forestfire smoke human reproductive outcome mean birth weight girl g boy g considered statistically different pvalue among model analyzed mean considered statistically different treated function month year pvalue girl boy r statistic indicate regression model considered able explain girl boy variation mean birth weight
Statistics,evaluating success data analysis,fundamental problem practice teaching data science evaluate quality given data analysis different evaluation science question underlying data analysis previously defined set principle describing data analysis used create data analysis characterize variation data analysis introduce metric quality evaluation call success data analysis different potential metric completeness validity honesty define successful data analysis matching principle analyst audience analysis developed paper propose statistical model general framework evaluating success data analysis argue framework used guide practicing data scientist student data science course build successful data analysis
Statistics,incorporating open data introductory course statistic,guideline assessment instruction statistic education gaise college report emphasized six recommendation teach introductory course statistic among use real data context purpose many educator created database consisting multiple data set use class sometimes making hundred data set available yet context purpose component data may remain elusive generic database made available describe use open data introductory course country city continue share data open data portal hence educator find regional data engages student effectively present excerpt case study show application statistical method data crime housing rainfall tourist travel others data wrangling discussion result recognized important case study component thus open data based case study attend gaise college report recommendation reproducible textsf r code made available case study example us open data advanced course statistic also described
Statistics,frequentist inference without repeated sampling,frequentist inference typically described term hypothetical repeated sampling advantage interpretation us single random sample contemporary example given indicate probability random phenomenon interpreted classical probability interpretation applied statistical inference using urn model classical limiting relative frequency interpretation used communicate statistical inference effectiveness discussed recent description pvalues confidence interval power viewed lens classical probability based single random sample population
Statistics,detecting classifying moment basketball match using sensor tracked data,data analytics sport crucial evaluate performance single player whole team literature proposes number tool offence defence scenario data coming tracking location player respect may used enrich amount useful information basketball however action interleaved inactive period paper describes methodological approach automatically identify active period game classify offensive defensive method based application threshold player kinematic parameter whose value undergo tuning strategy similar receiver operating characteristic curve using ground truth extracted video game
Statistics,reckless guide pvalues local evidence global error,chapter demystifies pvalues hypothesis test significance test introduces concept local evidence global error rate local evidence embodied textit data concern hypothesis interest textit experiment whereas global error rate property statistical analysis sampling procedure shown using simple example local evidence global error rate considered together making inference power analysis experimental design hypothesis testing explained along locally focussed expected pvalues issue relating multiple testing harking phacking explained shown many situation effect local evidence global error rate conflict conflict always overcome fresh dataset replication key experiment statistic complicated science singular right way either universally acceptable compromise may exist statistic offer wide array tool assisting scientific inference calibrating uncertainty statistical inference substitute scientific inference pvalues useful index evidence deserve place statistical toolbox basic pharmacologist
Statistics,computing expected value sample information efficiently expertise skill required four modelbased method,objective value information voi analysis help policymakers make informed decision whether conduct design future study historically computationally expensive method compute expected value sample information evsi restricted use voi simple decision model study design recently four evsi approximation method made analysis feasible accessible provide practical recommendation analyst computing evsi evaluating novel method method member collaborative network value information convoi compared input analyst expertise skill software required four recently developed approximation method information also collected strength limitation approximation method result four evsi method require decisionanalytic model probabilistic sensitivity analysis psa output one method also requires model rerun obtain new psa output evsi estimation compute evsi analyst must familiar least one following skill advanced regression modeling likelihood specification bayesian modeling method different strength limitation eg method handle evaluation study design outcome efficiently others quantify uncertainty evsi estimate method programmed statistical language r two method provide online application conclusion paper help inform choice four efficient evsi estimation method enabling analyst ass method strength limitation select appropriate evsi method given situation skill
Statistics,jump ball rating fall elite status sensitivity analysis three quarterback rating statistic,quarterback performance difficult rank much effort spent creating new rating system however input statistic rating subject randomness factor outside quarterback control investigate variance perform sensitivity analysis three quarterback rating statistic traditional rating smith burke wage win rating comparison made team level nfl team thus giving case even game compute quarterback rating offense additional touchdown fewer interception additional sack percent increase passing completion rate sensitivity analysis provides insight whether elite passing team could seem mediocre vice versa based random outcome result indicate traditional rating sensitive statistic respect touchdown interception completion whereas burke rating sensitive sack analysis suggests team passing offense ranking highly sensitive aspect football quarterback hand eg deflected pass lead interception thus margin show argument whether specific quarterback entered elite remains mediocre irrelevant
Statistics,multivariate timebetweenevents monitoring overview overlooked underlying complexity,review method monitoring multivariate timebetweenevents tbe data present underlying complexity overlooked literature helpful classify multivariate tbe monitoring application two fundamentally different scenario one scenario involves monitoring individual vector tbe data involves monitoring several possibly correlated temporal point process event could occur different rate discus performance measure advise use timebetweensignal based metric design comparison method reevaluate existing multivariate tbe monitoring method offer advice direction future research
Statistics,timebased analysis nba hot hand fallacy,debate surrounding hot hand nba ongoing many year however many previous work theme focused next sequential shot attempt often select player work look detail effect made missed shot next series shot twoyear span time shot shown critical factor analysis also multiyear streakiness analyzed indication player really sustain good bad fortune year year
Statistics,selection induced contrast estimate sice effect attempt quantify impact patient selection criterion randomized clinical trial,defining inclusionexclusion ie criterion trial one important step trial design increasingly complex ie criterion potentially create information imbalance transparency issue people design run trial consume information produced trial order better understand quantify impact category ie criterion observed treatment effect concept named selection induced contrast estimate sice effect introduced formulated paper sice effect exist controlled clinical trial treatment affect correlation marker used selection response interest effect demonstrated simulation real clinical trial data although statistical element behind sice effect well studied explicitly formulating studying effect benefit several area including better transparency ie criterion metaanalysis multiple clinical trial treatment effect interpretation realworld medical practice etc
Statistics,analyzing factor associated fatal road crash machine learning approach,road traffic injury account substantial human economic burden globally understanding risk factor contributing fatal injury paramount importance study proposed model adopts hybrid ensemble machine learning classifier structured sequential minimal optimization decision tree identify risk factor contributing fatal road injury model constructed trained tested validated using lebanese road accident platform lrap database road crash incident fatality occurrence outcome variable sensitivity analysis conducted examine influence multiple factor fatality occurrence seven nine selected independent variable significantly associated fatality occurrence namely crash type injury severity spatial clusterid crash time hour evidence gained model data analysis adopted policymakers key stakeholder gain insight major contributing factor associated fatal road crash translate knowledge safety program enhanced road policy
Statistics,playing whole game data collection analysis exercise google calendar,provide computational exercise suitable early introduction undergraduate statistic data science course allows student play whole game data science performing data collection data analysis many teaching resource exist data analysis resource abundant data collection given inherent difficulty task proposed exercise center around student use google calendar collect data goal answering question spend time one hand exercise involves answering question near universal appeal hand data collection mechanism beyond reach typical undergraduate student benefit exercise provides opportunity discussion ethical question consideration data provider data analyst face today age largescale internetbased data collection
Statistics,optimal dose calibration radiotherapy,paper tool provided theory optimal experimental design applied nonlinear calibration model motivated need estimating radiation dos using radiochromic film radiotherapy purpose calibration model case nonlinear explanatory variable worked explicitly model case experimental design found dependent variable inverse function theorem used obtain information matrix optimized optimal design response variable computed two different perspective first fitting model estimating parameter predicting proper dose applied patient first common point view general context optimal experimental design latter actually main objective calibration problem practitioner algorithm computing optimal design also provided
Statistics,new data source production official statistic,past year witnessed rise new data source potential production official statistic large classified survey administrative digital data apart difference generation collection claim lack statistical metadata economic value lack ownership data holder pose several entangled challenge lurking incorporation new data routinely production official statistic argue every challenge must duly overcome international community bring new statistical product based source challenge naturally classified different entangled issue regarding access data statistical methodology quality information technology management identify relevant necessarily tackled new data source definitively considered fully incorporated production official statistic
Statistics,multiple repairable system dependent competing risk nonparametric frailty,aim article analyze data multiple repairable system presence dependent competing risk order model dependence structure adopted wellknown shared frailty model model provides suitable theoretical basis generating dependence component failure time dependent competing risk model known dependence effect scenario influence estimate model parameter hence assumption causespecific intensity follow plp propose frailtyinduced dependence approach incorporate dependence among causespecific recurrent process moreover misspecification frailty distribution may lead error estimating parameter interest considered bayesian nonparametric approach model frailty density order offer flexibility provide consistent estimate plp model well insight heterogeneity among system simulation study real case study provided illustrate proposed approach demonstrate validity
Statistics,comparison clinical episode outcome bundled payment care improvement bpci initiative participant nonparticipant,objective evaluate difference major outcome bundled payment care improvement bpci participating provider nonparticipating provider major joint replacement lower extremity mjrle acute myocardial infarction ami episode method differenceindifferences approach estimated differential change outcome medicare beneficiary mjrle ami bpci participating hospital baseline january september intervention october december period beneficiary episode mjrle ami matched comparison hospital main outcome measure medicare payment los readmission episode includes anchor hospitalization post discharge period result mean total medicare payment mjrle episode post discharge period declined p medicare beneficiary episode initiated bpciparticipating provider beneficiary comparison provider reduction mainly due reduced institutional postacute care pac payment slight reduction carrier payment los estimated readmission rate statistically different bpci comparison population finding suggest pac use reduced without adverse effect recovery mjrle lack statistically significant difference effect ami could explained smaller sample size heterogenous recovery path ami conclusion finding suggest currently designed bundled payment effective reducing payment mjrle episode care necessarily ami saving came decline pac finding consistent result reported bpci model evaluation cm
Statistics,hierarchical bayesian propulsion power model marine vessel,marine traffic major contributor emission worldwide assessing magnitude emission global scale challenging task however emission reduced together improved cost efficiency way vessel operated mathematical model predicting ship consumption central role task nowadays many ship equipped data collection system enable databased calibration consumption model typically calibration procedure carried independently particular ship using data collected ship question paper demonstrate hierarchical bayesian modeling approach fit single model many vessel assumption parameter vessel type similar characteristic eg vessel size likely close benefit approach twofold borrow information parameter well informed vesselspecific data using data similar ship use final hierarchical model predict behavior vessel nt data based characteristic paper discus basic concept present first simple version model apply stan statistical modeling tool model fitting use real data cruise ship collected via widely used commercial eniram platform using bayesian statistical method obtain uncertainty model prediction prediction accuracy model compared existing datafree modeling approach
Statistics,correcting misclassification error crowdsourced ecological data bayesian perspective,many research domain use data elicited citizen scientist direct measure process expensive infeasible however participant may report incorrect estimate classification due lack skill demonstrate bayesian hierarchical model used learn latent variable interest accounting participant ability model described context ecological application involves crowdsourced classification georeferenced coralreef image great barrier reef australia latent variable interest proportion coral cover common indicator coral reef health participant ability expressed term sensitivity specificity correctly classified set point image model also incorporates spatial component allows prediction latent variable location surveyed show model outperforms traditional weightedregression approach used account uncertainty citizen science data approach produce accurate regression coefficient provides better characterization latent process interest new method implemented probabilistic programming language stan applied wide number problem rely uncertain citizen science data
Statistics,modeling random nonrandom decision uncertainty rating data fuzzy beta model,modeling human rating data subject raters decision uncertainty attractive problem applied statistic view complex interplay emotion decision making rating process final raters choice seldom reflect true underlying raters response rather imprecisely observed sense subject nonrandom component uncertainty namely decision uncertainty purpose article illustrate statistical approach analyse rating data integrates random nonrandom component rating process particular beta fuzzy number used model raters nonrandom decision uncertainty variable dispersion beta linear model instead adopted model random counterpart rating response main idea quantify characteristic latent nonfuzzy rating response mean random observation subject fuzziness fuzzy version expectationmaximization algorithm adopted estimate model parameter compute standard error finally characteristic proposed fuzzy beta model investigated mean simulation study well two case study behavioral social context
Statistics,ecological nonlinear state space model selection via adaptive particle markov chain monte carlo adpmcmc,develop novel advanced particle markov chain monte carlo algorithm capable sampling posterior distribution nonlinear state space model unobserved latent state unknown model parameter apply novel methodology five population growth model including model strong weak allee effect test efficiently sample complex likelihood surface often associated model utilising real also synthetically generated data set examine extent observation noise process error may frustrate effort choose model novel algorithm involves adaptive metropolis proposal combined sir particle mcmc algorithm adpmcmc show adpmcmc algorithm sample complex highdimensional space efficiently therefore superior standard gibbs metropolis hastings algorithm known converge slowly applied nonlinear state space ecological model considered paper additionally show adpmcmc algorithm used recursively estimate bayesian cramerrao lower bound tichavsk derive expression cramerrao bound estimate model considered result demonstrate number important feature common population growth model notably multimodal posterior surface dependence static dynamic parameter conclude sampling posterior distribution model use bayes factor highlight observation noise significantly diminishes ability select among model particularly designed reproduce allee effect
Statistics,simultaneous modelbased clustering visualization fisher discriminative subspace,clustering highdimensional space nowadays recurrent problem many scientific domain remains difficult task clustering accuracy result understanding point view paper present discriminative latent mixture dlm model fit data latent orthonormal discriminative subspace intrinsic dimension lower dimension original space constraining model parameter within group family parsimonious dlm model exhibited allows fit onto various situation estimation algorithm called fisherem algorithm also proposed estimating mixture parameter discriminative subspace experiment simulated real datasets show proposed approach performs better existing clustering method providing useful representation clustered data method well applied clustering mass spectrometry data
Statistics,keeping greed good sparse regression design uncertainty application biomass characterization,paper consider classic measurement error regression scenario independent design variable observed several source additive noise show motivating example replicated measurement design dependent variable may leveraged enhance sparse regression algorithm specifically estimate variance use scale design variable demonstrate efficacy scaling several point view validate empirically biomass characterization data set using two widely used sparse algorithm least angle regression lars dantzig selector d
Statistics,method finding structured sparse solution nonnegative least square problem application,demixing problem many area hyperspectral imaging differential optical absorption spectroscopy doas often require finding sparse nonnegative linear combination dictionary element match observed data show aspect problem misalignment doas reference uncertainty hyperspectral endmembers modeled expanding dictionary grouped element imposing structured sparsity assumption combination within group sparse even dictionary highly coherent difficult obtain good solution using convex greedy method nonnegative least square nnls orthogonal matching pursuit use penalty related hoyer measure ratio norm sparsity penalty added objective nnlstype model solving resulting nonconvex model propose scaled gradient projection algorithm requires solving sequence strongly convex quadratic program discus close connection convex splitting method difference convex programming also present promising numerical result example doas analysis hyperspectral demixing problem
Statistics,many community,stochastic blockmodels variant thereof among widely used approach community detection social network relational data stochastic blockmodel partition node network disjoint set called community approach inherently related clustering mixture model raise similar model selection problem number community bayesian information criterion bic popular solution however stochastic blockmodels conditional independence assumption given community endpoint among different edge usually violated practice regard propose composite likelihood bic clbic select number community show robust possible misspecifications underlying stochastic blockmodel assumption derive requisite methodology illustrate approach using simulated real data supplementary material containing relevant computer code available online
Statistics,modelling computation using ncorm mixture density regression,normalized compound random measure flexible nonparametric prior related distribution consider building general nonparametric regression model using normalized compound random measure mixture model posterior inference made using novel pseudomarginal metropolishastings sampler normalized compound random measure mixture model algorithm make use new general approach unbiased estimation laplace functionals compound random measure includes completely random measure special case approach illustrated problem density regression
Statistics,frequencydomain stochastic modeling stationary bivariate complexvalued signal,three equivalent way representing two jointly observed realvalued signal bivariate vector signal single complexvalued signal two analytic signal known rotary component representation unique advantage depending system interest application goal paper provide joint framework three representation context frequencydomain stochastic modeling framework allows u extend many established statistical procedure bivariate vector time series complexvalued rotary representation include procedure parametrically modeling signal coherence estimating model parameter using whittle likelihood performing semiparametric modeling choosing class nested model using model choice also provide new method testing impropriety complexvalued signal test noncircular anisotropic secondorder statistical structure signal represented complex plane finally demonstrate usefulness methodology capturing anisotropic structure signal observed fluid dynamic simulation turbulence
Statistics,optimized linear imputation,often realworld datasets especially high dimensional data feature value missing since data analysis statistical method handle gracefully missing value first step analysis requires imputation missing value indeed long standing interest method imputation missing value preprocessing step one recent effective approach irmi stepwise regression imputation method us linear regression model realvalued feature basis feature dataset however proposed iterative formulation lack convergence guarantee propose closely related method stated single optimization problem block coordinatedescent solution guaranteed converge local minimum experiment show result synthetic benchmark datasets comparable result irmi method whenever converges however set experiment described irmi often converge performance method shown markedly superior comparison method
Statistics,kernelestimated nonparametric overlapbased syncytial clustering,commonlyused clustering algorithm usually find ellipsoidal spherical regularstructured cluster challenged underlying group lack formal structure definition syncytial clustering name introduce method merge group obtained standard clustering algorithm order reveal complex group structure data develop distributionfree fullyautomated syncytial clustering algorithm used k mean algorithm approach estimate cumulative distribution function normed residual appropriately fit k group model calculates estimated nonparametric overlap pair cluster group high pairwise overlap merged long estimated generalized overlap decrease methodology always top performer identifying group regular irregular structure several datasets applied datasets scatter incomplete record approach also used identify distinct kind gamma ray burst burst transient source experiment catalog distinct kind activation functional magnetic resonance imaging study
Statistics,online bayesian parameter estimation general nonlinear statespace model tutorial new result,online estimation play important role process control monitoring obtaining theoretical solution simultaneous stateparameter estimation problem nonlinear stochastic system involves solving complex multidimensional integral amenable analytical solution basic sequential montecarlo smc particle filtering pf algorithm simultaneous estimation exist well recognized need making online algorithm nondegenerate fast applicable process missing measurement overcome deficiency traditional algorithm work proposes bayesian approach online state parameter estimation extension handle missing data realtime also provided simultaneous estimation performed filtering extended vector state parameter using adaptive sequentialimportanceresampling sir filter kernel density estimation method approach us online optimization algorithm based kullbackleibler kl divergence allow adaptation sir filter combined stateparameter estimation optimal tuning rule control width kernel variance artificial noise added parameter also proposed approach illustrated numerical example
Statistics,fractionallysupervised classification,traditionally three specie classification unsupervised supervised semisupervised supervised semisupervised classification differ whether weight given unlabelled observation classification procedure unsupervised classification clustering observation unlabeled hence full weight given unlabelled observation observation unlabelled difficult textit apriori choose optimal level supervision consequence suboptimal choice nontrivial flexible fractionallysupervised approach classification introduced level supervision ranging unsupervised supervised attained approach us weighted likelihood wherein weight control relative role labelled unlabelled data building classifier comparison approach traditional specie presented using simulated real data gaussian mixture model used vehicle illustrate fractionallysupervised classification approach however broadly applicable variation postulated model easily made
Statistics,mixture common skewt factor analyzer,mixture common skewt factor analyzer model introduced modelbased clustering highdimensional data assuming common component factor loading model allows clustering performed presence large number mixture component number dimension large wellmodelled mixture factor analyzer model variant thereof furthermore assuming component density follow skewt distribution allows robust clustering skewed data alternating expectationconditional maximization algorithm employed parameter estimation demonstrate excellent clustering performance model applied real simulated datathis paper mark first time skewed common factor used
Statistics,quantifying uncertainty random forest via confidence interval hypothesis test,work develops formal statistical inference procedure machine learning ensemble method ensemble method based bootstrapping bagging random forest improved predictive accuracy individual tree fail provide framework distributional result easily determined instead aggregating full bootstrap sample consider predicting averaging tree built subsamples training set demonstrate resulting estimator take form ustatistic prediction individual feature vector asymptotically normal allowing confidence interval accompany prediction practice subset subsamples used computational speed estimator take form incomplete ustatistics equivalent result derived demonstrate setup provides framework testing significance feature moreover internal estimation method develop allows u estimate variance parameter perform inference procedure additional computational cost simulation illustration real dataset provided
Statistics,microclustering cluster size grow sublinearly size data set,generative model clustering implicitly assume number data point cluster grows linearly total number data point finite mixture model dirichlet process mixture model pitman yor process mixture model make assumption infinitely exchangeable clustering model however task assumption undesirable example performing entity resolution size cluster often unrelated size data set consequently cluster contains negligible fraction total number data point task therefore require model yield cluster whose size grow sublinearly size data set address requirement defining emph microclustering property introducing new model exhibit property compare model several commonly used clustering model checking model fit using real simulated data set
Statistics,changepoint detection presence outlier,many traditional method identifying changepoints struggle presence outlier noise heavytailed often infer additional changepoints order fit outlier overcome problem data often need preprocessed remove outlier though difficult application data need analysed online present approach changepoint detection robust presence outlier idea adapt existing penalised cost approach detecting change use loss function le sensitive outlier argue loss function bounded classical biweight loss particularly suitable show bounded loss function robust arbitrarily extreme outlier present efficient dynamic programming algorithm find optimal segmentation penalised cost criterion importantly algorithm used setting data need analysed online show consistently estimate number changepoints accurately estimate location using biweight loss function demonstrate usefulness approach application analysing welllog data detecting copy number variation detecting tampering wireless device
Statistics,modelling preference data wallenius distribution,wallenius distribution generalisation hypergeometric distribution weight assigned ball different colour naturally defines model ranking category used classification purpose since general resulting likelihood analytically available adopt approximate bayesian computational abc approach estimating importance category illustrate performance estimation procedure simulated datasets finally use new model analysing two datasets movie rating italian academic statistician journal preference latter novel dataset collected author
Statistics,bayesian computer model analysis robust bayesian analysis,harness power bayesian emulation technique designed aid analysis complex computer model examine structure complex bayesian analysis technique facilitate robust bayesian analysis andor sensitivity analysis complex problem hence allow global exploration impact choice made likelihood prior specification show previously intractable problem robustness study overcome using emulation technique method allow scientist quickly extract approximation posterior result corresponding particular subjective specification utility flexibility method demonstrated reanalysis real application bayesian method employed capture belief river flow discus obvious extension direction future research approach open
Statistics,mixture modeling related sample ψ stick breaking kernel perturbation,great interest recently applying nonparametric kernel mixture hierarchical manner model multiple related data sample jointly setting several data feature commonly present related sample often share mixture component differing weight ii mixture component vary across sample iii often shared mixture component across sample aligned perfectly term location spread rather display small misalignment either due systematic crosssample difference often due uncontrolled extraneous cause properly incorporating feature mixture modeling enhance efficiency inference whereas ignoring reduces efficiency jeopardize validity inference due issue confounding introduce two technique incorporating feature modeling related data sample using kernel mixture first technique called psi stick breaking joint generative process mixing weight breaking stick shared sample component vary size across sample idiosyncratic stick sample component vary size second technique imbue random perturbation kernel thereby accounting crosssample misalignment technique used either separately together parametric nonparametric kernel mixture derive efficient bayesian inference recipe based mcmc sampling model featuring technique illustrate work simulated data real flow cytometry data set predictionestimation crosssample calibration testing multisample difference
Statistics,variation ensembled random survival forest application cancer research,paper describe novel implementation adaboost prediction survival function take different variation algorithm compare algorithm based system run time root mean square error construction includes right censoring data competing risk data take different data set illustrate performance algorithm
Statistics,nonparametric independence screening via favored smoothing bandwidth,propose flexible nonparametric regression method ultrahighdimensional data first step propose fast screening method based favored smoothing bandwidth marginal local constant regression iterative procedure developed recover important covariates regression function theoretically prove favored smoothing bandwidth based screening posse model selection consistency property simulation study well real data analysis show competitive performance new procedure
Statistics,fast spatial inference homogeneous ising model,ising model important statistical modeling inference many application however normalizing constant mean number active vertex mean spin interaction intractable provide accurate approximation make possible calculate quantity numerically simulation study indicate good performance compared markov chain monte carlo method tiny fraction time methodology also used perform bayesian inference functional magnetic resonance imaging activation detection experiment
Statistics,robust detection covariatetreatment interaction clinical trial,detection interaction treatment effect patient descriptor clinical trial critical optimizing drug development process increasing volume data accumulated clinical trial provides unique opportunity discover new biomarkers goal personalized medicine also requires innovative robust biomarker detection method capable detecting nonlinear sometimes weak signal propose set novel univariate statistical test based theory random walk able capture nonlinear nonmonotonic covariatetreatment interaction also propose novel combined test leverage power proposed univariate test single generalcase tool present result synthetic trial well realworld clinical trial compare method stateoftheart technique demonstrate utility robustness approach
Statistics,provable convex coclustering tensor,cluster analysis fundamental tool pattern discovery complex heterogeneous data prevalent clustering method mainly focus vector matrixvariate data applicable generalorder tensor arise frequently modern scientific business application moreover gap statistical guarantee computational efficiency existing tensor clustering solution due nature nonconvex formulation work bridge gap developing provable convex formulation tensor coclustering convex coclustering coco estimator enjoys stability guarantee computationally storage efficient establish nonasymptotic error bound coco estimator reveals surprising blessing dimensionality phenomenon exist vector matrixvariate cluster analysis theoretical finding supported extensive simulated study finally apply coco estimator cluster analysis advertisement click tensor data major online company clustering result provide meaningful business insight improve advertising effectiveness
Statistics,multiscale fisher independence test multivariate dependence,identifying dependency multivariate data common inference task arises numerous application however existing nonparametric independence test typically require computation scale least quadratically sample size making difficult apply massive data moreover resampling usually necessary evaluate statistical significance resulting test statistic finite sample size worsening computational burden introduce scalable resamplingfree approach testing independence two random vector breaking task simple univariate test independence collection contingency table constructed sequential coarsetofine discretization sample space transforming inference task multiple testing problem completed almost linear complexity respect sample size address increasing dimensionality introduce coarsetofine sequential adaptive procedure exploit spatial feature dependency structure effectively examine sample space derive finitesample theory guarantee inferential validity adaptive procedure given sample size particular show approach achieve strong control familywise error rate without resampling largesample approximation demonstrate substantial computational advantage procedure comparison existing approach well decent statistical power various dependency scenario extensive simulation study illustrate divideandconquer nature procedure exploited test independence learn nature underlying dependency finally demonstrate use method analyzing large data set flow cytometry experiment
Statistics,easytouse empirical likelihood abc method,many scientifically wellmotivated statistical model natural engineering environmental science specified generative process case may possible write likelihood model analytically approximate bayesian computation abc method allow bayesian inference situation typically computationally intensive recently computationally attractive empirical likelihood based abc method suggested literature method heavily rely availability set suitable analytically tractable estimating equation propose easytouse empirical likelihood abc method input required choice summary statistic observed value ability simulate summary statistic parameter value model shown posterior obtained using proposed method consistent performance explored using various example
Statistics,distributed sequential method analyzing massive data,analyse large data set containing lengthy variable adopt sequential estimation idea propose parallel divideandconquer method conduct several conventional sequential estimation procedure separately properly integrate result maintaining desired statistical property additionally using criterion statistical experiment design adopt adaptive sample selection together adaptive shrinkage estimation method simultaneously accelerate estimation procedure identify effective variable confirm cogency method theoretical justification numerical result derived synthesized data set apply proposed method three real data set including pertaining appliance energy use particulate matter concentration
Statistics,global fitting response surface via estimating multiple contour simulator,computer simulator nowadays widely used understand complex physical system many area aerospace renewable energy climate modeling manufacturing one fundamental issue study computer simulator known experimental design select input setting computer simulator run corresponding response collected extra care taken selection process computer simulator computationally expensive run selection shall acknowledge achieve goal analysis article focus goal producing accurate prediction important risk assessment decision making propose two new method design approach sequentially select input setting achieve goal approach make novel application simultaneous sequential contour estimation numerical example employed demonstrate effectiveness proposed approach
Statistics,simultaneous transformation rounding star model integervalued data,propose simple yet powerful framework modeling integervalued data count score rounded data datagenerating process defined simultaneously transforming rounding star continuousvalued process produce flexible family integervalued distribution capable modeling zeroinflation bounded censored data underdispersion transformation modeled unknown greater distributional flexibility rounding operation ensures coherent integervalued datagenerating process efficient mcmc algorithm developed posterior inference provides mechanism adaptation successful bayesian model algorithm continuous data integervalued data setting using star framework design new bayesian additive regression tree bart model integervalued data demonstrates impressive predictive distribution accuracy synthetic data large healthcare utilization dataset interpretable regressionbased inference develop star additive model offer greater flexibility scalability existing integervalued model star additive model applied study recent decline amazon river dolphin
Statistics,efficient parameter estimation sampled random field,provide computationally statistically efficient method estimating parameter stochastic gaussian model observed spatial grid need rectangular standard method plagued computational intractability designing method implemented realistically sized problem issue long time motivated use fourier transform whittle likelihood approximation challenge frequencydomain method determine account observational boundary effect missing data shape observed spatial grid paper address effect explicitly proposing new quasilikelihood estimator prove consistency asymptotic normality estimator show proposed method solves boundary issue whittle estimation finite sample yielding parameter estimate significantly reduced bias error demonstrate effectiveness method incomplete lattice comparison recent method finally apply method estimate parameter matern process used model data venus topography
Statistics,classification matrixvariate distribution,matrixvariate distribution intuitively model dependence structure matrixvalued observation arise application multivariate time series spatiotemporal repeated measure paper develops expectationmaximization algorithm discriminant analysis classification matrixvariate distribution methodology show promise simulated datasets applied forensic matching fractured surface classification functional magnetic resonance satellite hand gesture image
Statistics,clusterwise supervised learning procedure based aggregation distance,nowadays many machine learning procedure available shelve may used easily calibrate predictive model supervised data however input data consists one unknown cluster different underlying predictive model exist fitting model challenging task propose paper procedure three step automatically solve problem kfc procedure aggregate different model adaptively data first step procedure aim catching clustering structure input data may characterized several statistical distribution provides several partition given assumption distribution partition second step fit specific predictive model based data cluster overall model computed consensual aggregation model corresponding different partition comparison performance different simulated real data ass excellent performance method large variety prediction problem
Statistics,tuning parameter calibration prediction personalized medicine,personalized medicine become important part medicine instance predicting individual drug response based genomic information however many current statistical method tailored task overlook individual heterogeneity patient paper look personalized medicine linear regression standpoint introduce alternative version ridge estimator target individual establishing tuning parameter calibration scheme minimizes prediction error individual patient stark contrast classical scheme crossvalidation minimize prediction error average show pipeline optimal term oracle inequality fast highly effective simulation real data
Statistics,imbalanced classification objectiveoriented review,common issue classification scientific research industry existence imbalanced class sample size different class imbalanced training data naively implementing classification method often lead unsatisfactory prediction result test data multiple resampling technique proposed address class imbalance issue yet general guidance use technique article provide objectiveoriented review common resampling technique binary classification imbalanced class size learning objective consider include classical paradigm minimizes overall classification error costsensitive learning paradigm minimizes costadjusted weighted type type ii error neymanpearson paradigm minimizes type ii error subject type error constraint paradigm investigate combination resampling technique stateoftheart classification method pair resampling technique classification method use simulation study study performance different evaluation metric extensive simulation experiment demonstrate classification paradigm complex dynamic among resampling technique base classification method evaluation metric imbalance ratio practitioner takeaway message imbalanced data one usually consider combination resampling technique base classification method
Statistics,tuningfree ridge estimator highdimensional generalized linear model,ridge estimator regularize squared euclidean length parameter estimator mathematically computationally attractive involve tuning parameter difficult calibrate paper show ridge estimator modified tuning parameter avoided altogether also show modified version improve empirical prediction accuracy standard ridge estimator combined crossvalidation provide first theoretical guarantee
Statistics,hyperparameter selection subsampling bootstrap,massive data analysis becomes increasingly prevalent subsampling method like blb bag little bootstrap serf powerful tool assessing quality estimator massive data however performance subsampling method highly influenced selection tuning parameter eg subset size number resamples per subset article develop hyperparameter selection methodology used select tuning parameter subsampling method specifically careful theoretical analysis find analytically simple elegant relationship asymptotic efficiency various subsampling estimator hyperparameters lead optimal choice hyperparameters specifically arbitrarily specified hyperparameter set improve new set hyperparameters extra cpu time cost resulting estimator statistical efficiency much improved simulation study real data analysis demonstrate superior advantage method
Statistics,fast optimal bayesian approximation targeted prediction,prediction critical decisionmaking uncertainty lends validity statistical inference targeted prediction goal optimize prediction specific decision task interest represent via functionals using tool predictive decision analysis design framework constructing optimal scalable simple approximation targeted prediction bayesian model wide variety approximation penalized loss function derive convenient representation optimal targeted approximation yield efficient interpretable solution customized outofsample predictive metric developed evaluate compare among targeted predictor careful use posterior predictive distribution introduce procedure identifies set nearoptimal predictor acceptable model include different model form subset covariates provide unique insight feature level complexity needed accurate targeted prediction simulation demonstrate excellent prediction estimation variable selection capability targeted approximation constructed physical activity data national health nutrition examination survey nhanes better predict understand characteristic intraday physical activity
Statistics,innovative additive outlier robust kalman filtering robust particle filter,paper propose cebass particle mixture kalman filter robust innovative additive outlier able fully capture multimodality distribution hidden state furthermore particle sampling approach resamples past state enables cebass handle innovative outlier immediately visible observation trend change filter computationally efficient derive new accurate approximation optimal proposal distribution particle proposed algorithm shown compare well existing approach applied machine temperature server data
Statistics,inherent difficulty nonbayesian likelihoodbased inference revealed examination recent book aitkin,many decade statistician made attempt prepare bayesian omelette without breaking bayesian egg obtain probabilistic likelihoodbased inference without relying informative prior distribution recent example murray aitkin recent book em statistical inference present approach statistical hypothesis testing based comparison posterior distribution likelihood competing model aitkin develops illustrates method using simple example inference iid data twoway test independence analyze note consequence inferential paradigm adopted therein discussing approach incompatible bayesian perspective find relevant applied work
Statistics,linear regression numeric symbolic variable ordinary least square approach based wasserstein distance,paper present linear regression model modal symbolic data observed variable histogram variable according definition given framework symbolic data analysis parameter model estimated using classic least square method appropriate metric introduced order measure error observed predicted distribution particular wasserstein distance proposed property metric exploited predict response variable direct linear combination independent histogram variable measure goodness fit discussed application real data corroborates proposed method
Statistics,class conjugate prior defined unit simplex,dirichlet distribution dirichlet process infinite dimensional generalization primarily used conjugate prior categorical multinomial distribution bayesian statistic extension proposed broaden application different purpose article explore class prior distribution closely related dirichlet distribution incorporating additional information data generating mechanism example given show potential use model
Statistics,conversation eugenio regazzini,eugenio regazzini born august cremona italy took degree university l bocconi milano held position university torino bologna milano university l bocconi assistant professor lecturer professor since currently professor probability mathematical statistic university pavia period head institute application mathematics computer science italian national research council cnr milano head department mathematics university pavia respectively twelve year served member scientific board italian mathematical union umi elected fellow ims fellow istituto lombardo accademia di scienze e lettere research activity probability statistic covered wide spectrum topic including finitely additive probability foundation bayesian paradigm exchangeability partial exchangeability distribution functionals random probability measure stochastic integration history probability statistic overall one authoritative developer de finetti legacy last five year extended scientific interest probabilistic method mathematical physic particular studied asymptotic behavior solution equation interest kinetic theory gas present interview taken occasion birthday
Statistics,identifying number cluster discrete mixture model,research cluster analysis categorical data continues develop new clustering algorithm proposed however context determination number cluster rarely addressed paper propose new approach clustering categorical data estimation number cluster carried simultaneously assuming data originate finite mixture multinomial distribution develop method select number mixture component based minimum message length mml criterion implement new expectationmaximization em algorithm estimate model parameter proposed emmml approach rather selecting one among set preestimated candidate model requires running em several time seamlessly integrates estimation model selection single algorithm performance proposed approach compared wellknown criterion bayesian information criterionbic resorting synthetic data two real application european social survey emmml computation time clear advantage proposed method also real data solution much parsimonious solution provided competing method reduces risk model order overestimation increase interpretability
Statistics,note bayesian logistic regression spatial exponential family gibbs point process,recently attractive logistic regression inference method exponential family gibbs spatial point process introduced combined technique quadratic tangential variational approximation derived new bayesian technique analysing spatial point pattern technique described detail demonstrated numerical example
Statistics,testing order constraint qualitative difference bayes factor normalized maximum likelihood,compared bayes factor normalized maximum likelihood simple case selecting orderconstrained versus full binomial model comparison revealed two qualitative difference testing order constraint regarding data dependence model preference
Statistics,teacher know bootstrap resampling undergraduate statistic curriculum,three goal article show enormous potential bootstrapping permutation test help student understand statistical concept including sampling distribution standard error bias confidence interval null distribution pvalues dig deeper understand method work nt thing watch deal issue teaching change statistical practice comparing method common test interval see inaccurate latter confirm asymptotics n nt enough think n resampling provides diagnostics accurate alternative sadly common bootstrap percentile interval badly undercovers small sample better alternative tone informal story joke
Statistics,comparative review generalization gumbel extreme value distribution application wind speed data,generalized extreme value distribution particular case gumbel extreme value distribution widely applied extreme value analysis gumbel distribution certain drawback nonheavytailed distribution characterized constant skewness kurtosis generalized extreme value distribution frequently used context encompasses three possible limiting distribution normalized maximum infinite sample independent identically distributed observation however generalized extreme value distribution might suitable model observed maximum come large number observation hence form generalization gumbel distribution might preferable goal collect present literature distribution contain gumbel distribution embedded identify flexible skewness kurtosis heavytailed could competitive generalized extreme value distribution generalization gumbel distribution described compared using application wind speed data set monte carlo simulation show distribution suffer overparameterization coincide generalized gumbel distribution smaller number parameter ie nonidentifiable study suggests generalized extreme value distribution mixture two extreme value distribution considered practical application
Statistics,computer experiment functional input scalar output normbased approach,framework designing analyzing computer experiment presented constructed dealing functional real number input real number output designing experiment functional real number input two stage approach suggested first stage consists constructing candidate set functional input second stage optimal combination found candidate set latin hypercube real number input searched resulting design considered generalization latin hypercubes gp model explored metamodel functional input incorporated kriging model applying norm order define distance two functional input order make calculation norm computationally feasible use bsplines promoted
Statistics,estimating tail index using model averaging,idea model averaging used find weight peakoverthreshold problem using possible range threshold range largest observation chosen considered possible threshold time performing estimation weight based information criterion threshold calculated weighted estimate threshold shape parameter calculated
Statistics,new universal resample stable bootstrapbased stopping criterion pls component construction,develop new robust stopping criterion partial least square regression plsr component construction characterised high level stability new criterion defined universal one since suitable plsr extension generalized linear regression plsglr criterion based nonparametric bootstrap process computed algorithmically allows test successive component preset significant level alpha order ass performance robustness respect different noise level perform intensive datasets simulation preset known number component extract case n p n number subject p number original predictor datasets n p use ttests compare performance approach others classical criterion property robustness particularly tested resampling process real allelotyping dataset conclusion criterion present also better global predictive performance plsr plsglr logistic poisson framework
Statistics,modeling asymptotically independent spatial extreme based laplace random field,tackle modeling threshold exceedance asymptotically independent stochastic process construction based laplace random field defined gaussian random field scaled stochastic variable following exponential distribution framework yield useful asymptotic property remaining statistically convenient univariate distribution tail half exponential type part limiting generalized pareto distribution threshold exceedance normalizing marginal tail distribution data standard laplace field used capture spatial dependence among extreme asymptotic property laplace field explored compared classical framework asymptotic dependence multivariate joint tail decay rate laplace field slower gaussian field covariance structure hence provide conservative estimate extreme joint risk maintaining asymptotic independence statistical inference illustrated extreme wind gust netherlands comparison gaussian dependence model show better goodnessoffit term akaike criterion specific application fit welladapted weibull distribution univariate tail model normalization univariate tail distribution done simple power transformation data
Statistics,robust estimation stochastic frontier model,study proposes robust estimator stochastic frontier model integrating idea basu et al biometrika model verify suggested estimator strongly consistent asymptotic normal regularity condition investigate robust property use simulation study demonstrate estimator strong robust property little loss asymptotic efficiency relative maximum likelihood estimator real data analysis performed illustrating use estimator
Statistics,multivariate sn estimator,note introduce sn estimator multivariate sn new robust estimator multivariate ranking like mve mcd search h subset minimizes criterion difference new criterion measure degree overlap univariate projection data primary advantage new criterion lie relative independence configuration outlier second advantage easily lends socalled symmetricizing transformation whereby observation enter objective function pairwise difference make proposal well suited model asymmetric distribution sn therefore generally applicable either mve mcd sde also construct fast algorithm sn estimator simulate bias various adversary configuration outlier
Statistics,basic univariate bivariate statistic symbolic data critical review,proof problem basic statistic proposed numeric symbolic data
Statistics,effective degree freedom flawed metaphor,applied statistician fitting procedure degree freedom synonymous model complexity capacity overfitting data particular often used parameterize biasvariance tradeoff model selection argue contrary folk intuition model complexity degree freedom synonymous may correspond poorly exhibit theoretically explore various example fitting procedure degree freedom monotonic model complexity parameter exceed total dimension response space even simple setting degree freedom exceed dimension ambient space arbitrarily large amount show degree freedom nonconvex projection method unbounded
Statistics,new approach inference multisurvey study unknown population size,investigate poisson sampling design presence unknown selection probability applied population unknown size multiple sampling occasion fixedpopulation model adopted extended upon inference complete minimal sufficient statistic derived sampling model parameter fixedpopulation parameter vector raoblackwell version population quantity estimator detailed application applied emprical population extended inferential framework found much potential utility empirical study
Statistics,simple adaptive dispersion regression model count data,regression count data widely performed model poisson negative binomial nb zeroinflated regression challenge often faced practitioner selection right model take account dispersion typically occurs count datasets highly desirable unified model automatically adapt underlying dispersion easily implemented practice paper discrete weibull regression model shown able adapt simple way different type dispersion relative poisson regression overdispersion underdispersion covariatespecific dispersion maximum likelihood used efficient parameter estimation description model parameter inference model diagnostics accompanied simulated real data analysis
Statistics,note basis dimension selection generalized additive modelling,two new approach checking dimension basis function using penalized regression smoother presented first approach test adequacy basis dimension based estimate residual variance calculated differencing residual neighbour according smooth covariates second approach based estimated degree freedom smooth model residual respect model covariates comparison basis dimension selection algorithm based smoothness selection criterion gcv aic reml procedure computationally efficient enough routine use part model checking
Statistics,sensitivity analysis unmeasured confounding metaanalyses,randomeffects metaanalyses observational study produce biased estimate synthesized study subject unmeasured confounding propose sensitivity analysis quantifying extent unmeasured confounding specified magnitude could reduce certain threshold proportion true effect size scientifically meaningful also develop converse method estimate strength confounding capable reducing proportion scientifically meaningful true effect chosen threshold method apply bias factor assumed normally distributed across study assessed across range fixed value estimator derived using recently proposed sharp bound confounding bias within single study make assumption regarding unmeasured confounders functional form relationship exposure outcome interest provide r package confoundedmeta freely available online graphical user interface compute point estimate inference produce plot conducting sensitivity analysis method facilitate principled use randomeffects metaanalyses observational study ass strength causal evidence hypothesis
Statistics,analytic moment laplace transform formula quasistationary distribution shiryaev diffusion interval,derive analytic closedform moment laplace transform formula quasistationary distribution classical shiryaev diffusion restricted interval absorption given
Statistics,statistical inference big picture,statistic moved beyond frequentistbayesian controversy past leave ability interpret result suggest philosophy compatible statistical practice labeled statistical pragmatism serf foundation inference statistical pragmatism inclusive emphasizes assumption connect statistical model observed data argue introductory course often mischaracterize process statistical inference propose alternative big picture depiction
Statistics,discussion single twostage crosssectional time series benchmarking procedure sae,congratulate author stimulating valuable manuscript providing careful review stateof theart crosssectional timeseries benchmarking procedure small area estimation develop novel twostage benchmarking method hierarchical time series model evaluate procedure estimating monthly total unemployment using data u census bureau discus three topic linearity model misspecification computational complexity model comparison aspect small area estimation practice specifically pose following question author may wish answer robust model misspecification time perhaps move away linear model type considered battese et al fay herriot asymptotic computational complexity comparison made model benchmarking constraint inherently fixed random
Statistics,review nonparametric hypothesis test isotropy property spatial data,important aspect modeling spatiallyreferenced data appropriately specifying covariance function random field practitioner working spatial data presented number choice regarding structure dependence observation one choice determining whether isotropic covariance function appropriate isotropy implies spatial dependence depend direction spatial separation sampling location misspecification isotropy property directional dependence lead misleading inference eg inaccurate prediction parameter estimate researcher may use graphical diagnostics directional sample variograms decide whether assumption isotropy reasonable graphical technique difficult ass open subjective interpretation misleading hypothesis test assumption isotropy may desirable end number test directional dependence developed using spatial spectral representation random field provide overview nonparametric method available test hypothesis isotropy symmetry spatial data summarize test property discus important consideration recommendation choosing implementing test compare several method via simulation study propose number open research question several reviewed method implemented r using package sptest available cran
Statistics,publication bias metaanalysis confidence interval rosenthals failsafe number,purpose present paper ass efficacy confidence interval rosenthal failsafe number although rosenthal estimator highly used researcher statistical property largely unexplored first developed statistical theory allowed u produce confidence interval rosenthal failsafe numberthis produced discerning whether number study analysed metaanalysis fixed random case produce different variance estimator given number study given distribution provided five variance estimator confidence interval examined normal approximation nonparametric bootstrap accuracy different confidence interval estimate tested method simulation different distributional assumption half normal distribution variance estimator best probability coverage finally provide table lower confidence interval rosenthal estimator
Statistics,conversation nan laird,nan mckenzie laird harvey v fineberg professor biostatistics harvard h chan school public health made fundamental contribution statistical method longitudinal data analysis missing data metaanalysis addition widely known work statistical genetics statistical method psychiatric epidemiology paper dempster rubin em algorithm among top highly cited paper science nature applied work medical practice error widely cited among medical malpractice community nan born gainesville florida shortly thereafter parent angus mckenzie laird myra adelia doyle moved tallahassee florida nan sister victoria mell nan started college rice university transferred university georgia received b statistic elected phi beta kappa graduation nan worked massachusetts institute technology draper laboratory worked kalman filtering apollo man moon program enrolled statistic department harvard university received phd joined faculty harvard school public health upon receiving phd remains research professor retirement interview conducted boston massachusetts july link nan full cv found surlwwwhsphharvardedunanlaird
Statistics,statistical sensitiveness science,research often necessitates sample yet obtaining large enough sample always possible researcher may use one two method deciding upon required sample size rulesofthumb quick yet uncertain estimation power mathematically precise yet potential overestimate underestimate sample size effect size unknown misestimated sample size negative repercussion form increased cost abandoned project abandoned publication nonsignificant result describe procedure estimating sample size adequate testing approach common behavioural social biomedical science test significance developed fisher procedure focus desired minimum effect size research hand find minimum sample size required capturing effect size statistically significant result similar fashion power analysis sensitiveness analysis also extended finding minimum effect given sample size priori well calculating sensitiveness posteriori article provides full tutorial carrying sensitiveness analysis well empirical support via simulation
Statistics,boxcox symmetric distribution application nutritional data,introduce boxcox symmetric class distribution useful modeling positively skewed possibly heavytailed data new class distribution includes boxcox boxcox colegree boxcox power exponential distribution class logsymmetric distribution special case provides easy parameter interpretation make convenient regression modeling purpose additionally provides enough flexibility handle outlier usefulness boxcox symmetric model illustrated application nutritional data
Statistics,bayesian lower bound dense sparse outlier noise rmt framework,robust estimation important timely research subject paper investigate performance lower bound meansquareerror mse estimator bayesian linear model corrupted noise distributed according iid student tdistribution class prior parametrized degree freedom relevant modelize either dense sparse accounting outlier noise using hierarchical normalgamma representation student tdistribution van tree bayesian cramerrao bound bcrb amplitude parameter derived furthermore random matrix theory rmt framework assumed ie number measurement number unknown parameter grow jointly infinity asymptotic finite ratio using powerful result rmt closedform expression bcrb derived studied finally propose framework fairly compare two model corrupted noise different degree freedom fixed common target signaltonoise ratio snr particular focus effort comparison bcrbs associated two model corrupted sparse noise promoting outlier dense gaussian noise respectively
Statistics,statistical inference course based pvalues,introductory statistical inference text course treat point estimation hypothesis testing interval estimation problem separately primary emphasis largesample approximation present alternative approach teaching course built around pvalues emphasizing provably valid inference sample size detail computation marginalization also provided several illustrative example along course outline
Statistics,spatial temporal exponentialfamily point process model evolution social system,develop class exponentialfamily point process based latent social space model coevolution social structure behavior time temporal dynamic modeled discrete markov process specified individual transition distribution actor system given time prove distribution analytic closed form certain condition use result develop likelihoodbased inference provide computational framework enable simulation inference practice finally demonstrate value model analyzing alcohol drug use time context adolescent friendship network
Statistics,approximate variational estimation model network formation,develop approximate estimation method exponential random graph model ergms whose likelihood proportional intractable normalizing constant usual approach approximates constant monte carlo simulation however convergence may exponentially slow propose deterministic method based variational meanfield approximation ergm normalizing constant compute lower upper bound approximation error network size adapting nonlinear large deviation result translates bound distance true likelihood meanfield likelihood monte carlo simulation suggest practice deterministic method performs better conservative theoretical approximation bound imply large class model
Statistics,combinatorics distance covariance inclusionminimal maximizers quasiconcave set function diverse variable selection,paper show negative sample distance covariance function quasiconcave set function sample random variable statistically independent use property propose greedy algorithm combinatorially optimize diversity low statistical dependence promoting function distance covariance greedy algorithm obtains inclusionminimal maximizers diversity promoting objective inclusionminimal maximizers multiple solution set globally optimal maximizers proper subset maximizing set solution set present result upon applying approach obtain diverse feature covariatesvariablespredictors feature selection setting regression classification problem also combine diverse feature selection algorithm distance covariance based relevant feature selection algorithm produce subset covariates relevant yet ordered nonincreasing level diversity subset
Statistics,bootstrap clustering two dimension,propose bootstrap procedure data may exhibit clustering two dimension use insight theory generalized ustatistics analyze largesample property statistic sample average observation pooled across cluster asymptotic distribution statistic may nonstandard clustering mean show proposed bootstrap procedure pointwise consistent fixed datagenerating process dgp b uniformly consistent exclude case clustering without clustering mean c provides refinement dgp limiting distribution gaussian
Statistics,tutorial kernel density estimation recent advance,tutorial provides gentle introduction kernel density estimation kde recent advance regarding confidence band geometrictopological feature begin discussion basic property kde convergence rate various metric density derivative estimation bandwidth selection introduce common approach construction confidence intervalsbands discus handle bias next talk recent advance inference geometric topological feature density function using kde finally illustrate one use kde estimate cumulative distribution function receiver operating characteristic curve provide r implementation related tutorial end
Statistics,power analysis linear regression model regressors matrix sampled,multiple matrix sampling survey methodology technique randomly chooses relatively small subset item presented survey respondent purpose reducing respondent burden data produced missing completely random mcar special missing data technique used linear regression multivariate statistical analysis derive asymptotic variance regression parameter estimate allow u conduct power analysis linear regression model fit data obtained via multiple matrix sampling design idea demonstrated variation big five inventory psychological trait exploration regression parameter space demonstrates instability sample size requirement substantial loss precision matrixsampled regressors simulation nonnormal data demonstrates advantage semiparametric multiple imputation scheme
Statistics,review dynamic network model latent variable,present selective review statistical modeling dynamic network focus model latent variable specifically latent space model latent class model stochastic blockmodels investigate observed feature unobserved structure network begin overview static model introduce dynamic extension dynamic model also discus application studied literature data source listed appendix based review summarize list open problem challenge dynamic network modeling latent variable
Statistics,open data open review open dialogue making social science plausible,nowadays protecting trust social science also mean engaging open community dialogue help safeguard robustness improve efficiency research method combination open data open review open dialogue may sound simple implementation real world straightforward however view begley elli statement scientific process demand highest standard quality ethic rigour worth implementing importantly feasible work likely help restore plausibility social science research therefore feel likely triplet open data open review open dialogue gradually emerge become policy requirement regardless research funding source
Statistics,lossbased approach twopiece locationscale distribution application dependent data,twopiece locationscale model used modeling data presenting departure symmetry paper propose objective bayesian methodology tail parameter two particular distribution family skewed exponential power distribution skewed generalised logistic distribution apply proposed objective approach time series model linear regression model error term follow distribution object study performance proposed approach illustrated simulation experiment real data analysis methodology yield improvement density forecast shown analysis carry electricity price nordpool market
Statistics,quantification weight fingerprint evidence using rocbased approximate bayesian computation algorithm model selection,century fingerprint used considerable success identify criminal verify identity individual categorical conclusion scheme used fingerprint examiner generally inference process followed forensic scientist heavily criticised scientific legal literature instead scholar proposed characterise weight forensic evidence using bayes factor key element inference process forensic science quantifying magnitude support equally important determining model supported unfortunately complexity fingerprint pattern render likelihoodbased inference impossible paper use approximate bayesian computation model selection algorithm quantify weight fingerprint evidence supplement abc algorithm using receiver operating characteristic curve mitigate effect curse dimensionality modified algorithm computationally efficient make easier monitor convergence number simulation increase use method quantify weight fingerprint evidence forensic science note applied forensic pattern evidence
Statistics,multiple factor analysis distributional data,framework symbolic data analysis sda distributionvariables particular case multivalued variable unit represented set distribution eg histogram density function quantile function one variable factor analysis fa method primary exploratory tool dimension reduction visualization present work use multiple factor analysis mfa approach analysis data described distributional variable distributional variable induces set new numeric variable related quantiles distribution call new variable textit quantile variable set quantile variable related distributional one block mfa approach thus mfa performed juxtaposed table quantile variable show criterion decomposed analysis approximation variability based suitable metric distribution squared wasserstein distance application simulated real distributional data corroborate method interpretation result factorial plane performed new interpretative tool related several characteristic distribution location scale shape
Statistics,spatially varying coefficient modeling large datasets eliminating n spatial regression,spatially varying coefficient svc modeling popular applied science computational burden substantial especially true multiscale property svc considered given background study develops moran eigenvectorbased spatially varying coefficient msvc modeling approach estimate multiscale svcs computationally efficiently estimation accelerated rank reduction ii precompression iii sequential likelihood maximization step ii eliminate sample size n likelihood function step likelihood maximization cost independent n step iii accelerates likelihood maximization multiscale svcs estimated even number svcs k large msvc approach compared geographically weighted regression gwr monte carlo simulation experiment simulation result show approach far faster gwr n large despite numerically estimating parameter gwr numerically estimate parameter proposed approach applied land price analysis illustration developed svc estimation approach implemented r package spmoran
Statistics,sounding spider efficient way representing uncertainty high dimension,article proposes visualization method multidimensional data based animated functional hypothetical outcome plot fhops ii kiviat plot iii data sonification uncertainty quantification uq framework analysis coupled standard statistical analysis tool probability density function pdf used augment understanding uncertainty numerical code input translate uncertainty quantity interest qoi contrast static representation advanced technique functional highest density region hdr boxplot functional boxplot fhops dynamic visualization enables practitioner infer dynamic physic enables see functional correlation may exist technique allows represent qoi propose version kiviat plot encode input parameter new visualization take advantage information fhops data sonification allows analyse large datasets within highdimensional parameter space functional qoi canvas proposed method assessed showed benefit two related environmental datasets
Statistics,optimal design hierarchical generalized group testing,choosing optimal strategy hierarchical group testing important problem practitioner interested disease screening limited resource example screening infectious disease large population important use algorithm minimize cost potentially expensive assay black et al described intractable problem unless number individual screen small proposed approximation optimal strategy difficult implement large population size article develop optimal design respect expected total number test obtained using novel dynamic programming algorithm show algorithm substantially efficient approach proposed black et al addition compare two design imperfect test r code provided practitioner
Statistics,statistical reform replication crisis,replication crisis prompted many call statistical reform within psychological science examine issue within frequentist statistic may led replication crisis examine alternative bayesian statistic many suggested replacement frequentist approach bayesian approach offer radically different perspective evidence inference frequentist approach prioritising error control bayesian approach offering formal method quantifying relative strength evidence hypothesis suggest rather mere statistical reform needed better understanding different mode statistical inference better understanding statistical inference relates scientific inference
Statistics,gaussian process mixture estimating heterogeneous treatment effect,develop gaussianprocess mixture model heterogeneous treatment effect estimation leverage use transformed outcome approach present attempt improve point estimation uncertainty quantification relative past work used transformed variable related method well traditional outcome modeling earlier work modeling treatment effect heterogeneity using transformed outcome relied tree based method single regression tree random forest umbrella nonparametric model outcome modeling performed using bayesian additive regression tree various flavor weighted single tree approach work well large sample available suffer smaller sample result sensitive model misspecification method attempt garner improvement inference quality via correctly specified model rooted bayesian nonparametrics furthermore begin model assumes treatment assignment mechanism known extension learnt data presented application observational study approach applied simulated real data demonstrate theorized improvement inference respect two causal estimands conditional average treatment effect average treatment effect leveraging correctly specified model able accurately estimate treatment effect reducing variance
Statistics,flexible construction negative binomial model,work present construction stationary markov model negativebinomial marginal distribution simple closed form expression corresponding transition probability given linking proposal wellknown class birth death process thus revealing interesting characterization advantage closed form expression tested simulated real data
Statistics,organic fiducial inference,substantial generalisation put forward theory subjective fiducial inference outlined earlier paper particular theory extended deal case data discrete categorical rather continuous case important predata knowledge model parameter system directly expressing handling predata knowledge via referred global local predata function parameter concerned distinct involves attempting directly represent knowledge form prior distribution function parameter using bayes theorem regard individual attribute identified three separate type fiducial argument namely strong moderate weak fiducial argument form integral part theory developed various practical example application theory presented including example involving binomial poisson multinomial data fiducial distribution function parameter model example interpreted term generalised definition subjective probability set previously
Statistics,shannon entropy generalization towards statistic reliability information science,starting pioneering work shannon weiner plethora work reported entropy different direction entropyrelated review work direction statistic reliability information science best knowledge reported far tried collect possible work direction period people interested entropy specially new researcher get benefited
Statistics,deterministic bootstrapping class bootstrap method,algorithm described enables efficient deterministic approximate computation bootstrap distribution linear bootstrap method tn alleviating need repeated resampling observation resp inputderived data essence algorithm computes distribution function linear mixture independent random variable finite discrete distribution algorithm applicable elementary bootstrap scenario targetting mean parameter interest block bootstrap well certain residual bootstrap scenario moreover algorithm promise much broader applicability nonbootstrapped hypothesis testing
Statistics,exponential random graph model little network,statistical model social network enabled researcher study complex social phenomenon give rise observed pattern relationship among social actor gain rich understanding interdependent nature social tie actor much research focused social network within medium large social group date advance statistical model social network particular exponentialfamily random graph model ergms rarely applied study small network despite small network data team family personal network common many field paper revisit estimation ergms small network propose using exhaustive enumeration possible developed r package implement estimation pooled ergms small network using maximum likelihood estimation mle called ergmito based result extensive simulation study ass property mle estimator conclude several benefit direct mle estimation compared approximate method creates opportunity valuable methodological innovation applied modeling social network ergms
Statistics,atlantic causal inference conference acic data analysis challenge,brief note document data generating process used data analysis challenge associated atlantic causal inference conference acic focus challenge estimation inference conditional average treatment effect cates presence targeted selection lead strong confounding associated data file plot found first author web page
Statistics,estimating average treatment effect utilizing fractional imputation confounders subject missingness,problem missingness observational data ubiquitous confounders missing random multiple imputation commonly used however method requires congeniality condition valid inference may satisfied estimating average causal treatment effect alternatively fractional imputation proposed kim implemented handling missing value regression context article develop fractional imputation method estimating average treatment effect confounders missing random show fractional imputation estimator average treatment effect asymptotically normal permit consistent variance estimate via simulation study compare fractional imputation accuracy precision multiple imputation
Statistics,model based level shift detection autocorrelated data stream using moving window,standard control chart technique detect level shift data stream assume independence observation data today collected high frequency assumption seldom valid overcome propose adapt offline test procedure detection outlier based onestep prediction error proposed tsay online framework considering moving window present two algorithm combination estimate appropriate test value control chart test method ar process exposed level shift different size compare cusum applied onestep prediction error find even though method perform comparable tuned correctly method higher probability identifying correct change point process furthermore complicated process method easier tune range window size tested independent process
Statistics,introduction geodetic time series analysis,contribution chapter book geodetic time series analysis book dedicated art fitting trajectory model geodetic time series order extract accurate geophysical information realistic error bar geodymanics environmental geodesy related study vast amount literature published topic past year specifically interested parametric algorithm estimating functional stochastic model using various bayesian statistical tool maximum likelihood monte carlo markov chain kalman filter least square variance component estimation information criterion chapter focus parameter trajectory model estimated meant give researcher new topic easy introduction theory reference key book article detail found addition hope refreshes detail experienced reader pay special attention modelling noise received much attention literature last year highlight numerical aspect
Statistics,reducedbias estimation spatial econometric model incompletely geocoded data,application stateoftheart spatial econometric model requires information spatial coordinate statistical unit completely accurate usually case context areal data microgeographic pointlevel data however information inevitably affected locational error generated intentionally data producer privacy protection due inaccuracy geocoding procedure unfortunate circumstance potentially limit use spatial econometric modelling framework analysis micro data indeed recent contribution see eg arbia espa giuliani shown presence locational error may nonnegligible impact result particular wrong spatial coordinate lead downward bias increased variance estimation model parameter contribution aim developing strategy reduce bias produce reliable inference spatial econometrics model location error validity proposed approach assessed mean monte carlo simulation study different realcase scenario study result show method promising make spatial econometric modelling microgeographic data possible
Statistics,texttt code proof prepare emph weather condition,computational tool data analysis released daily repository comprehensive r archive network integrate tool solve problem research increasingly complex requiring frequent update mitigate emph kafkaesque computational challenge research manuscript proposes emph toolchain walkthrough opinionated documentation scientific workflow practical complement proofbased argument gray marwick arxiv reproducible data analysis focus practicality setting reproducible research compendia unit test measure texttt code proof confidence computational algorithm
Statistics,symbolic formula linear mixed model,statistical model mathematical representation often simplified idealised datagenerating process paper focus particular type statistical model called linear mixed model lmms widely used many discipline egagriculture ecology econometrics psychology mixed model also commonly known multilevel nested hierarchical panel data model incorporate combination fixed random effect lmms special case inclusion random effect particular give lmms considerable flexibility accounting many type complex correlated structure often found data flexibility however given rise number way enduser specify precise form lmm wish fit statistical software paper review software design specification lmm special case linear model focusing particular use highlevel symbolic model formula two popular contrasting rpackages asreml
Statistics,sharp hypothesis bispatial inference,fundamental class inferential problem characterised substantial degree predata prior belief value model parameter equal lay close specified value may example value indicates absence effect standard way tackling problem type including bayesian method often highly inadequate practice address issue inferential framework called bispatial inference put forward viewed generalisation radical reinterpretation existing approach inference based p value shown obtain postdata density function given parameter convenient combine special type bispatial inference constructed around onesided p value previously outlined form fiducial inference finally using called postdata opinion curve bispatialfiducial theory naturally extended deal general scenario number parameter may unknown application theory illustrated various example especially relevant analysis clinical trial data
Statistics,inference synthetic control method multiple treated unit,although synthetic control method scm widely applied commonlyused inference method placebo test often problematic especially treatment uniquely assigned paper discus problem placebo test multivariate treatment case improve power inference propose andrewstype procedure potentially solve drawback placebo test simulation conducted show andrew test often valid powerful compared placebo test
Statistics,variable selection transportability,transportability provides principled framework address problem applying study result new population consider problem selecting variable include transport estimator provide brief overview transportability framework illustrate selection diagram vital first step variable selection graph alone identify sufficient strictly necessary set variable generating unbiased transport estimate next conduct simulation experiment assessing impact including unnecessary variable performance parametric gcomputation transport estimator result highlight type variable included affect bias variance mean squared error estimate find addition variable cause outcome whose distribution differ source target population increase variance mean squared error transported estimate hand inclusion variable cause outcome regardless whether modify causal contrast interest differ distribution population reduces variance estimate without increasing bias finally exclusion variable cause outcome modify causal contrast interest increase bias finding suggest variable selection approach transport prioritize identifying including cause outcome study population rather focusing variable whose distribution may differ study sample target population
Statistics,integrated organic inference ioi reconciliation statistical paradigm,recognised bayesian approach inference adequately cope type predata belief population quantity interest commonly held practice particular generally encounter difficulty lack belief parameter model within certain partition parameter space concerned address issue fairly comprehensive theory inference put forward called integrated organic inference based fusion fisherian bayesian reasoning depending predata knowledge held given model parameter inference made parameter conditional parameter using one three method inference namely organic fiducial inference bispatial inference bayesian inference full conditional postdata density result combined using framework allows joint postdata density parameter sensibly formed without requiring full conditional density compatible various example application theory presented finally theory defended possible criticism partially term previously defined generalised subjective probability
Statistics,liquid scorecard,traditional credit scorecard generalized additive model gam step function component function shape step function may constrained order satisfy pile palatability interpretability legal explainability constraint fico used linear programming find traditional scorecard approximately maximizes divergence subject pile constraint paper introduce liquid scorecard allows component function least partially smooth curve use quadratic programming bspline theory find liquid scorecard exactly maximizes divergence subject pile constraint fico us aspect technology develop famous fico credit score
Statistics,exploiting disagreement highdimensional variable selector uncertainty visualization,propose combined selection uncertainty visualizer csuv estimate set true covariates highdimensional linear regression visualizes selection uncertainty exploiting dis agreement among different base selector proposed method selects covariates get selected frequently different variable selection method subsampled data method generic used different existing variable selection method demonstrate variable selection performance using real simulated data variable selection method uncertainty illustration tool publicly available r package csuv http githubcomchristineyuencsuv graphical tool also available online via http csuvshinyappsiocsuv
Statistics,space filling split plot design using fast flexible filling,article adaption algorithm creation experimental design lekivetz jones suggested dealing constraint around randomization splitplot design experiment used level factor modified easily others splitplot design deal context ioptimal doptimal design continuous response output space filling design strategy suggested proposed design evaluated based different design criterion well analytical example
Statistics,review book new science cause effect,book review published aronow peter fredrik avje book new science cause effect journal american statistical association
Statistics,globally intensityreweighted estimator k pair correlation function,introduce new estimator inhomogeneous k function pair correlation function spatial point process well cross k function cross pair correlation function bivariate spatial point process assumption secondorder intensityreweighted stationarity estimator rely global normalization factor depends aggregation intensity function whilst existing estimator depend locally intensity function individual observed point advantage new global estimator existing local estimator demonstrated theoretical consideration simulation study
Statistics,monte carlo comparison categorical test independence,test frequently applied test testing independence two categorical variable however one best knowledge compared extensively ultimately answer question use applicability case zero frequency debated non parametric permutation test suggested work perform extensive monte carlo simulation study attempting answer aforementioned point expected large sample sized case indistinguishable small sample sized case leq though provide strong evidence supporting use test regardless zero frequency case unconditional independence also suggest use permutation based test testing conditional independence cost computationally expensive test exhibited inferior performance use limited
Statistics,sequential adaptive strategy populationbased sampling rare clustered disease,innovative sampling strategy proposed applies largescale populationbased survey targeting rare trait unevenly spread geographical area interest proposal characterised ability tailor data collection specific feature challenge survey hand based integrating adaptive component sequential selection aim intensify detection positive case upon exploiting spatial clusterisation provide flexible framework managing logistical budget constraint account selection bias readytoimplement weighting system provided release unbiased accurate estimate empirical evidence illustrated tuberculosis prevalence survey recommended many country supported emblematic example need improved sampling design simulation result also given illustrate strength weakness proposed sampling strategy respect traditional crosssectional sampling
Statistics,different approach choosing threshold peak threshold,abstract extreme value methodology choice threshold play important role efficient modelling observation exceeding threshold threshold must chosen high enough ensure unbiased extreme value index choosing threshold high result uncontrolled variance paper investigates generalized model assist choice optimal threshold value gamma positive domain bayesian approach considered deriving posterior distribution unknown generalized parameter using property posterior distribution allows method choose optimal threshold without visual inspection
Statistics,monitoring process riskadjusted medical outcome using multistage mewma chart,statistical process control programme healthcare focus surveillance outcome final stage procedure mortality failure rate approach ignores multistage nature procedure patient progress several stage prior final stage paper develop multistage control chart based multivariate exponentially weighted moving average ewma test statistic derived score equation allows simultaneous monitoring intermediate final stage outcome healthcare process adjustment underlying patient risk factor dependence outcome variable use ewma test statistic allows quick detection small gradual change part process three advantage approach better understanding outcome different stage relate explicit monitoring upstream stage outcome may help curtail trend lead poorer endstage outcome understanding impact stage help determine effective allocation quality improvement resource simulation performed test control chart various type hypothesised shift result summarised using outofcontrol average run length
Statistics,new ecdf twosample test statistic,empirical cumulative distribution function ecdfs used test hypothesis two sample come distribution since seminal contribution kolmogorov smirnov paper describes statistic usable condition kolmogorovsmirnov provides power extant test vein demonstrate valid conservative procedure producing finitesample pvalues outline close relationship statistic two main predecessor also provide public r package cran twosamples implementing testing procedure nlog n time n memory using package function perform several simulation study showing power improvement
Statistics,method find efficient robust sampling strategy model uncertainty,consider problem deciding sampling strategy particular sampling design propose risk measure whose minimizing value guide choice method make use superpopulation model take account uncertainty parameter method illustrated real dataset yielding satisfactory result baseline use strategy couple probability proportionaltosize sampling difference estimator known optimal superpopulation model fully known show even moderate misspecifications model strategy robust outperformed alternative
Statistics,frequentismasmodel,statistician aware probability model interpreted frequentist manner really true objective reality idealisation argue often ignored actually applying frequentist method interpreting result keeping awareness essential difference reality model lead appropriate use interpretation frequentist model method called frequentismasmodel elaborated showing connection existing work appreciating special role iid model subject matter knowledge giving account condition model true useful giving detailed interpretation test confidence interval confronting implicit compatibility logic inverse probability logic bayesian inference reinterpreting role model assumption appreciating robustness role interpretative equivalence model epistemic often referred bayesian probability share issue model idealisation really true modelling reasoning uncertainty meaning essential advantage frequentism often claimed bayesian statistic combined frequentismasmodel leading gelman hennig call falsificationist bayes
Statistics,reliability decision based test fourier analysis boolean decision function,item test often used basis making decision test therefore required good psychometric property like unidimensionality many case sum score used combination threshold decide pas fail instance consider whether decision function appropriate without latent variable model property decision function desirable consider reliability stability decision function ie decision change upon perturbation change fraction outcome item measurement error concerned question whether sum score best way aggregate item use idea test theory social choice theory graphical model computer science probability theory answer question conclude weighted sum score desirable property fit test theory observable similar condition like conditional association ii property decision stable reliable iii satisfies rousseau criterion input match decision use fourier analysis boolean function investigate whether decision function stable figure set item proportionally large influence decision apply technique invoke idea graphical model use pseudolikelihood factorisation probability distribution
Statistics,bayesian baseline belief uncommon event,plausibility uncommon event miracle based testimony event much discussed analyzing probability involved mostly assumed common event taken data calculation however usually testimony common event difference significant effect inductive part inference large influence one view reliability testimony work full bayesian solution given realistic case one large number testimony common event one testimony uncommon event seen order large amount testimony common event testimony probably quite reliable reason testimony quite reliable based testimony common event probability uncommon event given testimony also higher hence one openminded considering plausibility uncommon event
Statistics,behavioralclinical phenotyping type diabetes selfmonitoring data,objective evaluate unsupervised clustering method identifying individuallevel behavioralclinical phenotype relate personal biomarkers behavioral trait type diabetes selfmonitoring data material method used hierarchical clustering hc identify group meal similar nutrition glycemic impact individual collected selfmonitoring data evaluated cluster correspondence gold standard generated certified diabetes educator cdes participant face validity rated cdes impact cdes ability identify pattern another participant result gold standard g included pattern across participant rediscovered using hc g pattern consistent pattern identified hc meal cluster followed pattern another included subgroup broader clusers cluster rated likert scale validity significance actionable reviewing cluster cdes identified pattern consistent data reduction contradiction pattern participant record discussion hierarchical clustering blood glucose macronutrient consumption appears suitable discovering behavioralclinical phenotype cluster corresponded gold standard rated positively cdes face validity cluster visualization helped cdes identify robust pattern nutrition glycemic impact creating new possibility visual analytic solution conclusion machine learning method use diabetes selfmonitoring data create personalized behavioralclinical phenotype may prove useful delivering personalized medicine
Statistics,uncovering sociological effect heterogeneity using machine learning,individual respond uniformly treatment event intervention sociologist routinely partition sample subgroup explore effect treatment vary covariates like race gender socioeconomic status analyst determine key subpopulation based theoretical prior datadriven discovery also routine yet analysis sociologist typically go problematic seldom move u beyond expectation bias explore new meaningful subgroup emerging machine learning method allow researcher explore source variation may previously considered envisaged paper use causal tree recursively partition sample uncover source treatment effect heterogeneity use honest estimation splitting sample training sample grow tree estimation sample estimate leafspecific effect assessing central topic social inequality literature college effect wage compare learn conventional approach exploring variation effect causal tree given use observational data use leafspecific matching sensitivity analysis address confounding offer interpretation effect based observed unobserved heterogeneity encourage researcher follow similar practice work variation sociological effect
Statistics,hierarchical clustering smart meter electricity load based quantile autocovariances,order improve efficiency sustainability electricity system country worldwide deploying advanced metering infrastructure particular household smart meter residential sector technology able record electricity load time series high frequency rate information exploited develop new clustering model group individual household similar consumption pattern end work propose three hierarchical clustering methodology allow capturing different characteristic time series based set dissimilarity measure computed different feature quantile autocovariances simple partial autocorrelations main advantage allow summarizing time series representative feature computationally efficient robust outlier easy automatize scalable hundred thousand smart meter series evaluate performance clustering model realworld smart meter dataset thousand halfhourly time series result show obtained cluster identify relevant consumption behavior household capture part geodemographic segmentation moreover apply supervised classification procedure explore feature relevant define cluster
Statistics,descriptive predictive analysis euroleague basketball game wisdom basketball crowd,study focus prediction basketball game euroleague competition using machine learning modelling prediction binary classification problem predicting whether match finish home win away win data collected euroleague official website season ie new format era feature extracted match data offtheshelf supervised machine learning technique applied calibrate validate model find simple machine learning model give accuracy greater test set worse sophisticated benchmark model additionally importance study lie wisdom basketball crowd demonstrate predicting power collective group basketball enthusiast outperform machine learning model discussed study argue accuracy level group expert set benchmark future study prediction european basketball game using machine learning
Statistics,network tomography identifiability fourier domain estimation,statistical problem network tomography infer distribution mathbf x mutually independent component measurement model mathbf amathbf x given binary matrix representing routing topology network consideration challenge dimension mathbf x much larger mathbf thus problem often called illposed paper study statistical aspect network tomography first address identifiability issue prove mathbf x distribution identifiable shift parameter mild condition use mixture model characteristic function derive fast algorithm estimating distribution mathbf x based general method moment extensive model simulation real internet trace driven simulation proposed approach shown favorable comparing previous method using simple discretization inferring link delay heterogeneous network
Statistics,penalized approach bivariate logistic regression model association ordinal response,bivariate ordered logistic model bolms appealing jointly model marginal distribution two ordered response association given set covariates number category response increase number global odds ratio reparametrizations estimated also increase estimating association structure becomes crucial type data fact data could rich fully modelled ordinary bolm sometimes wellknown dale model could parsimonious provide good fit addition crosstabulation response contains zero number model configuration including bivariate version partial proportional odds model ppom estimation bolm fisherscoring algorithm may either fail estimate irregular association structure work propose use nonparametric approach maximum likelihood estimation bolm apply penalty difference adjacent row column effect result estimation le demanding ordinary bolm permitting fit ppoms andor smoothing marginal association parameter polynomial curve surface score chosen data model selection based penalized loglikelihood ratio whose limiting distribution studied simulation aic proposal compared goodman model dale model term goodnessoffit parsimony literature data set finally application original data set liver disease patient proposed
Statistics,parametric statistical method trusted fmri based group study,widely used task fmri analysis use parametric method depend variety assumption individual aspect fmri model evaluated evaluated comprehensive manner empirical data work total million random task fmri group analysis performed using resting state fmri data compute empirical familywise error rate software package spm fsl afni well standard nonparametric permutation method variation nominal familywise error rate parametric statistical method shown conservative voxelwise inference invalid clusterwise inference particular cluster size inference cluster defining threshold p generates familywise error rate conduct number follow analysis investigation suggest cause invalid cluster inference spatial auto correlation function follow assumed gaussian shape comparison nonparametric permutation test based small number assumption found produce valid result voxel well cluster wise inference using real task data compare result one parametric method permutation test find stark difference conclusion drawn two using cluster inference finding speak need validating statistical method used neuroimaging field
Statistics,sequential test estimate overrunning based p value combination,often sequential trial additional data become available stopping boundary reached method incorporating information overrunning developed based adding weighted z method combining p value yield combined p value primary test medianunbiased estimate confidence bound parameter test amount overrunning information proportional amount available upon terminating sequential test exact inference method provided otherwise approximate method given evaluated context observing brownian motion drift either linear stopping boundary continuous time discretetime groupsequential boundary method compared available method exemplified data two sequential clinical trial
Statistics,factorial graphical lasso dynamic network,dynamic network model describe growing number important scientific process cell biology epidemiology sociology finance many aspect dynamical network require statistical consideration paper focus determining network structure estimating dynamic network difficult task since number component involved system large result number parameter estimated bigger number observation however characteristic many network sparse example molecular structure gene make interaction component highlystructured therefore sparse process penalized gaussian graphical model used estimate sparse network however literature focussed static network lack specific temporal constraint propose structured gaussian dynamical graphical model structure consist specific time dynamic known presence absence link block equality constraint parameter thus number parameter estimated reduced accuracy estimate including identification network tuned show constrained optimization problem solved taking advantage efficient solver logdetppa developed convex optimization moreover model selection method checking sensitivity inferred network described finally synthetic real data illustrate proposed methodology
Statistics,improving autodependogram using kulbackleibler divergence,autodependogram graphical device recently proposed literature analyze autodependencies defined computing classical pearson chisquare statistic independence various lag order point presence lagdepedencies paper proposes improvement diagram obtained substituting chisquare statistic estimator kulbackleibler divergence bivariate density two delayed variable product marginal distribution simulation study wellestablished time series model show new autodependogram powerful previous one application financial data also shown
Statistics,randomized pickfreeze sparse sobol index estimation high dimension,article investigates new procedure estimate influence variable given function defined highdimensional space precisely concerned describing function large number p parameter depends small number proposed method unconstrained ell minimization based sobol method prove mathcal slog p evaluation f one find relevant parameter
Statistics,robust pca fasthcs,principal component analysis pca widely used analyze highdimensional data sensitive outlier robust pca method seek fit unaffected outlier therefore trusted reveal fasthcs highdimensional congruent subset robust pca algorithm suitable highdimensional application including case number variable exceeds number observation detailing fasthcs algorithm carry extensive simulation study three real data application result show fasthcs systematically robust outlier stateoftheart method
Statistics,fully datadriven method estimating shape point cloud,given random sample point unknown distribution propose new datadriven method estimating probability support mild assumption r convex smallest r convex set contains sample point natural estimator main problem using estimator practice r unknown geometric characteristic set stochastic algorithm proposed selecting data hypothesis sample uniformly generated new datadriven reconstruction able achieve convergence rate convex hull estimating convex set much flexible smoothness shape condition practical performance estimator illustrated real data example simulation study
Statistics,sparse wavelet estimation quantile regression multiple functional predictor,manuscript study quantile regression partial functional linear model response scalar predictor include scalar multiple function wavelet basis adopted better approximate functional slope effectively detect local feature sparse group lasso penalty imposed select important functional predictor capture shared information among estimation problem reformulated standard secondorder cone program solved interior point method also give novel algorithm using alternating direction method multiplier admm recently employed many researcher solving penalized quantile regression problem asymptotic property convergence rate prediction error bound established simulation real data fmri data investigated show superiority proposed method
Statistics,nonparametric covariance estimation mixed longitudinal study application midlife woman health,motivated application mixed longitudinal study group subject entering study different age crosssectional followed successive year longitudinal consider nonparametric covariance estimation sample noisy partiallyobserved functional trajectory proposed algorithm based sequentialaggregation scheme noniterative basic matrix operation closedform solution step good performance proposed method supported theory numerical experiment also apply proposed procedure midlife woman working memory study based data study woman health across nation swan
Statistics,optimal uncertainty quantification moment class using canonical moment,gain robustness quantification risk measurement accounting source uncertainty tainting input computer code evaluate maximum quantile class distribution defined constraint moment methodology based theory canonical moment appears wellsuited framework practical optimization
Statistics,additive bayesian variable selection censoring misspecification,study interplay two important issue bayesian model selection bm censoring model misspecification consider additive accelerated failure time aaft cox proportional hazard probit model general concave loglikelihood structure fundamental question solution one hope bm provide inevitably model misspecified show asymptotically bm keep covariate predictive power either outcome censoring time discard covariates misspecification refers assuming wrong model functional effect response including using finite basis truly nonparametric effect omitting truly relevant covariates argue using simple model computationally practical yet attain good power detect potentially complex effect despite misspecification misspecification censoring asymptotically negligible effect suitablydefined false positive impact power exponential portray issue via simple description earlylate censoring drop predictive accuracy due misspecification method point view consider local prior novel structure combine local nonlocal prior enforce sparsity develop algorithm capitalize aaft tractability approximation aaft probit likelihood giving significant computational gain simple augmented gibbs sampler hierarchically explore linear nonlinear effect implementation r package mombf illustrate proposed method others based likelihood penalty via extensive simulation misspecification censoring present two application concerning effect gene expression colon breast cancer
Statistics,spatial spatiotemporal garch model unified approach,timeseries analysis particularly finance generalised autoregressive conditional heteroscedasticity garch model widely applied statistical tool modelling volatility cluster ie period increased decreased risk contrast spatial dependence conditional second moment spatial spatiotemporal process considered rather uncritical model proposed modelling local cluster increased risk paper introduce unified spatial spatiotemporal garchtype model cover previously proposed spatial autoregressive conditional heteroscedasticity arch model also introduces novel spatial garch spgarch egarch process common modelling framework maximumlikelihood estimator derived addition theoretical contribution suggest model selection strategy verified series monte carlo simulation study eventually use unified model demonstrated empirical example particular focus realestate price berlin zipcode area data spatial autoregressive model applied show locally varying model uncertainty captured spatial garchtype model
Statistics,exponential increase power independence homogeneity chisquare test auxiliary information,paper extension work exponential increase power two nonparametric test z test chisquare goodnessoffit test subject auxiliary information possible improve exponentially relative size sample power famous chisquare test independence homogeneity improving power statistical test using auxiliary information make possible either reduce probability accepting null hypothesis alternative hypothesis reduce size sample necessary reach predefined power suggested method computational simple statistical application presented illustrate result framework work nonparametric applied kind data area using statistic
Statistics,statistical inference dynamic treatment regime,dynamic treatment regime growing interest across clinical science regime provide one way operationalize thus inform sequential personalized clinical decision making dynamic treatment regime sequence decision rule decision rule per stage clinical intervention decision rule map uptodate patient information recommended treatment briefly review variety approach using data construct decision rule review interesting challenge nonregularity often arises area nonregularity mean parameter indexing optimal dynamic treatment regime nonsmooth functionals underlying generative distribution consequence regular asymptotically unbiased estimator parameter exists nonregularity arises inference parameter optimal dynamic treatment regime illustrate effect nonregularity asymptotic bias via sensitivity asymptotic limiting distribution local perturbation propose evaluate locally consistent adaptive confidence interval aci parameter optimal dynamic treatment regime use data adaptive intervention child adhd study illustrative example conclude highlighting discussing emerging theoretical problem area
Statistics,dynamic question ordering online survey,online survey potential support adaptive question later question depend earlier response past work taken rulebased approach uniformly across respondent envision richer interpretation adaptive question call dynamic question ordering dqo question order personalized approach could increase engagement therefore response rate well imputation quality present dqo framework improve survey completion imputation general surveytaking setting want maximize survey completion focus ordering question engage respondent collect hopefully information least information characterizes respondent accurate imputation another scenario goal provide personalized prediction since possible give reasonable prediction subset question concerned motivating user answer question instead want order question get information reduces prediction uncertainty burdensome illustrate framework example providing energy estimate prospective tenant also discus dqo national survey consider connection statisticsbased questionordering approach cognitive survey methodology
Statistics,generalized score matching nonnegative data,common challenge estimating parameter probability density function intractability normalizing constant case maximum likelihood estimation may implemented using numerical integration approach becomes computationally intensive score matching method hyv arinen avoids direct calculation normalizing constant yield closedform estimate exponential family continuous distribution mathbb r hyv arinen extended approach distribution supported nonnegative orthant mathbb r paper give generalized form score matching nonnegative data improves estimation efficiency example consider general class pairwise interaction model addressing overlooked inexistence problem generalize regularized score matching method lin et al improve theoretical guarantee nonnegative gaussian graphical model
Statistics,fitting mixture distribution data tutorial,paper stepbystep tutorial fitting mixture distribution data merely assumes reader background calculus linear algebra required background briefly reviewed explaining main algorithm explaining main algorithm first fitting mixture two distribution detailed example fitting two gaussians poissons respectively continuous discrete case introduced thereafter fitting several distribution general case explained example several gaussians gaussian mixture model poissons provided modelbased clustering one application mixture distribution also introduced numerical simulation also provided gaussian poisson example sake better clarification
Statistics,multivariate forecasting evaluation sensitive strictly proper scoring rule,recent year probabilistic forecasting emerging topic growing need suitable method evaluation multivariate prediction analyze sensitivity common scoring rule especially regarding quality forecasted dependency structure additionally propose scoring rule based copula uniquely describes dependency structure every probability distribution continuous marginal distribution efficient estimation considered scoring rule evaluation method dieboldmariano test discussed detailed simulation study compare performance renowned scoring rule one propose besides extended synthetic study based recently published result also consider real data example find energy score probably widely used multivariate scoring rule performs comparably well detecting forecast error also regarding dependency contradicts study result also show proposed copula score provides strong distinction model correct incorrect dependency structure close comprehensive discussion proposed methodology
Statistics,stable mixed graph,paper study class graph three type edge capture modified independence structure directed acyclic graph dag marginalisation unobserved variable conditioning selection variable using separation criterion include mc summary ancestral graph modification mc graph define class ribbonless graph rg permit use separation criterion rg contain summary ancestral graph subclass rg generated dag marginalisation conditioning derive simple algorithm generate rg given dag rg also generate summary ancestral graph simple way extension rggenerating algorithm enables u develop parallel theory three class study relationship well use class
Statistics,markov equivalence subclass loopless mixed graph,paper discus four problem regarding markov equivalence subclass loopless mixed graph classify four problem finding condition internal markov equivalence markov equivalence within subclass external markov equivalence markov equivalence subclass representational markov equivalence possibility graph subclass markov equivalent graph another subclass finding algorithm generate graph certain subclass markov equivalent given graph particularly focus class maximal ancestral graph subclass namely regression graph bidirected graph undirected graph directed acyclic graph present novel result representational markov equivalence algorithm
Statistics,markov property mixed graph,paper unify markov theory variety different type graph used graphical markov model introducing class loopless mixed graph show independence model induced separation graph compositional graphoids focus particular subclass ribbonless graph special case include undirected graph bidirected graph directed acyclic graph well ancestral graph summary graph define maximality graph well pairwise global markov property prove global pairwise markov property maximal ribbonless graph equivalent independence model compositional graphoid
Statistics,marginalization conditioning lwf chain graph,paper deal problem marginalization conditioning two disjoint subset node set chain graph cgs lwf markov property purpose define class chain mixed graph cmgs three type edge class provide separation criterion class cmgs stable marginalization conditioning contains class lwf cgs subclass provide method generating graph marginalization conditioning given cmg given lwf cg define study class anterial graph also stable marginalization conditioning contains lwf cgs simpler structure cmgs
Statistics,markov property discrete determinantal point process,determinantal point process dpps probabilistic model repulsion used represent occurrence random subset finite base set dpps allow model global negative association mathematically elegant direct way discrete dpps become popular computationally tractable model solving several machine learning task require selection diverse object successfully applied numerous reallife problem despite popularity statistical property model adequately explored note derive markov property discrete dpps show expressed using graphical model
Statistics,principal component sensitive distributional change,pca often used anomaly detection statistical process control task bivariate data prove minor projection least varying projection pcarotated data sensitive distributional change sensitivity defined hellinger distance distribution change particular almost always case one parameter bivariate normal distribution change ie change sparse simulation indicate minor projection sensitive large range change prechange setting higher dimension well motivates using minor projection detecting sparse distributional change highdimensional data
Statistics,estimation correlation binary sequence model,consider binary sequence generated thresholding hidden continuous sequence hidden variable assumed compound symmetry covariance structure single parameter characterizing common correlation study parameter estimation problem oneparameter model demonstrate maximizing likelihood function yield consistent estimate correlation formally prove nonestimability parameter deriving nonvanishing minimax lower bound counterintuitive phenomenon provides interesting insight onebit information latent variable sufficient consistently recover common correlation hand show trinary data generated hidden variable consistently estimate correlation parametric convergence rate thus reveal phase transition phenomenon regarding discretization latent continuous variable preserving estimability correlation numerical experiment performed validate conclusion
Statistics,fast adaptive sparse precision matrix estimation high dimension,paper proposes new method estimating sparse precision matrix high dimensional setting popular study fast computation adaptive procedure problem propose novel approach called sparse columnwise inverse operator address two issue analyze adaptive procedure based cross validation establish convergence rate frobenius norm convergence rate matrix norm also established method also enjoys advantage fast computation largescale problem via coordinate descent algorithm numerical merit illustrated using simulated real datasets particular performs favorably hiv brain tissue dataset adhd restingstate fmri dataset
Statistics,qualitative inequality squared partial correlation gaussian random vector,describe various set conditional independence relationship sufficient qualitatively comparing nonvanishing squared partial correlation gaussian random vector sufficient condition satisfied several graphical markov model rule comparing degree association among vertex gaussian graphical model also developed apply rule compare conditional dependency gaussian tree particular tree show dependence completely characterized length path joining dependent vertex vertex conditioned also apply result postulate rule model selection polytree model rule apply mutual information gaussian random vector well
Statistics,experiment hierarchical bayesian record linkage,record linkage rl exact file matching goal identify link entity information two file rl important activity area including counting population enhancing survey frame data conducting epidemiological followup study rl challenging file large accurate personal identification id number present file unit information recorded error without unique id number one must rely comparison name address date information find link latent class model used automatically score value information determining match status data fitting model come comparison made within group unit pas initial file blocking requirement data distribution vary across block article examines use prior information hierarchical latent class model context rl
Statistics,independent component analysis via energybased kernelbased mutual dependence measure,apply distancebased jin matteson kernelbased pfister et al mutual dependence measure independent component analysis ica generalize dcovica matteson tsay mdmica minimizing empirical dependence measure objective function deflation parallel manner solving minimization problem introduce latin hypercube sampling lh mckay et al global optimization method bayesian optimization bo mockus improve initialization newtontype local optimization method performance mdmica evaluated various simulation study image data example ica model correct mdmica achieves competitive result compared existing approach ica model misspecified estimated independent component le mutually dependent observed component using mdmica prone even mutually dependent observed component using approach
Statistics,testing conditional mean independence covariates martingale difference divergence,crucial problem statistic decide whether additional variable needed regression model propose new multivariate test investigate conditional mean independence given x conditioning known effect z ie e yx z e yz assuming e yz z linearly related reformulate equivalent notion conditional mean independence transformation approximated practice apply martingale difference divergence shao zhang measure conditional mean dependence show estimation error approximation negligible impact asymptotic distribution test statistic regularity assumption implementation test demonstrated simulation financial data example
Statistics,objective bayesian analysis change point problem,paper present lossbased approach change point analysis particular look problem two perspective first focus definition prior number change point known priori second contribution aim estimate number change point using lossbased approach recently introduced literature latter considers change point estimation model selection exercise show performance proposed approach simulated data real data set
Statistics,generalizing distance covariance measure test multivariate mutual dependence,propose three measure mutual dependence multiple random vector measure zero random vector mutually independent first measure generalizes distance covariance pairwise dependence mutual dependence two measure sum squared distance covariance measure share similar property asymptotic distribution distance covariance capture nonlinear nonmonotone mutual dependence random vector inspired complete incomplete vstatistics define empirical measure simplified empirical measure tradeoff complexity power testing mutual independence implementation test demonstrated simulation result real data example
Statistics,optimization testing linear nongaussian component analysis,independent component analysis ica decomposes multivariate data mutually independent component ic ica model subject constraint one component gaussian required model identifiability linear nongaussian component analysis lngca generalizes ica model linear latent factor model number nongaussian component signal gaussian component noise observation linear combination independent component although individual gaussian component identifiable gaussian subspace identifiable introduce estimator along optimization approach nongaussian gaussian component estimated simultaneously maximizing discrepancy nongaussian component gaussianity minimizing discrepancy gaussian component gaussianity number nongaussian component unknown develop statistical test determine based resampling discrepancy estimated component variety simulation study demonstrate improvement estimator competing estimator illustrate effectiveness test determine number nongaussian component apply method real data example demonstrate practical value
Statistics,largescale model selection misspecification,model selection crucial highdimensional learning inference contemporary big data application pinpointing best set covariates among sequence candidate interpretable model existing work assumes implicitly model correctly specified fixed dimensionality yet feature model misspecification high dimensionality prevalent practice paper exploit framework model selection principle misspecified model originated lv liu investigate asymptotic expansion bayesian principle model selection setting highdimensional misspecified model natural choice prior probability encourages interpretability incorporates kullbackleibler divergence suggest highdimensional generalized bayesian information criterion prior probability hgbicp largescale model selection misspecification new information criterion characterizes impact model misspecification high dimensionality model selection establish consistency covariance contrast matrix estimation model selection consistency hgbicp ultrahigh dimension mild regularity condition advantage new method supported numerical study
Statistics,snap semismooth newton algorithm pathwise optimization optimal local convergence rate oracle property,propose semismooth newton algorithm pathwise optimization snap lasso enet sparse highdimensional linear regression snap derived suitable formulation kkt condition based newton derivative solves semismooth kkt equation efficiently actively continuously seeking support regression coefficient along solution path warm start knot path snap converges locally superlinearly enet criterion achieves optimal local convergence rate lasso criterion ie snap converges one step cost two matrixvector multiplication per iteration certain regularity condition design matrix minimum magnitude nonzero element target regression coefficient show snap hit solution sign regression coefficient achieves sharp estimation error bound finite step high probability computational complexity snap shown lars coordinate descent algorithm per iteration simulation study real data analysis support theoretical result demonstrate snap faster accurate lars coordinate descent algorithm
Statistics,statistical inference meanfield variational bayes,conduct nonasymptotic analysis meanfield variational inference approximating posterior distribution complex bayesian model may involve latent variable show meanfield approximation posterior wellapproximated relative kullbackleibler divergence discrepancy measure normal distribution whose center maximum likelihood estimator mle particular result imply center meanfield approximation match mle higherorder term essentially loss efficiency using point estimator parameter regular parametric model latent variable also propose new class variational weighted likelihood bootstrap vwlb method quantifying uncertainty meanfield variational inference proposed vwlb viewed new sampling scheme produce independent sample approximating posterior comparing traditional sampling algorithm markov chain monte carlo vwlb implemented parallel free tuning
Statistics,permutation inference canonical correlation analysis,canonical correlation analysis cca become key tool population neuroimaging allowing investigation association many imaging nonimaging measurement variable often source variability direct interest previous work used cca residual model remove effect proceeded directly permutation inference show simple permutation test lead inflated error rate reason residualisation introduces dependency among observation violate exchangeability assumption even absence nuisance variable however simple permutation test cca also lead excess error rate canonical correlation first reason simple permutation scheme ignore variability already explained previous canonical variable propose solution problem case nuisance variable show transforming residual lower dimensional basis exchangeability hold result valid permutation test general case without nuisance variable propose estimating canonical correlation stepwise manner removing iteration variance already explained dealing different number variable side also discus address multiplicity test proposing admissible test conservative provide complete algorithm permutation inference cca
Statistics,continuous shrinkage prior revisited collapsing behavior remedy,modern genomic study increasingly focused identifying gene clinically associated health response commonly used bayesian shrinkage prior designed primarily detect handful signal dimension predictor high article investigate performance popular continuous shrinkage prior presence relatively large number true signal draw attention undesirable phenomenon posterior mean rendered close null vector caused sharp underestimation globalscale parameter phenomenon triggered absence tailindex controlling mechanism bayesian shrinkage prior provide remedy developing globallocaltail shrinkage prior automatically learn tailindex provide accurate inference even presence moderately large number signal collapsing behavior horseshoe remedy exemplified numerical example two gene expression datasets
Statistics,new correlation coefficient aggregating nonstrict incomplete ranking,introduce correlation coefficient designed deal variety ranking format including containing nonstrict ie withties incomplete ie unknown preference correlation coefficient designed enforce neutral treatment incompleteness whereby assumption made individual preference involving unranked object new measure regarded generalization seminal kendall tau correlation coefficient proven satisfy set metriclike axiom equivalent recently developed ranking distance function associated kemeny aggregation effort unify enhance robust ranking methodology work prof equivalence additional distance correlationcoefficient pairing space nonstrict incomplete ranking connection induce new exact optimization methodology specialized branch bound algorithm exact integer programming formulation moreover bridging complementary theory reinforces singular suitability featured correlation coefficient solve general consensus ranking problem latter premise bolstered accompanying set experiment random instance generated via herein developed sampling technique connected classic mallow distribution ranking data associated experiment branch bound algorithm demonstrate data becomes noisier featured correlation coefficient yield relatively fewer alternative optimal solution aggregate ranking tend closer underlying ground truth shared majority
Statistics,consistency plugin functional predictor ornsteinuhlenbeck process hilbert banach space,new result functional prediction ornsteinuhlenbeck process autoregressive hilbertvalued banachvalued framework derived specifically consistency maximum likelihood estimator autocorrelation operator associated plugin predictor obtained framework
Statistics,asymptotic property componentwise arh plugin predictor,paper present new result prediction linear process function space autoregressive hilbertian process framework order one arh process framework adopted componentwise estimator autocorrelation operator formulated momentbased estimation diagonal coefficient respect orthogonal eigenvectors autocovariance operator assumed known meansquare convergence theoretical autocorrelation operator space hilbertschmidt operator proved consistency follows space associated arh plugin predictor mean absolute convergence corresponding conditional expectation considered hilbert space obtained hence consistency space also hold simulation study undertaken illustrate finitelarge sample behavior formulated componentwise estimator predictor performance presented approach compared alternative approach previous current arh framework literature including case unknown eigenvectors
Statistics,distributionally robust optimization correlated data vector autoregressive process,present distributionally robust formulation stochastic optimization problem noniid vector autoregressive data use wasserstein distance define robustness space distribution show using duality theory problem equivalent finite convexconcave saddle point problem performance method demonstrated synthetic real data
Statistics,stickbreaking process exchangeable length variable,investigate general class stickbreaking process exchangeable length variable generalize wellknown bayesian nonparametric prior unexplored direction give condition assure respective specie sampling process discrete almost surely corresponding prior full support rich subclass find probability stickbreaking weight decreasingly ordered general formula distribution latent allocation variable derived mcmc algorithm proposed density estimation purpose
Statistics,empirical study stochastic variational algorithm beta bernoulli process,stochastic variational inference svi emerging promising candidate scaling inference bayesian probabilistic model large datasets however performance method assessed primarily context bayesian topic model particularly latent dirichlet allocation lda deriving several new algorithm using synthetic image genomic datasets investigate whether understanding gleaned lda applies setting sparse latent factor model specifically beta process factor analysis bpfa demonstrate big picture consistent using gibbs sampling within svi maintain certain posterior dependency extremely effective however find different posterior dependency important bpfa relative lda particularly approximation able model intralocal variable dependence perform best
Statistics,bayesian evidence model selection,paper review concept bayesian evidence bayes factor also known log odds ratio application model selection theory presented along discussion analytic approximate numerical technique specific attention paid laplace approximation variational bayes importance sampling thermodynamic integration nested sampling recent variant analogy statistical physic many technique originate discussed order provide reader deeper insight may lead new technique utility bayesian model testing domain science demonstrated presenting four specific practical example considered within context signal processing area signal detection sensor characterization scientific model selection molecular force characterization
Statistics,iteratively reweighted adaptive lasso conditional heteroscedastic time series application ararch type process,shrinkage algorithm great importance almost every area statistic due increasing impact big data especially time series analysis benefit efficient rapid estimation technique lasso however currently lasso type estimator autoregressive time series model still focus model homoscedastic residual therefore iteratively reweighted adaptive lasso algorithm estimation time series model conditional heteroscedasticity presented highdimensional setting asymptotic behaviour resulting estimator analysed found proposed estimation procedure performs substantially better homoscedastic counterpart special case algorithm suitable compute estimated multivariate ararch type model efficiently extension model like periodic ararch threshold ararch armagarch discussed finally different simulation result application electricity market data return metal price shown
Statistics,computationally efficient algorithm statistical image processing implementation r,series earlier paper subject proposed novel statistical hypothesis testing method detection object noisy image method us result percolation theory random graph theory developed algorithm allowed detect object unknown shape presence nonparametric noise unknown level unknown distribution boundary shape constraint imposed object weak bulk condition object interior required algorithm linear complexity exponential accuracy present paper describe implementation nonparametric hypothesis testing method provide program used statistical experiment image processing program written statistical programming language r
Statistics,conformal knn anomaly detector univariate data stream,anomaly timeseries data give essential often actionable information many application paper consider modelfree anomaly detection method univariate timeseries adapts nonstationarity data stream provides probabilistic abnormality score based conformal prediction paradigm despite simplicity method performs par complex predictionbased model numenta anomaly detection benchmark yahoo dataset
Statistics,delayed acceptance abcsmc,approximate bayesian computation abc established technique statistical inference used case likelihood function computationally expensive available relies use amodel specified form asimulator approximates likelihood aparameter value theta simulating auxiliary data set x evaluating distance x true data however abc computationally feasible case using simulator theta expensive paper investigates situation case acheap approximate simulator available approach employ delayed acceptance markov chain monte carlo mcmc within abc sequential monte carlo smc sampler order afirst stage kernel use cheap simulator rule part parameter space worth exploring true simulator run second stage kernel areasonable chance accepting proposed value theta show approach used quite automatically tuning parameter application stochastic differential equation model latent doubly intractable distribution presented
Statistics,modelbased clustering timeevolving network temporal exponentialfamily random graph model,dynamic network general language describing timeevolving complex system discrete time network model provide emerging statistical technique various application fundamental research question detect community structure timeevolving network however due significant computational challenge difficulty modeling community timeevolving network little progress current literature effectively find community timeevolving network work propose novel modelbased clustering framework timeevolving network based discrete time exponentialfamily random graph model choose number community use conditional likelihood construct effective model selection criterion furthermore propose efficient variational expectationmaximization em algorithm find approximate maximum likelihood estimate network parameter mixing proportion using variational method minorizationmaximization mm technique method appealing scalability largescale timeevolving network power method demonstrated simulation study empirical application international trade network collaboration network large american research university
Statistics,dirichlet process mixture model discrete choice,present mixed multinomial logit mnl model leverage truncated stickbreaking process representation dirichlet process flexible nonparametric mixing distribution proposed model dirichlet process mixture model accommodates discrete representation heterogeneity like latent class mnl model yet unlike latent class mnl model proposed discrete choice model require analyst fix number mixture component prior estimation complexity discrete mixing distribution inferred evidence posterior inference proposed dirichlet process mixture model discrete choice derive expectation maximisation algorithm simulation study demonstrate proposed model framework flexibly capture differentlyshaped taste parameter distribution furthermore empirically validate model framework case study motorist route choice preference find proposed dirichlet process mixture model discrete choice outperforms latent class mnl model mixed mnl model common parametric mixing distribution term insample fit outofsample predictive ability compared extant modelling approach proposed discrete choice model substantially abbreviates specification search relies le restrictive parametric assumption require analyst specify complexity discrete mixing distribution prior estimation
Statistics,smoothing interpolating noisy gps data smoothing spline,comprehensive methodology provided smoothing noisy irregularly sampled data nongaussian noise using smoothing spline demonstrate spline order tension parameter chosen priori physical reasoning also show allow nongaussian noise outlier typical gps signal demonstrate effectiveness method gps trajectory data obtained oceanographic floating instrument known drifter
Statistics,convolutional poisson gamma belief network,text analysis one often resort lossy representation either completely ignores word order embeds word lowdimensional dense feature vector paper propose convolutional poisson factor analysis cpfa directly operates lossless representation process word document sequence highdimensional onehot vector boost performance propose convolutional poisson gamma belief network cpgbn couple cpfa gamma belief network via novel probabilistic pooling layer cpfa form word phrase capture specific phraselevel topic cpgbn build hierarchy increasingly general phraselevel topic efficient inference develop gibbs sampler weibull distribution based convolutional variational autoencoder experimental result demonstrate cpgbn extract highquality text latent representation capture word order information hence leveraged building block enrich wide variety existing latent variable model ignore word order
Statistics,matrix free likelihood method exploratory factor analysis highdimensional gaussian data,paper proposes novel profile likelihood method estimating covariance parameter exploratory factor analysis highdimensional gaussian datasets fewer observation number variable implicitly restarted lanczos algorithm limitedmemory quasinewton method implemented develop matrixfree framework likelihood maximization simulation result show method substantially faster expectationmaximization solution without sacrificing accuracy method applied fit factor model data suicide attempter suicide ideators control group
Statistics,compromisefree bayesian neural network,conduct thorough analysis relationship outofsample performance bayesian evidence marginal likelihood bayesian neural network bnns well looking performance ensemble bnns using boston housing dataset using stateoftheart nested sampling numerically sample full nongaussian multimodal network posterior obtain numerical estimate bayesian evidence considering network model trainable parameter network zero four hidden layer either tanh relu activation function without hierarchical prior ensemble bnns obtained determining posterior distribution network posterior sample individual bnns reweighted associated bayesian evidence value good correlation outofsample performance evidence well remarkable symmetry evidence versus model size outofsample performance versus model size plane network relu activation function consistently higher evidence tanh function reflected outofsample performance ensembling architecture act improve performance relative individual bnns
Statistics,efficient k mode algorithm clustering categorical datasets,mining cluster datasets important endeavor many application k mean algorithm popular efficient distributionfree approach clustering numericalvalued data applied categoricalvalued observation k mode algorithm address lacuna taking k mean objective function replacing dissimilarity measure using mode instead mean modified objective function unlike many clustering algorithm k mode k mean scalable require calculation pairwise dissimilarity provide fast computationally efficient implementation k mode otqt prove find superior clustering existing algorithm also examine five initialization method three type k selection method many novel appropriate k mode examining performance real simulated datasets show simple random initialization best intializer novel k selection method accurate two method adapted k mean new otqt algorithm accurate almost always faster existing algorithm
Statistics,graph gamma process generalized linear dynamical system,introduce graph gamma process ggp linear dynamical system model realvalued multivariate time series temporal pattern discovery latent representation model used decompose time series parsimonious set multivariate subsequence subsequence different data dimension often share similar temporal pattern may exhibit distinct magnitude hence allowing superposition subsequence exhibit diverse behavior different data dimension generalize proposed model replacing gaussian observation layer negative binomial distribution model multivariate count time series generated proposed ggp infinite dimensional directed sparse random graph constructed taking logical operation countably infinite binary adjacency matrix share set countably infinite node adjacency matrix associated weight indicate activation strength place finite number edge finite subset node belonging node community use generated random graph whose number nonzerodegree node finite define sparsity pattern dimension latent state transition matrix generalized linear dynamical system activation strength node community relative overall activation strength used extract multivariate subsequence revealing data pattern captured corresponding community synthetic realworld time series proposed nonparametric bayesian dynamic model initialized random consistently exhibit good predictive performance comparison variety baseline model revealing interpretable latent state transition pattern decomposing time series distinctly behaved subsequence
Statistics,big learning bayesian method,explosive growth data availability cheap computing resource sparked increasing interest big learning emerging subfield study scalable machine learning algorithm system application big data bayesian method represent one important class statistic method machine learning substantial recent development adaptive flexible scalable bayesian learning article provides survey recent advance big learning bayesian method termed big bayesian learning including nonparametric bayesian method adaptively inferring model complexity regularized bayesian inference improving flexibility via posterior regularization scalable algorithm system based stochastic subsampling distributed computing dealing largescale application
Statistics,modelbased clustering nonparametric weighted network application water pollution analysis,water pollution major global environmental problem pose great environmental risk public health biological diversity work motivated assessing potential environmental threat coal mining increased sulfate concentration river network belong simple parametric distribution however existing network model mainly focus binary discrete network weighted network known parametric weight distribution propose principled nonparametric weighted network model based exponentialfamily random graph model local likelihood estimation study modelbased clustering application largescale water pollution network analysis require parametric distribution assumption network weight proposed method greatly extends methodology applicability statistical network model furthermore scalable large complex network largescale environmental study power proposed method demonstrated simulation study real application sulfate pollution network analysis ohio watershed located pennsylvania united state
Statistics,estimator archimedean copula high dimension,performance known new parametric estimator archimedean copula investigated special focus large dimension numerical difficulty particular methodofmomentslike estimator based pairwise kendall tau multivariate extension blomqvist beta minimum distance estimator maximumlikelihood estimator simulated maximumlikelihood estimator maximumlikelihood estimator based copula diagonal studied performance compared largescale simulation study known unknown margin pseudoobservations small high dimension small large dependency various different archimedean family sample size high dimension one hundred considered first time computational problem arising large dimension addressed detail method implemented open source r package pkg copula thus easily accessed studied
Statistics,data science course undergraduate thinking data,data science emerging interdisciplinary field combine element mathematics statistic computer science knowledge particular application domain purpose extracting meaningful information increasingly sophisticated array data available many setting data tend nontraditional sense often live large complex andor messy first course statistic undergraduate level typically introduces student variety technique analyze small neat clean data set however whether pursue formal training statistic many student end working data considerably complex need facility statistical computing technique importantly student require framework thinking structurally data describe undergraduate course liberal art environment provides student tool necessary apply data science course emphasizes modern practical useful skill cover full data analysis spectrum asking interesting question acquiring managing manipulating processing querying analyzing visualizing data well communicating finding written graphical oral form
Statistics,setting stage data science integration data management skill introductory second course statistic,many argued statistic student need additional facility express statistical computation introducing student commonplace tool data management visualization reproducible analysis data science applying realworld scenario prepare think statistically era increasingly big data imperative student develop datarelated capacity beginning introductory course believe integration precursor data science curriculaearly oftenwill help statistician part dialogue regarding big data big question
Statistics,teaching precursor data science introductory second course statistic,statistic student need develop capacity make sense staggering amount information collected increasingly datacentered world data science important part modern statistic introductory second statistic course often neglect fact paper discus way provide practical foundation student learn compute data defined nolan temple lang well develop data habit mind finzer describe introductory second course integrate two key precursor data science use reproducible analysis tool access large database introducing student commonplace tool data management visualization reproducible analysis data science applying realworld scenario prepare think statistically era big data
Statistics,efficient forward simulation fisherwright population stochastic population size neutral single step mutation haplotype,population genetics forensic genetics important know haplotype distributed population simulation population dynamic help facilitating research distribution haplotype forensic genetics haplotype example consist lineage marker short tandem repeat locus chromosome ystr dominating model describing population dynamic simple yet powerful fisherwright model describe efficient algorithm exact forward simulation exact fisherwright population approximative coalescent model efficiency come convenient data structure changing traditional view individual haplotype algorithm implemented opensource r package fwsim able simulate large population focus haploid model assume stochastic population size flexible growth specification selection neutral single step mutation process selfreproducing individual assumption make algorithm ideal studying lineage marker ystr
Statistics,analytic evaluation fractional moment quasistationary distribution shiryaev martingale interval,consider quasistationary distribution classical shiryaev diffusion restricted interval absorption fixed derive analytically closedform formula distribution fractional moment em arbitrary given order sinmathbb r formula consistent previously found polunchenko pepelyshev case sinmathbb n also show virtue formula th fractional moment quasistationary distribution becomes exponential distribution mean limit atoinfty limiting exponential distribution stationary distribution reciprocal shiryaev diffusion
Statistics,montecarlomeasurementsjl nonlinear propagation arbitrary multivariate distribution mean method overloading,manuscript outline software package facilitates working probability distribution mean montecarlo method way allows propagation multivariate probability distribution arbitrary function provide emph type represents probability distribution internal vector unweighted sample texttt particle subtype texttt real number behaves like regular real number calculation mean method overloading make software easy work present minimal friction user highlight design facilitates optimal usage simd instruction showcase package uncertainty propagation offtheshelf ode solver well robust probabilistic optimization automatic differentiation
Statistics,coronavirus arima based timeseries analysis forecast near future,novel coronavirus currently major worldwide threat infected million people globally leading hundredthousands death grave circumstance important predict future infected case support prevention disease aid healthcare service preparation following notion developed model employed forecasting future case india study indicates ascending trend case coming day time series analysis also present exponential increase number case supposed present prediction model assist government medical personnel prepared upcoming condition readiness healthcare system
Statistics,model selection undirected graphical model elastic net,structure learning random field attracted considerable attention due difficulty importance area remote sensing computational biology natural language processing protein network social network analysis consider problem estimating probabilistic graph structure associated gaussian markov random field gmrf ising model potts model extending previous work regularized neighborhood estimation include elastic net penalty additionally show numerical evidence edge density play role graph recovery process finally introduce novel method augmenting neighborhood estimation leveraging pairwise neighborhood union estimate
Statistics,contemporary overview probabilistic latent variable model,paper provide conceptual overview latent variable model within probabilistic modeling framework overview emphasizes compositional nature interconnectedness seemingly disparate model commonly encountered statistical practice
Statistics,greater data science baccalaureate institution,donoho jcgs press paper spirited call action statistician point losing ground field data science refusing accept data science domain least domain becoming distinctly defined call writing john tukey bill cleveland leo breiman among others remind u statistician dealing data science year encourages acceptance direction field also ensuring statistic tightly integrated faculty baccalaureate institution growth undergraduate statistic program dramatic keen ensure statistic place data science data science education paper donoho primarily focused graduate education undergraduate institution considering many question
Statistics,fuzzy roc,fuzzy roc extends receiver operating curve roc visualization situation data point falling indeterminacy region classified address two challenge definition sensitivity specificity bound indeterminacy visual summarization large number possibility arising different choice indeterminacy zone
Statistics,lassopenalized bic mixture model selection,efficacy familybased approach mixture modelbased clustering classification depends selection parsimonious model current wisdom suggests bayesian information criterion bic mixture model selection however bic wellknown limitation including tendency overestimate number component well proclivity often drastically underestimating number component higher dimension former problem might soluble merging component latter impossible mitigate clustering classification application paper lassopenalized bic lpbic introduced overcome problem approach illustrated based application extension mixture factor analyzer lpbic used select number component number latent factor lpbic shown match outperform bic several situation
Statistics,better subset regression,find efficient screening method high dimensional linear regression model paper study relationship model fitting screening performance sparsity assumption show subset includes true submodel always yield smaller residual sum square ie better model fitting general asymptotic setting indicates screening important variable could follow better fitting better screening rule ie pick better subset better model fitting seek better subset consider optimization problem associated best subset regression em algorithm called orthogonalizing subset screening accelerating version proposed searching best subset although two algorithm guarantee subset yield best monotonicity property make subset better model fitting initial subset generated popular screening method thus subset better screening performance asymptotically simulation result show method competitive high dimensional variable screening even finite sample size
Statistics,mahalanobis distance functional data application classification,paper present general notion mahalanobis distance functional data extends classical multivariate concept situation observed data point belonging curve generated stochastic process precisely new semidistance functional observation generalize usual mahalanobis distance multivariate datasets introduced development us regularized square root inverse operator hilbert space main characteristic functional mahalanobis semidistance shown afterwards new version several well known functional classification procedure developed using mahalanobis distance functional data measure proximity functional observation performance several well known functional classification procedure compared method used conjunction mahalanobis distance functional data positive result monte carlo study analysis two real data example
Statistics,learning local dependence ordered data,many application data come natural ordering ordering often induce local dependence among nearby variable however complex data width dependence may vary making simple assumption constant neighborhood size unrealistic propose framework learning local dependence based estimating inverse cholesky factor covariance matrix penalized maximum likelihood estimation matrix yield simple regression interpretation local dependence variable predicted neighbor proposed method involves solving convex penalized gaussian likelihood problem hierarchical group lasso penalty problem decomposes independent subproblems solved efficiently parallel using firstorder method method yield sparse symmetric positive definite estimator precision matrix encoding gaussian graphical model derive theoretical result found existing method attaining structure particular condition signed support recovery estimation consistency rate multiple norm mild regression problem empirical result show method performing favorably compared existing method apply method genomic data flexibly model linkage disequilibrium method also applied improve performance discriminant analysis sound recording classification
Statistics,debiased whittle likelihood,whittle likelihood widely used computationally efficient pseudolikelihood however known produce biased parameter estimate large class model propose method debiasing whittle estimate secondorder stationary stochastic process debiased whittle likelihood computed mathcal nlog n operation standard approach demonstrate superior performance method simulation study application largescale oceanographic dataset case debiased approach reduces bias two order magnitude achieving estimate close exact maximum likelihood fraction computational cost prove method yield estimate consistent optimal convergence rate n weaker assumption standard theory require power spectral density continuous frequency describe method easily combined standard method bias reduction tapering differencing reduce bias parameter estimate
Statistics,graphguided banding covariance matrix,regularization become primary tool developing reliable estimator covariance matrix highdimensional setting curb curse dimensionality numerous method assume population covariance inverse covariance matrix sparse making particular structural assumption desired pattern sparsity highlyrelated yet complementary literature study specific setting measured variable known ordering case banded population matrix often assumed banded approach conceptually computationally easier asking patternless sparsity applicable specific situation data measured time onedimensional space work proposes generalization notion bandedness greatly expands range problem banded estimator apply develop convex regularizers occupying broad middle ground former approach patternless sparsity latter reliance known ordering framework defines bandedness respect known graph measured variable graph available diverse situation provide theoretical computational applied treatment two new estimator r package called ggb implement new method
Statistics,two provably consistent divide conquer clustering algorithm large network,article advance divideandconquer strategy solving community detection problem network propose two algorithm perform clustering number small subgraphs finally patch result single clustering main advantage algorithm bring significantly computational cost traditional algorithm including spectral clustering semidefinite program modularity based method likelihood based method etc without losing accuracy even improving accuracy time algorithm also nature parallelizable thus exploiting fact traditional algorithm accurate corresponding optimization problem much simpler small problem divideandconquer method provide omnibus recipe scaling traditional algorithm large network prove consistency algorithm various subgraph selection procedure perform extensive simulation realdata analysis understand advantage divideandconquer approach various setting
Statistics,valid inference corrected outlier removal,ordinary least square ols estimation linear regression model wellknown highly sensitive outlier common practice identify remove outlier looking data fit ols form confidence interval pvalues remaining data original data collected standard detectandforget approach shown problematic paper highlight fact lead invalid inference show recently developed tool selective inference used properly account outlier detection removal inferential procedure apply general class outlier removal procedure includes several commonly used approach conduct simulation corroborate theoretical result apply method three real data set illustrate inferential result differ traditional detectandforget strategy companion r package outference implement new procedure interface match function commonly used inference lm r
Statistics,structural risk minimization c mathbb r regression,one mean fitting function highdimensional data providing smoothness constraint recently following smooth function approximation problem proposed given finite set e subset mathbb r function f e rightarrow mathbb r interpolate given information function widehat f dot c mathbb r class firstorder differentiable function lipschitz gradient widehat f f e value mathrm lip nabla widehat f minimal algorithm provided construct approximating function widehat f estimate optimal lipschitz constant mathrm lip nabla widehat f noiseless setting address statistical aspect reconstructing approximating function widehat f closelyrelated class c mathbb r given sample noisy data observe independent identically distributed sample f xi e xi noise term set e subset mathbb r fixed known obtain uniform bound relating empirical risk true risk class mathcal f widetilde f c mathbb r mid mathrm lip nabla f leq widetilde quantity widetilde grows number sample rate governed metric entropy class c mathbb r finally provide implementation using vaidya algorithm supporting result via numerical experiment simulated data
Statistics,classification pointofview conditional kendall tau,show problem estimating conditional kendall tau rewritten classification task conditional kendall tau conditional dependence parameter characteristic given pair random variable goal predict whether pair concordant value discordant value conditionally covariates prove consistency asymptotic normality family penalized approximate maximum likelihood estimator including equivalent logit probit regression framework detail specific algorithm adapting usual machine learning technique including nearest neighbor decision tree random forest neural network setting estimation conditional kendall tau finite sample property estimator sensitivity component datagenerating process assessed simulation study finally apply estimator dataset european stock index
Statistics,subset multivariate collective point anomaly detection,recent year growing interest identifying anomalous structure within multivariate data stream consider problem detecting collective anomaly corresponding interval one data stream behaves anomalously first develop test single collective anomaly power simultaneously detect anomaly either rare affecting data stream common show detect multiple anomaly way computationally efficient avoids approximation inherent binary segmentationlike approach approach call mvcapa shown consistently estimate number location collective anomaly property previously shown competing method mvcapa made robust point anomaly allow anomaly imperfectly aligned show practical usefulness allowing imperfect alignment resulting increase power detect region copy number variation
Statistics,iterative algorithm discrete structure recovery,propose general modeling algorithmic framework discrete structure recovery applied wide range problem framework able study recovery clustering label rank player sign regression coefficient unified perspective simple iterative algorithm proposed discrete structure recovery generalizes method including lloyd algorithm iterative feature matching algorithm linear convergence result proposed algorithm established paper appropriate abstract condition stochastic error initialization illustrate general theory applying three representative problem clustering gaussian mixture model approximate ranking sign recovery compressed sensing show minimax rate achieved case
Statistics,modelling highdimensional categorical data using nonconvex fusion penalty,propose method estimation highdimensional linear model nominal categorical data estimator called scope fuse level together making corresponding coefficient exactly equal achieved using minimax concave penalty difference order statistic coefficient categorical variable thereby clustering coefficient provide algorithm exact efficient computation global minimum resulting nonconvex objective case single variable potentially many level use within block coordinate descent procedure multivariate case show oracle least square solution exploit unknown level fusion limit point coordinate descent high probability provided true level certain minimum separation condition known minimal univariate case demonstrate favourable performance scope across range real simulated datasets r package catreg implementing scope linear model also version logistic regression available cran
Statistics,optimal thinning mcmc output,use heuristic ass convergence compress output markov chain monte carlo suboptimal term empirical approximation produced typically number initial state attributed burn removed whilst remainder chain thinned compression also required paper consider problem retrospectively selecting subset state fixed cardinality sample path approximation provided empirical distribution close optimal novel method proposed based greedy minimisation kernel stein discrepancy suitable problem heavy compression required theoretical result guarantee consistency method effectiveness demonstrated challenging context parameter inference ordinary differential equation software available stein thinning package python matlab
Statistics,nonsparse pca high dimension via cone projected power iteration,paper propose cone projected power iteration algorithm recover principal eigenvector noisy positive semidefinite matrix true principal eigenvector assumed belong convex cone proposed algorithm fast tractable error specifically method achieves polynomial time complexity certain convex cone equipped fast projection monotone cone attains small error noisy matrix small conerestricted operator norm supplement result minimax lower bound error spiked covariance model numerical experiment simulated real data show method achieves shorter run time smaller error comparison ordinary power iteration sparse principal component analysis algorithm principal eigenvector convex cone
Statistics,nonparametric independence screening sparse ultrahigh dimensional additive model,variable screening procedure via correlation learning proposed fan lv reduce dimensionality sparse ultrahigh dimensional model even true model linear marginal regression highly nonlinear address issue extend correlation learning marginal nonparametric learning nonparametric independence screening called ni specific member sure independence screening several closely related variable screening procedure proposed nonparametric additive model shown mild technical condition proposed independence screening method enjoy sure screening property extent dimensionality reduced independence screening also explicitly quantified methodological extension iterative nonparametric independence screening inis also proposed enhance finite sample performance fitting sparse additive model simulation result real data analysis demonstrate proposed procedure work well moderate sample size large dimension performs better competing method
Statistics,penalty shrinkage preliminary test estimator full model hypothesis,paper considers multiple regression model compare full model hypothesis analytically well simulation performance characteristic popular penalty estimator ridge regression lasso adaptive lasso scad elastic net versus least square estimator restricted estimator preliminary test estimator steintype estimator dimension parameter space smaller sample space dimension find rr uniformly dominates lse pte se prse lasso alasso scad en uniformly dominates lse observed neither penalty estimator steintype estimator dominate one another
Statistics,proximal robbinsmonro method,need parameter estimation massive datasets reinvigorated interest stochastic optimization iterative estimation procedure stochastic approximation forefront recent development yield procedure simple general fast however standard stochastic approximation often numerically unstable deterministic optimization hand increasingly us proximal update achieve numerical stability principled manner theoretical gap thus emerged standard stochastic approximation subsumed framework robbins monro framework stochastic approximation proximal update paper conceptualize proximal version classical robbinsmonro procedure theoretical analysis demonstrates proposed procedure important stability benefit classical robbinsmonro procedure retains best known convergence rate exact implementation proximal robbinsmonro procedure challenging show approximate implementation lead procedure easy implement still dominate classical procedure achieving numerical stability practically without tradeoff moreover approximate proximal robbinsmonro procedure applied even objective calculated analytically generalize stochastic proximal procedure currently use
Statistics,randomized maximumcontrast selection subagging largescale regression,introduce general method sparse largescale variable selection largescale regression setting number parameter number sample extremely large proposed method based careful combination penalized estimator applied random projection sample space lowdimensional space one special case study detail random projection divided nonoverlapping block consisting small portion original data within block select projection yielding smallest outofsample error random ensemble estimator aggregate result according new maximalcontrast voting scheme determine final selected set theoretical result illuminate effect performance increasing number nonoverlapping block moreover demonstrate statistical optimality retained along computational speedup proposed method achieves minimax rate approximate recovery estimator using full set sample furthermore theoretical result allow number subsamples grow subsample size require irrepresentable condition estimator also compared empirically several popular highdimensional estimator via extensive simulation study reveals excellent finitesample performance
Statistics,projection pursuit framework testing general highdimensional hypothesis,article develops framework testing general hypothesis highdimensional model number variable may far exceed number observation existing literature considered le handful hypothesis testing individual coordinate model parameter however problem testing general complex hypothesis remains widely open propose new inference method developed around hypothesis adaptive projection pursuit framework solves testing problem general case proposed inference centered around new class estimator defined projection initial guess unknown onto space defined null projection automatically take account structure null hypothesis allows u study formal inference number longstanding problem example directly conduct inference sparsity level model parameter minimum signal strength especially significant given fact former fundamental condition underlying theoretical development highdimensional statistic latter key condition used establish variable selection property moreover proposed method asymptotically exact satisfactory power property testing general functionals highdimensional parameter simulation study lend support theoretical claim additionally show excellent finitesample size power property proposed test
Statistics,convex banding covariance matrix,introduce new sparse estimator covariance matrix highdimensional model variable known ordering estimator solution convex optimization problem equivalently expressed estimator taper sample covariance matrix toeplitz sparselybanded dataadaptive matrix result adaptivity convex banding estimator enjoys theoretical optimality property attained previous banding tapered estimator particular convex banding estimator minimax rate adaptive frobenius operator norm log factor commonlystudied class covariance matrix general class furthermore correctly recovers bandwidth true covariance exactly banded convex formulation admits simple efficient algorithm empirical study demonstrate practical effectiveness illustrate exactlybanded estimator work well even true covariance matrix close banded matrix confirming theoretical result method compare favorably existing method term accuracy speed illustrate practical merit convex banding estimator showing used improve performance discriminant analysis classifying sound recording
Statistics,hierarchical sparse modeling choice two group lasso formulation,demanding sparsity estimated model become routine practice statistic many situation wish require sparsity pattern attained honor certain problemspecific constraint hierarchical sparse modeling hsm refers situation constraint specify one set parameter set zero whenever another set zero recent year numerous paper developed convex regularizers form sparsity structure arises many area statistic including interaction modeling time series analysis covariance estimation paper observe method fall two framework group lasso gl latent overlapping group lasso log systematically compared context hsm purpose paper provide sidebyside comparison two framework hsm term statistical property computational efficiency call special attention gl aggressive shrinkage parameter deep hierarchy property shared log term computation introduce finitestep algorithm exactly solves proximal operator log certain simple hsm structure later exploit develop novel pathbased block coordinate descent scheme general hsm structure algorithm greatly improve computational performance log finally compare two method context covariance estimation introduce new sparselybanded estimator using log show achieves statistical advantage existing glbased method simpler express efficient compute
Statistics,piecewise deterministic markov process continuoustime monte carlo,recently exciting development monte carlo method development new mcmc sequential monte carlo smc algorithm based continuoustime rather discretetime markov process led fundamentally new monte carlo algorithm used sample say posterior distribution interestingly continuoustime algorithm seem particularly well suited bayesian analysis bigdata setting need access small subset data point iteration yet still guaranteed target true posterior distribution whilst continuoustime mcmc smc method developed independently show related fact involve simulating piecewise deterministic markov process furthermore show method developed date specific case potentially much wider class continuoustime monte carlo algorithm give informal introduction piecewise deterministic markov process covering aspect relevant new monte carlo algorithm view making development new continuoustime monte carlo accessible focus subsampling idea used algorithm aim give insight new algorithm implemented issue affect efficiency
Statistics,statistical inference high dimensional regression via constrained lasso,paper propose new method estimation constructing confidence interval lowdimensional component highdimensional model proposed estimator called constrained lasso classo estimator obtained simultaneously solving two estimating equation one imposing zerobias constraint lowdimensional parameter forming penalized procedure highdimensional nuisance parameter carefully choosing zerobias constraint resulting estimator low dimensional parameter shown admit asymptotically normal limit attaining cram e rrao lower bound semiparametric sense propose tuningfree iterative algorithm implementing classo show algorithm initialized lasso estimator desparsified estimator proposed van de geer et al emph ann statist bf asymptotically equivalent first iterate algorithm analyse asymptotic property classo estimator show globally linear convergence algorithm also demonstrate encouraging empirical performance classo numerical study
Statistics,α variational inference statistical guarantee,propose family variational approximation bayesian posterior distribution called alpha vb provable statistical guarantee standard variational approximation special case alpha vb alpha novel class variational inequality developed linking bayes risk variational approximation objective function variational optimization problem implying maximizing evidence lower bound variational inference effect minimizing bayes risk within variational density family operating frequentist setup variational inequality imply point estimate constructed alpha vb procedure converge optimal rate true parameter wide range problem illustrate general theory number example including meanfield variational approximation low highdimensional bayesian linear regression spike slab prior mixture gaussian model latent dirichlet allocation mixture gaussian variational approximation regular parametric model
Statistics,rare feature selection high dimension,common modern prediction problem many predictor variable count rarely occurring event lead design matrix many column highly sparse challenge posed rare feature received little attention despite prevalence diverse area ranging natural language processing eg rare word biology eg rare specie show theoretically empirically explicitly accounting rareness feature greatly reduce effectiveness analysis next propose framework aggregating rare feature denser feature flexible manner creates better predictor response strategy leverage side information form tree encodes feature similarity apply method data tripadvisor predict numerical rating hotel based text associated review method achieves high accuracy making effective use rare word contrast lasso unable identify highly predictive word rare companion r package called rare implement new estimator using alternating direction method multiplier
Statistics,hard soft thresholding optimal iterative thresholding algorithm,iterative thresholding algorithm seek optimize differentiable objective function sparsity rank constraint alternating gradient step reduce objective thresholding step enforce constraint work examines choice thresholding operator asks whether possible achieve stronger guarantee possible hard thresholding develop notion relative concavity thresholding operator quantity characterizes worstcase convergence performance thresholding operator target optimization problem surprisingly find commonly used thresholding operator hard thresholding soft thresholding suboptimal term worstcase convergence guarantee instead general class thresholding operator lying hard thresholding soft thresholding shown optimal strongest possible convergence guarantee among thresholding operator example general class includes ellq thresholding appropriate choice q newly defined em reciprocal thresholding operator also investigate implication improved optimization guarantee statistical setting sparse linear regression show new class thresholding operator attain optimal rate computationally efficient estimator matching lasso
Statistics,distributed linear regression averaging,distributed statistical learning problem arise commonly dealing large datasets setup datasets partitioned machine compute locally communicate short message communication often bottleneck paper study onestep iterative weighted parameter averaging statistical linear model data parallelism linear regression machine send result central server take weighted average parameter optionally iterate sending back weighted average local ridge regression centered work compared linear regression full data study performance loss estimation test error confidence interval length high dimension number parameter comparable training data size find performance loss onestep weighted averaging also give result iterative averaging also find different problem affected differently distributed framework estimation error confidence interval length increase lot prediction error increase much le rely recent result random matrix theory develop new calculus deterministic equivalent tool broader interest
Statistics,heteroskedastic pca algorithm optimality application,principal component analysis pca singular value decomposition svd widely used statistic econometrics machine learning applied mathematics well studied case homoskedastic noise noise level contamination homogeneous paper consider pca svd presence heteroskedastic noise commonly used model factor analysis arises naturally range application introduce general framework heteroskedastic pca propose algorithm called heteropca involves iteratively imputing diagonal entry remove bias due heteroskedasticity procedure computationally efficient provably optimal generalized spiked covariance model key technical step deterministic robust perturbation analysis singular subspace independent interest effectiveness proposed algorithm demonstrated suite application including heteroskedastic lowrank matrix denoising poisson pca svd based heteroskedastic incomplete data
Statistics,stein point markov chain monte carlo,important task machine learning statistic approximation probability measure empirical measure supported discrete point set stein point class algorithm task proceed sequentially minimising stein discrepancy empirical measure target hence require solution nonconvex optimisation problem obtain new point paper remove need solve optimisation problem instead selecting new point based markov chain sample path significantly reduces computational cost stein point lead suite algorithm straightforward implement new algorithm illustrated set challenging bayesian inference problem rigorous theoretical guarantee consistency established
Statistics,binary expansion randomized ensemble test beret,recently binary expansion testing framework introduced test independence two continuous random variable utilizing symmetry statistic complete sufficient statistic dependence develop new test ensemble method us sum squared symmetry statistic distance correlation simulation study suggest method improves power preserving clear interpretation binary expansion testing extend method test independence random vector arbitrary dimension random projection proposed binary expansion randomized ensemble test transforms multivariate independence testing problem univariate problem simulation study data example analysis show proposed method provides relatively robust performance compared existing method
Statistics,highdimensional multiscale online changepoint detection,introduce new method highdimensional online changepoint detection setting p variate gaussian data stream may undergo change mean procedure work performing likelihood ratio test simple alternative different scale coordinate aggregating test statistic across scale coordinate algorithm online sense worstcase computational complexity per new observation namely obigl log ep bigr independent number previous observation practice may even significantly faster prove patience average run length null procedure least desired nominal level provide guarantee response delay alternative depend sparsity vector mean change simulation confirm practical effectiveness proposal
Statistics,adaptive independent metropolishastings fast estimation mixture normal,construct adaptive independent metropolishastings sampler us mixture normal proposal distribution take full advantage potential adaptive sampling algorithm update mixture normal frequently starting early chain algorithm built speed reliability sampling performance evaluated real simulated example article outline condition adaptive sampling hold give readily accessible proof condition sampling scheme generates iterates converge target distribution
Statistics,three month journeying hawaiian monk seal,hawaiian monk seal monachus schauinslandi endemic hawaiian island endangered specie marine mammal life entirely within jurisdiction united state specie number around declining owing among thing poor juvenile survival evidently related poor foraging success consequently data collected recently foraging habitat movement behavior monk seal throughout northwestern main hawaiian island work directed exploring data set located relatively shallow offshore submerged bank penguin bank search model seal journey work end fitting stochastic differential equation sde mimic aspect behavior seal working location data collected one seal sde found developing time varying potential function two point attraction time location irregularly spaced close together geographically leading difficulty interpretation synthetic plot generated using model employed ass reasonableness spatially temporally one aspect animal stay mainly southwest molokai work led estimation length location seal foraging trip
Statistics,regularized estimation largescale gene association network using graphical gaussian model,graphical gaussian model popular tool estimation undirected gene association network microarray data key issue number variable greatly exceeds number sample estimation matrix partial correlation since moorepenrose inverse sample covariance matrix lead poor estimate scenario standard method inappropriate adequate regularization technique needed article investigate general framework combining regularized regression method estimation graphical gaussian model framework includes various existing method well two new approach based ridge regression adaptive lasso respectively method extensively compared qualitatively quantitatively within simulation study application six diverse real data set addition proposed algorithm implemented r package parcor available r repository cran
Statistics,skellam shrinkage waveletbased intensity estimation inhomogeneous poisson data,ubiquity integrating detector imaging application implies variety realworld data well modeled poisson random variable whose mean turn proportional underlying vectorvalued signal interest article first show socalled skellam distribution arises fact haar wavelet filterbank transform coefficient corresponding measurement type distributed sum difference poisson count provide two main theorem skellam shrinkage one showing nearoptimality shrinkage bayesian setting providing unbiased risk estimation frequentist context result serve yield new estimator haar transform domain including unbiased risk estimate shrinkage haarfisz variancestabilized data along accompanying lowcomplexity algorithm inference conclude simulation study demonstrating efficacy skellam shrinkage estimator standard univariate wavelet test function well variety test image taken image processing literature confirming offer substantial performance improvement existing alternative
Statistics,alternative marginal likelihood estimator phylogenetic model,bayesian phylogenetic method generating noticeable enthusiasm field molecular systematics many phylogenetic model often stake different approach used compare within bayesian framework bayes factor defined ratio marginal likelihood two competing model play key role bayesian model selection focus alternative estimator marginal likelihood whose computation still challenging problem several computational solution proposed none considered outperforming others simultaneously term simplicity implementation computational burden precision estimate practitioner researcher often led available software privileged far simplicity harmonic mean estimator hm arithmetic mean estimator however known resulting estimate bayesian evidence favor one model biased often inaccurate infinite variance reliability corresponding conclusion doubtful new implementation generalized harmonic mean ghm idea recycles mcmc simulation posterior share computational simplicity original hm estimator unlike overcomes infinite variance issue alternative estimator applied simulated phylogenetic data produce fully satisfactory result outperforming simple estimator currently provided publicly available software
Statistics,sparse covariance estimation heterogeneous sample,standard gaussian graphical model ggms implicitly assume conditional independence among variable common observation sample however practice observation usually collected form heterogeneous population assumption satisfied leading turn nonlinear relationship among variable tackle problem explore mixture ggms particular consider infinite mixture model ggms infinite hidden markov model ggm emission distribution model allow u divide heterogeneous population homogenous group cluster conditional independence structure main advantage considering infinite mixture allow u easily estimate number number subpopulation sample illustration study trend exchange rate fluctuation preeuro era example demonstrates model flexible providing extremely interesting interesting insight reallife application
Statistics,online spot volatilityestimation decomposition nonlinear market microstructure noise model,technique online estimation spot volatility highfrequency data developed algorithm work directly transaction data update volatility estimate immediately occurrence new transaction furthermore nonlinear market microstructure noise model proposed reproduces several stylized fact highfrequency data computationally efficient particle filter used allows approximation unknown efficient price combination recursive em algorithm estimation volatility curve neither assume transaction time equidistant use interpolated price also make distinction volatility per time unit volatility per transaction provide estimator precisely use model random time change spot volatility decomposed spot volatility per transaction time trading intensity thus highlighting influence trading intensity volatility
Statistics,combining spatial information source accounting systematic error proxy,environmental research increasingly us highdimensional remote sensing numerical model output help fill spacetime gap traditional observation output often noisy proxy process interest thus one need separate ass signal noise often called discrepancy proxy given complicated spatiotemporal dependency extend popular twolikelihood hierarchical model using flexible representation discrepancy employ littleused markov random field approximation thin plate spline capture smallscale discrepancy computationally efficient manner better modeling smooth process standard conditional autoregressive model increased flexibility reduces identifiability lack identifiability inherent scientific context model particulate matter air pollution using satellite aerosol atmospheric model output proxy estimated discrepancy occur variety spatial scale smallscale discrepancy particularly important example indicate little predictive improvement modeling observation alone similarly simulation informative proxy presence discrepancy resulting identifiability issue prevent improvement prediction result highlight resolve critical question best use proxy information minimizing potential proxyinduced error
Statistics,seasonal fractional longmemory process semiparametric estimation approach,paper explores seasonal longmemory time series property using seasonal fractional arima model seasonal data one two seasonal period shortmemory counterpart stationarity invertibility parameter condition established model studied estimate memory parameter method given reisen rodrigues palma b generalized deal time series two seasonal fractional longmemory parameter asymptotic property established accuracy method investigated monte carlo experiment good performance estimator indicates alternative competitive procedure estimate seasonal longmemory time series data artificial series considered example application proposed estimation method
Statistics,posterior model probability computed modelspecific gibbs output,reversible jump markov chain monte carlo rjmcmc extends ordinary mcmc method use bayesian multimodel inference show rjmcmc implemented gibbs sampling alternating update model indicator vectorvalued palette parameter denoted bm psi like artist us palette mix dab color specific need create modelspecific parameter set available bm psi description remove mystery rjmcmc also provides basis fitting model one time using ordinary mcmc computing model weight bayes factor postprocessing monte carlo output illustrate procedure using several example
Statistics,scalable inference customer similarity interaction data using dirichlet process,sociological theory homophily people similar one another likely interact one another marketer often access data interaction among customer homophily guiding principle inference could made underlying similarity however larger network face quadratic explosion number potential interaction need modeled scalability problem render probability model social interaction computationally infeasible smallest network paper develop probabilistic framework modeling customer interaction grounded theory homophily flexible enough account random variation interacts particular present novel bayesian nonparametric approach using dirichlet process moderate scalability problem marketing researcher encounter working networked data find framework powerful way draw insight latent similarity customer discus marketer apply insight segmentation targeting activity
Statistics,dual φ divergence estimation normal model,class robust estimator obtained dual representation phi divergence studied empirically normal location model member class estimator compared found efficient true model offer attractive alternative maximum likelihood term robustness
Statistics,phylogenetic ornsteinuhlenbeck regression curve,regression curve studying trait relationship developed herein adaptive evolution model considered ornsteinuhlenbeck system whose parameter estimated novel engagement generalized leastsquares optimization algorithm implemented ecological data
Statistics,efficient bayesian multivariate surface regression,method choosing fixed set knot location additive spline model fairly well established statistical literature method principle directly extendable nonadditive surface model le likely successful setting curse dimensionality especially couple covariates propose regression model multivariate gaussian response combine additive spline interactive spline highly efficient mcmc algorithm update knot location jointly use shrinkage prior avoid overfitting different estimated shrinkage factor additive surface part model also different shrinkage parameter different response variable make possible model adapt varying degree nonlinearity different part data parsimonious way simulated data application firm leverage data show approach computationally efficient allowing freely estimated knot location offer substantial improvement outofsample predictive performance
Statistics,emulating gravity model infer spatiotemporal dynamic infectious disease,probabilistic model infectious disease dynamic useful understanding mechanism underlying spread infection likelihood function model expensive evaluate traditional likelihoodbased inference may computationally intractable furthermore traditional inference may lead poor parameter estimate fitted model may capture important biological characteristic observed data propose novel approach resolving issue inspired recent work emulation calibration complex computer model motivating example gravity time series susceptibleinfectedrecovered tsir model approach focus characteristic process scientific interest find gaussian process approximation gravity model using key summary statistic obtained model simulation demonstrate via simulated example new approach computationally expedient provides accurate parameter inference result good model fit apply method analyze measles outbreak england wale two period prevaccination period vaccination period based result able obtain important scientific insight transmission measles general method applicable problem traditional likelihoodbased inference computationally intractable produce poor model fit also alternative approximate bayesian computation abc simulation model expensive
Statistics,capturing timevarying driver epidemic using stochastic dynamical system,epidemic often modelled using nonlinear dynamical system observed partial noisy data paper consider stochastic extension order capture unknown influence changing behavior public intervention seasonal effect etc model assign diffusion process timevarying parameter inferential procedure based suitably adjusted adaptive particle mcmc algorithm performance proposed computational method validated simulated data adopted model applied pandemic england addition estimating effective contact rate trajectory methodology applied real time provide evidence related public health decision diffusion driven seirtype model age structure also introduced
Statistics,bayesian nonstationary spatial modeling large datasets,proliferation modern highresolution measuring instrument mounted satellite plane groundbased vehicle monitoring station need arisen statistical method suitable analysis large spatial datasets observed large spatial domain statistical analysis datasets provide two main challenge first traditional spatialstatistical technique often unable handle large number observation computationally feasible way second large heterogeneous spatial domain often appropriate assume process interest stationary entire domain address first challenge using model combining lowrank component allows flexible modeling mediumtolongrange dependence via set spatial basis function tapered remainder component allows modeling local dependence using compactly supported covariance function addressing second challenge propose two extension model result increased flexibility first model parameterized based nonstationary matern covariance parameter vary smoothly across space second fully bayesian model component parameter considered random including number location shape basis function used lowrank component using simulated data realworld dataset highresolution soil measurement show extension result substantial improvement current stateoftheart
Statistics,inference sde model via approximate bayesian computation,model defined stochastic differential equation sdes allow representation random variability dynamical system relevance class model growing many applied research area already standard tool model eg financial neuronal population growth dynamic however inference multidimensional sde model still challenging computationally theoretically approximate bayesian computation abc allow perform bayesian inference model sufficiently complex likelihood function either analytically unavailable computationally prohibitive evaluate computationally efficient abcmcmc algorithm proposed halving running time simulation focus case sde describes latent dynamic statespace model however methodology limited statespace framework simulation study pharmacokineticspharmacodynamics model stochastic chemical reaction considered matlab package implementing abcmcmc algorithm provided
Statistics,pointprocess response model spike train single neuron neural circuit optogenetic stimulation,optogenetics new tool study neuronal circuit genetically modified allow stimulation flash light study recording single neuron within neural circuit optogenetic stimulation data experiment present statistical challenge modeling high frequency point process neuronal spike input another high frequency point process light flash develop generalized linear model approach model relationship two point process employing additive pointprocess response function resulting model pointprocess response optogenetics pro provides explicit nonlinear transformation link input point process output one response function may provide important interpretable scientific insight property biophysical process governs neural spiking response optogenetic stimulation validate compare pro model using real dataset simulation model yield superior areaunderthe curve value high predicting every future spike experiment recurrent layer v circuit prefrontal cortex pro model provides evidence neuron integrate input sophisticated manner another use model enables understanding neural circuit altered various disease condition andor experimental condition comparing pro parameter
Statistics,estimating hidden population size using respondentdriven sampling data,respondentdriven sampling rds approach sampling design inference hardtoreach human population typically sampling frame available population member difficult identify recruit broader sampling frame common example include injecting drug user men sex men female sex worker analysis rds data focused estimating aggregate characteristic disease prevalence however rds often conducted setting population size unknown great independent interest paper present approach estimating size target population based data collected rds proposed approach us successive sampling approximation rds leverage information ordered sequence observed personal network size inference us bayesian framework allowing incorporation prior knowledge flexible class prior population size proposed aid elicitation extensive simulation study provides insight performance method estimating population size broad range condition study show approach also improves estimation aggregate characteristic particular choice prior produce interval estimate good frequentist property finally method demonstrates sensible result used estimate number subpopulation risk hiv two city el salvador
Statistics,clustering classification via clusterweighted factor analyzer,modelbased clustering classification clusterweighted model constitutes convenient approach random vector interest constitutes response variable set p explanatory variable x however applicability may limited p high overcome problem paper assumes latent factor structure x mixture component lead clusterweighted factor analyzer cwfa model imposing constraint variance covariance matrix x novel family sixteen cwfa model introduced modelbased clustering classification alternating expectationconditional maximization algorithm maximum likelihood estimation parameter model family described initialize algorithm hierarchical procedure proposed us nested structure model within family thus guarantee natural ranking among sixteen likelihood artificial real data show model good clustering classification performance algorithm able recover parameter well
Statistics,generalized linear gaussian clusterweighted modeling,clusterweighted modeling cwm flexible mixture approach modeling joint probability data coming heterogeneous population weighted sum product marginal distribution conditional distribution paper introduce wide family cluster weighted model conditional distribution assumed belong exponential family canonical link referred generalized linear gaussian cluster weighted model moreover show suitable sense mixture generalized linear model considered nested generalized linear gaussian cluster weighted model proposal illustrated many numerical study based simulated real data set
Statistics,fast approximate bayesian computation discretely observed markov model using factorised posterior distribution,many modern statistical application involve inference complicated stochastic model likelihood function difficult even impossible calculate hence conventional likelihoodbased inferential echniques used setting bayesian inference performed using approximate bayesian computation abc however spite many recent development abc methodology many application computational cost abc necessitates choice summary statistic tolerance potentially severely bias estimate posterior propose new piecewise abc approach suitable discretely observed markov model involves writing posterior density parameter product factor function subset data using abc within factor approach advantage sidestepping need choose summary statistic enables stringent tolerance set making posterior le approximate investigate two method estimating posterior density based abc sample factor first use gaussian approximation factor second use kernel density estimate method merit gaussian approximation simple fast probably adequate many application hand using instead kernel density estimate benefit consistently estimating true abc posterior number abc sample tends infinity illustrate piecewise abc approach three example case approach enables exact matching simulation data offer fast accurate inference
Statistics,removing batch effect prediction problem frozen surrogate variable analysis,batch effect responsible failure promising genomic prognos tic signature major ambiguity published genomic result retraction widelypublicized finding batch effect correction developed move artifact designed used population study genomic technology beginning used clinical application sam ples analyzed one time diagnostic prognostic predictive applica tions currently batch correction method developed specifically prediction paper propose new method called frozen surrogate variable analysis fsva borrows strength training set individual sample batch correction show fsva improves prediction ac curacy simulation public genomic study fsva available part sva bioconductor package
Statistics,reducedrank covariance estimation vector autoregressive modeling,consider reducedrank modeling white noise covariance matrix large dimensional vector autoregressive var model first propose reducedrank covariance estimator setting independent observation available derive reducedrank estimator based latent variable model vector observation give analytical form maximum likelihood estimate simulation result show reducedrank covariance estimator outperforms two competing covariance estimator estimating large dimensional covariance matrix independent observation describe integrate proposed reducedrank estimator fitting large dimensional var model consider two scenario require different model fitting procedure var modeling context reducedrank covariance estimator provides interpretable description dependence structure var process also lead improvement modelfitting forecasting unrestricted covariance estimator two real data example presented illustrate fitting procedure
Statistics,fast estimation diffusion tensor rician noise em algorithm,paper present fast computational method expectation maximization algorithm maximum likelihood ml estimation diffusion tensor imaging rice noise model extend ml framework maximum posterior map estimation describe numerical similarity ml map estimator novel method implemented applied using synthetic real data wide range b amplitude comparison popular method made accuracy methodology computation
Statistics,classification method hilbert data based surrogate density,unsupervised supervised classification approach hilbert random curve studied rest use surrogate probability density defined distributionfree mixture context asymptotic factorization smallball probability surrogate density estimated kernel approach principal component data focus illustration classification algorithm computational implication particular attention tuning parameter involved asymptotic result sketched application simulated real datasets show proposed method work
Statistics,informational approach global optimization presence noisy evaluation result application optimization renewable energy integration strategy,consider problem global optimization function f noisy evaluation adopt bayesian sequential approach evaluation point chosen reduce uncertainty position global optimum f measured entropy corresponding random variable informational approach global optimization villemonteix et al evaluation noisy error coming estimation entropy using conditional simulation becomes non negligible compared variation input domain propose solution problem choosing evaluation point several evaluation going made point method applied optimization strategy integration renewable energy electrical distribution network
Statistics,distribution associated general run pattern hidden markov model,paper give method computing distribution associated pattern state sequence hidden markov model conditional observing part observation sequence probability computed general class pattern competing pattern generalized later pattern thus theory includes special case result large class problem wide application unobserved state sequence assumed markovian general order dependence auxiliary markov chain associated state sequence used simplify computation two example given illustrate use methodology whereas first application illustrate basic step applying theory second detailed application dna sequence show method adapted include restriction related biological knowledge
Statistics,bayesian treed gaussian process model application computer modeling,motivated computer experiment design rocket booster paper explores nonstationary modeling methodology couple stationary gaussian process treed partitioning partitioning simple effective method dealing nonstationarity methodological development statistical computing detail make approach efficient described detail addition providing analysis rocket booster simulator approach demonstrated effective arena
Statistics,estimating covariance many asset history highly variable length,quantitative portfolio allocation requires accurate tractable estimation covariance large number asset whose history greatly vary length data said follow monotone missingness pattern likelihood convenient factorization upon assuming asset return multivariate normally distributed history least long total asset count maximum likelihood ml estimate easily obtained performing repeated ordinary least square ols regression one asset thing get interesting asset historical return ols becomes unstable due rank deficient design matrix called big p small n problem explore remedy involve making change basis principal component partial least square regression applying shrinkage method like ridge regression lasso enables estimation covariance large set asset history essentially arbitrary length offer improvement accuracy interpretation extend method showing external factor incorporated allows adaptive use factor without restrictive assumption common factor model method demonstrated randomly generated data benchmarked performance balanced portfolio using real historical financial return accompanying r package called monomvn containing code implementing estimator described herein made freely available cran
Statistics,smart statistical framework optimal design matrix generation application fmri,general linear model glm well established tool analyzing functional magnetic resonance imaging fmri data fmri analysis via glm proceed massively univariate fashion design matrix used analyzing data voxel major limitation approach locally varying nature signal interest well associated confounds local variability result potentially large bias uncontrolled increase variance contrast interest main contribution paper two fold develop statistical framework called smart enables estimation optimal design matrix explicitly controlling bias variance decomposition set potential design matrix develop validate numerical algorithm computing optimal design matrix general fmri data set implication framework include ability match optimally magnitude underlying signal true magnitude also matching null signal zero size thereby optimizing sensitivity specificity signal detection enabling capture multiple profile interest using single contrast opposed ftest way optimizes bias variance enables passing first level parameter estimate variance higher level group analysis possible using ftests demonstrate application approach vivo pharmacological fmri data capturing acute response drug infusion taskevoked block design fmri estimation haemodynamic response function hrf response eventrelated fmri framework quite general potentially wide applicability variety discipline
Statistics,perfect simulation spatial point process using dominated coupling past application multiscale areainteraction point process,consider perfect simulation algorithm locally stable point process based dominated coupling past version algorithm developed feasible process neither purely attractive purely repulsive process include multiscale areainteraction process capable modelling point pattern whose clustering structure varies across scale prove correctness algorithm existence process application redwood seedling data discussed
Statistics,stable graphical model estimation random forest discrete continuous mixed variable,conditional independence graph concise representation pairwise conditional independence among many variable graphical random forest grafo novel method estimating pairwise conditional independence relationship among mixedtype ie continuous discrete variable number edge tuning parameter graphical model estimator obvious number constitutes good choice stability selection help choosing parameter respect bound expected number false positive error control performance grafo evaluated compared various method p possibly mixedtype variable sample size n n maximum likelihood furthermore grafo applied data swiss health survey order evaluate well reproduce interconnection functional health component personal environmental factor hypothesized world health organization international classification functioning disability health icf finally grafo used identify risk factor may associated adverse neurodevelopment child suffer trisomy experienced openheart surgery grafo performs well mixed data thanks stability selection provides error control mechanism false positive selection
Statistics,fibregenerated point process field orientation,paper introduces new approach analyzing spatial point data clustered along around system curve fibre data arise catalogue galaxy location recorded location earthquake aerial image minefield pore pattern fingerprint finding underlying curvilinear structure pointpattern data set may facilitate better understanding arise also aid reconstruction missing data base space fibre set integral line orientation field using empirical bayes approach estimate field orientation anisotropic feature data sample posterior distribution fibre exploring model different number cluster fitting fibre cluster proceed bayesian approach permit inference various property cluster associated fibre result perform well number different curvilinear structure
Statistics,adaptive interacting wanglandau algorithm automatic density exploration,statistician wellaccustomed performing exploratory analysis modeling stage analysis notion conducting preliminary generalpurpose exploratory analysis monte carlo stage generally modelfitting stage analysis area feel deserves much attention towards aim paper proposes generalpurpose algorithm automatic density exploration proposed exploration algorithm combine expands upon component various adaptive markov chain monte carlo method wanglandau algorithm heart additionally algorithm run interacting parallel chain feature decrease computational cost well stabilizes algorithm improving ability explore density performance studied several application bayesian variable selection example author demonstrate convergence gain obtained interacting chain ability algorithm adaptive proposal induce modejumping illustrated trimodal density bayesian mixture modeling application lastly ising model author demonstrate ability algorithm overcome high correlation encountered spatial model
Statistics,bayesian vertex nomination,consider attributed graph whose vertex colored green red observed red color vertex unobserved typically unknown total number red vertex small vertex nomination problem nominate one unobserved vertex red edge set graph subset set unordered pair vertex suppose edge also colored green red observed edge context statistic vertex defined number observed red vertex connected content statistic number red edge incident assuming statistic independent vertex red edge likely red vertex coppersmith priebe proposed likelihood model based statistic formulate bayesian model using proposed likelihood together prior distribution chosen unknown parameter unobserved vertex color resulting posterior distribution nominated vertex one highest posterior probability red inference conducted using metropoliswithingibbs algorithm performance illustrated simulation study result show bayesian model performs significantly better chance ii probability correct nomination increase increasing posterior probability nominated vertex red iii bayesian model either match performs better method coppersmith priebe application example provided using enron email corpus vertex represent enron employee associate observed red vertex known fraudsters red edge represent email communication perceived fraudulent wish identify one latent vertex likely fraudster
Statistics,bayesian nonparametric weighted sampling inference,historically challenge perform bayesian inference designbased survey context present paper develops bayesian model sampling inference presence inverseprobability weight use hierarchical approach model distribution weight nonsampled unit population simultaneously include predictor nonparametric gaussian process regression use simulation study evaluate performance procedure compare classical designbased estimator apply method fragile family child wellbeing study study find bayesian nonparametric finite population estimator robust classical designbased estimator without loss efficiency work induce regularization small cell thus way automatically smoothing highly variable weight
Statistics,efficient estimation highdimensional multivariate normal copula model discrete spatial response,distributional transform dt amongst computational method used estimation highdimensional multivariate normal copula model discrete response advantage likelihood derived conveniently theory copula model continuous margin clear analysis adequacy method investigate smallsample asymptotic efficiency method estimating highdimensional multivariate normal copula model univariate bernoulli poisson negative binomial margin show dt approximation lead biased estimate discretisation highdimensional discrete response implement maximum simulated likelihood method based evaluating multidimensional integral likelihood randomized quasi monte carlo method efficiency calculation show method nearly efficient maximum likelihood fully specified highdimensional multivariate normal copula model method illustrated spatially aggregated count data set shown substantial gain efficiency via maximum simulated likelihood method
Statistics,bayesian nonparametric inference infectious disease data,propose framework bayesian nonparametric estimation rate new infection occur assuming epidemic partially observed developed methodology relies modelling rate new infection occur function depends time two different type prior distribution proposed namely using stepfunctions bsplines methodology illustrated using simulated real datasets show certain aspect epidemic seasonality superspreading event picked without explicitly incorporate parametric model
Statistics,false discovery rate smoothing,present false discovery rate smoothing empiricalbayes method exploiting spatial structure large multipletesting problem fdr smoothing automatically find spatially localized region significant test statistic relaxes threshold statistical significance within region tightens elsewhere manner control overall falsediscovery rate given level result increased power cleaner spatial separation signal noise approach requires solving nonstandard highdimensional optimization problem efficient augmentedlagrangian algorithm presented simulation study fdr smoothing exhibit stateoftheart performance modest computational cost particular shown far robust existing method spatially dependent multiple testing also apply method data set fmri experiment spatial working memory detects pattern much biologically plausible detected standard fdrcontrolling method code fdr smoothing publicly available python r
Statistics,estimating static parameter linear gaussian multiple target tracking model,present offline online maximum likelihood estimation mle technique inferring static parameter multiple target tracking mtt model linear gaussian dynamic present batch online version expectationmaximisation em algorithm short long data set respectively show monte carlo approximation method implemented performance assessed numerical example using simulated data various scenario comparison bayesian estimation procedure also provided
Statistics,modelling receiver operating characteristic curve using gaussian mixture,receiver operating characteristic curve widely applied measuring performance diagnostic test many direct indirect approach proposed modelling roc curve tractability gaussian distribution typically used model population propose using gaussian mixture model leading flexible approach better account atypical data monte carlo simulation used circumvent issue absence closedform show method performs favourably compared crude binormal curve semiparametric frequentist binormal roc using famous labroc procedure
Statistics,functional regression,functional data analysis fda involves analysis data whose ideal unit observation function defined continuous domain observed data consist sample function taken population sampled discrete grid ramsay silverman textbook sparked development field accelerated past year become one fastest growing area statistic fueled growing number application yielding type data one unique characteristic fda need combine information across within function ramsay silverman called replication regularization respectively article focus functional regression area fda received attention application methodological development first introduction basis function key building block regularization functional regression method followed overview functional regression method split three type functional predictor regression scalaronfunction functional response regression functiononscalar functiononfunction regression role replication regularization discussed methodological development described roughly chronological manner time deviating historical timeline group together similar method primary focus modeling methodology highlighting modeling structure developed various regularization approach employed end brief discussion describing potential area future development field
Statistics,improved em algorithm solving mle constrained diffusion kurtosis imaging human brain,displacement distribution water molecular characterized mathematically gaussianity without considering potential diffusion barrier compartment however true real scenario biological tissue comprised cell membrane various intracellular extracellular space compartment water diffusion referred nongaussian distribution diffusion kurtosis imaging dki recently considered one sensitive biomarker extension diffusion tensor imaging quantifies degree nongaussianity diffusion work proposes efficient scheme maximum likelihood estimation mle dki start rician noise model signal intensity augmenting vonmises distributed latent phase variable rician likelihood transformed tractable joint density without loss generality fast computational method expectationmaximization em algorithm mle proposed dki guarantee physical relevance diffusion kurtosis apply ternary quartic tq parametrization utilize positivity imposes upper bound kurtosis fisherscoring method used achieving fast convergence individual diffusion compartment addition use barrier method constrain lower bound kurtosis proposed estimation scheme conducted synthetic real data objective healthy human brain compared method popular one promising performance shown result
Statistics,bayesian inference diffusion driven mixedeffects model,stochastic differential equation sdes provide natural framework modelling intrinsic stochasticity inherent many continuoustime physical process process observed multiple individual experimental unit sde driven mixedeffects model allow quantification well within individual variation performing bayesian inference model using discrete time data may incomplete subject measurement error challenging problem focus paper extend recently proposed mcmc scheme include sde driven mixedeffects framework fundamental approach development novel construct allows efficient sampling conditioned sdes may exhibit nonlinear dynamic observation time apply resulting scheme synthetic data generated simple sde model orange tree growth real data consisting observation aphid number recorded variety different treatment regime addition provide systematic comparison approach inference scheme based tractable approximation sde linear noise approximation
Statistics,generalizing frailty assumption survival analysis,paper study cox regression hazard model unobservable random frailty specific distribution postulated frailty variable marginal lifetime distribution allows parametric nonparametric model laplace approximation method gradient search smooth manifold embedded euclidean space applied noniterative profile likelihood optimization method proposed estimating regression coefficient proposed method compared expectedmaximization method developed based gamma frailty assumption also case frailty model misspecified
Statistics,detection outlying proportion,paper introduce new method detecting outlier set proportion based construction suitable twoway contingency table application algorithm detection outlying cell table exploit special structure relevant contingency table increase efficiency method main property algorithm together guide choice parameter investigated simulation simple case theoretical justification provided several example synthetic data example based pseudoreal data biological experiment demonstrate good performance algorithm
Statistics,finding functional neuromaging literature trust,recent cluster failure paper eklund colleague cast doubt accuracy widely used statistical test functional neuroimaging leverage nonparametric method control false discovery rate offer nuanced quantitative guidance finding existing literature trusted show task study examined eklund et al cluster originally reported significant indeed trustworthy false discovery rate benchmark
Statistics,gaussian process model mortality rate improvement factor,develop gaussian process gp framework modeling mortality rate mortality improvement factor gp regression nonparametric datadriven approach determining spatial dependence mortality rate jointly smoothing raw rate across dimension calendar year age gp model quantifies uncertainty associated smoothed historical experience generates full stochastic trajectory outofsample forecast framework well suited updating projection newly available data arrives dealing edge issue credibility lower present detailed analysis gaussian process model performance u mortality experience based cdc datasets investigate interaction mean residual modeling bayesian nonbayesian gp methodology accuracy insample outofsample forecasting stability model parameter also document general decline along strong agedependency mortality improvement factor past year contrasting finding society actuary soa model fully reflect recent trend
Statistics,statistical analysis autoregressive fractionally integrated moving average model,practice several time series exhibit longrange dependence persistence observation leading development number estimation prediction methodology account slowly decaying autocorrelations autoregressive fractionally integrated moving average arfima process one bestknown class longmemory model package afmtools r implemented statistical tool analyzing arfima model particular package contains function parameter estimation exact autocovariance calculation predictive ability testing impulse response function amongst others finally implemented method illustrated application reallife time series
Statistics,extremely efficient generation gamma random variable α,gamma distribution wellknown widely used many signal processing communication application letter simple extremely efficient acceptreject algorithm introduced generation independent random variable gamma distribution shape parameter alpha proposed method us another gamma distribution integer alphap alpha sample easily drawn proposal function reason new technique attains higher acceptance rate ar alpha method currently available literature ar tends alpha diverges
Statistics,penalized simulated maximum likelihood approach parameter estimation stochastic differential equation,consider problem estimating parameter stochastic differential equation sdes discretetime observation either completely partially observed transition density two observation generally unknown propose importance sampling approach auxiliary parameter transition density unknown embed auxiliary importance sampler penalized maximum likelihood framework produce accurate computationally efficient parameter estimate simulation study three different model illustrate promising improvement new penalized simulated maximum likelihood method new procedure designed challenging case state variable unobserved moreover observed state sparse time commonly arises ecological study apply new approach two epidemic chronic wasting disease mule deer
Statistics,nonparametric estimation dynamic range music signal,dynamic range important parameter measure spread sound power music signal measure recording quality various descriptive measure sound power none strong statistical foundation start nonparametric model sound wave additive stochastic term role catch transient energy component recovered simple rateoptimal kernel estimator requires single datadriven tuning distribution variance approximated consistent random subsampling method able cope massive size typical dataset based latter propose statistic estimation method able represent dynamic range concept consistently behavior statistic assessed based large numerical experiment simulate dynamic compression selection real music signal application method real data also show proposed method predict subjective expert opinion hifi quality recording
Statistics,data augmentation rician noise model bayesian diffusion tensor imaging,mapping white matter tract essential step towards understanding brain function diffusion magnetic resonance imaging dmri noninvasive technique detect vivo anisotropy diffusion water molecule correspond nervous fiber living brain process spectral data displacement distribution water molecule collected magnetic resonance scanner statistical point view inverting fourier transform sparse noisy spectral measurement lead nonlinear regression problem diffusion tensor imaging dti simplest modeling approach postulating gaussian displacement distribution volume element voxel typically inference based linearized lognormal regression model fit spectral data low frequency however approximation fails fit high frequency measurement contain information detail displacement distribution low signal noise ratio paper directly work rice noise model cover full range b value using data augmentation represent likelihood reduce nonlinear regression problem framework generalized linear model construct bayesian hierarchical model order perform simultaneously estimation regularization tensor field finally bayesian paradigm implemented using markov chain monte carlo
Statistics,highdimensional unsupervised classification via parsimonious contaminated mixture,contaminated gaussian distribution represents simple heavytailed elliptical generalization gaussian distribution unlike oftenconsidered tdistribution also allows automatic detection mild outlying bad point way observation typically assigned group finite mixture model context starting distribution propose contaminated factor analysis model method dimensionality reduction detection bad point higher dimension mixture contaminated gaussian factor analyzer mcgfa model follows therefrom extends recently proposed mixture contaminated gaussian distribution highdimensional data introduce family parsimonious model formed introducing constraint covariance contamination structure general mcgfa model outline variant expectationmaximization algorithm parameter estimation various implementation issue discussed novel family model compared wellestablished approach simulated real data
Statistics,efficient bayesian inference multivariate factor stochastic volatility model,discus efficient bayesian estimation dynamic covariance matrix multivariate time series factor stochastic volatility model particular propose two interweaving strategy yu meng journal computational graphical statistic substantially accelerate convergence mixing standard mcmc approach similar marginal data augmentation technique proposed acceleration procedure exploit nonidentifiability issue frequently arise factor model new interweaving strategy easy implement come almost extra computational cost nevertheless boost estimation efficiency several order magnitude shown extensive simulation study conclude application algorithm exchange rate data set illustrates superior performance new approach realworld data
Statistics,clustering airbnb review,last decade online customer review increasingly exert influence consumer decision booking accommodation online renewal importance concept wordof mouth reflected growing interest investigating consumer experience analyzing online review process text mining sentiment analysis clustering approach developed boston airbnb review submitted english language collected approach based mixture latent variable model provides appealing framework handling clustered binary data address problem discovering meaningful segment consumer coherent underlying topic sentiment behind review penalized mixture latent trait approach developed reduce number parameter identify variable informative clustering introduction componentspecific rate parameter avoids overpenalization occur inferring shared rate parameter clustered data divided guest four group property driven guest host driven guest guest recent overall negative stay guest negative experience
Statistics,fast calibrated additive quantile regression,propose novel framework fitting additive quantile regression model provides well calibrated inference conditional quantiles fast automatic estimation smoothing parameter model structure diverse usable distributional gam maintaining equivalent numerical efficiency stability proposed method statistically rigorous computationally efficient based general belief updating framework bissiri et al loss based inference compute adapting stable fitting method wood et al show pinball loss statistically suboptimal relative novel smooth generalisation also give access fast estimation method provide novel calibration method efficiently selecting learning rate balancing loss smoothing prior inference thereby obtaining reliable quantile uncertainty estimate work motivated probabilistic electricity load forecasting application used demonstrate proposed approach method described implemented qgam r package available comprehensive r archive network cran
Statistics,new look inverse gaussian distribution,inverse gaussian ig one famous considered distribution positive support propose convenient modebased parameterization yielding reparametrized ig rig distribution allowssimplifies use ig distribution various statistical field give example nonparametric statistic robust statistic modelbased clustering nonparametric statistic define smoother based rig kernel construction estimator welldefined free boundary bias adopt likelihood crossvalidation select smoothing parameter robust statistic propose contaminated ig distribution heavytailed generalization rig distribution accommodate mild outlier automatically detected model via maximum posteriori probability obtain maximum likelihood estimate parameter illustrate expectationmaximization em algorithm finally modelbased clustering semiparametric density estimation present finite mixture rig distribution use em algorithm obtain ml estimate parameter mixture model application economic insurance data finally illustrated exemplify enhance use proposed model
Statistics,bayesian hierarchical weighting adjustment survey inference,combine bayesian prediction weighted inference unified approach survey inference general principle bayesian analysis imply model survey outcome conditional variable affect probability inclusion incorporate weighting variable framework multilevel regression poststratification byproduct generating modelbased weight smoothing investigate deep interaction introduce structured prior distribution smoothing stability estimate computation done via stan implemented open source r package rstanarm ready public use simulation study illustrate modelbased prediction weighting inference outperform classical weighting apply proposal new york longitudinal study wellbeing new approach generates robust weight increase efficiency finite population inference especially subset population
Statistics,note implementing special case lear covariance model standard software,repeated measure analysis require proper choice correlation model ensure accurate inference optimal efficiency linear exponent autoregressive lear correlation model provides flexible twoparameter correlation structure accommodates variety data type correlation withinsampling unit decrease exponentially time space lear model subsumes three classic temporal correlation structure namely compound symmetry continuoustime ar maintaining parsimony providing appealing statistical computational property also supply plausible correlation structure power analysis across many experimental design however commonly used statistical package provide straightforward way implement model limiting use appropriate programming skill present reparameterization lear model allows easily implementing standard software special case data equally spaced temporal spatial interval
Statistics,poisson trick extension fitting multinomial regression model,article concerned fitting multinomial regression model using socalled poisson trick work motivated chen kuo malchowm ller svarer criticized computationally inefficient sometimes producing nonsense result first discus case independent data offer parsimonious fitting strategy covariates categorical propose new approach modelling correlated response based extension gammapoisson model likelihood expressed closedform parameter estimated via expectationconditional maximization ecm algorithm implemented using function fitting generalized linear model readily available standard statistical software package compared existing method approach avoids need approximate intractable integral thus inference exact respect approximating gammapoisson model proposed method illustrated via reanalysis yogurt data discussed chen kuo
Statistics,variation em algorithm marshallolkin bivariate pareto distribution location scale,recently asimit et al used em algorithm estimate marshallolkin bivariate pareto distribution distribution seven parameter describe alternative approach em algorithm numerical simulation performed verify performance different proposed algorithm reallife data analysis also shown illustrative purpose
Statistics,dvine copula mixed model joint metaanalysis comparison diagnostic test,particular disease may two diagnostic test developed test subject several study quadrivariate generalized linear mixed model glmm recently proposed joint metaanalyse compare two diagnostic test propose dvine copula mixed model joint metaanalysis comparison two diagnostic test general model includes quadrivariate glmm special case also operate original scale sensitivity specificity method allows direct calculation sensitivity specificity test well parameter summary receiver operator characteristic sroc curve along comparison srocs test methodology demonstrated extensive simulation study illustrated metaanalysing two example test diagnosis particular disease compared study suggests improvement glmm fit data since model also provide tail dependency asymmetry
Statistics,bayesian matching unlabelled point set using procrustes configuration model,problem matching unlabelled point set using bayesian inference considered two recently proposed model likelihood compared based procrustes sizeandshape full configuration bayesian inference carried matching point set using markov chain monte carlo simulation improvement existing procrustes algorithm proposed improves convergence rate using occasional large jump burnin period procrustes configuration method compared simulation study using real data interest estimate strength match protein binding site performance method generally quite similar connection two model made using laplace approximation
Statistics,bootstrapping data array arbitrary order,paper study bootstrap strategy estimating variance mean taken large multifactor crossed random effect data set apply bootstrap reweighting independently level factor giving observation product independently sampled factor weight exact bootstrap exists problem mccullagh bernoulli show proposed bootstrap mildly conservative meaning biased toward overestimating variance sufficient condition allow unbalanced heteroscedastic input earlier result resampling bootstrap apply two factor use multinomial weight poorly suited online computation proposed reweighting approach implemented parallel online setting result method apply number factor method illustrated using factor data set comment length facebook
Statistics,plackettluce regression new bayesian model polychotomous data,multinomial logistic regression one popular model modelling effect explanatory variable subject choice set specified option model found numerous application machine learning psychology economy bayesian inference model non trivial requires either resort metropolishastings algorithm rejection sampling within gibbs sampler paper propose alternative model multinomial logistic regression model build plackettluce model popular model multiple comparison show introduction suitable set auxiliary variable lead expectationmaximization algorithm find maximum posteriori estimate parameter provide full bayesian treatment deriving gibbs sampler requires sample highly standard distribution also propose variational approximate inference scheme simple implement one property plackettluce regression model learns sparse set feature weight compare method sparse bayesian multinomial logistic regression show competitive especially presence polychotomous data
Statistics,unbiased estimate gradient stochastic network performance measure,three class stochastic network performance measure considered performance measure defined expected value random variable normally obtained analytically function network parameter closed form give similar representation random variable provide useful way analytical study function gradient representation used obtain sufficient condition gradient estimate unbiased condition rather general usually met simulation study stochastic network application result discussed practical algorithm calculating unbiased estimate gradient also presented
Statistics,combining dynamic prediction joint model longitudinal timetoevent data using bayesian model averaging,joint modeling longitudinal timetoevent data active area statistic research received lot attention recent year recently new attractive application type model obtain individualized prediction survival probability andor future longitudinal response advantageous feature prediction dynamically updated extra longitudinal response collected subject interest providing real time risk assessment using recorded information aim paper twofold first highlight importance modeling association structure longitudinal event time response greatly influence derived prediction second illustrate improve accuracy derived prediction suitably combining joint model different association structure second goal achieved using bayesian model averaging setting intriguing feature model weight fixed rather subject timedependent implying different followup time prediction subject may based different model
Statistics,miniminimax uncertainty quantification emulator,consider approximating black box function f emulator hat f based n noiseless observation f let w point domain f big might error hat f w f w f could arbitrarily rough error could arbitrarily large need constraint f besides data suppose f lipschitz known constant find lower bound number observation required ensure best emulator hat f based n data hat f w f w le epsilon general know whether f lipschitz much le know lipschitz constant assume optimistically f lipschitzcontinuous smallest constant consistent n data find maximum regular f hat f w f w best possible emulator hat f call miniminimax uncertainty w reality f might lipschitz might attain lipschitz constant data hence miniminimax uncertainty w could much smaller hat f w f w miniminimax uncertainty large even f satisfies optimistic regularity assumption hat f w f w could large matter cleverly choose hat f community atmosphere model maximum w miniminimax uncertainty based set f smaller would single observation f centroid parameter space also find lower confidence bound quantiles miniminimax uncertainty mean domain f community atmosphere model lower confidence bound appreciable fraction maximum
Statistics,error analysis bayesian identification nonlinear statespace model,last two decade several method based sequential monte carlo smc markov chain monte carlo mcmc proposed bayesian identification stochastic nonlinear statespace model ssms well known performance simulation based identification method depends numerical approximation used design propose use posterior cramerrao lower bound pcrlb mean square error mse bound using pcrlb systematic procedure developed analyse estimate delivered bayesian identification method term bias mse efficiency efficacy utility proposed approach illustrated numerical example
Statistics,input design bayesian identification nonlinear statespace model,propose algorithm designing optimal input online bayesian identification stochastic nonlinear statespace model proposed method relies minimization posterior cramer rao lower bound derived model parameter respect input sequence render optimization problem computationally tractable input parametrized multidimensional markov chain input space proposed approach illustrated simulation example
Statistics,optimal design analysis exponentially weighted moving average chart exponential data,study optimal design exponentially weighted moving average ewma chart proper choice smoothing factor initial value headstart decision statistic particular problem addressed quickest detection abrupt change parameter discretetime exponential model pre postchange parameter value assumed known changepoint known changepoint detection scenario examine performance conventional onesided ewma chart respect two optimality criterion pollak minimax criterion associated maximal conditional expected delay detection shiryaev multicyclic setup associated stationary expected delay detection using integralequations approach derive exact closedform formula required performance measure based formula find optimal smoothing factor headstart solving corresponding two bivariate constraint optimization problem finally performance optimized ewma chart compared shiryaev robert r procedure minimax setting original shiryaev robert procedure multicyclic setting main conclusion ewma chart fully optimized turn competitive procedure performance nearly indistinguishable knowntobebest shiryaev robert r shiryaev robert procedure
Statistics,accelerating asymptotically exact mcmc computationally intensive model via local approximation,construct new framework accelerating markov chain monte carlo posterior sampling problem standard method limited computational cost likelihood numerical model embedded therein approach introduces local approximation model metropolishastings kernel borrowing idea deterministic approximation theory optimization experimental design previous effort integrating approximate model inference typically sacrifice either sampler exactness efficiency work seek address limitation exploiting useful convergence characteristic local approximation prove ergodicity approximate markov chain showing sample asymptotically emph exact posterior distribution interest describe variation algorithm employ either local polynomial approximation local gaussian process regressors theoretical result reinforce key observation underlying paper likelihood emph local regularity number model evaluation per mcmc step greatly reduced without biasing monte carlo average numerical experiment demonstrate multiple orderofmagnitude reduction number forward model evaluation used representative ode pde inference problem synthetic real data
Statistics,logistic biplots ordinal data application job satisfaction doctorate degree holder spain,biplot method allow simultaneous representation individual variable data matrix binary nominal data logistic biplots recently developed extend classical linear representation continuous data data ordinal linear binary nominal logistic biplots adequate technique categorical principal component analysis catpca item response theory irt ordinal item used instead paper extend biplot ordinal data resulting method termed ordinal logistic biplot olb row score computed ordinal logistic response along dimension column parameter produce logistic response surface projected onto space spanned row score define linear biplot proportional odds model used obtaining multidimensional model known graded response model item response theory literature study geometry representation construct computational algorithm estimation parameter calculation prediction direction ordinal logistic biplots extend catpca irt sense give graphical representation irt similar biplot catpca main theoretical result applied study job satisfaction doctorate phd holder spain holder doctorate degree research qualification crucial creation commercialization dissemination knowledge innovation proposed method used extract useful information spanish data international survey career doctorate holder cdh jointly carried eurostat organisation economic cooperation development oecd unesco institute statistic uis
Statistics,fast exact bootstrap principal component analysis p million,many suggested bootstrap procedure estimating sampling variability principal component analysis pca result however number measurement per subject p much larger number subject n challenge calculating storing leading principal component bootstrap sample computationally infeasible address outline method fast exact calculation bootstrap principal component eigenvalue score method leverage fact bootstrap sample occupy n dimensional subspace original sample result bootstrap principal component limited n dimensional subspace efficiently represented low dimensional coordinate subspace several uncertainty metric computed solely based bootstrap distribution low dimensional coordinate without calculating storing p dimensional bootstrap component fast bootstrap pca applied dataset sleep electroencephalogram eeg recording dataset brain magnetic resonance image mri papprox million brain mri dataset method allows standard error first principal component based bootstrap sample calculated standard laptop minute opposed approximately day standard method
Statistics,computationally efficient spatial modeling annual maximum hour precipitation application data iceland,propose computationally efficient statistical method obtain distributional property annual maximum hour precipitation km km regular grid iceland latent gaussian model built take account observation spatial variation output local meteorological model covariate based meteorological model constructed observational site grid point order assimilate available scientific knowledge precipitation statistical model model applied two data set extreme precipitation one uncorrected data set one data set corrected phase wind observation assumed follow generalized extreme value distribution latent level implement spde spatial model location scale parameter likelihood efficient mcmc sampler exploit model structure constructed yield fast continuous spatial prediction spatially varying model parameter quantiles
Statistics,empirical bayesian analysis simultaneous changepoints multiple data sequence,copy number variation cancer cell volatility fluctuation stock price commonly manifested changepoints occurring position across related data sequence introduce bayesian modeling framework basic employ changepoint prior capture cooccurrence tendency data type design efficient algorithm sample maximize basic changepoint posterior develop monte carlo expectationmaximization procedure select prior hyperparameters empirical bayes fashion use resulting basic framework analyze dna copy number variation cancer cell line identify important event affected price volatility p stock
Statistics,nonparametric distributed learning architecture big data algorithm application,dramatic increase size complexity modern datasets made traditional centralized statistical inference prohibitive addition computational challenge associated big data learning presence numerous data type eg discrete continuous categorical etc make automation scalability difficult question immediate concern design dataintensive statistical inference architecture without changing basic statistical modeling principle developed small data last century address problem present metalp flexible distributed statistical modeling framework
Statistics,local global robustness conjugate bayesian analysis,paper study influence perturbation conjugate prior bayesian inference perturbed prior defined inside larger family local mixture model effect posterior inference studied perturbation sense generalizes linear perturbation studied cite intuitive naturally normalized flexible statistical application global local sensitivity analysis considered geometric approach employed optimizing sensitivity direction function difference posterior mean divergence function posterior predictive model sensitivity measure function defined convex space nontrivial boundary shown smooth manifold
Statistics,variable selection latent class analysis application low back pain diagnosis,identification relevant clinical criterion related low back pain disorder may aid evaluation nature pain suffered way usefully informs patient assessment treatment data concerning low back pain categorical nature form checklist item denotes presence absence clinical condition latent class analysis modelbased clustering method multivariate categorical response applied data preliminary diagnosis type pain work propose variable selection method latent class analysis applied selection useful variable detecting group structure data method based comparison two different model allows discarding variable group information variable carrying information already selected one consider swapstepwise algorithm step model compared approximation bayes factor method applied selection clinical criterion useful clustering patient different class shown perform parsimonious variable selection give clustering performance comparable expertbased classification patient three class pain
Statistics,analysis distributional variation multiscale betabinomial modeling,many statistical analysis involve comparison multiple data set collected different condition order identify difference underlying distribution common challenge multisample comparison presence various confounders extraneous cause condition interest also contribute difference across distribution result false finding ie identified difference replicable followup investigation consider anova approach addressing issue multisample comparison collecting replicate data set condition thereby allowing identification interesting distributional variation extraneous one introduce multiscale bayesian hierarchical model analysis distributional variation andova design based collection betabinomial test targeting variation different scale different location across sample space instead treating test independently model employ graphical structure introduce dependency among individual test thereby allowing borrowing strength among derive efficient inference recipe combination numerical integration message passing evaluate ability method effectively address andova extensive simulation utilize method analyze dnaseseq data set identifying difference transcriptional factor binding
Statistics,copulabased model multivariate ordinal panel data application wellbeing composition,novel copulabased multivariate panel ordinal model developed estimate structural relation among component wellbeing ordinal timeseries modelled using copulabased markov model relate marginal distribution response time observation observation time conditional distribution ordinal timeseries joined using multivariate copula maximum simulated likelihood based evaluating multidimensional integral likelihood randomized quasi monte carlo method used estimation asymptotic calculation show method nearly efficient maximum likelihood fully specified multivariate copula model finding highlight importance one relative position evaluating wellbeing direct effect socioeconomic characteristic wellbeing strong indirect effect impact component wellbeing temporal resilience habit formation behavioural trait explain dependence joint tail time across wellbeing component
Statistics,supplementary material sample time series frequently decision support via multirate spectrum estimation discussion,technical report includes assortment technical detail extended discussion related paper sample time series frequently decision support via multirate spectrum estimation discussion introduces model estimating logspectral density stationary discrete time process given systematically missing data model cost implication changing sampling rate
Statistics,fast approximation small pvalues permutation test partitioning permutation,researcher genetics life science commonly use permutation test evaluate difference group permutation test desirable property including exactness data exchangeable applicable even distribution test statistic analytically intractable however permutation test computationally intensive propose asymptotic approximation resampling algorithm quickly estimating small permutation pvalues eg difference ratio mean twosample test method based distribution test statistic within across partition permutation define article present method demonstrate use simulation application cancer genomic data simulation find resampling algorithm computationally efficient another leading alternative particularly extremely small pvalues eg application cancer genomic data find method successfully identify downregulated gene focus difference ratio mean speculate approach may work setting
Statistics,new insight rental housing market across united state web scraping analyzing craigslist rental listing,current source data rental housing census commercial database focus large apartment complex reflect recent market activity full scope u rental market address gap collected cleaned analyzed mapped visualized million craigslist rental housing listing data reveal finegrained spatial temporal pattern within across metropolitan housing market u find metropolitan area singledigit percentage listing fair market rent nontraditional source volunteered geographic information offer planner realtime localscale estimate rent housing characteristic currently lacking alternative source census data
Statistics,generalized stability approach regularized graphical model,selecting regularization parameter penalized highdimensional graphical model principled datadriven computationally efficient manner continues one key challenge highdimensional statistic present substantial computational gain conceptual generalization stability approach regularization selection star stateoftheart graphical model selection scheme using property poissonbinomial distribution convex nonasymptotic distributional modeling propose lower upper bound star graph regularization path result greatly reduced computational cost without compromising regularization selection also generalize star criterion single edge induced subgraph graphlet stability show simultaneously requiring edge graphlet stability lead superior graph recovery performance independent graph topology novel insight render gaussian graphical model selection routine task standard multicore computer
Statistics,inverse problem timeseries valued computer model via scalarization,expensive evaluate computer simulator even estimate overall surface challenging problem paper focus estimation inverse solution ie find set input combination simulator generates give good approximation predetermined simulator output ranjan et al proposed expected improvement criterion sequential design framework inverse problem scalar valued simulator paper focus inverse problem timeseries valued simulator used simulated two real example performance comparison
Statistics,fast bayesian wholebrain fmri analysis spatial prior,spatial wholebrain bayesian modeling taskrelated functional magnetic resonance imaging fmri great computational challenge currently proposed method therefore inference subregions brain separately approximate inference without comparison true posterior distribution popular method standard method bayesian single subject analysis spm software introduced penny et al method process data slicebyslice us approximate variational bayes vb estimation algorithm enforces posterior independence activity coefficient different voxels introduce fast practical markov chain monte carlo mcmc scheme exact inference model slicewise whole brain using prior activity coefficient algorithm exploit sparsity us modern technique efficient sampling highdimensional gaussian distribution leading speedup without mcmc would practical option using mcmc first time able evaluate approximate vb posterior exact mcmc posterior show vb lead spurious activation addition develop improved vb method drop assumption independent voxels posteriori algorithm shown much faster mcmc original vb large datasets negligible error compared mcmc posterior
Statistics,bayesian hierarchical model monthly maximum instantaneous flow,propose comprehensive bayesian hierarchical model monthly maximum instantaneous flow river catchment gumbel distribution used probabilistic model observation assumed come several catchment suggested latent model gaussian designed monthly maximum making better use data standard approach using annual maximum latent level linear mixed model used location scale parameter gumbel distribution accounting seasonal dependence covariates catchment specification prior distribution make use penalised complexity pc prior ensure robust inference latent parameter main idea behind pc prior shrink toward base model thus avoiding overfitting pc prior also provide convenient framework prior elicitation based simple notion scale prior distribution regression coefficient also elicited based hydrological meteorological knowledge posterior inference done using mcmc split sampler efficient gibbs blocking scheme tailored latent gaussian model proposed model applied observed data eight river catchment iceland crossvalidation study demonstrates good predictive performance
Statistics,bayesian inference stochastic differential equation mixed effect model tumor xenography study,consider bayesian inference stochastic differential equation mixed effect model sdemems exemplifying tumor response treatment regrowth mouse produce extensive study sdemem fitted using exact inference based pseudomarginal mcmc approximate inference via bayesian synthetic likelihood bsl investigate twocompartments sdemem corresponding fraction tumor cell killed survived treatment respectively case study data considers tumor xenography study two treatment group one control containing mouse result case study simulation indicate sdemem able reproduce observed growth pattern bsl robust tool inference sdemems finally compare fit sdemem similar ordinary differential equation model due small sample size strong prior information needed identify model parameter sdemem determined two model better term predicting tumor growth curve simulation study find sample mouse per group bsl able identify model parameter distinguish treatment group
Statistics,parallel local approximation mcmc expensive model,performing bayesian inference via markov chain monte carlo mcmc exceedingly expensive posterior evaluation invoke evaluation computationally expensive model system partial differential equation recent work conrad et al jasa described framework constructing refining local approximation model mcmc simulation posterior adapted approximation harness regularity model reduce computational cost inference preserving asymptotic exactness markov chain describe two extension work first prove sampler running parallel collaboratively construct shared posterior approximation ensuring ergodicity associated chain providing novel opportunity exploiting parallel computation mcmc second focusing metropolis adjusted langevin algorithm describe proposal distribution successfully employ gradient relevant information extracted approximation investigate practical performance strategy using two challenging inference problem first subsurface hydrology second glaciology using local approximation constructed via parallel chain successfully reduce run time needed characterize posterior distribution problem day hour month day respectively dramatically improving tractability bayesian inference
Statistics,stay go latent threshold approach largescale mixture innovation model,paper proposes straightforward algorithm carry inference large timevarying parameter vector autoregressions tvpvars mixture innovation component coefficient system significantly decrease computational burden approximating latent indicator drive timevariation coefficient latent threshold process depends absolute size shock merit approach illustrated two application first forecast u term structure interest rate demonstrate forecast gain proposed mixture innovation model relative benchmark model second apply approach u macroeconomic data find significant evidence timevarying effect monetary policy tightening
Statistics,joining splitting model markov melding,analysing multiple evidence source often feasible via modular approach separate submodels specified smaller component available evidence introduce generic framework enables fully bayesian analysis setting propose generic method forming suitable joint model joining submodels convenient computational algorithm fitting joint model stage rather single monolithic model approach also enables splitting large joint model smaller submodels allowing inference original joint model conducted via multistage algorithm motivate demonstrate approach two example joining component evidence synthesis influenza splitting large ecology model
Statistics,tractable bayesian variable selection beyond normality,bayesian variable selection often assumes normality effect model misspecification sufficiently understood sound reason behind assumption particularly large p ease interpretation analytical computational convenience flexible framework exist including semi nonparametric model often cost tractability propose simple extension normal model allows skewness thickerthannormal tail preserve tractability lead easy interpretation logconcave likelihood facilitates optimization integration characterize asymptotically parameter estimation bayes factor rate particular studying effect model misspecification suitable condition misspecified bayes factor consistent induce sparsity asymptotic rate correct model however rate detect signal altered exponential factor often resulting loss sensitivity deficiency ameliorated inferring error distribution data simple strategy improve inference substantially work focus likelihood thus combined likelihood penalty prior focus nonlocal prior induce extra sparsity ameliorate finitesample effect caused misspecification result highlight practical importance focusing likelihood rather solely prior come bayesian variable selection methodology available r package mombf
Statistics,flexible tweedie regression model continuous data,tweedie regression model provide flexible family distribution deal nonnegative highly rightskewed data well symmetric heavy tailed data handle continuous data probability mass zero estimation inference tweedie regression model based maximum likelihood method challenged presence infinity sum probability function nontrivial restriction power parameter space paper propose two approach fitting tweedie regression model namely quasi pseudolikelihood discus asymptotic property two approach perform simulation study compare method maximum likelihood method particular show quasilikelihood method provides asymptotically efficient estimation regression parameter computational implementation alternative method faster easier orthodox maximum likelihood relying simple newton scoring algorithm simulation study showed quasi pseudolikelihood approach present estimate standard error coverage rate similar maximum likelihood method furthermore secondmoment assumption required quasi pseudolikelihood method enables u extend tweedie regression model class quasitweedie regression model wedderburn style moreover allows eliminate nontrivial restriction power parameter space thus provides flexible regression model deal continuous data provide texttt r implementation illustrate application tweedie regression model using three data set
Statistics,supremumnorm based test equality several covariance function,paper propose new test equality several covariance function functional data test statistic taken supremum value sum squared difference estimated individual covariance function pooled sample covariance function hoping obtain powerful test existing test testing problem asymptotic random expression test statistic null hypothesis obtained approximate null distribution proposed test statistic describe parametric bootstrap method nonparametric bootstrap method asymptotic random expression proposed test also studied local alternative shown proposed test root n consistent intensive simulation study conducted demonstrate finite sample performance proposed test turn proposed test indeed powerful existing test functional data highly correlated proposed test illustrated three real data example
Statistics,fast symmetric additive covariance smoothing,propose fast bivariate smoothing approach symmetric surface wide range application show applied estimate covariance function longitudinal data well multiple additive covariance functional data complex correlation structure symmetric smoother handle possibly noisy data sampled common dense grid well irregularly sparsely sampled data estimation based bivariate penalized spline smoothing using mixed model representation symmetry used reduce computation time compared usual nonsymmetric smoother outline application approach functional principal component analysis demonstrate practical value two application approach evaluated extensive simulation provide documented open source software implementing fast symmetric bivariate smoother building established algorithm additive model
Statistics,detecting performance degradation softwareintensive system presence trend longrange dependence,contemporary softwareintensive system reach increasingly large scale imperative failure detection scheme developed help prevent costly system downtime promising direction towards construction scheme exploitation easily available measurement system performance characteristic average number processed request queue size per unit time work investigate holistic methodology detection abrupt change time series data presence quasiseasonal trend longrange dependence focus failure detection computer system propose trend estimation method enjoying optimality property presence longrange dependent noise estimate considered normal system behaviour detect changepoints anomaly develop approach based ensemble weak detector demonstrate performance proposed changepoint detection scheme using artificial dataset publicly available abilene dataset well proprietary geoinformation system dataset
Statistics,estimation inference large linear mixed effect model,linear mixed model large imbalanced crossed random effect structure pose severe computational problem maximum likelihood estimation bayesian analysis cost grow fast n n observation problem arise setting underlying factor satisfy many many relationship instead nested one electronic commerce application n quite large method account correlation structure greatly underestimate uncertainty propose method moment approach take account correlation structure computed n cost method moment amenable parallel computation require parametric distributional assumption tuning parameter convergence diagnostics regression coefficient give condition consistency asymptotic normality well consistent variance estimate variance component give condition consistency use consistent estimate mildly conservative variance estimate computation done n work illustrate algorithm data stitch fix crossed random effect correspond client item
Statistics,achieving shrinkage timevarying parameter model framework,shrinkage timevarying parameter tvp model investigated within bayesian framework aim automatically reduce timevarying parameter static one model overfitting achieved placing double gamma shrinkage prior process variance efficient markov chain monte carlo scheme developed exploiting boosting based ancillaritysufficiency interweaving strategy method applicable tvp model univariate well multivariate time series application include tvp generalized phillips curve eu area inflation modelling multivariate tvp cholesky stochastic volatility model joint modelling return index
Statistics,generalized exponential smoothing prediction hierarchical time series,shang hyndman proposed grouped functional time series forecasting approach combination individual forecast obtained using generalized least square method modify methodology using generalized exponential smoothing technique disaggregated functional time series order obtain robust predictor discus property proposal basing result obtained via simulation study analysis real data related prediction demand electricity australia
Statistics,sequential monte carlo transformation,paper introduces methodology performing bayesian inference sequentially sequence posterior space different dimension show may achieved use sequential monte carlo smc sampler del moral et al making use full flexibility framework order method computationally efficient particular introduce innovation using deterministic transformation move particle effectively target distribution different dimension approach combined adaptive method yield extremely flexible general algorithm bayesian model comparison suitable use application acceptance rate reversible jump markov chain monte carlo rjmcmc low demonstrate approach wellstudied problem model comparison mixture model novel application inferring coalescent tree sequentially data arrives
Statistics,phylogenetic factor analysis,phylogenetic comparative method explore relationship quantitative trait adjusting shared evolutionary history adjustment often occurs brownian diffusion process along branch phylogeny generates model residual trait highdimensional trait inferring pairwise correlation within multivariate diffusion limiting circumvent problem propose phylogenetic factor analysis pfa assumes small unknown number independent evolutionary factor arise along phylogeny factor generate cluster dependent trait set bayesian framework pfa provides measure uncertainty factor number grouping combine continuous discrete trait integrates missing measurement incorporates phylogenetic uncertainty help molecular sequence develop gibbs sampler based dynamic programming estimate pfa posterior distribution threefold faster multivariate diffusion orderofmagnitude efficiently presence latent trait propose novel marginal likelihood estimator previously impractical model discrete data find pfa also provides better fit multivariate diffusion evolutionary question columbine flower development placental reproduction transition triggerfish fin morphometry
Statistics,extending growth mixture model using continuous nonelliptical distribution,growth mixture model gmms incorporate conventional random effect growth modeling latent trajectory class finite mixture modeling therefore offer way handle unobserved heterogeneity subject development gmms gaussian random effect dominate literature data asymmetric andor heavier tail one latent class required capture observed variable distribution therefore gmm continuous nonelliptical distribution proposed capture skewness heavier tail data set specifically multivariate skewt distribution generalized hyperbolic distribution introduced extend gmms extending gmms four statistical model considered differing distribution measurement error random effect mathematical development gmms nonelliptical distribution relies expression normal variancemean mixture resultant relationship generalized inverse gaussian distribution parameter estimation outlined within expectationmaximization framework performance gmms nonelliptical distribution illustrated simulated real data
Statistics,sparse bayesian vector autoregressions huge dimension,develop bayesian vector autoregressive var model multivariate stochastic volatility capable handling vast dimensional information set three feature introduced permit reliable estimation model first assume reducedform error var feature factor stochastic volatility structure allowing conditional equationbyequation estimation second apply recently developed globallocal shrinkage prior var coefficient cure curse dimensionality third utilize recent innovation efficiently sample highdimensional multivariate gaussian distribution make simulationbased fully bayesian inference feasible dimensionality large time series length moderate demonstrate merit approach extensive simulation study apply model u macroeconomic data evaluate forecasting capability
Statistics,bayesian nonparametric estimation survival function multiplesamples information,many real problem dependence structure general exchangeability required instance setting partial exchangeability reasonable assumption reason vector dependent bayesian nonparametric prior recently gained popularity provide flexible model tractable computational theoretical point view paper focus use estimating survival function multiplesamples information methodology allows model dependence among survival time different group observation extend previous work arbitrary dimension theoretical result posterior behaviour underlying dependent vector completely random measure provided performance model tested simulated dataset arising distributional clayton copula
Statistics,metalearning resampling recommendation system,one possible approach tackle class imbalance classification task resample training dataset ie drop element synthesize new one exist several widelyused resampling method recent research showed choice resampling method significantly affect quality classification raise resampling selection problem exhaustive search optimal resampling timeconsuming hence limited use paper describe alternative approach resampling selection follow metalearning concept build resampling recommendation system ie algorithm recommending resampling datasets basis property
Statistics,multisample estimation bacterial composition matrix metagenomics data,metagenomics sequencing routinely applied quantify bacterial abundance microbiome study bacterial composition estimated based sequencing read count due limited sequencing depth dna dropout many rare bacterial taxon might captured final sequencing read result many zero count naive composition estimation using count normalization lead many zero proportion tend result inaccurate estimate bacterial abundance diversity paper take multisample approach estimation bacterial abundance order borrow information across sample across specie empirical result real data set suggest composition matrix multiple sample approximately low rank motivates regularized maximum likelihood estimation nuclear norm penalty efficient optimization algorithm using generalized accelerated proximal gradient euclidean projection onto simplex space developed theoretical upper bound minimax lower bound estimation error measured kullbackleibler divergence frobenius norm established simulation study demonstrate proposed estimator outperforms naive estimator method applied analysis human gut microbiome dataset
Statistics,bayesian nonparametrics stochastic epidemic model,vast majority model spread communicable disease parametric nature involve underlying assumption disease spread population article consider use bayesian nonparametric approach analysing data disease outbreak specifically focus method estimating infection process simple model assumption process explicit timedependence
Statistics,ancillaritysufficiency interweaving strategy asis boosting mcmc estimation stochastic volatility model,bayesian inference stochastic volatility model using mcmc method highly depends actual parameter value term sampling efficiency draw posterior utilizing standard centered parameterization break volatility volatility parameter latent state equation small noncentered version model show deficiency highly persistent latent variable series novel approach ancillaritysufficiency interweaving recently shown aid overcoming issue broad class multilevel model paper demonstrate interweaving strategy applied stochastic volatility model order greatly improve sampling efficiency parameter throughout entire parameter range moreover method combining best different world allows inference parameter constellation previously infeasible estimate without need select particular parameterization beforehand
Statistics,sparse inverse covariance estimation highthroughput microrna sequencing data poisson lognormal graphical model,introduce poisson lognormal graphical model count data present normality transformation data arising distribution model transformation feasible highthroughput microrna mirna sequencing data directly account known overdispersion relationship present data set model allows network dependency modeled provide algorithm utilizes onestep em based result order allow provable increase performance determining network structure model shown provide increase performance simulation setting range network structure model applied highthroughput mirna sequencing data patient breast cancer cancer genome atlas tcga selecting highly connected mirna molecule fitted network find nearly known involved regulation breast cancer
Statistics,generalized biplots multidimensional scaled projection,dimension reduction visualization staple data analytics method principal component analysis pca multidimensional scaling md provide low dimensional ld projection high dimensional hd data preserving hd relationship observation traditional biplots assign meaning ld space pca projection displaying ld ax attribute ax however specific linear projection used pca md projection allow arbitrary stress dissimilarity function require special care labeling ld space propose iterative scheme plot ld axis attribute based userspecified stress dissimilarity metric discus detail general biplot methodology relationship pcaderived biplots provide example using real data
Statistics,bayesian analysis three parameter singular marshallolkin bivariate pareto distribution,paper provides bayesian analysis singular marshallolkin bivariate pareto distribution consider three parameter singular marshallolkin bivariate pareto distribution consider two type prior reference prior gamma prior bayes estimate parameter calculated based slice cum gibbs sampler lindley approximation credible interval also provided method prior distribution data analysis kept illustrative purpose
Statistics,model averaging use economics,method model averaging become important tool deal model uncertainty example situation large amount different theory exist common economics model averaging natural formal response model uncertainty bayesian framework paper deal bayesian model averaging important role prior assumption bayesian procedure highlighted addition frequentist model averaging method also discussed numerical method implement method explained point reader freely available computational resource main focus uncertainty regarding choice covariates normal linear regression model paper also cover challenging setting particular emphasis sampling model commonly used economics application model averaging economics reviewed discussed wide range area among growth economics production modelling finance forecasting macroeconomic quantity
Statistics,general framework datadriven uncertainty quantification complex input dependency using vine copula,system subject uncertain input produce uncertain response uncertainty quantification uq deal estimation statistic system response given computational model system probabilistic model input engineering application common assume input mutually independent coupled gaussian elliptical dependence structure copula paper overcome limitation modelling dependence structure multivariate input vine copula vine copula model multivariate dependence built simpler paircopulas vine representation flexible enough capture complex dependency paper formalises framework needed build vine copula model multivariate input combine virtually uq method framework allows fully automated datadriven inference probabilistic input model available input data procedure exemplified two finite element model truss structure subject input nongaussian dependence structure case analyse moment model response using polynomial chaos expansion perform structural reliability analysis calculate probability failure system using first order reliability method importance sampling reference solution obtained monte carlo simulation result show gaussian assumption yield biased statistic vine copula representation achieves significantly precise estimate even structure need fully inferred limited amount observation
Statistics,bayesian nonparametric model biomedical data analysis,dissertation develop nonparametric bayesian model biomedical data analysis particular focus inference tumor heterogeneity inference missing data first present bayesian feature allocation model tumor subclone reconstruction using mutation pair key innovation lie use short read mapped pair proximal single nucleotide variant snvs contrast existing method use marginal read unpaired snvs context using mutation pair order recover phylogenetic relationship subclones develop bayesian treed feature allocation model contrast commonly used feature allocation model allow latent feature dependent using tree structure introduce dependence finally propose nonparametric bayesian approach monotone missing data longitudinal study nonignorable missingness contrast existing method method allows incorporating information auxiliary covariates able capture complex structure among response missingness auxiliary covariates model validated simulation study applied realworld biomedical datasets
Statistics,spatial statistical downscaling constructing highresolution nature run global observing system simulation experiment,observing system simulation experiment os widely used rigorous costeffective way guide development new observing system evaluate performance new data assimilation algorithm nature run nrs output deterministic model play essential role building osse system global atmospheric process used create synthetic observation high spatial resolution represent true atmosphere forecast verified however nrs generated resolution coarser actual observation propose principled statistical downscaling framework construct highresolution nrs via conditional simulation coarseresolution numerical model output use nonstationary spatial covariance function model basis function representation approach explicitly address changeofsupport problem also allows fast computation large volume numerical model output also propose datadriven algorithm select required basis function adaptively order increase flexibility nonstationary covariance function model article demonstrate technique downscaling coarseresolution physical nr native resolution circ text latitude time circ text longitude global surface text co concentration equalarea hexagon
Statistics,sophisticated small versus simple sizeable pay introduce drifting coefficient bayesian var,ass relationship model size complexity timevarying parameter var framework via thorough predictive exercise euro area united kingdom united state turn sophisticated dynamic drifting coefficient important small data set simpler model tend perform better sizeable data set combine best world novel shrinkage prior help mitigate curse dimensionality resulting competitive forecast scenario considered furthermore discus dynamic model selection improve upon best performing individual model point time
Statistics,wto r package computing weighted topological overlap consensus network integrated visualization tool,network analysis gene coexpression network metabolic network ecological network become central approach systemslevel study biological data several software package exist generating analyzing network either correlation score absolute value transformed score called weighted topological overlap wto however since gene regulatory process downregulate gene great interest explicitly consider positive negative correlation constructing gene coexpression network present r package calculating wto contrast existing package explicitly address sign wto value thus especially valuable analysis gene regulatory network package includes calculation pvalues raw adjusted pairwise gene score package also allows calculation network time series without replicates since network independent datasets biological repeat related study due technical biological noise data additionally incorporated novel method calculating consensus network cn two network r package compare new wto package state art package demonstrate application wto cn function using independently derived datasets healthy human prefrontal cortex sample showcase example time series application utilized metagenomics data set work developed software package allows computation wto network cns visualization tool r statistical environment publicly available cran repository open source license http cranrprojectorgwebpackageswto
Statistics,zeromodified poissonlindley distribution application zeroinflated zerodeflated count data,main object article present extension zeroinflated poissonlindley distribution called zeromodified poissonlindley additional parameter pi zeromodified poissonlindley natural interpretation term either zerodeflatedinflated proportion inference dealt using likelihood approach particular maximum likelihood estimator distribution parameter compared small large sample also consider alternative biascorrection mechanism based efron bootstrap resampling model applied real data set found perform better competing model
Statistics,bayesian graphical compositional regression microbiome data,important task microbiome study test existence give characterization difference microbiome composition across group sample important challenge problem include large withingroup heterogeneity among sample existence potential confounding variable ignored increase chance false discovery reduce power identifying true difference propose probabilistic framework overcome issue combining three idea phylogenetic treebased decomposition crossgroup comparison problem series local test ii graphical model link local test allow information sharing across taxon iii bayesian testing strategy incorporates covariates integrates withingroup variation avoiding potentially unstable point estimate derive efficient inference algorithm based numerical integration junctiontree message passing conduct extensive simulation study investigate performance approach compare stateoftheart method number representative setting apply method american gut data analyze association dietary habit human gut microbiome composition presence covariates illustrate importance incorporating covariates microbiome crossgroup comparison
Statistics,variable selection functional additive regression model,paper considers problem variable selection regression model case functional variable may mixed type variable scalar multivariate directional etc proposal begin simple null model sequentially selects new variable incorporated model based use distance correlation proposed cite sake simplicity paper us additive model however proposed algorithm may ass type contribution linear non linear variable algorithm shown quite promising result applied simulation real data set
Statistics,deterministic balancing score algorithm avoid common pitfall propensity score matching,propensity score matching psm defacto standard estimating causal effect observational study show psm implementation susceptible several major drawback illustrate finding using case study patient derive four formal property optimal statistical matching algorithm meet propose deterministic balancing score exact matching dbsem meet aforementioned property exact matching furthermore investigate one main problem psm common psm result one valid set matched pair bootstrapped psm selection possible valid set matched pair exact matchings provide mathematical proof dbsem result delivers expected value valid set matched pair investigated dataset
Statistics,reduced basis kriging big spatial field,spatial statistic common method prediction gaussian random field grf maximum likelihood estimation combined kriging massive data set kriging computationally intensive term cpu time memory fixed rank kriging proposed solution method however still involves operation large matrix develop alteration method utilizing approximation made fixed rank kriging combined restricted maximum likelihood estimation sparse matrix methodology experiment show methodology provide additional gain computational efficiency fixedrank kriging without loss accuracy prediction methodology applied climate data archived united state national climate data center good result
Statistics,efficient bandwidth estimation twodimensional filtered backprojection reconstruction,generalized crossvalidation approach estimate reconstruction filter bandwidth twodimensional filtered backprojection presented method writes reconstruction equation equivalent backprojected filtering form derives result eigendecomposition symmetric twodimensional circulant matrix applies make bandwidth estimation computationally efficient operation within context standard backprojected filtering reconstruction performance evaluation wide range simulated emission tomography experiment give promising result superior performance hold low high total expected count pointing method applicability even weaker signalnoise situation approach also applies general class elliptically symmetric filter reconstruction performance often better even obtained true optimal radially symmetric filter
Statistics,modelbased clustering population network,recently obtaining data population network typically rare however advancement automatic monitoring device growing social scientific interest network data become widely available sociological experiment involving cognitive social structure fmri scan revealing largescale brain network group patient growing awareness urgently need tool analyse population network particularly model variation network due covariates propose modelbased clustering method based mixture generalized linear mixed model employed describe joint distribution population network parsimonious manner identify subpopulation network share certain topological property interest degree distribution community structure effect covariates presence edge etc maximum likelihood estimation proposed model efficiently carried implementation em algorithm ass performance method simulated data conclude example application advice network small business
Statistics,efficient data augmentation multivariate probit model panel data application general practitioner decisionmaking contraceptive,article considers problem estimating multivariate probit model panel data setting emphasis sampling highdimensional correlation matrix improving overall efficiency data augmentation approach reparameterise correlation matrix principled way carry efficient bayesian inference using hamiltonian monte carlo also propose novel antithetic variable method generate sample posterior distribution random effect regression coefficient resulting significant gain efficiency apply methodology analysing stated preference data obtained australian general practitioner evaluating alternative contraceptive product analysis suggests joint probability discussing combination contraceptive product patient show medical practice variation among general practitioner indicates resistance even discus product let alone recommend
Statistics,bayesian spatial analysis hardwood tree count forest via mcmc,paper perform bayesian inference analyze spatial tree count data timiskaming abitibi river forest ontario canada consider bayesian generalized linear geostatistical model implement markov chain monte carlo algorithm sample posterior distribution spatial prediction new site forest change amount training data reduced studied compared logistic regression model without spatial effect finally discus stratified sampling approach selecting subset data allows potential better prediction
Statistics,spatiotemporal data fusion massive sea surface temperature data modis amsre instrument,remote sensing data widely used study various geophysical process advance remotesensing technology massive amount remote sensing data collected space time different satellite instrument typically different footprint measurementerror characteristic data coverage combine datasets different satellite instrument propose dynamic fused gaussian process dfgp model enables fast statistical inference filtering smoothing massive spatiotemporal datasets datafusion context based upon spatiotemporalrandomeffects model dfgp methodology represents underlying true process two component linear combination small number basis function random coefficient general covariance matrix together linear combination large number basis function markov random coefficient model underlying geophysical process different spatial resolution rely changeofsupport property also allows efficient computation dfgp model estimate model parameter devise computationally efficient stochastic expectationmaximization sem algorithm ensure scalability massive datasets dfgp model applied total million sea surface temperature datasets tropical pacific ocean oneweek time period modis amsre instrument
Statistics,varimax rotation based gradient projection need random start loading matrix optimal performance,gradient projection rotation gpr promising method rotate factor component loading different criterion since condition optimal performance gprvarimax widely unknown simulation study investigates gpr towards varimax criterion principal component analysis condition simulation study comprise two sample size n n orthogonal simple structure population model based four number component without kaisernormalization six number random start loading matrix gprvarimax rotation gprvarimax rotation always performed better least random matrix used start loading instead identity matrix gprvarimax worked better small number component larger n compared smaller n sample loading kaisernormalized rotation ensure optimal stationary performance gprvarimax recovering orthogonal simple structure recommend using least iteration start loading matrix rotation three component iteration six component nine component rotation based sample size least case kaisernormalization different start loading matrix nine component gprvarimax rotation based least case kaisernormalization least different start loading matrix
Statistics,parameter estimation absolute continuous four parameter geometric marshallolkin bivariate pareto distribution,paper formulate four parameter absolute continuous geometric marshallolkin bivariate pareto distribution study parameter estimation em algorithm also explore bayesian analysis slice cum gibbs sampler approach numerical result shown verify performance algorithm illustrate procedure real life data analysis
Statistics,bayesian analysis absolute continuous marshallolkin bivariate pareto distribution location scale parameter,paper provides two different novel approach slice sampling estimate parameter absolute continuous marshallolkin bivariate pareto distribution location scale parameter carry bayesian analysis taking gamma prior shape scale parameter truncated normal location parameter credible interval coverage probability also provided method reallife data analysis shown illustrative purpose
Statistics,crossvalidated kernel ensemble robust hypothesis test nonlinear effect gaussian process,r package cvek introduces robust hypothesis test nonlinear effect gaussian process cvek ensemblebased estimator adaptively learns form maineffect kernel data construct companion variance component test package cvek implement estimator two testing procedure namely asymptotic test bootstrap test additionally implement variety tuning parameter criterion including akaike information criterion generalized cross validation generalized maximum profile marginal likelihood leaveoneout cross validation moreover three kind ensemble strategy create ultimate ensemble kernel simple averaging empirical risk minimization exponential weighting null distribution test statistic approximated using scaled chisquare distribution therefore statistical inference based result package hypothesis testing performed extensive simulation demonstrate robustness correct implementation estimator
Statistics,optimal replacement policy cumulative damage model strength degradation application,many reallife scenario system failure depends dynamic stressstrength interference strength degrades stress accumulates concurrently time paper consider problem finding optimal replacement strategy balance cost replacement cost failure result minimum expected cost per unit time cumulative damage model strength degradation existing recommendation applicable restricted distributional assumption andor fixed strength theoretical evaluation expected cost per unit time turn complicated simulationbased algorithm proposed evaluate expected cost rate find optimal replacement strategy proposed method easy implement wider domain application illustration proposed method applied real case study mailbox cellphone battery experiment
Statistics,estimating atmospheric motion wind satellite image data using spacetime drift model,geostationary satellite collect highresolution weather data comprising series image used estimate wind speed direction different altitude derived motion wind dmw algorithm commonly used process data estimate atmospheric wind tracking feature image taken goesr series noaa geostationary meteorological satellite however wind estimate dmw algorithm sparse come uncertainty measure motivates u statistically model wind motion spatial process drifting time propose covariance function depends spatial temporal lag drift parameter capture wind speed wind direction estimate parameter local maximum likelihood method allows u compute standard error estimate enabling spatial smoothing estimate using gaussian kernel weighted inverse estimated variance conduct extensive simulation study determine situation method performs well proposed method applied brightness temperature data colorado reduces prediction error brightness temperature compared dmw algorithm
Statistics,simultaneous inference mixed small area parameter,address simultaneous inference mixed parameter key ingredient small area estimation assume linear mixed model framework firstly analyse statistical property maxtype statistic use construct simultaneous prediction interval well implement multiple testing procedure secondly derive band based volumeoftube formula addition adapt simultaneous inference method regression nonparametric curve estimation compare approach simultaneous interval necessary compare cluster since presently available interval statistically valid analysis proposed testing procedure used validate certain statement set mixed parameter test pairwise difference proposal accompanied simulation experiment data example small area household income demonstrate excellent performance utility technique
Statistics,posteriorbased proposal speeding markov chain monte carlo,markov chain monte carlo mcmc widely used bayesian inference model complex system performance however often unsatisfactory model many latent variable due socalled poor mixing necessitating development application specific implementation paper introduces posteriorbased proposal pbps new type mcmc update applicable huge class statistical model whose conditional dependence structure represented directed acyclic graph pbps generates large joint update parameter latent variable space whilst retaining good acceptance rate typically evaluation approach standard gibbs random walk update stateoftheart hamiltonian particle mcmc method carried widely varying model type individualbased model disease diagnostic test data financial stochastic volatility model mixed model used statistical genetics population model used ecology whilst different method worked better worse different scenario pbps found either near fastest significantly faster next best approach factor pbps therefore represent additional general purpose technique usefully applied wide variety context
Statistics,maximum likelihood estimation semiparametric twocomponent mixture model using logconcave approximation,motivated study biological science detect differentially expressed gene semiparametric twocomponent mixture model one known component studied paper assuming density unknown component logconcave contains broad family density develop semiparametric maximum likelihood estimator propose em algorithm compute new estimation method find mixing proportion distribution unknown component simultaneously establish identifiability proposed semiparametric mixture model prove existence consistency proposed estimator compare estimator several existing estimator simulation study apply method two real data set biological science astronomy
Statistics,jaccardtanimoto similarity test estimation method,binary data used broad area biological science using binary presenceabsence data evaluate specie cooccurrences help elucidate relationship among organism environment summarize similarity occurrence specie routinely use jaccardtanimoto coefficient ratio intersection union natural identify statistically significant jaccardtanimoto coefficient suggest nonrandom cooccurrences specie however statistical hypothesis testing using similarity coefficient seldom used studied introduce hypothesis test similarity biological presenceabsence data using jaccardtanimoto coefficient several key improvement presented including unbiased estimation expectation centered jaccardtanimoto coefficient account occurrence probability derived exact asymptotic solution developed bootstrap measurement concentration algorithm compute statistical significance binary similarity comprehensive simulation study demonstrate proposed method produce accurate pvalues false discovery rate proposed estimation method order magnitude faster exact solution proposed method implemented open source r package called jaccard http cranrprojectorgpackagejaccard introduce suite statistical method jaccardtanimoto similarity coefficient enable straightforward incorporation probabilistic measure analysis specie cooccurrences due generality proposed method implementation applicable wide range binary data arising genomics biochemistry area science
Statistics,robust approximate bayesian inference synthetic likelihood,bayesian synthetic likelihood bsl established method conducting approximate bayesian inference model due intractability likelihood function exact bayesian approach either infeasible computationally demanding implicit application bsl assumption data generating process dgp produce simulated summary statistic capture behaviour observed summary statistic demonstrate compatibility actual assumed dgp satisfied ie model misspecified bsl yield unreliable parameter inference circumvent issue propose new bsl approach detect presence model misspecification simultaneously deliver useful inference even significant model misspecification two simulated two real data example demonstrate performance new approach bsl document superior accuracy standard bsl assumed model misspecified
Statistics,baseline drift estimation air quality data using quantile trend filtering,address problem estimating smoothly varying baseline trend time series data problem arises wide range field including chemistry macroeconomics medicine however study motivated analysis data low cost air quality sensor method extend quantile trend filtering framework enable estimation multiple quantile trend simultaneously ensuring quantiles cross handle computational challenge posed long time series propose parallelizable alternating direction method moment admm algorithm admm algorthim enables estimation trend piecewise manner reducing computation time extending limit method larger data size also address smoothing parameter selection propose modified criterion based extended bayesian information criterion simulation study motivating application low cost air quality sensor data demonstrate model provides better quantile trend estimate existing method improves signal classification lowcost air quality sensor output
Statistics,scaling bayesian probabilistic record linkage posthoc blocking application california great register,probabilistic record linkage prl process determining record two database correspond underlying entity absence unique identifier bayesian solution problem provide powerful mechanism propagating uncertainty due uncertain link record via posterior distribution however computational consideration severely limit practical applicability existing bayesian approach propose new computational approach providing fast algorithm deriving point estimate linkage structure properly account onetoone matching restricted mcmc algorithm sample approximate posterior distribution advance make possible perform bayesian prl larger problem ass sensitivity result varying prior specification demonstrate method subset ocr dataset california great register collection million voter registration comprise panel data set party registration collected advent scientific survey
Statistics,optimizing interim analysis timing bayesian adaptive commensurate design,developing product rare disease statistical challenge arise due limited number patient available participation drug trial clinical research bayesian adaptive clinical trial design offer possibility increased statistical efficiency reduced development cost ethical hazard prevention via incorporation evidence external source historical data expert opinion realworld evidence flexibility specification interim look paper propose novel bayesian adaptive commensurate design borrows adaptively historical information also us particular payoff function optimize timing study interim analysis trial payoff function many sample saved via early stopping probability making correct early decision either futility efficacy calibrate bayesian algorithm acceptable longrun frequentist property type error power via simulation design stage illustrate approach using pediatric trial design setting testing effect new drug rare genetic disease optimia r package available http provides easytouse implementation approach
Statistics,estimating variance time series linear regression model using empirical blups convex optimization,propose twostage estimation method variance component time series model known fdslrms whose observation described linear mixed model lmm based estimating variance fundamental quantity time series forecasting approach called kriging empirical plugin best linear unbiased prediction unobservable random component fdslrm method providing invariant nonnegative quadratic estimator used absolutely continuous probability distribution time series data result applying convex optimization lmm methodology resolved two problem theoretical existence equivalence least square estimator nonnegative doolse maximum likelihood estimator mle possible starting point method practical lack computational implementation fdslrm computing mle case n observed time series value also discovered new algorithm order mathcal n default precision time accurate time faster best current python r based computational package namely cvxpy cvxr nlme sommer mixed illustrate result three real data set electricity consumption tourism cyber security easily available reproducible sharable modifiable form interactive jupyter notebook
Statistics,unconstrained representation orthogonal matrix application common principle component,many statistical problem involve estimation left dtimes dright orthogonal matrix textbf q estimation often challenging due orthonormality constraint textbf q cope problem propose simple decomposition orthogonal matrix abbreviate plr decomposition produce onetoone correspondence textbf q left dtimes dright unit lower triangular matrix textbf l whose dleft entry diagonal unconstrained real value decomposition applied regardless objective function consideration use classical unconstrained optimization method find minimum maximum objective function respect textbf l illustrative purpose apply plr decomposition common principle component analysis cpca maximum likelihood estimation common orthogonal matrix multivariate leptokurticnormal distribution assumed group compared commonly used normal distribution leptokurticnormal additional parameter governing excess kurtosis make estimation textbf q cpca robust mild outlier usefulness plr decomposition leptokurticnormal cpca illustrated two biometric data analysis
Statistics,low rank gaussian process prediction model large datasets,spatial prediction requires expensive computation invert spatial covariance matrix depends also considerable storage need work concentrate computationally efficient algorithm prediction using large datasets recent prediction model spatial data known fixed rank kriging much faster kriging easily implemented le assumption process however fixed rank kriging requires estimation matrix must positive definite original estimation procedure guarantee property present result show matrix subtraction given form give positive definite matrix motivated result present iterative fixed rank kriging algorithm ensures positive definiteness matrix required prediction show mild condition algorithm numerically converges modified fixed rank kriging procedure implemented predict missing chlorophyll observation large region ocean color prediction compared made well known method spatial prediction
Statistics,characterization valuation uncertainty calibrated parameter stochastic decision model,evaluated implication different approach characterize uncertainty calibrated parameter stochastic decision model dm quantified value uncertainty decision making used microsimulation dm colorectal cancer crc screening conduct costeffectiveness analysis cea colonoscopy screening calibrated natural history model crc epidemiological data different degree uncertainty obtained joint posterior distribution parameter using bayesian approach conducted probabilistic sensitivity analysis psa model parameter different characterization uncertainty calibrated parameter estimated value uncertainty different characterization value information analysis analysis conducted using high performance computing resource running extremescale model exploration swift emews framework posterior distribution high correlation among parameter parameter weibull hazard function age onset adenoma highest posterior correlation considering full posterior distribution maximumaposteriori estimate calibrated parameter little difference spread distribution cea outcome similar expected value perfect information evpi respectively wtp ignoring correlation posterior distribution calibrated parameter produced widest distribution cea outcome highest evpi wtp different characterization uncertainty calibrated parameter implication expect value reducing uncertainty cea ignoring inherent correlation among calibrated parameter psa overestimate value uncertainty
Statistics,linear aggregation treebased estimator,regression tree ensemble method popular method nonparametric regression combining strong predictive performance interpretable estimator order improve utility smooth response surface study regression tree random forest linear aggregation function introduce new algorithm find best axisaligned split fit optimal linear aggregation function corresponding node implement method provably fastest way algorithm enables u create interpretable tree obtain better predictive performance wide range data set also provide software package implement algorithm applying algorithm several realworld data set showcase favorable performance extensive simulation study term emse demonstrate improved interpretability resulting estimator large realworld data set
Statistics,adaptive variable selection sequential prediction multivariate dynamic model,discus bayesian model uncertainty analysis forecasting sequential dynamic modeling multivariate time series perspective decisionmaker specific forecasting objective guide thinking relevant model based formal bayesian decisiontheoretic reasoning develop timeadaptive approach exploring weighting combining selecting model differ term predictive variable included adaptivity allows change set favored model time guided specific forecasting goal synthetic example illustrates decisionguided variable selection differs traditional bayesian model uncertainty analysis standard model averaging applied study one motivating application longterm macroeconomic forecasting highlight utility new approach term improving prediction well ability identify interpret different set relevant model time respect specific defined forecasting goal
Statistics,linear regression stationary error r package slm,paper introduces r package slm stand stationary linear model package contains set statistical procedure linear regression general context error process strictly stationary short memory work setting hannan proved asymptotic normality normalized least square estimator lse mild condition error process propose different way estimate asymptotic covariance matrix lse correct type error rate usual test parameter well confidence interval procedure evaluated different set simulation two example real datasets studied
Statistics,copula density estimation finite mixture parametric copula density,copula density estimation method based finite mixture heterogeneous parametric copula density proposed specifically mixture component clayton frank gumbel normal copula density capable capturing lower tail strong central upper tail heavy tail symmetrical elliptical dependence respectively model parameter estimated interiorpoint algorithm constrained maximum likelihood problem interiorpoint algorithm compared commonly used em algorithm simulation real data application show proposed approach effective model complex dependency data dimension beyond two three
Statistics,clustering continuoustime hidden markov model,develop clustering procedure longitudinal trajectory based continuoustime hidden markov model cthmm generalized linear observation model specifically paper carry infinite mixture modelbased clustering cthmm achieve inference using markov chain monte carlo mcmc specifically bayesian nonparametric inference using dirichlet process mixture model utilize restricted gibbs sampling splitmerge proposal expedite mcmc algorithm employ proposed algorithm simulated data well large real data example result demonstrate desired performance new sampler
Statistics,spatial matérn prior fast wholebrain fmri analysis,bayesian wholebrain functional magnetic resonance imaging fmri analysis threedimensional spatial smoothing prior shown produce stateoftheart activity map without presmoothing data proposed inference algorithm computationally demanding however proposed spatial prior several le appealing property improper infinite spatial range paper proposes statistical inference framework functional magnetic resonance imaging fmri analysis based class matern covariance function framework us gaussian markov random field gmrf representation matern field via stochastic partial differential equation spde approach lindgren et al allows flexible interpretable spatial prior maintaining sparsity required fast inference highdimensional wholebrain setting develop accelerated stochastic gradient descent sgd optimization algorithm empirical bayes eb inference spatial hyperparameters conditional inferred hyperparameters make fully bayesian treatment main parameter interest brain activity coefficient apply matern prior experimental simulated taskfmri data clearly demonstrate reasonable choice previously used prior using prior simulation cross validation visual inspection resulting activation map additionally illustrate potential spde formulation derive anisotropic version matern prior
Statistics,python library empirical calibration,dealing biased data sample common task across many statistical field survey sampling bias often occurs due unrepresentative sample causal study observational data treated versus untreated group assignment often correlated covariates ie random empirical calibration generic weighting method present unified view correcting reducing data bias task mentioned provide python library ec compute empirical calibration weight problem formulated convex optimization solved efficiently dual form compared existing software ec efficient robust ec also accommodates different optimization objective support weight clipping allows inexact calibration improves usability demonstrate usage across various experiment simulated realworld data
Statistics,trialr bayesian clinical trial design r stan,manuscript introduces proglang r package called pkg trialr implement collection clinical trial method proglang stan proglang r article explore three method detail first continual reassessment method conducting phase dosefinding trial seek maximum tolerable dose second efftox dosefinding design scrutinises dos joint efficacy toxicity outcome third augmented binary method modelling probability treatment success phase ii oncology trial reference repeated measure continuous tumour size binary indicator treatment failure emphasise article benefit stem access posterior sample including flexible inference powerful visualisation hope package encourages use bayesian method clinical trial
Statistics,adaptive approximate bayesian computation tolerance selection,approximate bayesian computation abc method increasingly used inference situation likelihood function either computationally costly intractable evaluate extension basic abc rejection algorithm improved computational efficiency procedure broadened applicability abcpopulation monte carlo abcpmc approach beaumont et al become popular choice approximate sampling posterior abcpmc sequential sampler iteratively decreasing value tolerance specifies close simulated data need real data acceptance propose method adaptively selecting sequence tolerance improves computational efficiency algorithm common technique addition define stopping rule byproduct adaptation procedure assist automating termination sampling proposed automatic abcpmc algorithm easily implemented present several example demonstrating benefit term computational efficiency
Statistics,bayesian semiparametric gaussian copula approach multivariate normality test,paper bayesian semiparametric copula approach used model underlying multivariate distribution f true first dirichlet process constructed unknown marginal distribution f true gaussian copula model utilized capture dependence structure f true result bayesian multivariate normality test developed combining relative belief ratio energy distance several interesting theoretical result approach derived finally several simulated example real data set proposed approach reveals excellent performance
Statistics,spatiotemporal reconstruction global using gaussian markov random field,atmospheric inverse modelling method reconstructing historical flux greenhouse gas land atmosphere using observed atmospheric concentration atmospheric tracer transport model small number observed atmospheric concentration relation number unknown flux component make inverse problem illconditioned assumption flux needed constrain solution common practise model flux using latent gaussian field mean structure based estimated flux combination process modelling natural flux statistical bookkeeping anthropogenic emission reconstruct global co flux field modelling flux using gaussian markov random field gmrf resulting flexible computational beneficial model maternlike spatial covariance temporal covariance defined autoregressive model seasonal dependence contrast previous inversion flux defined spatially continuous domain traditionally discrete flux representation replaced integrated flux resolution specified transport model formulation remove aggregation error flux covariance due traditional representation area integral flux discrete point provides model closer resembling reallife spacetime continuous flux
Statistics,bayesian variable selection gaussian copula regression model,develop novel bayesian method select important predictor regression model multiple response diverse type particular sparse gaussian copula regression model used account multivariate dependency combination discrete continuous response association set predictor utilize parameter expansion data augmentation strategy construct markov chain monte carlo algorithm estimation parameter latent variable model based centered parametrization gaussian latent variable design efficient proposal distribution update jointly latent binary vector important predictor corresponding nonzero regression coefficient proposed strategy tested simulated data applied two real data set response consist lowintensity count binary ordinal continuous variable
Statistics,efficient bayesian parcor approach dynamic modeling multivariate time series,bayesian lattice filtering smoothing approach proposed fast accurate modeling inference multivariate nonstationary time series approach offer computational feasibility interpretable timefrequency analysis multivariate context proposed framework allows u obtain posterior estimate timevarying spectral density individual time series component well posterior measurement timefrequency relationship across multiple component timevarying coherence partial coherence proposed formulation considers multivariate dynamic linear model mdlms forward backward timevarying partial autocorrelation coefficient tvvparcor computationally expensive scheme posterior inference multivariate dynamic parcor model avoided using approximation mdlm context approximate inference corresponding timevarying vector autoregressive tvvar coefficient obtained via whittle algorithm key aspect proposed tvvparcor representation lower dimension therefore efficient tvvar representation performance tvvparcor model illustrated simulation study analysis multivariate nonstationary temporal data arising neuroscience environmental application model performance evaluated using goodnessoffit measurement timefrequency domain also assessing quality shortterm forecasting
Statistics,conjugate nearest neighbor gaussian process model efficient statistical interpolation large spatial data,key challenge spatial statistic analysis massive spatiallyreferenced data set analysis often proceed gaussian process specification produce rich robust inference involve dense covariance matrix lack computationally exploitable structure matrix computation required fitting model involve floating point operation cubic order number spatial location dynamic memory storage quadratic order recent development spatial statistic offer variety massively scalable approach bayesian inference hierarchical model particular gained popularity due richness flexibility accommodating spatial process current contribution provide computationally efficient exact algorithm spatial interpolation massive data set using scalable spatial process combine lowrank gaussian process efficient sparse approximation following recent work model lowrank process using gaussian predictive process gpp residual process sparsityinducing nearestneighbor gaussian process nngp key contribution implement model using exact conjugate bayesian modeling avoid expensive iterative algorithm simulation study evaluate performance proposed approach robustness model especially long range prediction implement approach remotely sensed light detection ranging lidar data collected u forest service tanana inventory unit tiu remote portion interior alaska
Statistics,ice model calibration using semicontinuous spatial data,rapid change earth cryosphere caused human activity lead significant environmental impact computer model provide useful tool understanding behavior projecting future arctic antarctic ice sheet however model typically subject large parametric uncertainty due poorly constrained model input parameter govern behavior simulated ice sheet computer model calibration provides formal statistical framework infer parameter using observational data quantify uncertainty projection due uncertainty parameter calibration ice sheet model often challenging relevant model output observational data take form semicontinuous spatial data point mass zero rightskewed continuous distribution positive value current calibration approach handle data introduce hierarchical latent variable model handle binary spatial pattern positive continuous spatial pattern separate component overcome challenge due highdimensionality use likelihoodbased generalized principal component analysis impose lowdimensional structure latent variable spatial dependence apply methodology calibrate physical model antarctic ice sheet demonstrate overcome aforementioned modeling computational challenge result calibration obtain improved future icevolume change projection
Statistics,que será será uncertainty estimation featurebased time series forecast,interval forecast significant advantage accounting uncertainty estimation point forecast highlighting importance providing prediction interval pi well point forecast forecasting activity paper general featurebased framework outlined examine relationship time series feature interval forecasting accuracy provide reliable forecast well uncertainty estimation specifically framework divided training testing phase training part use collection time series train model explore time series feature affect interval forecasting accuracy different forecasting method make proposed framework interpretable term contribution feature model uncertainty prediction effect analysis applied assign weight various benchmark method purpose reducing uncertainty forecast testing part calculate point forecast pi new series using trained model weight determination process obtained training phase illustrate whether point interval forecast featurebased forecasting framework outperforms individual benchmark method simple equally weighted combination different confidence level competition data improved computational efficiency
Statistics,multifidelity computer model emulation highdimensional output application storm surge,hurricanedriven storm surge one deadly costly natural disaster making precise quantification surge hazard great importance inference system done physicsbased computer model process surge simulator implemented wide range fidelity level computational burden varying several order magnitude due nature system danger posed surge make greater fidelity highly desirable however model highvolume output tend come great computational cost make detailed study coastal flood hazard prohibitive need make development emulator combining highdimensional output multiple complex computer model different fidelity level important propose parallel partial autoregressive cokriging model predict highlyaccurate storm surge computationally efficient way large spatial domain emulator capability predicting storm surge accurately highfidelity computer model given storm characteristic allows accurate assessment hazard storm surge large spatial domain
Statistics,adaptive bayesian slope highdimensional model selection missing value,consider problem variable selection highdimensional setting missing observation among covariates address relatively understudied problem propose new synergistic procedure adaptive bayesian slope effectively combine slope method sorted regularization together spikeandslab lasso method position approach within bayesian framework allows simultaneous variable selection parameter estimation despite missing value spikeandslab lasso coefficient regarded arising hierarchical model consisting two group spike inactive slab active however instead assigning independent spike prior covariate deploy joint slope spike prior take account ordering coefficient magnitude order control false discovery extensive simulation demonstrate satisfactory performance term power fdr estimation bias wide range scenario finally analyze real dataset consisting patient paris hospital underwent severe trauma show excellent performance predicting platelet level methodology implemented c wrapped r package abslope public use
Statistics,predicting phenotype brain connection structure,article focus problem predicting response variable based networkvalued predictor particular motivation developing interpretable accurate predictive model cognitive trait neuropsychiatric disorder based individual brain connection network connectome current method focus reducing complex highdimensional brain network lowdimensional set prespecified feature prior applying standard predictive algorithm method sensitive feature choice inevitably discard information instead propose nonparametric bayes class model utilize information entire adjacency matrix defining connection among brain region adaptively defining flexible predictive algorithm maintaining interpretability proposed bayesian connectomics bacon model class utilizes poissondirichlet process detect lowerdimensional bidirectional covariate subject pattern adjacency matrix small n large p problem transformed small n small q problem facilitating effective stochastic search predictor spikeandslab prior cluster predictor strike balance regression model parsimony flexibility resulting improved inference test case prediction describe basic property bacon model class develop efficient algorithm posterior computation resulting method shown outperform existing approach simulation applied creative reasoning data set
Statistics,generalized evolutionary point process model specification model comparison,generalized evolutionary point process offer class point process model allows either excitation inhibition based upon history process regard propose modeling comprises generalization nonlinear hawkes process working within bayesian framework model fitting implemented markov chain monte carlo entail discussion computation likelihood point pattern furthermore class model discus strategy model comparison using simulation illustrate well distinguish model point pattern specification conditionally independent event time eg poisson process specifically demonstrate model correctly identify true relationship ie excitation inhibitioncontrol consider novel extension log gaussian cox process incorporates evolutionary behavior illustrate model comparison approach prefers evolutionary log gaussian cox process compared simpler model also examine real dataset consisting violent crime event police district chicago year data exhibit strong daily seasonality change across year account data attribute find significant mild selfexcitation implying event occurrence increase intensity future event
Statistics,estimating spatiallysmoothed fiber orientation distribution,diffusionweighted magnetic resonance imaging dmri invivo noninvasive imaging technology probe anatomical architecture biological sample anatomy white matter fiber tract brain revealed help understanding connectivity pattern among different brain region paper propose novel nearestneighbor adaptive regression model narm adaptive estimation fiber orientation distribution fod function based dmri data spatial homogeneity used improve fod estimation incorporating neighborhood information specifically formulate fod estimation problem weighted linear regression problem weight chosen account spatial proximity potential heterogeneity due different fiber configuration weight adaptively updated stopping rule based nearest neighbor distance designed prevent oversmoothing narm extended accommodate dmri data multiple bvalues comprehensive simulation result demonstrate narm lead satisfactory fod reconstruction performs better voxelwise estimation well competing smoothing method applying narm real dmri datasets demonstrate effectiveness narm recovering realistic crossing fiber pattern producing coherent fiber tracking result establishing practical value narm analyzing dmri data providing reliable information brain structural connectivity
Statistics,modeling material stress using integrated gaussian markov random field,equation physical constitutive model material stress within tantalum grain solved numerically using tetrahedrally meshed volume resulting output included scalar vonmises stress tetrahedron within finite element discretization paper define intricate statistical model spatial field vonmises stress us given grain geometry fundamental way model relates threedimensional field integral latent stochastic process defined vertex one twodimensional grain boundary intuitive neighborhood structure said boundary node suggested use latent gaussian markov random field gmrf however despite potential computational gain afforded gmrfs integral nature model sheer number data point pose substantial challenge full bayesian analysis overcome problem encourage efficient exploration posterior distribution number technique combined parallel computing sparse matrix method modification block update strategy within sampling routine addition use auxiliary variable approach accommodate presence outlier data
Statistics,drhotnet r package detecting differential risk hotspot linear network,one common application spatial data analysis detecting zone certain investigation level pointreferenced event study especially concentrated detection kind zone usually referred hotspot essential certain field criminology epidemiology traffic safety traditionally hotspot detection procedure developed areal unit analysis although working spatial scale suitable enough many research practical purpose detecting hotspot accurate level instance road segment level may convenient sometimes furthermore typical hotspot detection procedure entirely focused determination zone event overall highly concentrated le common far procedure prioritize location zone specific type event overrepresented relation type observed denoted differential risk hotspot r package drhotnet provides several functionality facilitate detection differential risk hotspot along linear network paper drhotnet depicted usage r console shown detailed analysis crime dataset
Statistics,computer model emulation highdimensional functional output largescale observing system uncertainty experiment,observing system uncertainty experiment osues widely used costeffective way make retrieval quality assessment nasa orbiting carbon mission one important component retrieval algorithm fullphysics forward model describes relationship atmospheric variable carbon dioxide radiance measured remote sensing instrument forward model complicated computationally expensive largescale osue requires evaluation model numerous time make infeasible operational usage tackle issue develop statistical emulator facilitate efficient largescale osues remote sensing emulator represents radiance output irregular wavelength via linear combination basis function random coefficient random coefficient modeled nearestneighbor gaussian process builtin input dimension reduction via active subspace proposed emulator reduces dimensionality input space output space fast computation achieved within fully bayesian inference framework validation experiment demonstrate emulator outperforms reduced order model approximates fullphysics forward model
Statistics,practical introduction regression discontinuity design foundation,element accompanying element matias cattaneo nicolas idrobo rocio titiunik provide accessible practical guide analysis interpretation regression discontinuity rd design encourages use common set practice facilitates accumulation rdbased empirical evidence element author discus foundation canonical sharp rd design following feature score continuously distributed one dimension ii one cutoff iii compliance treatment assignment perfect accompanying element author discus practical conceptual extension basic rd setup
Statistics,nonparametric universal copula modeling,handle ubiquitous problem dependence learning copula quickly becoming pervasive tool across wide range datadriven discipline encompassing neuroscience finance econometrics genomics social science machine learning healthcare many copula connection function invented abe sklar response query maurice frechet year stand article provides history key development offer unified perspective
Statistics,estimation auction model shape restriction,introduce several new estimation method leverage shape constraint auction model estimate various object interest including distribution bidder valuation bidder ex ante expected surplus seller counterfactual revenue basic approach applies broadly unlike literature work wide range auction format allows asymmetric bidder though approach restrictive focus analysis first price sealed bid auction independent private valuation highlight two nonparametric estimation strategy one based least square criterion maximum likelihood criterion also provide first direct estimator strategy function establish several theoretical property method guide empirical analysis inference addition providing asymptotic distribution estimator identify way methodological choice tailored object interest object like bidder ex ante surplus seller counterfactual expected revenue additional symmetric bidder show input parameter free estimator achieve semiparametric efficiency bound object like bidder inverse strategy function provide easily implementable boundary corrected kernel smoothing transformation method order ensure squared error integrable entire support valuation extensive simulation study illustrates analytical result demonstrates respective advantage least square maximum likelihood estimator finite sample compared estimation strategy based kernel density estimation simulation indicate smoothed version estimator enjoy large degree robustness choice input parameter
Statistics,missing data analysis imputation via latent gaussian markov random field,paper recast problem missing value covariates regression model latent gaussian markov random field gmrf model fully bayesian framework proposed approach based definition covariate imputation submodel latent effect gmrf structure show formulation work continuous covariates provide insight could extended categorical covariates resulting bayesian hierarchical model naturally fit within integrated nested laplace approximation inla framework use model fitting hence work fill important gap inla methodology allows treat model missing value covariates fully bayesian framework relying inla model fitting possible formulate joint model data imputed covariates missingness mechanism way able tackle general problem assessing missingness mechanism conducting sensitivity analysis different alternative model nonobserved covariates finally illustrate proposed approach two example modeling health risk factor disease mapping rely two different imputation mechanism based typical multiple linear regression spatial model respectively given speed model fitting inla able fit joint model short time easily conduct sensitivity analysis
Statistics,stochastic approximation em exploratory item factor analysis,stochastic approximation em algorithm saem described estimation item person parameter given test data coded dichotomous ordinal variable method hinge upon eigenanalysis missing variable sampled augmented data augmented data approach introduced albert seminal work applying gibbs sampling item response theory similar maximum likelihood factor analysis factor structure bayesian approach depends sufficient statistic computed missing latent data second feature saem algorithm use robbinsmonro procedure establishing convergence contrary expectation maximization method costly integral must calculated method wellsuited highly multidimensional data annealing method implemented prevent convergence local maximum likelihood multiple calculation error applied within framework markov chain monte carlo presented delineate uncertainty parameter estimate given nature efa exploratory factor analysis algorithm formalized leveraging tracywidom distribution retention factor extracted eigenanalysis sufficient statistic covariance augmented data matrix simulation condition dichotomous polytomous data one ten dimension factor loading used ass statistical accuracy gauge computational time efa approach irtspecific implementation saem algorithm finally three application methodology also reported demonstrate effectiveness method enabling timely analysis well substantive interpretation method applied real data
Statistics,numerically stable algorithm integrating bayesian model using markov melding,statistical analysis consider multiple data source markov melding provides method combining sourcespecific bayesian model model often contain different quantity information due variation richness modelspecific data availability modelspecific prior information show make multistage markov chain monte carlo sampler employed markov melding unstable unreliable propose robust multistage algorithm estimate required prior marginal selfdensity ratio using weighted sample dramatically improving accuracy tail distribution thus stabilising algorithm providing reliable inference demonstrate approach using evidence synthesis inferring hiv prevalence evidence synthesis influenza
Statistics,supervised functional pca covariate dependent mean covariance structure,incorporating covariate information functional data analysis method substantially improve modeling prediction performance however many functional data analysis method make use covariate supervision information often high computational cost assume score related covariates assumption usually violated practice article propose functional data analysis framework relates mean covariance function covariate information facilitate modeling ensure covariance function positive semidefinite represent using spline design map euclidean space symmetric positive semidefinite matrix manifold model combined roughness penalty encourage smoothness estimated function temporal covariate domain also develop efficient method fast evaluation objective gradient function crossvalidation used choose tuning parameter demonstrate advantage approach simulation study astronomical data analysis
Statistics,bayesian spatial homogeneity pursuit regression count value data,spatial regression model ubiquitous many different area environmental science geoscience public health exploring relationship response variable covariates complex spatial pattern important work paper propose novel spatially clustered coefficient regression model count value data based nonparametric bayesian method proposed method detects spatial homogeneity poisson regression coefficient markov random field constraint mixture finite mixture prior provides consistent estimator number cluster regression coefficient geographically neighborhood information theoretical property proposed method established efficient markov chain monte carlo algorithm developed using multivariate log gamma distribution base distribution extensive simulation study carried examine empirical performance proposed method additionally analyze georgia premature death data illustration effectiveness approach
Statistics,probabilistic elicitation expert knowledge assessment computer simulation,present new method probabilistic elicitation expert knowledge using binary response human expert assessing simulated data statistical model parameter subject uncertainty binary response describe either absolute realism individual simulation relative realism pair simulation two alternative version approach version provides nonparametric representation expert belief distribution value model parameter without demanding assertion opinion parameter value framework also integrates use active learning efficiently query expert possibility additionally provide useful misspecification diagnostic validate method automatic expert judging binomial distribution human expert judging distribution voter across political party united state norway method provide flexible meaningful representation human expert belief correctly identifying higher dispersion voter party norway
Statistics,pursuing source heterogeneity modeling clustered population,researcher often deal heterogeneous population mixed regression relationship increasingly era data explosion problem many candidate predictor interest identify predictor associated outcome also distinguish true source heterogeneity ie identify predictor different effect among cluster thus true contributor formation cluster clarify concept source heterogeneity account potential scale difference cluster propose regularized finite mixture effect regression achieve heterogeneity pursuit feature selection simultaneously name suggests problem formulated effectsmodel parameterization cluster label missing effect predictor outcome decomposed common effect term set clusterspecific term constrained sparse estimation effect lead identification variable common effect heterogeneous effect propose efficient algorithm show approach achieve estimation selection consistency simulation study demonstrate effectiveness method various practical scenario three application presented namely imaging genetics study linking genetic factor brain neuroimaging trait alzheimer disease public health study exploring association suicide risk among adolescent school district characteristic sport analytics study understanding salary level baseball player associated performance contractual status
Statistics,variable selection multiplyimputed datasets choosing stacked grouped method,penalized regression method lasso elastic net used many biomedical application simultaneous regression coefficient estimation variable selection desired however missing data complicates implementation method particularly missingness handled using multiple imputation applying variable selection algorithm imputed dataset likely lead different set selected predictor making difficult ascertain final active set without resorting ad hoc combination rule paper consider general class penalized objective function construction force selection variable across multiplyimputed datasets pooling objective function across imputation optimization performed jointly imputed datasets rather separately dataset consider two objective function formulation exist literature refer stacked grouped objective function building existing work derive implement efficient cyclic coordinate descent majorizationminimization optimization algorithm continuous binary outcome data b incorporate adaptive shrinkage penalty c compare method simulation develop r package miselect easy implementation simulation demonstrate stacked objective function approach tend computationally efficient better estimation selection property apply method data university michigan al patient repository umapr aim identify association persistent organic pollutant al risk
Statistics,model selection criterion standard censored regression model based bootstrap sample augmentation mechanism,statistical regression technique extraordinarily essential data fitting tool explore potential possible generation mechanism random phenomenon therefore model selection variable selection becoming extremely important identify appropriate model optimal explanation effect interesting response paper discus compare bootstrapbased model selection criterion standard censored regression model tobit regression model circumstance limited observation information monte carlo numerical evidence demonstrates performance model selection criterion based bootstrap sample augmentation strategy become competitive alternative one akaike information criterion aic bayesian information criterion bic etc circumstance inadequate observation information meanwhile numerical simulation experiment demonstrate model identification risk due deficiency data information high censoring rate rather limited number observation adequately compensated increasing scientific computation cost term bootstrap sample augmentation strategy also apply recommended bootstrapbased model selection criterion tobit regression model fit real fidelity dataset
Statistics,super scalable algorithm short segment detection,many application copy number variant cnv detection goal identify short segment observation different mean median background segment usually short hidden long sequence hence challenging find study super scalable short segment detection algorithm paper nonparametric method cluster location observation exceed threshold segment detection computationally efficient rely gaussian noise assumption moreover develop framework assign significance level detected segment demonstrate advantage proposed method theoretical simulation real data study
Statistics,rocnreg r package receiver operating characteristic curve inference without covariate information,receiver operating characteristic roc curve popular tool used evaluate discriminatory capability diagnostic testsbiomarkers measured continuous scale distinguishing two alternative disease state eg diseased nondiseased circumstance test performance discriminatory ability may vary according subjectspecific characteristic different test setting case informationspecific accuracy measure covariatespecific covariateadjusted roc curve needed ignoring covariate information may lead biased erroneous result paper introduces r package rocnreg allows estimating pooled unadjusted roc curve covariatespecific roc curve covariateadjusted roc curve different method semi parametric nonparametric perspective within bayesian frequentist paradigm estimated roc curve pooled covariatespecific covariateadjusted several summary measure accuracy partial area roc curve youden index obtained package also provides function obtain rocbased optimal threshold value using several criterion namely youden index criterion criterion set target value false positive fraction bayesian method provide tool assessing model fit via posterior predictive check model choice carried via several information criterion numerical graphical output provided method package illustrated analysis data endocrine study aim ass capability body mass index detect presence absence cardiovascular disease risk factor package available cran http cranrprojectorgpackagerocnreg
Statistics,poissontweedie mixedeffects model flexible approach analysis longitudinal rnaseq data,present new modelling approach longitudinal count data motivated increasing availability longitudinal rnasequencing experiment distribution rnaseq count typically exhibit overdispersion zeroinflation heavy tail moreover longitudinal design repeated measurement subject typically positively correlated propose generalized linear mixed model based poissontweedie distribution flexibly handle aforementioned feature longitudinal rnaseq count develop computational approach accurately evaluate likelihood proposed model perform maximum likelihood estimation approach implemented r package ptmixed freely downloaded cran ass performance ptmixed simulated data present application dataset longitudinal rnasequencing measurement healthy dystrophic mouse applicability poissontweedie mixedeffects model restricted longitudinal rnaseq data extends scenario nonindependent measurement discrete overdispersed response variable available
Statistics,integrative bayesian model using postselective inference case study radiogenomics,identifying direct link gene pathway clinical endpoint highly fatal disease cancer formidable task integrative analysis play crucial role modeling link relying upon association wealth intermediary variable genomic measurement motivated harness phenotypic information tumor towards molecular characterization lowgrade glioma develop data driven bayesian framework define sharp model calibrate accurately efficiently uncertainty associated promising biological finding bayesian method propose article amenable flexible class adaptive model determined via complex interplay signal sifted variable selection algorithm domain specific knowledge ii advantage computationally efficient high dimensional inference due focus sharp model fewer parameter compared nonadaptive counterpart iii exhibit significantly better reconciliation model adaptivity inferential power stateofart approach constrained curse dimensionality main workforce involves carefully constructed conditional likelihood utilizes reparameterization map obtain compact formula selectioncorrected posterior deploying method investigate radiogenomic characteristic diffuse lowgrade glioma successfully uncover association several biologically important gene pathway patient survival time
Statistics,simultaneous nongaussian component analysis sing data integration neuroimaging,advance technology allow acquisition complementary information increasingly common scientific study collect multiple datasets largescale neuroimaging study often include multiple modality eg task functional mri restingstate fmri diffusion mri andor structural mri aim understand relationship datasets study seek understand whether region brain activated working memory task relate restingstate correlation neuroimaging popular approach us principal component analysis dimension reduction prior feature extraction via joint independent component analysis may discard biological feature low variance introduce simultaneous nongaussian component analysis sing dimension reduction feature extraction achieved simultaneously shared information captured via subject score apply method working memory task restingstate correlation human connectome project find joint structure evident learned spatial correspondence moreover subject score related fluid intelligence
Statistics,linear mixed model formulation spatiotemporal random process computational advance separable productsum covariance,describe spatiotemporal random process using linear mixed model show many commonly used model viewed special case general framework pay close attention model separable productsum covariance proposed linear mixed model formulation facilitates implementation novel algorithm using stegle eigendecompositions recursive application shermanmorrisonwoodbury formula helmertwolf blocking efficiently invert separable productsum covariance matrix even every spatial location observed every time point show algorithm provides noticeable improvement standard cholesky decomposition approach via simulation ass performance separable productsum covariance identify scenario separable covariance noticeably inferior productsum covariance also compare likelihoodbased semivariogrambased estimation discus benefit drawback use proposed approach analyze daily maximum temperature data oregon usa summer end offering guideline choosing among covariance estimation method based property observed data
Statistics,detecting abrupt change presence local fluctuation autocorrelated noise,whilst plethora algorithm detecting change mean univariate timeseries almost struggle real application autocorrelated noise mean fluctuates locally abrupt change one wish detect case default implementation often based assumption constant mean change independent noise lead substantial overestimation number change propose principled approach detect abrupt change model local fluctuation random walk process autocorrelated noise via ar process estimate number location changepoints minimising penalised cost based model develop novel efficient dynamic programming algorithm decaf solve minimisation problem despite additional challenge dependence across segment due autocorrelated noise make existing algorithm inapplicable theory empirical result show approach greater power detecting abrupt change existing approach apply method measuring gene expression level bacteria
Statistics,confidence interval prediction accuracy measure multivariable prediction model based bootstrapbased optimism correction method,assessing prediction accuracy multivariable prediction model optimism correction essential preventing biased result however published paper clinical prediction model point estimate prediction accuracy measure corrected adequate bootstrapbased correction method confidence interval corrected eg delong confidence interval usually used assessing cstatistic naive method adjust optimism bias account statistical variability estimation parameter prediction model therefore coverage probability true value prediction accuracy measure seriously nominal level eg article provide two generic bootstrap method namely locationshifted bootstrap confidence interval twostage bootstrap confidence interval generally applied bootstrapbased optimism correction method ie harrell bias correction method addition widely applied various method prediction model development involving modern shrinkage method ridge lasso regression numerical evaluation simulation proposed confidence interval showed favourable coverage performance besides current standard practice based optimismuncorrected method showed serious undercoverage property avoid erroneous result optimismuncorrected confidence interval used practice adjusted method recommended instead also developed r package predboot implementing method http githubcomnomahipredboot effectiveness proposed method illustrated via application gustoi clinical trial
Statistics,multivariate tailinflated normal distribution application finance,paper introduces multivariate tailinflated normal mtin distribution elliptical heavytails generalization multivariate normal mn mtin belongs family mn scale mixture choosing convenient continuous uniform mixing distribution moreover closedform probability density function characterized one additional inflation parameter respect nested mn governing tailweight first four moment also computed interestingly always exist excess kurtosis assume positive value method moment maximum likelihood ml considered estimation concern latter direct approach well variant em algorithm illustrated existence ml estimate also evaluated since inflation parameter estimated data robust estimate mean vector covariance matrix nested mn distribution automatically obtained downweighting simulation performed compare estimation methodsalgorithms investigate ability aic bic select among set candidate elliptical model illustrative purpose mtin distribution finally fitted multivariate financial data usefulness also shown comparison wellestablished multivariate elliptical distribution
Statistics,hamiltonian mcmc method estimating rare event probability highdimensional problem,accurate efficient estimation rare event probability significant importance since often occurrence event widespread impact focus work precisely quantifying probability often encountered reliability analysis complex engineering system based introduced framework termed approximate sampling target postprocessing adjustment astpa herein integrated supported gradientbased hamiltonian markov chain monte carlo hmcmc method basic idea construct relevant target distribution weighting highdimensional random variable space onedimensional output likelihood model using limitstate function sample target distribution exploit hmcmc algorithm family mcmc method adopts physical system dynamic rather solely using proposal probability distribution generate distant sequential sample develop new quasinewton mass preconditioned hmcmc scheme qnphmcmc particularly efficient suitable highdimensional space eventually compute rare event probability original postsampling step devised using inverse importance sampling procedure based already obtained sample statistical property estimator analyzed well performance proposed methodology examined detail compared subset simulation series challenging low highdimensional problem
Statistics,qgam bayesian nonparametric quantile regression modelling r,generalized additive model gam flexible nonlinear regression model fitted efficiently using approximate bayesian method provided mgcv r package gam method provided mgcv based assumption response distribution modelled parametrically discus flexible method entail parametric assumption particular article introduces qgam package extension mgcv providing fast calibrated bayesian method fitting quantile gam qgams r qgams based smooth version pinball loss koenker rather likelihood function hence jointly achieving satisfactory accuracy quantile point estimate coverage corresponding credible interval requires adopting specialized bayesian fitting framework fasiolo wood zaffran nedellec goude detail framework implemented qgam provide example illustrating package used practice
Statistics,analyzing effect observation function selection ensemble kalman filtering epidemic model,ensemble kalman filter enkf bayesian filtering algorithm utilized estimating unknown model state parameter nonlinear system important component enkf observation function connects unknown system variable observed data function take different form based modeling assumption respect available data relevant system parameter goal research analyze effect observation function selection enkf setting epidemic modeling variety observation function used literature particular four observation function different form various level complexity examined connection classic susceptibleinfectiousrecovered sir model result demonstrate importance choosing observation function well interprets available data corresponding enkf estimate several filtering scenario including state estimation known parameter combined state parameter estimation constant timevarying parameter numerical experiment illustrate modifying observation noise covariance matrix filter help account uncertainty observation function certain case
Statistics,mixture linear expert model censored data novel approach scalemixture normal distribution,classical mixture linear expert moe model one widespread statistical framework modeling classification clustering data built normality assumption error term mathematical computational convenience classical moe model two challenge sensitive atypical observation outlier might produce misleading inferential result censored data paper aimed resolve two challenge simultaneously proposing novel robust moe model modelbased clustering discriminant censored data scalemixture normal class distribution unobserved error term based novel model develop analytical expectationmaximization em type algorithm obtain maximum likelihood parameter estimate simulation study carried examine performance effectiveness robustness proposed methodology finally real data used illustrate superiority new model
Statistics,overlappingsample mendelian randomisation multiple exposure bayesian approach,background mendelian randomization mr widely applied causal inference medical research us genetic variant instrumental variable iv investigate putative causal relationship exposure outcome traditional mr method dominantly focussed twosample setting ivexposure association study ivoutcome association study independent however uncommon participant two study fully overlap onesample partly overlap overlappingsample method proposed method applicable three sample setting essence converted two overlapping sample problem onesample problem data individual incomplete assume individual drawn population unmeasured data missing random unobserved data treated au pair model parameter unknown quantity thus could imputed iteratively conditioning observed data estimated parameter using markov chain monte carlo generalised model allow pleiotropy multiple exposure assessed performance number simulation using four metric mean standard deviation coverage power result higher sample overlapping rate stronger instrument led estimate higher precision power pleiotropy notably negative impact estimate nevertheless overall coverage high model performed well sample setting conclusion model offer flexibility applicable sample setting important addition mr literature restricted one two sample scenario given nature bayesian inference easily extended complex mr analysis medical research
Statistics,merlin r package mixed effect regression linear nonlinear userdefined model,r package merlin performs flexible joint modelling hierarchical multioutcome data increasingly multiple longitudinal biomarker measurement possibly censored timetoevent outcome baseline characteristic available however limited software allows information incorporated one model paper present merlin allows estimation model unlimited number continuous binary count timetoevent outcome unlimited level nested random effect wide variety link function including expected value gradient shared random effect available order link different outcome biologically plausible way accompanying predictmerlin function allows individual population level prediction made even complex model option specify userdefined family making merlin ideal methodological research flexibility merlin illustrated using example patient followed heart valve replacement beginning linear model finishing joint multiple longitudinal competing risk survival model
Statistics,framework list representation enabling list stabilization incorporation gene exchangeability,analysis multivariate data set eg microarray study frequently result list gene associated response interest biological interpretation often complicated statistical instability obtained gene list respect sampling variation may partly due functional redundancy among gene implying multiple gene play exchangeable role cell paper use concept exchangeability random variable model functional redundancy thereby account instability attributable sampling variation present flexible framework incorporate exchangeability representation list proposed framework support straightforward robust comparison two list also used generate new stable gene ranking incorporating information experimental data using microarray data set lung cancer patient show proposed method provides robust gene ranking existing method respect sampling variation without compromising biological significance
Statistics,going grid computationally efficient inference loggaussian cox process,paper introduces new method performing computational inference loggaussian cox process likelihood approximated directly making novel use continuously specified gaussian random field show sufficiently smooth gaussian random field prior distribution approximation converge arbitrarily high order approximation based counting process partition domain achieves firstorder convergence given result improve general theory convergence stochastic partial differential equation model introduced lindgren et al new method demonstrated standard point pattern data set two interesting extension classical loggaussian cox process framework discussed first extension considers variable sampling effort throughout observation window implement method chakraborty et al second extension construct loggaussian cox process world ocean analysis performed using integrated nested laplace approximation fast approximate inference
Statistics,exact sampling counting fixedmargin matrix,uniform distribution matrix specified row column sum often natural choice null model testing structure twoway table binary nonnegative integer due difficulty sampling distribution many approximate method developed show exploiting certain symmetry exact sampling counting fact possible many nontrivial realworld case illustrate real datasets including ecological cooccurrence matrix contingency table
Statistics,powerlaw noise general spatial domain nonstandard mesh,powerlaw noise abound nature observed extensively time series spatially varying environmental parameter although recent year seen extension traditional stochastic partial differential equation include system driven fractional brownian motion spatially distributed scaleinvariance received comparatively little attention especially parameter defined nonstandard spatial domain paper discus generalization powerlaw noise general spatial domain outlining theoretical underpinnings well addressing numerical simulation arbitrary mesh three computational algorithm presented efficiently generating sample path accompanied numerous numerical illustration
Statistics,global sensitivity analysis model spatially dependent output,global sensitivity analysis complex numerical model often call estimation variancebased importance measure named sobol index metamodelbased technique developed order replace cpu timeexpensive computer code inexpensive mathematical function predicts computer code output common metamodelbased sensitivity analysis method wellsuited computer code scalar output however environmental domain many area application numerical model output often spatial map may also vary time paper introduce innovative method obtain spatial map sobol index minimal number numerical model computation based upon functional decomposition spatial output onto wavelet basis metamodeling wavelet coefficient gaussian process analytical example presented clarify various step methodology technique applied real hydrogeological case model input variable spatial map sobol index thus obtained
Statistics,surface response analysis determination confidence region atmospheric global warming study usa data,starting atmospheric measurement taken hawaii quadratic model interaction fitted using attributable variable surface response analysis returned eigenvalue eigenvectors critical point turn mixed type two positive eigenvalue one null rest negative data derived confidence region two variable various type elliptic hyperbolic degenerate based result indicate determine twodimensional confidence region statisticallysignificant variable relevant contributor atmospheric emission
Statistics,contributor carbon dioxide atmosphere europe,carbon dioxide along atmospheric temperature interacting cause defined global warming present study develop statistical model using real data identify attributable variable risk factor cause emission atmosphere europe scientist believe nineteen attributable variable cause atmosphere however study identified three individual risk factor five interaction among attributable variable cause almost emission atmosphere europe rank risk factor interaction according amount generate addition compare present finding european data similar study continental united state example u liquid fuel rank number one europe gas fuel fact liquid fuel europe least contributable variable atmosphere gas fuel rank seventh
Statistics,contributor carbon dioxide atmosphere europe surface response analysis,paper continuation statistical modeling nonlinear relationship atmospheric attributable variable account emission based data eu country order compare relevant finding obtained case u data current study initiated leading optimal secondorder model based three linear term five secondorder term conclude study present work finding canonical decomposition nonlinear model computing specific twodimensional confidence region lead use model order quantify net effect various risk factor compare result obtained u case
Statistics,multifidelity variance reduction pickfreeze sobol index estimation,many mathematical model involve input parameter precisely known global sensitivity analysis aim identify parameter whose uncertainty largest impact variability quantity interest output model one statistical tool used quantify influence input variable output sobol sensitivity index estimated using large sample evaluation output propose variance reduction technique based availability fast approximation output enable significant computational saving output costly evaluate
Statistics,asymptotic normality efficiency two sobol index estimator,many mathematical model involve input parameter precisely known global sensitivity analysis aim identify parameter whose uncertainty largest impact variability quantity interest output model one statistical tool used quantify influence input variable output sobol sensitivity index consider statistical estimation index finite sample model output present two estimator state central limit theorem show one estimator optimal asymptotic variance also generalize result case true output observable replaced noisy version
Statistics,sampling plan controlinspection scheme independent dependent sampling design application photovoltaics,evaluation produced item time delivery practice usually amended least one inspection later time point extend methodology acceptance sampling variable arbitrary unknown distribution additional sampling infor mation available setting based appropriate approximation operating characteristic derive new acceptance sampling plan control overall operating characteristic result cover case independent sampling well case dependent sampling particular study modified panel sampling design case spatial batch sampling latter advisable photovoltaic field monitoring study since allows detect analyze local cluster degraded damaged module finite sample property examined simulation study focusing accuracy estimation
Statistics,alternative continuous univariate distribution supported bounded interval bmt distribution,paper introduce bmt distribution unimodal alternative continuous univariate distribution supported bounded interval idea behind mathematical formulation new distribution come computer aid geometric design specifically bezier curve first review general property distribution given parametric equation extend definition bezier distribution proposing bmt cumulative distribution function derive probability density function closedform expression quantile function median interquartile range mode moment domain change c mentioned estimation parameter approached method maximum likelihood maximum product spacing test numerical estimation procedure using simulated data usefulness flexibility new distribution illustrated three real data set bmt distribution significant potential estimate domain parameter model data outside scope beta similar distribution
Statistics,learning large scale ordinary differential equation system,learning large scale nonlinear ordinary differential equation ode system data known computationally statistically challenging present framework together adaptive integral matching aim algorithm learning polynomial rational ode system sparse network structure framework allows time course data sampled multiple environment representing eg different intervention perturbation system algorithm aim combine initial penalised integral matching step adapted least square step based solving ode numerically r package episode implement aim together several algorithm available cran shown aim achieves stateoftheart network recovery silico phosphoprotein abundance data eighth dream challenge auroc demonstrated via range numerical example aim good statistical property computationally feasible even large system
Statistics,higherorder approximate confidence interval,standard confidence interval employed applied statistical analysis usually based asymptotic approximation approximation considerably inaccurate small moderate sized sample derive accurate confidence interval based higherorder approximate quantiles score function coverage approximation error n approximation error confidence interval based asymptotic normality mles n monte carlo simulation confirm theoretical finding implementation regression model real data application provided
Statistics,strong uniform consistency relative error regression function estimator censoring time series model,consider random vector x x ddimensional onedimensional suppose random variable subject random right censoring satisfies alpha mixing property aim paper study behavior kernel estimator relative error regression establish uniform almost sure consistency rate furthermore highlighted covariance term measure dependency simulation study show proposed estimator performs well finite sample size different case
Statistics,exact model comparison plausibility framework,plausibility formalization exact test parametric model generalizes procedure fisher exact test resulting test based cumulative probabilies probability density function goodnessoffit interpretation exact control alpha level finite sample size model comparison possible approach generalize plausibility incorporating weighing allows perform model comparison show one weighing scheme asymptotically equivalent likelihood ratio test lrt finite sample guarantee test size null hypothesis unlike lrt confirm theoretical property simulation mimic data set data application apply method retinoblastoma data set demonstrate parentoforigin effect weighted plausibility also application highdimensional data analysis pvalues penalized regression model derived demonstrate superior performance compared datasplitting procedure simulation study apply weighted plausibility highdimensional gene expression casecontrol prostate cancer data setpar discus flexibility approach relating weighted plausibility targeted learning bootstrap sparsity selection
Statistics,powerful portmanteau test detecting nonlinearity time series,new portmanteau test statistic proposed detecting nonlinearity time series data paper elaborate toeplitz autocorrelation matrix autocorrelation crosscorrelation residual squared residual block matrix derive new portmanteau test statistic using log determinant mth autocorrelations crosscorrelations block matrix asymptotic distribution proposed test statistic derived linear combination chisquared distribution approximated gamma distribution test applied identify linearity nonlinearity dependency stationary time series model shown convergence new test asymptotic distribution reasonable higher power test many situation demonstrate efficiency proposed test investigating linear nonlinear effect vodafone qatar daily return
Statistics,adaptive method sequential importance sampling application state space model,paper discus new adaptive proposal strategy sequential monte carlo algorithm also known particle filter relying criterion evaluating quality proposed particle choice proposal distribution major concern dramatically influence quality estimate thus show longused coefficient variation weight used estimating chisquare distance target instrumental distribution auxiliary particle filter byproduct analysis obtain auxiliary adjustment multiplier weight type chisquare distance minimal moreover establish empirical estimate linear complexity kullbackleibler divergence involved distribution guided result discus adaptive designing particle filter proposal distribution illustrate method numerical example
Statistics,curseofdimensionality revisited collapse particle filter large scale system,widely realized monte carlo method approximation via sample ensemble may fail large scale system work offer theoretical insight phenomenon context particle filter demonstrate maximum weight associated sample ensemble converges one sample size system dimension tends infinity specifically fairly weak assumption ensemble size grows subexponentially cube root system dimension convergence hold single update step statespace model independent identically distributed kernel important special case refined argument show simulation suggest convergence unity occurs unless ensemble grows superexponentially system dimension weight singularity also established model general multivariate likelihood eg gaussian cauchy although presented context atmospheric data assimilation numerical weather prediction result generally valid highdimensional particle filter
Statistics,asymptest r package performing parametric statistical test confidence interval based central limit theorem,paper describes r package implementing large sample test confidence interval based central limit theorem various parameter one two sample mean variance context considered statistic test expressed form facilitates presentation variance parameter case asymptotic robustness classical test depends departure data distribution normality measured term kurtosis distribution
Statistics,vanilla rao blackwellization metropolis hastings algorithm,casella robert biometrika presented general rao blackwellization principle acceptreject metropolis hastings scheme lead significant decrease variance resulting estimator high cost computation storage adopting completely different perspective introduce instead universal scheme guarantee variance reduction metropolis hastingsbased estimator keeping computation cost control establish central limit theorem improved estimator illustrate performance toy example probit model estimation
Statistics,monotonic convergence general algorithm computing optimal design,monotonic convergence established general class multiplicative algorithm introduced silvey titterington torsney comm statist theory method computing optimal design conjecture titterington appl stat confirmed consequence optimal design logistic regression used illustration
Statistics,gammabased clustering via ordered mean application geneexpression analysis,discrete mixture model provide wellknown basis effective clustering algorithm although technical challenge limited scope context geneexpression data analysis model presented mix finite catalog structure one representing equality inequality constraint among latent expected value computation depend probability independent gammadistributed variable attain possible ordering ordering event equivalent event independent negativebinomial random variable finding guide dynamicprogramming calculation structuring mixturemodel component according constraint among latent mean lead strict concavity mixture log likelihood addition beneficial numerical property clustering method show promising result empirical study
Statistics,bayesian estimation bivariate copula using jeffreys prior,bivariate distribution continuous margin uniquely decomposed via copula marginal distribution consider problem estimating copula function adopt bayesian approach space copula function construct finitedimensional approximation subspace parametrized doubly stochastic matrix major problem selection prior distribution space doubly stochastic matrix also known birkhoff polytope main contribution paper derivation simple formula jeffreys prior showing proper known literature complex problem like one treated result difficult obtain bayes estimator resulting jeffreys prior evaluated numerically via markov chain monte carlo methodology rather extensive simulation experiment carried many case result favour bayes estimator frequentist estimator standard kernel estimator deheuvels estimator term mean integrated squared error
Statistics,parametric family large binary space,context adaptive monte carlo algorithm directly generate independent sample distribution interest use proxy need close target generally proxy distribution parametric family sampling space target distribution continuous sampling problem high dimension often use multivariate normal distribution proxy easily parametrise moment quickly sample objective construct similarly flexible parametric family binary sampling space large exhaustive enumeration binary sampling problem difficult continuous counterpart since choice suitable proxy distribution obvious
Statistics,bayesian model selection beta autoregressive process,deal bayesian inference beta autoregressive process restrict attention class conditionally linear process process particularly suitable forecasting purpose difficult estimate due constraint parameter space provide full bayesian approach estimation include parameter restriction inference problem suitable specification prior distribution moreover bayesian framework parameter estimation model choice solved simultaneously particular suggest markovchain monte carlo mcmc procedure based metropolishastings within gibbs algorithm solve model selection problem following reversible jump mcmc approach
Statistics,sequential monte carlo large binary sampling space,monte carlo algorithm said adaptive automatically calibrates current proposal distribution using past simulation choice parametric family defines set proposal distribution critical good performance paper present parametric family adaptive sampling highdimensional binary space practical motivation problem variable selection linear regression context want sample bayesian posterior distribution model space using appropriate version sequential monte carlo raw version sequential monte carlo easily implemented using binary vector independent component highdimensional problem however simple proposal yield satisfactory result key efficient adaptive algorithm binary parametric family take correlation account analogously multivariate normal distribution continuous space provide review model binary data make one work context sequential monte carlo sampling computational study real life data hundred covariates suggest difficult instance sequential monte carlo approach clearly outperforms standard technique based markov chain exploration
Statistics,bayesian test normality versus dirichlet process mixture alternative,propose bayesian test normality univariate multivariate data alternative nonparametric model characterized dirichlet process mixture distribution alternative model based principle embedding predictive matching interpreted offer random granulation normal distribution mixture normal mixture component occupying smaller volume farther distribution center scalar parametrization based latent clustering used cover entire spectrum separation normal distribution alternative model efficient sequential importance sampler developed calculate bayes factor simulation indicate proposed test detect nonnormality without favoring nonparametric alternative normality hold
Statistics,chisquare classical exact test often wildly misreport significance remedy lie computer,discrete probability distribution model tested goodnessoffit close uniform forming pearson chisquare statistic involve division nearly zero often lead serious trouble practice even absence roundoff error present article illustrates via numerous example fortunately widespread availability computer avoiding trouble simple easy without problematic division nearly zero actual value taken goodnessoffit statistic humanly interpretable blackbox computer program rapidly calculate precise significance
Statistics,maximum entropy estimation probability distribution gaussian condition,describe method computationally estimate probability density function univariate random variable applying maximum entropy principle local condition given gaussian function estimation error optimal value parameter determined experimental result presented method estimate distribution well large enough selection used typically least value compared classical approach entropy maximisation local condition allow improving estimation locally method well suited heuristic optimisation approach
Statistics,convergence asymptotic normality variational bayesian approximation exponential family model missing value,study property variational bayes approximation exponential family model missing value shown iterative algorithm obtaining variational bayesian estimator converges locally true value probability sample size becomes inde nitely large moreover variational posterior distribution proved asymptotically normal
Statistics,graphical method inequality constraint marginalized dag,present graphical approach deriving inequality constraint directed acyclic graph dag model variable unobserved particular show observed distribution discrete model always restricted two observed variable neither adjacent graph share latent parent generalizes well known instrumental inequality method also provides inequality interventional distribution used bound causal effect constraint characterized term new graphical separation criterion providing easy intuitive method derivation
Statistics,efficient computational algorithm optimal allocation regression model,article discus optimal allocation problem experiment regression model used statistical analysis monotonic convergence general class multiplicative algorithm optimality discussed literature provide alternate proof monotonic convergence criterion simple computational algorithm furthermore show converges optimality also discus algorithm well conjecture monotonic convergence criterion monte carlo simulation used demonstrate reliability efficiency usefulness proposed algorithm
Statistics,prepivoting composite score statistic weighted bootstrap iteration,role played composite analogue log likelihood ratio hypothesis testing setting confidence region prominent canonical likelihood setting since asymptotic distribution depends unknown parameter approximate pivot based composite log likelihood ratio derived using asymptotic argument however actual distribution pivot may differ considerably asymptotic reference leading test confidence region whose level distant nominal one use bootstrap rather asymptotic distribution composite likelihood framework explored prepivoted test confidence set based suitable statistic turn accurate computationally appealing inferential tool
Statistics,bayesian ultrahighdimensional screening via mcmc,explore theoretical numerical property fully bayesian model selection method sparse ultrahighdimensional setting ie pgg n p number covariates n sample size method consists hierarchical bayesian model novel prior placed model space includes hyperparameter tn controlling model size efficient mcmc algorithm automatic stochastic search model theory show specifying tn correctly proposed method yield selection consistency ie posterior probability true model asymptotically approach one tn misspecified selected model still asymptotically nested true model theory also reveals insensitivity selection result respect choice tn implementation reasonable prior assumed tn allows u draw sample stochastically approach conduct selection estimation even inference unified framework additional prescreening dimension reduction step needed two novel g prior proposed make approach flexible simulation study given display numerical advantage method
Statistics,nonparametric estimation mean hilbert manifold extrinsic analysis mean shape contour,motivated problem nonparametric inference high level digital image analysis introduce general extrinsic approach data analysis hilbert manifold focus mean probability distribution sample space perform inference mean appeal concept neighborhood hypothesis functional data analysis derive onesample test consider analysis shape contour lying plane embedding corresponding sample space shape hilbert manifold space hilbertschmidt operator define extrinsic mean shape planar contour sample analogue apply general method problem considering computational restriction faced utilizing digital imaging data comparison computational cost provided another method analyzing shape contour
Statistics,improving inla approach approximate bayesian inference latent gaussian model,introduce new copulabased correction generalized linear mixed model glmms within integrated nested laplace approximation inla approach approximate bayesian inference latent gaussian model inla usually accurate rather extreme case glmms eg binomial poisson data seen problematic inaccuracy occur low degree smoothing borrowing strength within model therefore developed correction aiming push boundary applicability inla new correction implemented part rinla package add negligible computational cost empirical evaluation real simulated data indicate method work well
Statistics,exponentially titled empirical distribution function ranked set sample,study nonparametric estimation distribution function df continuous random variable based ranked set sampling design using exponentially tilted et empirical likelihood method propose et estimator df use construct new resampling algorithm unbalanced ranked set sample explore property proposed algorithm hypothesis testing problem underlying population mean show bootstrap test based et estimator df asymptotically normal exhibit small bias order n illustrate method evaluate finite sample performance algorithm perfect imperfect ranking scheme using real data set several monte carlo simulation study compare performance test statistic based et estimator based empirical likelihood estimator
Statistics,model building multiple dependent variable constraint,widely used method finding relationship several quantity multiple regression however restricted single dependent variable present general method allows model constructed multiple variable side equation computed easily using spreadsheet program underlying principle originating canonical correlation analysis maximising correlation two side model equation paper present fitting procedure make possible force estimated model satisfy constraint condition required posse may arise theory prior knowledge intuitively obvious also show least square approach problem inadequate produce model scale invariant
Statistics,default bayesian analysis multiway table dataaugmentation approach,paper proposes strategy regularized estimation multiway contingency table common metaanalyses multicenter clinical trial approach based data augmentation appeal heavily novel class polyagamma distribution main contribution build relevant distributional theory demonstrate three useful feature dataaugmentation scheme first lead simple em gibbssampling algorithm posterior inference circumventing need analytic approximation numerical integration metropolis hastings variational method second allows modeler much flexibility choosing prior traditionally come dirichlet logisticnormal family example approach allows user incorporate bayesian analogue classical penalizedlikelihood technique eg lasso bridge computing regularized estimate logodds ratio finally dataaugmentation scheme naturally suggests default strategy prior selection based logisticz model strongly related jeffreys prior binomial proportion illustrate method focus primarily particular case metaanalysismulticenter study jxkxn table general approach encompasses many common situation provide example
Statistics,robust clustering regression analysis via contaminated gaussian clusterweighted model,gaussian clusterweighted model cwm mixture regression model random covariates allows flexible clustering random vector composed response variable covariates mixture component adopts gaussian distribution covariates response given covariates robustify approach respect possible elliptical heavy tailed departure normality due presence atypical observation contaminated gaussian cwm introduced addition parameter gaussian cwm mixture component contaminated cwm parameter controlling proportion outlier one controlling proportion leverage point one specifying degree contamination respect response variable one specifying degree contamination respect covariates crucially parameter specified priori adding flexibility approach furthermore model estimated observation assigned group finer intragroup classification typical point outlier good leverage point bad leverage point concept primary importance robust regression analysis directly obtained relation mixturebased contaminated model analyzed identifiability condition provided expectationconditional maximization algorithm outlined parameter estimation various implementation operational issue discussed property estimator regression coefficient evaluated monte carlo experiment compared estimator gaussian cwm sensitivity study also conducted based real data set
Statistics,approximate bayesian computation state space model,new approach inference state space model proposed based approximate bayesian computation abc abc avoids evaluation likelihood function matching observed summary statistic statistic computed data simulated true process exact inference feasible statistic sufficient finite sample sufficiency unattainable state space setting seek asymptotic sufficiency via maximum likelihood estimator mle parameter auxiliary model prove auxiliary modelbased approach achieves bayesian consistency precise limiting sense proximity asymptotic sufficiency yielded mle replicated score multiple parameter setting separate treatment scalar parameter based integrated likelihood technique advocated way avoiding curse dimensionality attention given structure state variable driven continuous time process exact inference typically infeasible case result intractable transition abc method demonstrated using unscented kalman filter fast simple way producing approximation setting stochastic volatility model financial return used illustration
Statistics,outlier pattern outlier contingency table algebraic statistic,paper provide definition pattern outlier contingency table within modelbased framework particular make use loglinear model exact goodnessoffit test specify notion outlier pattern outlier language technique algebraic statistic essential tool make definition clear easily applicable numerical example show use definition
Statistics,priori truncation method posterior sampling homogeneous normalized completely random measure mixture model,paper adopts bayesian nonparametric mixture model mixing distribution belongs wide class normalized homogeneous completely random measure propose truncation method mixing distribution discarding weight unnormalized measure smaller threshold prove convergence law approximation provide theoretical property characterize posterior distribution blocked gibbs sampler devised versatility approximation illustrated two different application first normalized bessel random measure encompassing dirichlet process introduced goodness fit index show good performance mixing measure density estimation second describes incorporate covariates support normalized measure leading linear dependent model regression clustering
Statistics,nonasymptotic convergence analysis unadjusted langevin algorithm,paper study method sample target distribution pi mathbb r positive density respect lebesgue measure known normalisation factor method based euler discretization overdamped langevin stochastic differential equation associated pi constant decreasing step size euler discretization obtain nonasymptotic bound convergence target distribution pi total variation distance particular attention paid dependency dimension demonstrate applicability method high dimensional setting bound improve extend result dalalyan
Statistics,geometric ergodicity random walk metropolis positiondependent proposal covariance,consider metropolishastings method proposal kernel mathcal n x hg x x current state discussing specific case literature analyse ergodicity property resulting markov chain one dimension find suitable choice g x change ergodicity property compared random walk metropolis case mathcal n x hsigma either better worse higher dimension use specific example show judicious choice g x produce chain converge geometric rate limiting distribution probability concentrate ever narrower ridge x grows something true random walk metropolis
Statistics,scalable bayesian variable selection using nonlocal prior density ultrahighdimensional setting,bayesian model selection procedure based nonlocal alternative prior density extended ultrahigh dimensional setting compared variable selection procedure using precisionrecall curve variable selection procedure included comparison include method based g prior reciprocal lasso adaptive lasso scad minimax concave penalty criterion use precisionrecall curve eliminates sensitivity conclusion choice tuning parameter find bayesian variable selection procedure based nonlocal prior competitive procedure range simulation scenario subsequently explain favorable performance theoretical examination consistency property certain regularity condition apply demonstrate nonlocal procedure consistent linear model even number covariates p increase subexponentially sample size n model selection procedure based zellner g prior also found competitive penalized likelihood method identifying true model posterior distribution model space induced method much dispersed posterior distribution induced model space nonlocal prior method investigate asymptotic form marginal likelihood based nonlocal prior show attains unique term derived bayesian model selection procedure also propose scalable efficient algorithm called simplified shotgun stochastic search screening explore enormous model space show dramatically reduces computing time without losing capacity search interesting region model space algorithm available verb r package texttt cran
Statistics,hypothesis testing highdimensional sparse binary regression,paper study detection boundary minimax hypothesis testing context highdimensional sparse binary regression model motivated genetic sequencing association study rare variant effect investigate complexity hypothesis testing problem design matrix sparse observe new phenomenon behavior detection boundary occur case gaussian linear regression derive detection boundary function two component design matrix sparsity index signal strength function sparsity alternative alternative design matrix sparsity index high test asymptotically powerless irrespective magnitude signal strength binary design matrix sparsity index high result parallel gaussian case context derive detection boundary dense sparse regime dense regime show generalized likelihood ratio rate optimal sparse regime propose extended higher criticism test show rate optimal sharp illustrate finite sample property theoretical result using simulation study
Statistics,baseline zone estimation two dimension,consider problem estimating region nonparametric regression function baseline level two dimension baseline level typically corresponds minimummaximum function estimating region complement pertinent several problem arising edge estimation environmental statistic fmri related field assume baseline region convex estimate via fitting stump function approximate p value obtained test deviation regression function baseline level estimate obtained using algorithm originally developed constructing convex contour density studied two different sampling setting one several response obtained number different covariatelevels doseresponse involving limited number response value per covariate standard regression shape baseline region smoothness regression function boundary play critical role determining rate convergence estimate regression function pregular boundary convex baseline region estimate converges rate n doseresponse setting n total budget analogue standard regression setting converges rate n extension nonconvex baseline region explored well
Statistics,nonasymptotic estimation support recovery high dimensional sparse covariance matrix,propose general framework nonasymptotic covariance matrix estimation making use concentration inequalitybased confidence set specify framework estimation large sparse covariance matrix incorporation past thresholding estimator key emphasis support recovery technique go beyond past result thresholding estimator allowing wide range distributional assumption beyond merely subgaussian tail methodology furthermore adapted wide range estimator setting usage nonasymptotic dimensionfree confidence set yield good theoretical performance extensive simulation demonstrated superior performance compared method context support recovery able specify false positive rate optimize maximize true recovery
Statistics,critical analysis resampling strategy regularized particle filter,analyze performance different resampling strategy regularized particle filter regarding parameter estimation show particular building analytical insight obtained linear gaussian case resampling systematically prevent filtered density converging towards true posterior distribution discus several mean overcome limitation including kernel bandwidth modulation provide evidence resulting particle filter clearly outperforms traditional bootstrap particle filter result supported numerical simulation linear textbook example logistic map nonlinear plant growth model
Statistics,score test consistent,score test statistic using observed information easy compute numerically large sample distribution null hypothesis well known equivalent score test based expected information likelihoodratio test wald test however several author noted alternative longer hold particular statistic take negative value examine score test using observed information context comparing two binomial proportion imperfect detection common problem ecology studying occurrence specie demonstrate combination simulation theoretical analysis new modified rule propose reject null hypothesis observed score statistic larger usual chisquare cutoff negative power mostly greater test addition consistency largely restored new test easy use inference always possible
Statistics,consistency variational bayes inference estimation model selection mixture,mixture model widely used bayesian statistic machine learning particular computational biology natural language processing many field variational inference technique approximating intractable posterior thanks optimization algorithm extremely popular practice dealing complex model mixture contribution paper twofold first study concentration variational approximation posterior still open problem general mixture derive consistency rate convergence also tackle problem model selection number component study approach already used practice consists maximizing numerical criterion evidence lower bound prove strategy indeed lead strong oracle inequality illustrate theoretical result application gaussian multinomial mixture
Statistics,uniform stability particle approximation optimal filter derivative,sequential monte carlo method also known particle method widely used set computational tool inference nonlinear nongaussian statespace model many application may necessary compute sensitivity derivative optimal filter respect static parameter statespace model instance order obtain maximum likelihood model parameter interest compute optimal controller optimal control problem poyiadjis et al original particle algorithm compute filter derivative proposed shown using numerical example particle estimate numerically stable sense deteriorate time paper substantiate claim detailed theoretical study lp bound central limit theorem particle approximation filter derivative presented shown mixing condition lp bound asymptotic variance characterized central limit theorem uniformly bounded respect time index demon strate performance predicted theory several numerical example also use particle approximation filter derivative perform online maximum likelihood parameter estimation stochastic volatility model
Statistics,nonasymptotic bound estimation error mcmc algorithm,address problem upper bounding mean square error mcmc estimator analysis nonasymptotic first establish general result valid essentially ergodic markov chain encountered bayesian computation possibly unbounded target function f bound sharp sense leading term exactly sigma mathrm p f n sigma mathrm p f clt asymptotic variance next proceed specific additional assumption give explicit computable bound geometrically polynomially ergodic markov chain quantitative drift condition corollary provide result confidence estimation
Statistics,flexibility design multiple try metropolis scheme,multiple try metropolis mtm method generalization classical metropolishastings algorithm next state chain chosen among set sample according normalized weight literature several extension proposed work show remark upon flexibility design mtmtype method fulfilling detailed balance condition discus several possibility show different numerical result
Statistics,biasreduced estimator mean heavytailed distribution infinite second moment,use biasreduced estimator high quantiles heavytailed distribution introduce new estimator mean case infinite second moment asymptotic normality proposed estimator established checked simulation study four popular goodnessoffit test different sample size moreover compare term bias mean squared error estimator peng estimator peng evaluate accuracy resulting confidence interval
Statistics,exact berkjones statistic pvalue calculation,continuous goodnessoffit testing classical problem statistic despite low power detecting deviation tail distribution popular test based kolmogorovsmirnov statistic similar varianceweighted statistic andersondarling higher criticism statistic give weight tail deviation shown various work still mishandle extreme tail viable alternative paper study statistical property exact mn statistic berk jones derive asymptotic null distribution mn mn mn prove consistency well asymptotic optimality wide range rareweak mixture model additionally present new computationally efficient method calculate p value supremumbased onesided statistic including onesided mn mn rn rn statistic berk jones higher criticism statistic illustrate theoretical analysis several finitesample simulation
Statistics,estimation diagonal element sparse precision matrix,paper present several estimator diagonal element inverse covariance matrix called precision matrix sample iid random vector focus high dimensional vector sparse precision matrix well understood underlying distribution gaussian column precision matrix estimated independently form one another solving linear regression problem sparsity constraint approach lead computationally efficient strategy estimating precision matrix start estimating regression vector estimate diagonal entry precision matrix final step combine estimator getting estimator offdiagonal entry step estimating regression vector intensively studied past decade problem deriving statistically accurate estimator diagonal entry received much le attention goal present paper fill gap presenting four estimator seem natural one diagonal entry precision matrix performing comprehensive empirical evaluation estimator estimator consideration residual variance relaxed maximum likelihood symmetryenforced maximum likelihood penalized maximum likelihood show theoretically empirically aforementioned regression vector estimated without error symmetryenforced maximum likelihood estimator smallest estimation error however realistic setting regression vector estimated sparsityfavoring computationally efficient method quality estimator become relatively comparable slight advantage residual variance estimator
Statistics,sequential monte carlo fractional stochastic volatility model,paper consider fractional stochastic volatility model model volatility may exhibit longrange dependent roughantipersistent behavior propose dynamic sequential monte carlo methodology applicable long memory antipersistent process order estimate volatility well unknown parameter model establish central limit theorem state parameter filter study asymptotic property consistency asymptotic normality filter illustrate result simulation study apply method estimating volatility parameter longrange dependent model p data
Statistics,adaptive delayedacceptance mcmc target expensive likelihood,conducting bayesian inference delayed acceptance da metropolishastings mh algorithm da pseudomarginal mh algorithm applied computationally expensive calculate true posterior unbiased estimate thereof computationally cheap approximation available first acceptreject stage applied cheap approximation substituted true posterior mh acceptance ratio proposal pas first stage computationally expensive true posterior unbiased estimate thereof evaluated second acceptreject stage ensuring detailed balance satisfied respect intended true posterior scenario obvious computationally cheap approximation weighted average previous evaluation computationally expensive posterior provides generic approximation posterior k nearest neighbour nonzero weight evaluation approximate posterior made computationally cheap provided point posterior evaluated stored multidimensional binary tree known kdtree content kdtree potentially updated every computationally intensive evaluation resulting adaptive delayedacceptance pseudomarginal metropolishastings algorithm justified theoretically empirically guidance tuning parameter provided methodology applied discretely observed markov jump process characterising predatorprey interaction ode system describing dynamic autoregulatory gene network
Statistics,estimating standard error importance sampling estimator multiple markov chain,naive importance sampling estimator based sample single importance density numerically unstable instead consider generalized importance sampling estimator sample one probability distribution combined study problem markov chain monte carlo context independent sample replaced markov chain sample chain converge respective target distribution polynomial rate two finite moment condition show central limit theorem hold generalized estimator develop easy implement method calculate valid asymptotic standard error based batch mean also provide batch mean estimator calculating asymptotically valid standard error geyer reverse logistic estimator illustrate method using bayesian variable selection procedure linear regression particular generalized importance sampling estimator used perform empirical bayes variable selection batch mean estimator used obtain standard error highdimensional setting current method applicable
Statistics,summary statistic approximate bayesian computation,document due appear chapter forthcoming handbook approximate bayesian computation abc edited sisson fan beaumont since earliest work abc recognised using summary statistic essential produce useful inference result abc suffers curse dimensionality effect whereby using high dimensional input cause large approximation error output therefore crucial find low dimensional summary informative parameter inference model choice task hand chapter review method proposed select summary extending previous review paper blum et al recent development related theoretical result abc curse dimensionality sufficiency also discussed
Statistics,latent variable model selection gaussian conditional random field,consider problem learning conditional gaussian graphical model presence latent variable building recent advance field suggest method decomposes parameter conditional markov random field sum sparse lowrank matrix derive convergence bound estimator show wellbehaved highdimensional regime well sparsistent ie capable recovering graph structure show proximal gradient algorithm semidefinite programming technique employed fit model thousand variable extensive simulation illustrate condition required identifiability show wide range situation model performs significantly better counterpart example accommodating latent variable finally suggested method applied two datasets comprising individual level data genetic variant metabolite level show result replicate better alternative approach show enriched biological signal
Statistics,bounding error expectationpropagation,expectation propagation popular algorithm variational inference come theoretical guarantee article prove approximation error made ep bounded bound asymptotic interpretation number n datapoints allows u study ep convergence respect true posterior particular show ep converges rate mathcal n mean order magnitude faster traditional gaussian approximation mode also give similar asymptotic expansion moment order well excess kullbackleibler cost defined additional kl cost incurred using ep rather ideal gaussian approximation expansion highlight superior convergence property ep approach deriving result likely applicable many similar approximate inference method addition introduce bound moment logconcave distribution may independent interest
Statistics,computationally efficient change point detection highdimensional regression,largescale sequential data often exposed degree inhomogeneity form sudden change parameter datagenerating process consider problem detecting structural change highdimensional regression setting propose joint estimator number location change point parameter corresponding segment estimator computed using dynamic programming emphasize approximated using binary search algorithm n log n mathrm lasso n computational operation still enjoying essentially theoretical property mathrm lasso n denotes computational cost computing lasso sample size n establish oracle inequality estimator well binary search approximation covering also case large asymptotically growing number change point evaluate performance proposed estimation algorithm simulated data apply methodology real data
Statistics,exact distribution generalized shiryaevroberts stopping time minimax brownian motion setup,consider quickest changepoint detection problem aim detect onset prespecified drift live monitored standard brownian motion changepoint assumed unknown nonrandom object interest distribution stopping time associated generalized shryaevroberts gsr detection procedure set sense presence drift brownian motion surveillance specifically seek gsr stopping time survival function tail probability alarm triggered gsr procedure prior given point time distinguish two scenario drift never set prechange regime b drift effect ab initio postchange regime scenario obtain closedform formula respective survival function gsr statistic deterministic nonnegative headstart assumed arbitrarily given two formula found analytically direct solution respective kolmogorov forward equation via fourier spectral method achieve separation spacial temporal variable exploit obtained formula numerically characterize pre postchange distribution gsr stopping time depending three factor magnitude drift b detection threshold c gsr statistic headstart
Statistics,auxiliary likelihoodbased approximate bayesian computation state space model,computationally simple approach inference state space model proposed using approximate bayesian computation abc abc avoids evaluation intractable likelihood matching summary statistic observed data statistic computed data simulated true process based parameter draw prior draw produce match observed simulated summary retained used estimate inaccessible posterior reduction lowdimensional set sufficient statistic possible state space setting define summary maximum auxiliary likelihood function thereby exploit asymptotic sufficiency estimator auxiliary parameter vector derive condition approach including computationally efficient version based auxiliary score achieves bayesian consistency reduce welldocumented inaccuracy abc multiparameter setting propose separate treatment parameter dimension using integrated likelihood technique three stochastic volatility model exact bayesian inference either computationally challenging infeasible used illustration demonstrate approach compare favorably extensive set approximate exact comparators empirical illustration completes paper
Statistics,asymptotic property approximate bayesian computation,approximate bayesian computation allows statistical analysis model intractable likelihood paper consider asymptotic behaviour posterior distribution obtained method give general result rate posterior distribution concentrate set containing true parameter limiting shape asymptotic distribution posterior mean result hold given rate tolerance used within method mild regularity condition summary statistic condition linked identification true parameter implication practitioner discussed
Statistics,recursive partitioning multiscale modeling conditional density,introduce nonparametric prior conditional distribution univariate multivariate response given set predictor prior constructed form twostage generative procedure first stage recursively partition predictor space second stage generates conditional distribution multiscale nonparametric density model predictor partition block generated first stage design allows adaptive smoothing predictor space response space result full posterior conjugacy model allowing exact bayesian inference completed analytically forwardbackward recursive algorithm without need mcmc thus enjoying high computational efficiency scaling linearly sample size show prior enjoys desirable theoretical property full support posterior consistency illustrate apply model variety inference problem conditional density estimation well hypothesis testing model selection manner similar applying parametric conjugate prior attaining full nonparametricity also provided comparison two stateoftheart bayesian nonparametric model conditional density model fit computational time real data example flow cytometry containing observation given illustrate substantial computational efficiency method application multivariate problem
Statistics,parameter estimation wasserstein distance,statistical inference performed minimizing parameter space wasserstein distance model distribution empirical distribution data study asymptotic property minimum wasserstein distance estimator complementing result derived bassetti bodini regazzini particular result cover misspecified setting datagenerating process assumed part family distribution described model result motivated recent application minimum wasserstein estimator complex generative model discus difficulty arising approximation estimator illustrate behavior several numerical experiment two example taken literature approximate bayesian computation likelihood function analytically tractable two example involve misspecified model
Statistics,approximate penalized estimation piecewiseconstant signal graph,study recovery piecewiseconstant signal graph estimator minimizing edgepenalized objective although exact minimization objective may computationally intractable show statistical risk guarantee achieved alpha expansion algorithm computes approximate minimizer polynomial time establish graph small average vertex degree guarantee minimax rateoptimal class edgesparse signal spatially inhomogeneous graph propose minimization edgeweighted objective edge weighted effective resistance another measure contribution graph connectivity establish minimax optimality resulting estimator corresponding edgeweighted sparsity class show theoretically risk guarantee always achieved estimator minimizing totalvariation relaxation empirically based estimate accurate high signaltonoise setting
Statistics,exact test compare contingency table quasiindependence quasisymmetry,work define loglinear model compare several square contingency table quasiindependence quasisymmetry model relevant markov base theoretically characterized markov base exact test evaluate two table fit common model introduced two realdata example illustrate use model different field application
Statistics,asymptotics abc,present informal review recent work asymptotics approximate bayesian computation abc particular focus abc posterior point estimate obtained abc behave limit data result review show abc perform well term point estimation standard implementation overestimate uncertainty parameter use regression correction beaumont et al abc also accurately quantify uncertainty theoretical result also practical implication implement abc
Statistics,least square estimation single index model convex lipschitz link,consider estimation inference single index regression model unknown convex link function propose lipschitz constrained least square estimator llse parametric nonparametric component given independent identically distributed observation prove consistency find rate convergence llse error assumed q ge moment allowed depend covariates fact prove general theorem used find rate convergence l variety nonparametricsemiparametric regression problem assumption error moreover qge establish n rate convergence asymptotic normality estimator parametric component moreover llse proved semiparametrically efficient error happen homoscedastic furthermore develop r package texttt simest compute proposed estimator
Statistics,fast accurate bayesian model criticism conflict diagnostics using rinla,bayesian hierarchical model increasingly popular realistic modelling analysis complex data trend accompanied need flexible general computationally efficient method model criticism conflict detection usually bayesian hierarchical model incorporates grouping individual data point example individual repeated measurement data case following question arises group outlier conflict remaining group existing general approach aiming answer question tend extremely computationally demanding model fitting based mcmc show grouplevel model criticism conflict detection done quickly accurately integrated nested laplace approximation inla new method implemented part open source rinla package bayesian computing http rinlaorg
Statistics,symmetric rank covariance generalised framework nonparametric measure dependence,need test whether two random vector independent spawned large number competing measure dependence interested nonparametric measure invariant strictly increasing transformation kendall tau hoeffding recently discovered bergsma dassios sign covariance measure exhibit symmetry readily apparent definition making symmetry explicit define new class multivariate nonparametric measure dependence refer symmetric rank covariance new class generalises measure lead naturally multivariate extension bergsma dassios sign covariance symmetric rank covariance may estimated unbiasedly using ustatistics prove result computational efficiency largesample behavior algorithm develop computation include best knowledge first efficient algorithm wellknown hoeffding statistic multivariate setting
Statistics,alternative approach functional linear partial quantile regression,previously proposed partial quantile regression pqr prediction procedure functional linear model using partial quantile covariance technique developed simple partial quantile regression simpqr algorithm efficiently extract pqr basis estimating functional coefficient however although pqr approach considered attractive alternative projection onto principal component basis certain limitation uncovering corresponding asymptotic property mainly iterative nature nondifferentiability quantile loss function article propose implement alternative formulation partial quantile regression apqr functional linear model using block relaxation method finite smoothing technique proposed reformulation lead insightful result motivates new theory demonstrating consistency establishing convergence rate applying advanced technique empirical process theory two simulation two real data sample adni investigated show superiority proposed method
Statistics,goodnessoffit test complete spatial randomness based minkowski functionals binary image,propose class goodnessoffit test complete spatial randomness csr contrast standard test procedure utilizes transformation data binary image characterized geometric functionals suitable limiting regime derive asymptotic distribution test statistic null hypothesis almost sure limit certain alternative new test computationally efficient simulation show strong competitor test csr test applied real data set gammaray astronomy immediate extension presented encourage work
Statistics,conditionally conjugate meanfield variational bayes logistic model,variational bayes vb common strategy approximate bayesian inference simple method available specific class model including particular representation conditionally conjugate construction within exponential family model logit component apparently notable exception class due absence conjugacy logistic likelihood gaussian prior coefficient linear predictor facilitate approximate inference within widely used class model jaakkola jordan proposed simple variational approach relies family tangent quadratic lower bound logistic loglikelihoods thus restoring conjugacy approximate bound gaussian prior strategy still implemented successfully le attempt made formally understand reason underlying excellent performance cover key gap provide formal connection bound recent polyagamma data augmentation logistic regression result place computational method associated aforementioned bound within framework variational inference conditionally conjugate exponential family model thereby allowing recent advance class inherited also method relying jaakkola jordan
Statistics,multiple improvement multiple imputation likelihood ratio test,multiple imputation mi inference handle missing data first properly imputing missing value time combining analysis result applying completedata procedure completed datasets however existing method combining likelihood ratio test multiple defect combined test statistic negative practice reference null distribution standard f distribution ii invariant reparametrization iii fails ensure monotonic power due use inconsistent estimator fraction missing information fmi alternative hypothesis iv requires nontrivial access likelihood ratio test statistic function estimated parameter instead datasets paper show via theoretical derivation empirical investigation essentially problem straightforwardly addressed willing perform additional likelihood ratio test stacking completed datasets one big completed dataset particularly intriguing finding fmi estimated consistently likelihood ratio statistic testing whether completed datasets produced mi regarded effectively sample coming common model practical guideline provided based extensive comparison existing mi test
Statistics,asymptotic coverage probability bootstrap percentile confidence interval constrained parameter,asymptotic behaviour commonly used bootstrap percentile confidence interval investigated parameter subject linear inequality constraint concentrate important one twosample problem data generated general parametric distribution natural exponential family focus paper quantifying coverage probability parametric bootstrap percentile confidence interval particular limiting behaviour near boundary propose local asymptotic framework study subtle coverage behaviour framework discover true parameter close restriction boundary asymptotic coverage probability always exceed nominal level onesample case however remarkably nominal level twosample case using illustrative example show result provide theoretical justification guidance applying bootstrap percentile method constrained inference problem
Statistics,twostage approach analysis occupancy data ii heterogeneous model conditional likelihood,occupancy model involve probability site occupied probability occupancy detected homogeneous occupancy model occupancy detection probability site admits orthogonal parameter transformation yield twostage process calculate maximum likelihood estimate necessary simultaneously estimate occupancy detection probability twostage approach examined heterogeneous occupancy model occupancy detection probability depend covariates may vary site time longer orthogonal transformation approach effectively reduces parameter space allows fuller use r functionality permit use existing vector generalised linear model method fit model detection allows development iterative weighted least square approach fit model occupancy efficiency examined simulation study full maximum likelihood twostage approach compared several data set
Statistics,optimal subsampling algorithm big data regression,fast approximate maximum likelihood estimator massive data paper study optimal subsampling method aoptimality criterion osmac generalized linear model consistency asymptotic normality estimator general subsampling algorithm established optimal subsampling probability loptimality criterion derived furthermore using frobenius norm matrix concentration inequality finite sample property subsample estimator based optimal subsampling probability also derived since optimal subsampling probability depend full data estimate adaptive twostep algorithm developed asymptotic normality optimality estimator adaptive algorithm established proposed method illustrated evaluated numerical experiment simulated real datasets
Statistics,lugsail lag window estimating timeaverage covariance matrix,lag window commonly used time series econometrics steadystate simulation markov chain monte carlo estimate timeaverage covariance matrix presence high correlation estimator timeaverage covariance matrix almost always exhibit significant negative bias leading undesirable finitesample property propose new family lag window specifically designed improve finitesample performance offsetting negative bias opposite direction existing lag window adapted lugsail equivalent additional assumption use lag window within spectral variance estimator demonstrate advantage linear regression model autocorrelated heteroskedastic residual consider weighted batch mean estimator since spectral variance estimator computationally intensive large simulation output obtain bias variance result multivariate estimator significantly weaken mixing condition process superior finitesample property illustrated vector autoregressive process bayesian logistic regression model
Statistics,property simulationbased estimator high dimension,considering increasing size available data need statistical method control finite sample bias growing mainly due frequent setting number variable large allowed increase sample size bringing standard inferential procedure incur significant loss term performance moreover complexity statistical model also increasing thereby entailing important computational challenge constructing new estimator implementing classical one tradeoff numerical complexity statistical property often accepted however numerically efficient estimator altogether unbiased consistent asymptotically normal high dimensional problem would generally ideal paper set general framework estimator easily derived wide class model framework based concept underlie simulationbased estimation method indirect inference approach allows various extension compared previous result adapted possibly inconsistent estimator applicable discrete model andor model large number parameter consider algorithm namely iterative bootstrap ib efficiently compute simulationbased estimator showing convergence property within framework also prove property simulationbased estimator specifically unbiasedness consistency asymptotic normality number parameter allowed increase sample size therefore important implication proposed approach allows obtain unbiased estimator finite sample finally study approach applied three common model namely logistic regression negative binomial regression lasso regression
Statistics,fast inference procedure semivarying coefficient model via local averaging,semivarying coefficient model widely used application finance economics medical science many area functional coefficient commonly estimated local smoothing method eg local linear estimator implies one implement estimation procedure hundred time obtain estimate one function computation cost severe paper give insight tradeoff statistical efficiency computation simplicity proposes fast inference procedure semivarying coefficient model method coefficient function approximated piecewise constant simple rough approximation make estimator easy implement avoid repeat estimation work shall show though estimator asymptotically optimal efficient enough building inference procedure furthermore three test brought check whether certain coefficient constant result clearly show room improving asymptotic efficiency limited proper tradeoff statistical efficiency computation simplicity taken consideration improve performance inference procedure
Statistics,limitation limitation bayesian leaveoneout crossvalidation model selection,article invited discussion article gronau wagenmakers found http
Statistics,fast exact bayesian inference sparse signal normal sequence model,consider exact algorithm bayesian inference model selection prior including spikeandslab prior sparse normal sequence model best existing exact algorithm becomes numerically unstable sample size much attention alternative approach like approximate algorithm gibbs sampling variational bayes etc shrinkage prior eg horseshoe prior spikeandslab lasso empirical bayesian method however introducing algorithmic idea online sequential prediction show exact calculation feasible much larger sample size general model selection prior reach certain spikeandslab prior easily reach prove de finettilike result finite sample size characterizes exactly model selection prior expressed spikeandslab prior computational speed numerical accuracy proposed method demonstrated experiment simulated data differential gene expression data set compare effect multiple hyperparameter setting betabinomial prior experimental evaluation compute guaranteed bound numerical accuracy new algorithm show proposed method numerically reliable whereas alternative based long division
Statistics,local inversionfree estimation spatial gaussian process,maximizing likelihood widely used estimating unknown covariance parameter spatial gaussian process however evaluating optimizing likelihood function computationally intractable particularly large number possibly irregularly spaced observation due need handle inverse illconditioned large covariance matrix extending inversionfree method anitescu chen stein cite investigate broad class covariance parameter estimator based inversionfree surrogate loss block diagonal approximation scheme covariance structure class estimator yield spectrum negotiating tradeoff statistical accuracy computational cost present fixeddomain asymptotic property proposed method establishing sqrt n consistency asymptotic normality result isotropic matern gaussian process observed multidimensional irregular lattice simulation study also presented assessing scalability statistical efficiency proposed algorithm large data set
Statistics,spectral densitybased measurepreserving abc partially observed diffusion process illustration hamiltonian sdes,approximate bayesian computation abc become one major tool likelihoodfree statistical inference complex mathematical model simultaneously stochastic differential equation sdes developed established tool modelling time dependent real world phenomenon underlying random effect applying abc stochastic model two major difficulty arise first derivation effective summary statistic proper distance particularly challenging since simulation stochastic process parameter configuration result different trajectory second exact simulation scheme generate trajectory stochastic model rarely available requiring derivation suitable numerical method synthetic data generation obtain summary le sensitive intrinsic stochasticity model propose build statistical method eg choice summary statistic underlying structural property model focus existence invariant measure map data estimated invariant density invariant spectral density ensure model property kept synthetic data generation adopt measurepreserving numerical splitting scheme derived propertybased measurepreserving abc method illustrated broad class partially observed hamiltonian type sdes simulated data real electroencephalography eeg data proposed ingredient incorporated type abc algorithm directly applied sdes characterised invariant distribution measurepreserving numerical method derived
Statistics,highdimensional nonparametric density estimation via symmetry shape constraint,tackle problem highdimensional nonparametric density estimation taking class logconcave density mathbb r p incorporating within symmetry assumption facilitate scalable estimation algorithm mitigate curse dimensionality main symmetry assumption superlevel set density k homothetic ie scalar multiple convex body k subseteq mathbb r p k known prove k homothetic logconcave maximum likelihood estimator based n independent observation density worstcase risk bound respect eg squared hellinger loss n independent p moreover show estimator adaptive sense data generating density admits special form nearly parametric rate may attained also provide worstcase adaptive risk bound case k known positive definite transformation completely unknown must estimated nonparametrically estimation algorithm fast even n p order hundred thousand illustrate strong finitesample performance method simulated data
Statistics,infinity far bayesian nonparametric perspective finite mixture model,mixture model one widely used statistical tool dealing data heterogeneous population paper considers longstanding debate finite mixture infinite mixture brings two modelling strategy together showing finite mixture simply realization point process following bayesian nonparametric perspective introduce new class prior normalized independent point process investigate probabilistic property new class moreover design conditional algorithm finite mixture model random number component overcoming challenge associated reversible jump scheme recently proposed marginal algorithm illustrate model real data discus important application population genetics
Statistics,condition number hamiltonian monte carlo,hamiltonian monte carlo popular sampling technique smooth target density scale length target long known influence integration error sampling efficiency however quantitative measure intrinsic target lacking paper restrict attention multivariate gaussian leapfrog integrator obtain condition number corresponding sampling efficiency number based spectral schatten norm quantifies number leapfrog step needed efficiently sample demonstrate utility using condition number analyze hmc preconditioning technique also find condition number large inverse wishart matrix derive burnin heuristic
Statistics,divideandconquer informationbased optimal subdata selection algorithm,informationbased optimal subdata selection iboss computationally efficient method select informative data point large data set processing full data column however volume data set large processed available memory machine infeasible implement iboss procedure paper develops divideandconquer iboss approach solving problem full data set divided smaller partition loaded memory subset data selected partition using iboss algorithm derive finite sample property asymptotic property resulting estimator asymptotic result show full data set partitioned randomly number partition large resultant estimator estimation efficiency original iboss estimator also carry numerical experiment evaluate empirical performance proposed method
Statistics,multiscale scan statistic adaptive submatrix localization,consider problem localizing submatrix largerthanusual entry value inside data matrix without prior knowledge submatrix size establish optimization framework based multiscale scan statistic develop algorithm order approach optimizer also show estimator requires signal strength order minimax estimator oracle knowledge submatrix size exactly recover anomaly high probability perform simulation show estimator superior performance compared estimator require prior submatrix knowledge comparatively faster compute
Statistics,directing power towards conic parameter subspace,highdimensional parameter interest test based quadratic statistic known low power subset parameter space henceforth parameter subspace addition typically involve inverse covariance matrix difficult estimate highdimensional setting simultaneously address two issue proposing novel test statistic large conic parameter subspace interest test statistic generalizes wald statistic nest many wellknown test statistic given parameter subspace statistic free tuning parameter suitable highdimensional setting subspace sufficiently small computed using regularized linear regression type regularization regularization parameter completely determined parameter subspace interest illustrate statistic subspace consist sparse nearlysparse vector computation corresponds regularized regression respectively
Statistics,phase transition unbiased estimation high dimensional setting,important challenge statistical analysis concern control finite sample bias estimator example maximum likelihood estimator bias result significant inferential loss problem typically magnified highdimensional setting number variable p allowed diverge sample size n however generally difficult establish whether estimator unbiased therefore asymptotic order common approach used lowdimensional setting quantify magnitude bias alternative introduce new stronger property possibly highdimensional setting called phase transition unbiasedness estimator satisfying property unbiased n greater finite sample size nast moreover propose phase transition unbiased estimator built upon idea matching initial estimator computed sample simulated data required initial estimator consistent thus chosen computational efficiency andor desirable property robustness estimator computed using suitable simulation based algorithm namely iterative bootstrap shown converge exponentially fast addition demonstrate consistency limiting distribution estimator highdimensional setting finally illustration use approach develop new estimator logistic regression model without random effect also enjoy property robustness data contamination also affected problem separability simulation exercise theoretical result confirmed setting sample size relatively small compared model dimension
Statistics,malawithingibbs sampler highdimensional distribution sparse conditional structure,markov chain monte carlo mcmc sampler numerical method drawing sample given target probability distribution discus one particular mcmc sampler malawithingibbs sampler theoretical practical perspective first show acceptance ratio step size sampler independent overall problem dimension target distribution sparse conditional structure ii structure reflected partial updating strategy malawithingibbs addition target density blockwise logconcave sampler convergence rate independent dimension practical perspective expect malawithingibbs useful solving highdimensional bayesian inference problem posterior exhibit sparse conditional structure least approximately context partitioning state correctly reflects sparse conditional structure must found illustrate process two numerical example also discus tradeoff block size used partial updating computational requirement may increase number block
Statistics,variable screening based gaussian centered lmoments,important challenge big data identification important variable paper propose method discovering variable nonstandard univariate marginal distribution conventional momentsbased summary statistic welladopted purpose sensitivity outlier lead selection based outlier rather distributional shape bimodality address type nonrobustness consider lmoments using practice however limitation take zero value gaussian distribution shape marginal distribution naturally compared remedy propose gaussian centered lmoments share advantage lmoments zero gaussian distribution strength gaussian centered lmoments conventional moment shown theoretical practical aspect performance screening important gene cancer genetics data
Statistics,functional marked point process natural structure unify spatiotemporal framework analyse dependent functional data,paper treat functional marked point process fmpps defined marked point process mark random element polish function space mark may represent eg spatial path function time able consider eg multivariate fmpps also attach additional euclidean mark point indicate fmpps quite naturally connect point process framework functional data analysis framework geostatistical framework show various existing model fit well fmpp framework addition introduce new family summary statistic weighted marked reduced moment measure together nonparametric estimator order study feature functional mark show generalise summary statistic finally apply tool analyse population structure demographic evolution sex ratio time spanish province
Statistics,hauckdonner effect wald test detection tipping point parameter space characterization,wald test remains ubiquitous statistical practice despite shortcoming inaccuracy small sample lack invariance reparameterization paper develops another lesserknown shortcoming called hauck donner effect hde whereby wald test statistic monotonely increasing function increasing distance parameter estimate null value resulting upward biased p value loss power aberration lead damaging consequence variable selection hde afflicts many type regression model corresponds estimate near boundary parameter space article present several new result main contribution propose general test detecting hde regardless underlying cause ii fundamentally characterize hde pairwise ratio wald rao score likelihood ratio test statistic distribution iii show parameter space may partitioned interior encased hde severity measure faint weak moderate strong extreme iv prove necessary condition hde table log odds ratio least v give practical guideline hdefree hypothesis testing overall practical postfit test conducted potentially model estimated iteratively reweighted least square generalized linear model glm vector glm vglm class latter encompasses many popular regression model
Statistics,fast stable parameter estimation linear dynamical system,dynamical system describe change process arise naturally underlying physical principle law motion conservation mass energy momentum model facilitate causal explanation driver impediment process describe behaviour observed data quantify model parameter measured directly paper address two question providing methodology estimating solution parameter linear dynamical system incomplete noisy observation process proposed procedure build parameter cascading approach linear combination basis function approximates implicitly defined solution dynamical system system parameter estimated approximating solution adheres data taking advantage linearity system simplified parameter cascading estimation procedure developing new iterative scheme achieve fast stable computation illustrate approach obtaining linear differential equation represents real data biomechanics comparing approach popular method estimating parameter linear dynamical system namely nonlinear leastsquares approach simulated annealing parameter cascading smooth functional tempering reveals considerable reduction computation improved bias sampling variance
Statistics,general bayesian bootstrap censored data based betastacy process,introduce novel procedure perform bayesian nonparametric inference rightcensored data emph betastacy bootstrap approximates posterior law summary survival distribution eg mean survival time often difficult nonparametric case precisely procedure approximates joint posterior law functionals betastacy process nonparametric process prior widely used survival analysis also represents missing link unifies common bayesian bootstrap complete censored data based nonparametric prior defined exact sampling algorithm require tuning markov chain monte carlo step illustrate betastacy bootstrap analyzing survival data real clinical trial
Statistics,asymptotically optimal bias reduction parametric model,important challenge statistical analysis concern control finite sample bias estimator problem magnified highdimensional setting number variable p diverges sample size n well nonlinear model andor model discrete data complex setting propose use general simulationbased approach show resulting estimator bias order mathcal hence providing asymptotically optimal bias reduction based initial estimator slightly asymptotically biased making approach generally applicable particularly relevant classical estimator maximum likelihood estimator numerically approximated show iterative bootstrap kuk provides computationally efficient approach compute bias reduced estimator illustrate theoretical result simulation study develop new bias reduced estimator logistic regression without random effect estimator enjoy additional property robustness data contamination problem separability
Statistics,new exponential dispersion model count data property application,fundamental paper cubic variance function vfs letac mora annals presented systematic rigorous comprehensive study natural exponential family nefs real line characterization vfs mean value parameterization presented section reason left unnoticed section deal construction vfs associated nefs counting distribution set nonnegative integer allows find corresponding generating measure edms based nefs introduce paper three new class edms based result class associated simple vfs derive mean value parameterization associated generating measure also prove desirable property class shown overdispersed zero inflated ascending order making competitive statistical model use statistical actuarial modeling numerical example real data compare performance one class demonstrates superiority
Statistics,sequential online subsampling thinning experimental design,consider design problem experimental condition design point xi presented form sequence iid random variable generated unknown probability measure mu given proportion alphain selected objective select good candidate xi fly maximize concave function phi corresponding information matrix optimal solution corresponds construction optimal bounded design measure xialpha leq mualpha difficulty mu unknown xialpha must constructed online construction proposed relies definition threshold tau directional derivative phi current information matrix value tau fixed certain quantile distribution directional derivative combination recursive quantile estimation yield nonlinear twotimescale stochastic approximation method applied long design sequence since current information matrix estimated quantile need stored convergence optimum design proved various illustrative example presented
Statistics,stratification optimal resampling sequential monte carlo,sequential monte carlo smc also known particle filter widely accepted powerful computational tool making inference dynamical system key step smc resampling play role steering algorithm towards future dynamic several strategy proposed used practice including multinomial resampling residual resampling liu chen optimal resampling fearnhead clifford stratified resampling kitagawa optimal transport resampling reich show one dimensional case optimal transport resampling equivalent stratified resampling sorted particle minimize resampling variance well expected squared energy distance original resampled empirical distribution multidimensional case variance stratified resampling sorting particle using hilbert curve gerber et al mathbb r improved rate compared original number particle improved rate lowest ordered stratified resampling scheme conjectured gerber et al also present almost sure bound wasserstein distance original hilbertcurveresampled empirical distribution light theoretical result propose stratified multipledescendant growth smg algorithm allows u explore sample space efficiently compared standard iid multipledescendant samplingresampling approach measured wasserstein metric numerical evidence provided demonstrate effectiveness proposed method
Statistics,homogeneity test covariance changepoints identification highdimensional functional data,consider inference problem highdimensional hd functional data dense number repeated measurement taken large number p variable small number n experimental unit spatial temporal dependence high dimensionality dense number repeated measurement make theoretical study computation challenging paper two aim first aim solve theoretical computational challenge detecting identifying change point among covariance matrix hd functional data second aim provide computationally efficient tuningfree tool guaranteed stochastic error control change point detection procedure developed form testing homogeneity covariance matrix weak convergence stochastic process formed test statistic established large p large small n setting mild set condition change point identification estimator proven consistent change point location sequence rate convergence depends data dimension sample size number repeated measurement signaltonoise ratio also show proposed computation algorithm significantly reduce computation time applicable realworld data fmri data large number hd repeated measurement simulation result demonstrate finite sample performance computational effectiveness proposed procedure observe empirical size test well controlled nominal level location multiple change point accurately identified application fmri data demonstrates proposed method identify event boundary preface movie sherlock proposed procedure implemented r package techphd
Statistics,tutorial multivariate k statistic computation,document aim provide accessible tutorial unbiased estimation multivariate cumulants using k statistic offer explicit general formula multivariate k statistic arbitrary order also prove k statistic unbiased using obius inversion rudimentary combinatorics many detailed example considered throughout paper conclude discussion k statistic computation including challenge time complexity examine couple possible avenue improve efficiency computation purpose document threefold provide clear introduction k statistic without relying specialized tool like umbral calculus construct explicit formula k statistic might facilitate future approximation faster algorithm serve companion paper python library pymoments implement formula
Statistics,fiducial approach nonparametric deconvolution problem discrete case,fiducial inference generalized hannig et al applied nonparametric gmodeling efron discrete case propose computationally efficient algorithm sample fiducial distribution use generated sample construct point estimate confidence interval study theoretical property fiducial distribution perform extensive simulation various scenario proposed approach give rise surprisingly good statistical performance term mean squared error point estimator coverage confidence interval furthermore apply proposed fiducial method estimate probability satellite site malignant using gastric adenocarcinoma data patient
Statistics,backfitting large scale crossed random effect regression,regression model crossed random effect error model expensive compute cost generalized least square gibbs sampling easily grow n worse n observation papaspiliopoulos et al present collapsed gibbs sampler cost n extremely stringent sampling model propose backfitting algorithm compute generalized least square estimate prove cost n greatly relaxed though still strict sampling assumption empirically backfitting algorithm cost n relaxed assumption illustrate new algorithm rating data set stitch fix
Statistics,maximum likelihood estimation likelihood ratio ordered family distribution,consider bivariate observation ldots xn yn subset mathfrak x timesmathbb r real set mathfrak x conditional xi yi independent random variable distribution p xi px xinmathfrak x unknown using empirical likelihood approach devise algorithm estimate unknown family distribution px xinmathfrak x sole assumption family increasing respect likelihood ratio order review latter concept realize assumption distribution px density common countable support needed benefit stronger regularization imposed likelihood ratio ordering usual stochastic ordering evaluated term estimation predictive performance simulated well real data
Statistics,principal separable component analysis via partial inner product,nonparametric estimation covariance lie heart functional data analysis whether curve surfacevalued data case twodimensional domain pose statistical computational challenge typically alleviated assuming separability however separability often questionable sometimes even demonstrably inadequate propose framework analysis covariance operator random surface generalises separability retaining major advantage approach based additive decomposition covariance series separable component decomposition valid covariance twodimensional domain leveraging key notion partial inner product generalise power iteration method general hilbert space show aforementioned decomposition efficiently constructed practice truncation decomposition retention principal separable component automatically induces nonparametric estimator covariance whose parsimony dictated truncation level resulting estimator calculated stored manipulated little computational overhead relative separability framework estimation method genuinely nonparametric since considered decomposition hold covariance consistency rate convergence derived mild regularity assumption illustrating tradeoff bias variance regulated truncation level merit practical performance proposed methodology demonstrated comprehensive simulation study
Statistics,combining particle mcmc raoblackwellized monte carlo data association parameter estimation multiple target tracking,consider state parameter estimation multiple target tracking problem data association uncertainty unknown number target show problem recast conditionally linear gaussian statespace model unknown parameter present algorithm computationally efficient inference resulting model proposed algorithm based combining raoblackwellized monte carlo data association algorithm particle markov chain monte carlo algorithm jointly estimate parameter data association particle marginal metropolishastings particle gibbs variant particle mcmc considered demonstrate performance method using simulated data realdata case study using multiple target tracking estimate brown bear population finland
Statistics,moment condition bayesian nonparametrics,model phrased though moment condition central much modern inference moment condition embedded within nonparametric bayesian setup handling model probabilistically straightforward posterior support manifold solve relevant issue building new probability computational tool using hausdorff measure analyze real simulated data new method involve simulating manifold applied widely including providing bayesian analysis quasilikelihoods linear nonlinear regression missing data hierarchical model
Statistics,resampling improvement importance sampling varying population size model,sequential importance sampling algorithm defined estimate likelihood model ancestral population process however algorithm based feature model constant population size become inefficient population size varies time making likelihoodbased inference difficult many demographic situation work modify previous sequential importance sampling algorithm improve efficiency likelihood estimation procedure still based feature model constant size us resampling technique new resampling probability distribution depending pairwise composite likelihood tested algorithm called sequential importance sampling resampling sisr simulated data set different demographic case case divided computational cost two accuracy inference case even one hundred study provides first assessment impact resampling technique parameter inference using sequential importance sampling extends range situation likelihood inference easily performed
Statistics,contending party logistic choice analysis inter intragroup blog citation dynamic u presidential election,u presidential election cycle marked debut internetbased medium blog social networking website institutionally recognized feature american political landscape using longitudinal sample dncrncdesignated blogcitation network able test influence various strategic institutional balancetheoretic mechanism exogenous factor seasonality political event propensity blog cite one another time capitalizing temporal resolution data utilize autoregressive network regression framework carry inference logistic choice process using combination deviancebased model selection criterion simulationbased model adequacy test identify combination process best characterizes choice behavior contending blog
Statistics,reliable peerreview process,propose enhanced peerreview process reviewer encouraged truthfully disclose review start modelling process using bayesian model uncertainty regarding quality manuscript taken account introduce scoring function evaluate reported review mild assumption show reviewer strictly maximize expected score telling truth also show score used order reach consensus
Statistics,statistical view team handball result home advantage team fitness prediction match outcome,analyze result german team handball bundesliga ten season modelfree statistical time series approach show home advantage nearly negligible compared total sum goal specific interest spent time evolution team fitness expressed term goal difference contrast soccer result indicate decay team fitness value season long time correlation behavior year nearly comparable able explain dominance team large value total number goal match method prediction match winner presented good accuracy real result analyze property promoted team indicate drastic level change bundesliga second league finding reflect good agreement recent discussion modern successful attack strategy
Statistics,statistical significance simulation study general scientist,scientist performs experiment normally acquire set measurement expected demonstrate result statistically significant thus confirming whatever hypothesis testing main method establishing statistical significance involves demonstrating low probability observed experimental result product random chance typically defined p indicates le chance observed result occurred randomly research study visually demonstrates commonly used definition statistical significance erroneously imply significant finding demonstrated generating random gaussian noise data analyzing data using statistical testing based established twosample ttest study demonstrates insignificant yet statistically significant finding possible moderately large sample size common many field modern science
Statistics,statistical analysis pattern evolution perceived emotion induced hindustani music study based listener response,objective study find underlying pattern perception emotion evolved india hindustani music used reference frame tracking changing perception emotion found different emotion perceived hindustani music form particular sequential pattern corresponding pitch period analyzed using standard deviation mean successive squared differencesthis sequential pattern emotion coincides corresponding sequential pattern tempo average number steady state basis result found range perception emotion diminished significantly day compared proportion response perceived emotion like anger serenity romantic sorrow also decreased great extent previously proportion response perceived emotion anxiety increased phenomenally standard deviation mean successive squared difference two good measure tracking changing perception emotion overall pattern change perceived emotion corresponded psychological sociological change human life
Statistics,distributed detection random process multiple access channel energy bandwidth constraint,analyze binary hypothesis testing problem built wireless sensor network wsn detecting stationary random process distributed space time circularlysymmetric complex gaussian distribution neymanpearson framework using analog scheme sensor transmit different linear combination measurement multiple access channel mac reach fusion center fc whose task decide whether process present considering energy constraint node transmission limited amount channel us compute miss error exponent proposed scheme using large deviation theory ldt show proposed strategy asymptotically optimal number sensor approach infinity among linear orthogonal scheme also show proposed scheme obtains significant energy saving low signaltonoise ratio regime typical scenario wsns finally monte carlo simulation process space validates analytical result
Statistics,measurement economic tail risk,paper attempt provide decisiontheoretic foundation measurement economic tail risk closely related utility theory also relevant statistical model uncertainty main result risk measure satisfy set economic axiom choquet expected utility statistical property elicitability ie exists objective function minimizing expected objective function yield risk measure mean functional median shortfall median tail loss distribution elicitability important backtesting also extend result address model uncertainty incorporating multiple scenario application argue median shortfall better alternative expected shortfall setting capital requirement basel accord
Statistics,spectral correlation hub screening multivariate time series,chapter discus correlation analysis stationary multivariate gaussian time series spectral fourier domain goal identify hub time series ie highly correlated specified number time series show fourier component time series different frequency asymptotically statistically independent property permit independent correlation analysis frequency alleviating computational statistical challenge highdimensional time series detect correlation hub frequency existing correlation screening method extended complex number accommodate complexvalued fourier component characterize number hub discovery specified correlation degree threshold regime increasing dimension fixed sample size theory specifies appropriate threshold apply sample correlation matrix detect hub also allows statistical significance attributed hub discovery numerical result illustrate accuracy theory usefulness proposed spectral framework
Statistics,modelling level adoption analytical tool implementation multicriteria evidential reasoning,future competitive advantage given organisation extract valuable information massive data make better decision case data come multiple source therefore challenge aggregate common framework order make meaningful useful paper first review important multicriteria decision analysis method mcda existing current literature offer novel practical consistent methodology based type mcda aggregate data two different source common framework two datasets different nature related topic aggregated common scale implementing set transformation rule allows u generate appropriate evidence assessing finally prioritising level adoption analytical tool four type company numerical example provided clarify form implementing methodology sixstep process offered guideline assist engineer researcher practitioner interested replicating methodology situation need aggregate transform multiple source data
Statistics,beating bookie number online sport betting market rigged,online sport gambling industry employ team data analyst build forecast model turn odds sport game favour several betting strategy proposed beat bookmaker expert prediction model arbitrage strategy odds bias exploitation return inconsistent remains shown betting strategy outperform online sport betting market designed strategy beat football bookmaker number instead building forecasting model compete bookmaker prediction exploited probability information implicit odds publicly available marketplace find bet mispriced odds strategy proved profitable historical simulation using closing odds historical simulation using minute minute odds period staked real money bookmaker made code data model publicly available result demonstrate football betting market inefficient bookmaker consistently beaten across thousand game simulated environment reallife betting provide detailed description betting experience illustrate sport gambling industry compensates market inefficiency discriminatory practice successful client
Statistics,seeing forest tree investigation network knowledge,paper ass empirical content one prevalent assumption economics network literature namely assumption decision maker full knowledge network interact using network data village ask individual ass whether five randomly chosen pair household village linked financial social informational relationship find network knowledge low highly localized declining steeply pair network distance respondent respondent even able offer guess status potential link given pair individual even willing offer guess respondent correctly identify link time also find onestep increase social distance pair corresponds increase probability misidentifying link investigate theoretical implication assumption showing prediction various model change substantially agent behave realistic assumption incomplete knowledge network taken together result suggest assumption full network knowledge may serve poor approximation real world ii innocuous allowing incomplete network knowledge may firstorder implication range qualitative quantitative result various context
Statistics,kernel method nonlinear connectivity detection,paper show presence nonlinear coupling time series may detected employing kernel feature space representation alone dispensing need go back solve preimage problem gauge model adequacy consequence canonical methodology model construction diagnostics granger connectivity inference applies change computation using kernel lieu secondorder moment
Statistics,turtleback diagram conditional probability,elaborate alternative representation conditional probability usual tree diagram term representation turtleback diagram resemblance pattern turtle shell adopting set theoretic view event sample space turtleback diagram us element venn diagram set intersection complement partition conditioning additional notion area set indicates probability whereas ratio area conditional probability part diagram drawn properly labeled calculation conditional probability involves simple arithmetic area relevant set discus turtleback diagram relation visual representation conditional probability detail several scenario turtleback diagram prove useful equivalence recursive space partition tree turtleback diagram seen equally expressive tree diagram representing abstract concept also provide empirical data use turtleback diagram undergraduate student elementary statistic probability course
Statistics,transmission macroeconomic shock risk parameter us stress testing,paper interested evaluating resilience financial portfolio extreme economic condition therefore use empirical measure characterize transmission process macroeconomic shock risk parameter propose use extensive family model called general transfer function model condense well characteristic transmission described impact measure procedure estimating parameter model described employing bayesian approach using prior information provided impact measure addition illustrate use estimated model credit risk data portfolio
Statistics,agreement bibliometrics peer review evidence italian research assessment exercise,paper appraises concordance bibliometrics peer review drawing evidence data two experiment realized italian governmental agency research evaluation experiment performed validating dual system evaluation consisting interchangeable use bibliometyrics peer review adopted agency research assessment exercise two experiment based stratified random sample journal article article scored bibliometrics peer review degree concordance two evaluation computed correct setting experiment defined developing designbased estimation cohen kappa coefficient testing procedure assessing homogeneity missing proportion stratum result experiment show research area hard science engineering life science degree agreement bibliometrics peer review weak individual article level thus outcome experiment validate use dual system evaluation italian research assessment general weak concordance indicates metric replace peer review level individual article hence use dual system evaluation reducing cost might introduce unknown bias research assessment exercise
Statistics,introducing bayesian analysis text circledr activelearning exercise undergraduate,present activelearning strategy undergraduate applies bayesian analysis candycovered chocolate text circledr exercise best suited small class size tutorial setting student introduced concept bayesian statistic exercise take advantage nonuniform distribution text circledr colour difference distribution made two different factory paper provide intended learning outcome lesson plan stepbystep guide instruction opensource teaching material also suggest extension exercise graduatelevel incorporates hierarchical bayesian analysis
Statistics,stochastic differential theory cricket,new formalism analyzing progression cricket game using stochastic differential equation sde introduced theory enables quantitative way representing every team using three key variable physical meaning associated contrast traditional system ratingranking team based combination different statical cumulants using formalism new method calculate winning probability progression number ball given
Statistics,multichannel signal detection interference noise signal mismatch happens,paper consider problem detecting multichannel signal interference noise signal mismatch happens first propose two selective detector since strong selectivity preferred situation however two detector would suitable candidate robust detector needed overcome shortcoming devise tunable detector parametrized nonnegative scaling factor referred tunable parameter adjusting tunable parameter proposed detector smoothly change capability rejecting robustly detecting mismatch signal moreover one selective detector tunable detector appropriate tunable parameter provide nearly detection performance existing detector absence signal mismatch obtain analytical expression probability detection pd probability false alarm pfas three proposed detector verified monte carlo simulation
Statistics,estimating infection rate anatomy inference problem,consequence missing data test infection imperfect accuracy test reported rate population infection sars virus lower actual rate infection hence reported rate severe illness conditional infection higher actual rate understanding time path pandemic hampered absence bound infection rate credible informative paper explains logical problem bounding rate report illustrative finding using data illinois new york italy combine data assumption infection rate untested population accuracy test appear credible current context find infection rate might substantially higher reported also find infection fatality rate italy substantially lower reported
Statistics,projection based conditional dependence measure application highdimensional undirected graphical model,measuring conditional dependence important topic statistic broad application including graphical model factor model setting new conditional dependence measure based projection proposed corresponding conditional independence test developed asymptotic null distribution unveiled number factor could highdimensional also shown new test control asymptotic significance level calculated efficiently generic method building dependency graph without gaussian assumption using new test elaborated numerical result real data analysis show superiority new method
Statistics,cdfdr comparison density approach local false discovery rate estimation,efron et al proposed empirical bayes formulation frequentist benjamini hochbergs false discovery rate method benjamini article attempt unify two culture using concept comparison density distribution function also shown almost existing local fdr method viewed proposing various model specification comparison density unifies vast literature false discovery method one concept notation
Statistics,inverse moment method sufficient forecasting using highdimensional predictor,consider forecasting single time series using highdimensional predictor presence possible nonlinear forecast function sufficient forecasting fan et al used sliced inverse regression estimate lowerdimensional sufficient index nonparametric forecasting using factor model however fan et al fundamentally limited inverse firstmoment method assuming restricted fixed number factor linearity condition factor monotone effect factor response work study inverse secondmoment method using directional regression inverse thirdmoment method extend methodology applicability sufficient forecasting number factor diverges dimension predictor proposed method relaxes distributional assumption predictor enhances capability capturing nonmonotone effect factor response provide highdimensional analysis inverse moment method exhaustiveness rate convergence also prove model selection consistency power proposed method demonstrated simulation study empirical study forecasting monthly macroeconomic data theoretical development prove invariance result inverse moment method make separate contribution sufficient dimension reduction
Statistics,finegray competing risk model highdimensional covariates estimation inference,purpose paper construct confidence interval regression coefficient finegray model competing risk data random censoring number covariates larger sample size despite strong motivation biomedical application highdimensional finegray model attracted relatively little attention among methodological theoretical literature fill gap developing confidence interval based onestep biascorrection regularized estimation develop theoretical framework partial likelihood independent identically distributed entry therefore present many technical challenge also study approximation error weighting scheme random censoring competing risk establish new concentration result timedependent process addition theoretical result algorithm present extensive numerical experiment application study noncancer mortality among prostate cancer patient using linked medicareseer data
Statistics,statistical shape analysis bayesian framework shape two three dimension,paper describe novel shape classification method embedded bayesian paradigm discus modelling resulting shape classification algorithm two three dimensional data shape conclude evaluating efficiency efficacy proposed algorithm kimia shape database two dimensional case
Statistics,pairwise covariatesadjusted block model community detection,one fundamental problem network study community detection stochastic block model sbm one widely used model network data different estimation method developed community detection consistency result unveiled however sbm restricted strong assumption node community stochastically equivalent may suitable practical application introduce pairwise covariatesadjusted stochastic block model pcabm generalization sbm incorporates pairwise covariate information study maximum likelihood estimate coefficient covariates well community assignment shown coefficient estimate covariates community assignment consistent suitable sparsity condition spectral clustering adjustment scwa introduced efficiently solve pcabm certain condition derive error bound community estimation scwa show community detection consistent pcabm compare favorably sbm degreecorrected stochastic block model dcbm wide range simulated real network covariate information accessible
Statistics,deep knockoff,paper introduces machine sampling approximate modelx knockoff arbitrary unspecified data distribution using deep generative model main idea iteratively refine knockoff sampling mechanism criterion measuring validity produced knockoff optimized criterion inspired popular maximum mean discrepancy machine learning thought measuring distance pairwise exchangeability original knockoff feature building upon existing modelx framework thus obtain flexible modelfree statistical tool perform controlled variable selection extensive numerical experiment quantitative test confirm generality effectiveness power deep knockoff machine finally apply new method real study mutation linked change drug resistance human immunodeficiency virus
Statistics,semiparametric method exposure misclassification propensity scorebased timetoevent data analysis,epidemiology identifying effect exposure variable relation timetoevent outcome classical research area practical importance incorporating propensity score cox regression model measure control confounding certain advantage outcome rare however situation involving exposure measured moderate substantial error identifying exposure effect using propensity score cox model remains challenging yet unresolved problem paper propose estimating equation method correct exposure misclassificationcaused bias estimation exposureoutcome association also discus asymptotic property derive asymptotic variance proposed estimator conduct simulation study evaluate performance proposed estimator various setting illustration apply method correct misclassificationcaused bias estimating association level lung cancer mortality using nationwide prospective cohort nurse health study nh proposed methodology applied using userfriendly r function published online
Statistics,prediction outlier detection classification problem,consider multiclass classification problem training data outofsample test data may different distribution propose method called bcops balanced conformal optimized prediction set bcops construct prediction set c x subset class label possibly empty try optimize outofsample performance aiming include correct class often possible also detecting outlier x method return prediction corresponding c x equal empty set proposed method combine supervisedlearning algorithm method conformal prediction minimize misclassification loss averaged outofsample distribution constructed prediction set finitesample coverage guarantee without distributional assumption also propose method estimate outlier detection rate given method prove asymptotic consistency optimality proposal suitable assumption illustrate method real data example
Statistics,estimating differential latent variable graphical model application brain connectivity,differential graphical model designed represent difference conditional dependence structure two group thus particular interest scientific investigation motivated modern application manuscript considers extended setting group generated latent variable gaussian graphical model due existence latent factor differential network decomposed sparse lowrank component symmetric indefinite matrix estimate two component simultaneously using twostage procedure initialization stage computes simple consistent estimator ii convergence stage implemented using projected alternating gradient descent algorithm applied nonconvex objective initialized using output first stage prove given initialization estimator converges linearly nontrivial minimax optimal statistical error experiment synthetic real data illustrate proposed nonconvex procedure outperforms existing method
Statistics,aggregation multiple knockoff,develop extension knockoff inference procedure introduced barber candes new method called aggregation multiple knockoff ako address instability inherent random nature knockoffbased inference specifically ako improves stability power compared original knockoff algorithm still maintaining guarantee false discovery rate control provide new inference procedure prove core property demonstrate benefit set experiment synthetic real datasets
Statistics,kernel assisted learning personalized dose finding,individualized dose rule recommends dose level within continuous safe dose range based patient level information physical condition genetic factor medication history traditionally personalized dose finding process requires repeating clinical visit patient frequent adjustment dosage thus patient constantly exposed risk underdosing overdosing process statistical method finding optimal individualized dose rule lower cost risk patient article propose kernel assisted learning method estimating optimal individualized dose rule proposed methodology also applied continuous decisionmaking problem advantage proposed method include robustness model misspecification capability providing statistical inference estimated parameter simulation study show method capable identifying optimal individualized dose rule produce favorable expected outcome population finally illustrate approach using data warfarin dosing study thrombosis patient
Statistics,batch kernel som related laplacian method social network analysis,large graph natural mathematical model describing structure data wide variety field web mining social network information retrieval biological network etc application automatic tool required get synthetic view graph reach good understanding underlying problem particular discovering group tightly connected vertex understanding relation group important practice paper show kernel version batch self organizing map used achieve goal via kernel derived laplacian matrix graph especially used conjunction classical method based spectral analysis graph proposed method used explore structure medieval social network modeled weighted graph directly built large corpus agrarian contract
Statistics,survey crossvalidation procedure model selection,used estimate risk estimator perform model selection crossvalidation widespread strategy simplicity apparent universality many result exist model selection performance crossvalidation procedure survey intends relate result recent advance model selection theory particular emphasis distinguishing empirical statement rigorous theoretical result conclusion guideline provided choosing best crossvalidation procedure according particular feature problem hand
Statistics,nonlinear time series modeling unified perspective algorithm application,new comprehensive approach nonlinear time series analysis modeling developed present paper introduce novel dataspecific middistribution based legendre polynomial lp like nonlinear transformation original time series enables u adapt existing stationary linear gaussian time series modeling strategy made applicable nongaussian nonlinear process robust fashion emphasis present paper empirical time series modeling via algorithm lptime demonstrate effectiveness theoretical framework using daily p return data proposed lptime algorithm systematically discovers stylized fact financial time series automatically previously noted many researcher one time
Statistics,feature augmentation via nonparametrics selection fan high dimensional classification,propose high dimensional classification method involves nonparametric feature augmentation knowing marginal density ratio powerful univariate classifier use ratio estimate transform original feature measurement subsequently penalized logistic regression invoked taking input newly transformed augmented feature procedure train model equipped local complexity global simplicity thereby avoiding curse dimensionality creating flexible nonlinear decision boundary resulting method called feature augmentation via nonparametrics selection fan motivate fan generalizing naive bayes model writing log ratio joint density linear combination marginal density related generalized additive model better interpretability computability risk bound developed fan numerical analysis fan compared competing method provide guideline best application domain real data analysis demonstrates fan performs competitively benchmark email spam gene expression data set moreover fan implemented extremely fast algorithm parallel computing
Statistics,kernel balancing flexible nonparametric weighting procedure estimating causal effect,absence unobserved confounders matching weighting method widely used estimate causal quantity including average treatment effect treated att unfortunately method necessarily achieve goal making multivariate distribution covariates control group identical treated leaving potentially multivariate function covariates different mean two group imbalanced function influence nontreatment potential outcome conditioning observed covariates fails att estimate may biased kernel balancing introduced target weaker requirement unbiased att estimation specifically expected nontreatment potential outcome treatment control group equal conditional expectation nontreatment potential outcome assumed fall space function associated choice kernel implying set basis function regression surface linear weight chosen control unit treated control group equal mean basis function result expectation nontreatment potential outcome must also equal treated control group weighting allowing unbiased att estimation subsequent difference mean outcome model using weight moreover weight produced precisely equalize particular kernelbased approximation multivariate distribution covariates treated control equivalent form stabilized inverse propensity score weighting though require assuming model treatment assignment mechanism r package kbal provided implement approach
Statistics,flexible model microclustering application entity resolution,generative model clustering implicitly assume number data point cluster grows linearly total number data point finite mixture model dirichlet process mixture model pitman yor process mixture model make assumption infinitely exchangeable clustering model however application assumption inappropriate example performing entity resolution size cluster unrelated size data set cluster contain negligible fraction total number data point application require model yield cluster whose size grow sublinearly size data set address requirement defining microclustering property introducing new class model exhibit property compare model within class two commonly used clustering model using four entityresolution data set
Statistics,extreme event evaluation using crp distribution,verification ensemble forecast extreme event remains challenging question general public well medium naturely pay particular attention extreme event conclude global predictive performance ensemble often unskillful needed ashing classical verification tool focus event lead unexpected behavior square effect thresholded weighted scoring rule developed use derivation continuous ranked probability score crp however property crp extreme event generate undesirable effect quality verification using theoretical argument simulation example illustrate pitfall conventional verification tool propose different direction ass ensemble forecast using extreme value theory considering proper score random variable
Statistics,minimax nonparametric twosample test,consider problem comparing probability density two group model complex pattern underlying density formulate problem nonparametric density hypothesis testing problem major difficulty conventional test may fail distinguish alternative null hypothesis controlled type error paper model logtransformed density tensor product reproducing kernel hilbert space rkhs propose probabilistic decomposition space decomposition quantify difference density two group component norm probabilistic decomposition based bernstein width sharp minimax lower bound distinguishable rate established nonparametric twosample test propose penalized likelihood ratio plr test possessing wilks phenomenon asymptotically chisquare distributed test statistic achieving established minimax testing rate simulation real application demonstrate proposed test outperforms conventional approach various scenario
Statistics,taylor moment expansion continuousdiscrete gaussian filtering smoothing,paper concerned nonlinear gaussian filtering smoothing continuousdiscrete statespace model dynamic model formulated stochastic differential equation sde measurement obtained discrete time instant propose novel taylor moment expansion tme gaussian filter smoother approximate moment sde temporal taylor expansion differently classical linearisation taylor approach taylor expansion formed moment function directly time variable using taylor expansion nonlinear function model analyse theoretical property including positive definiteness covariance estimate stability tme gaussian filter smoother numerical experiment demonstrate proposed tme gaussian filter smoother significantly outperform stateoftheart method term estimation accuracy numerical stability
Statistics,equivalence relation lp distance time series,introduce general framework defining equivalence measuring distance time series first concrete method prove existence equivalence relation space time series quotient space equipped metrizable topology illustrate algorithmically calculate distance among collection time series perform clustering analysis based distance apply insight analyse recent bushfires nsw australia introduce new method analyse time series crosscontextual setting
Statistics,fisher combined probability test highdimensional covariance matrix,testing large covariance matrix fundamental importance statistical analysis highdimensional data past decade three type test statistic studied literature quadratic form statistic maximum form statistic weighted combination known quadratic form statistic would suffer low power sparse alternative maximum form statistic would suffer low power dense alternative weighted combination method introduced enhance power quadratic form statistic maximum form statistic weight appropriately chosen paper provide new perspective exploit full potential quadratic form statistic maximum form statistic testing highdimensional covariance matrix propose scaleinvariant power enhancement test based fisher method combine pvalues quadratic form statistic maximum form statistic carefully studying asymptotic joint distribution quadratic form statistic maximum form statistic prove proposed combination method retains correct asymptotic size boost power general alternative moreover demonstrate finitesample performance simulation study real application
Statistics,age time migration dependent dynamic disease,paper generalizes previously published differential equation describes relation agespecific incidence remission mortality disease prevalence underlying model simple compartment model three state illnessdeath model contrast former work migration calendar timeeffects included application theoretical finding hypothetical example irreversible disease treated
Statistics,differentially private exponential random graph,propose method release analyze synthetic graph order protect privacy individual relationship captured social network proposed technique aim fitting estimating wide class exponential random graph model ergms differentially private manner thus offer rigorous privacy guarantee specifically use randomized response mechanism release network epsilon edge differential privacy maintain utility statistical inference treating original graph missing propose way use likelihood based inference markov chain monte carlo mcmc technique fit ergms produced synthetic network demonstrate usefulness proposed technique real data example
Statistics,extreme order statistic stationary process,let xi ile n independent copy stationary process x given positive constant u define set r th conjunction c r u tin x r n u x r n r th largest order statistic ldots xn tge numerous application brain mapping digital communication system interest approximation probability set conjunction c r u empty imposing albin condition x paper obtain exact asymptotic expansion probability u tends infinity establish tail asymptotics supremum generalized skewgaussian process gumbel limit theorem minimum order statistic stationary gaussian process byproduct derive version li shao normal comparison lemma minimum maximum gaussian random vector
Statistics,causation coefficient taxonomy correlationcausation relationship,paper introduces causation coefficient defined term probabilistic causal model coefficient suggested natural causal analogue pearson correlation coefficient permit comparing causation correlation simple yet rigorous manner together coefficient provide natural way classify possible correlationcausation relationship occur practice example relationship provided addition typical relationship correlation causation analyzed provide insight correlation causation often conflated finally example calculation causation coefficient shown real data set
Statistics,mostly harmless simulation using monte carlo study estimator selection,consider two recent suggestion perform empirically motivated monte carlo study help select treatment effect estimator unconfoundedness show theoretically neither likely informative except restrictive condition unlikely satisfied many context test empirical relevance also apply approach realworld setting estimator performance known approach worse random selecting estimator minimise absolute bias better selecting estimator minimise mean squared error however using simple bootstrap least good often better researcher would best advised use range estimator compare estimate robustness
Statistics,augmented lagrangian approach sparse principal component analysis,principal component analysis pca widely used technique data analysis dimension reduction numerous application science engineering however standard pca suffers fact principal component pc usually linear combination original variable thus often difficult interpret pc alleviate drawback various sparse pca approach proposed literature despite success achieving sparsity important property enjoyed standard pca lost method uncorrelation pc orthogonality loading vector also total explained variance attempt maximize optimistic paper propose new formulation sparse pca aiming finding sparse nearly uncorrelated pc orthogonal loading vector explaining much total variance possible also develop novel augmented lagrangian method solving class nonsmooth constrained optimization problem well suited formulation sparse pca show converges feasible point moreover regularity assumption converges stationary point additionally propose two nonmonotone gradient method solving augmented lagrangian subproblems establish global local convergence finally compare sparse pca approach several existing method synthetic random real data respectively computational result demonstrate sparse pc produced approach substantially outperform method term total explained variance correlation pc orthogonality loading vector
Statistics,defended also applied perceived absurdity bayesian inference,missionary zeal many bayesians old matched direction view among theoretician bayesian method absurdnot merely misguided obviously wrong principle consider several example beginning feller classic text probability theory continuing recent case perceived bayesian nature socalled doomsday argument analyze note intellectual background behind various misconception bayesian statistic without aiming complete historical coverage reason dismissal
Statistics,array variate elliptical random variable multiway kronecker delta covariance matrix structure,standard statistical method applied matrix random variable often fail describe underlying structure multiway data set paper discus concept array variate random variable introduce class elliptical array density elliptical contour
Statistics,maximum lilkelihood estimation β model,study maximum likelihood estimation statistical model undirected random graph known beta model degree sequence minimal sufficient statistic derive necessary sufficient condition based polytope degree sequence existence maximum likelihood estimator mle model parameter characterize combinatorial fashion sample point leading nonexistent mle nonestimability probability parameter nonexistent mle formulate condition guarantee mle exists probability tending one number node increase
Statistics,property moment estimator shape parameter gamma distribution,exact distribution moment estimator shape parameter gamma distribution small sample derived order preserving property estimator presented
Statistics,order preserving property moment estimator,balakrishnan mi considered order preserving property maximum likelihood estimator paper given condition moment estimator property preserving stochastic order considered property preserving usual stochastic order well likelihood ratio order mainly sufficient condition given one parameter family distribution also exponential family location family scale family
Statistics,cramerraoinduced bound candecompparafac tensor decomposition,paper present cramerrao lower bound crlb variance unbiased estimate factor matrix canonical polyadic cp candecompparafac cp decomposition tensor noisy observation ie tensor plus random gaussian iid tensor novel expression derived bound mean square angular error factor along selected dimension tensor arbitrary dimension expression need le operation computing bound best existing stateofthe art algorithm operation n r tensor order tensor rank insightful expression derived tensor rank rank arbitrary dimension tensor arbitrary dimension rank two factor matrix orthogonal column result used gauge performance different approximate cp decomposition algorithm prediction accuracy checking stability given decomposition tensor condition whether crlb finite novel expression derived hessian matrix needed popular damped gaussnewton method solving cp decomposition tensor missing element beside computing crlb tensor expression may serve design damped gaussnewton algorithm decomposition
Statistics,testing ifra orderingii,suppose f g two life distribution function said f ifra g written f g g f x starshaped infty paper problem testing f g f g f neq g considered case g known g unknown propose new test based ustatistics obtain asymptotic distribution test statistic new test compared well known test literature addition apply test real data set context reliability
Statistics,high dimensional bayesian inference gaussian directed acyclic graph model,paper consider gaussian model markov respect arbitrary dag first construct family conjugate prior cholesky parametrization covariance matrix model family many shape parameter dag vertex naturally extends work geiger heckerman distribution derive prior distribution covariance precision parameter gaussian dag markov model work thus extends work dawid lauritzen letac massam gaussian model markov respect decomposable graph arbitrary dag reason call distribution dagwishart distribution advantage distribution posse strong hyper markov property thus allow explicit estimation covariance precision parameter regardless dimension problem also allow u develop methodology model selection covariance estimation space dagmarkov model demonstrate via several numerical example proposed method scale well highdimensions
Statistics,generalization gaussian semiparametric estimator multivariate longrange dependent process,paper propose study general class gaussian semiparametric estimator gse fractional differencing parameter context longrange dependent multivariate time series establish large sample property estimator without assuming gaussianity class model considered satisfies simple condition spectral density function restricted small neighborhood zero frequency includes important class varfima process also present simulation study ass finite sample property proposed estimator based smoothed version gse support competitiveness
Statistics,spectrum estimation unified framework covariance matrix estimation pca large dimension,covariance matrix estimation principal component analysis pca two cornerstone multivariate analysis classic textbook solution perform poorly dimension data magnitude similar sample size even larger setting common remedy statistical problem nonlinear shrinkage eigenvalue sample covariance matrix optimal nonlinear shrinkage formula depends unknown population quantity thus available however possible consistently estimate oracle nonlinear shrinkage motivated asymptotic ground key tool end consistent estimation set eigenvalue population covariance matrix also known spectrum interesting challenging problem right extensive monte carlo simulation demonstrate method desirable finitesample property outperform previous proposal
Statistics,piterbarg maxdiscretisation theorem stationary vector gaussian process observed different grid,paper derive piterbarg maxdiscretisation theorem two different grid considering centered stationary vector gaussian process far literature result direction derived joint distribution maximum gaussian process grid mathfrak r cdots paper extend recent finding considering additionally maximum another grid mathfrak r derive joint limiting distribution maximum stationary gaussian vector process different choice grid letting tto infty
Statistics,fisher neymanpearson nhst tutorial teaching data testing,despite frequent call overhaul null hypothesis significance testing nhst controversial procedure remains ubiquitous behavioral social biomedical teaching research little change seems possible procedure becomes well ingrained mind current practice researcher thus optimal opportunity change time procedure taught undergraduate postgraduate level paper present tutorial teaching data testing procedure often referred hypothesis testing theory first procedure introduced approach data testing followed fisher test significance second approach followed neyman pearson test acceptance final procedure incongruent combination previous two theory current approach nsht researcher sticking latter two compromise solution improve nhst conclude tutorial
Statistics,risk contagion regular variation asymptotic tail independence,risk contagion concern entity dealing large scale risk suppose x denotes risk vector pertaining two component system relevant measurement risk contagion would quantify amount influence high value x measured variety way paper study two measure quantity e max called marginal mean excess mme well related quantity e xy called marginal expected shortfall me quantity indicator risk contagion useful various application ranging finance insurance systemic risk environmental climate risk work assumption multivariate regular variation hidden regular variation asymptotic tail independence risk vector x many broad useful model class satisfy assumption present several example derive asymptotic behavior mme me threshold tends infinity observe although assume asymptotic tail independence model mme me converge general condition reflects underlying weak dependence model still remains significant besides consistency empirical estimator introduce extrapolation method based extreme value theory estimate mme me high threshold little data available show estimator consistent illustrate methodology simulated real data set
Statistics,unifying markov property graphical model,several type graph different conditional independence interpretation also known markov property proposed used graphical model paper unify markov property introducing class graph four type edge line arrow arc dotted line single separation criterion show independence structure defined class specialize previously defined case suitable subclass graph considered addition define pairwise markov property subclass chain mixed graph includes chain graph lwf interpretation well summary graph consequently ancestral graph prove equivalence pairwise markov property global markov property compositional graphoid independence model
Statistics,tractable measure component overlap gaussian mixture model,ability quantify distinctness cluster structure fundamental certain simulation study particular comparing performance different classification algorithm intrinsic integral measure based overlap corresponding mixture component often analytically intractable also case gaussian mixture model unequal covariance matrix space dimension work focus gaussian mixture model sample level assume class assignment known derive measure component overlap based eigenvalue generalized eigenproblem represents fisher discriminant task explain rationale behind present simulation result show well reflect behavior integral measure linear approximation analyzed coefficient posse advantage analytically tractable numerically computable even complex setup
Statistics,graph margin bayesian network,directed acyclic graph dag model also called bayesian network impose conditional independence constraint multivariate probability distribution widely used probabilistic reasoning machine learning causal inference latent variable included model set possible marginal distribution remaining observed variable generally complex represented dag larger class mixed graphical model use multiple edge type introduced overcome however class represent model arise margin dag paper show ordinary mixed graph fundamentally insufficiently rich capture variety marginal model introduce new class hypergraphs called mdags latent projection operation obtain mdag margin dag show distinct marginal dag model represented least one mdag provide graphical result towards characterizing two marginal model finally show mdags correctly capture marginal structure causallyinterpreted dag intervention observed variable
Statistics,measurement error deconvolution space generalized function,paper considers convolution equation arise problem measurement error nonparametric regression error variable independence condition equation examined space generalized function account possible singularity make possible consider density arbitrary absolutely continuous distribution operate fourier transforms polynomially growing regression function result derived identification wellposedness topology generalized function deconvolution problem regression model condition consistency plugin estimation model derived
Statistics,propagation initial error parameter linear gaussian state space model,linear gaussian state space model parametrized theta subset mathbb r r r geq corresponding vector parameter model kalman filter give exactly solution optimal filtering weak assumption result supposes perfectly known real application assumption realistic since unknown estimated paper analysis kalman filter biased estimator show propagation bias estimation hidden state give expression propagation linear gaussian state space model extend result almost linear model estimated extended kalman filter illustration given autoregressive process measurement noise widely studied econometrics model economic financial data
Statistics,statistique et big data analytics volumétrie lattaque de clone,article assumes acquired skill expertise statistician unsupervised nmf kmeans svd supervised learning regression cart random forest skill knowledge statistician must acquire reach volume scale big data quick overview different strategy available especially imposed hadoop algorithm available learning method outlined order understand adapted strong stress mapreduce functionality
Statistics,solution indefinite integral standard normal probability density function,conventional wisdom assumes indefinite integral probability density function standard normal distribution expressed finite elementary term true expression antiderivative infinite elementary term differentiated directly yield standard normal density function derive function using infinite partial integration review relation cumulative distribution function standard normal distribution error function
Statistics,pairwise markov property regression graph,sequence regression one may generate joint probability distribution one start joint marginal distribution context variable possibly concentration graph structure continues ordered sequence conditional distribution named regression joint response involved random variable may discrete continuous type generating process specifies response conditioning set contains regressor variable lead least one valid ordering node corresponding regression graph three type edge one undirected dependence among context variable another undirected dependence among joint response one directed dependence response regressor variable regression graph several definition pairwise markov property interprets conditional independence associated missing edge graph different way explain property arise prove equivalence compositional graphoids point equivalence one global markov property
Statistics,sobol index problem defined nonrectangular domain,novel theoretical numerical framework estimation sobol sensitivity index model input confined nonrectangular domain eg presence inequality constraint developed two numerical method namely quadrature integration method may efficient problem low medium dimensionality mcqmc estimator based acceptancerejection sampling method proposed numerical estimation sobol sensitivity index several model test function constraint considered analytical solution sobol sensitivity index found solution used benchmark verifying numerical estimate method shown general efficient
Statistics,progress conjecture regarding triangular distribution,triangular distribution wellknown class distribution often used elementary example probability model maximum likelihood estimation mode parameter triangular distribution unit interval performed via order statisticsbased method conjectured method conducted using constant number likelihood function evaluation average sample size becomes large prove two theorem validate conjecture graphical numerical result presented supplement proof
Statistics,graphical model discrete continuous data,introduce general framework undirected graphical model generalizes gaussian graphical model wide range continuous discrete combination different type data model framework called exponential trace model amenable estimation based maximum likelihood introduce samplingbased approximation algorithm computing maximum likelihood estimator apply pipeline learn simultaneous neural activity spike data
Statistics,approximation via convolutiondefined mixture model,oftencited fact regarding mixing mixture distribution density function able approximate density function unknown distribution arbitrary degree accuracy provided mixing mixture distribution sufficiently complex fact often made concrete investigate review theorem provide approximation bound mixing distribution connection approximation bound mixing distribution estimation bound maximum likelihood estimator finite mixture location scale distribution reviewed
Statistics,faithfulness probability distribution graph,main question graphical model causal inference whether given probability distribution p usually underlying distribution data graph graph p faithful main goal paper provide theoretical answer problem work general independence model contain probabilistic independence model special case exploit generalization ordering called preordering node mixed graph allows u provide sufficient condition given independence model markov graph minimum possible number edge importantly necessary sufficient condition given probability distribution faithful graph present result general case mixed graph specialize definition result betterknown subclass undirected concentration bidirected covariance graph well directed acyclic graph
