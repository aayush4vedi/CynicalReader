ID,Source,Tag_Code,Tag_Name,Title,Url,ProcessdedTitle,WeightedContent,Content
1,Lobsters,scaling,Scaling and architecture,Organizing architectural katas,https://nelis.boucke.be/post/architectural-katas/,organizing architectural katas,architectural katas ted neward architectural katas poster architetural katas code katas architectural katas poster step architectural kata description architectural kata website practical organization architectural katas preparation invite kind environment project http wwwarchitecturalkatascom room view additional constraint model facilitating workshop agenda timing role facilitator conclusion twitter external link see also,architectural katas great way spice architectural skill team community practice productive discussion architecture always receive great feedback co organizing architectural katas regularly get question organizing others matteo pierro run session xpdays benelux organize katas sharing experience encourage others organizing kind katas architectural katas ivory tower architect defining architecture team bygone era general trend development team define software architecture least team strongly involved defining architecture people learn architecture well thin air need way learn practice also main motivation ted neward coming architectural katas illustrated following two quote poster architetural katas get great designer great designer design course fred brook supposed get great architect get chance architect fewer halfdozen time career ted neward ted searching way practice defining architecture inspiration code katas came concept architectural katas architectural katas poster step architectural kata architectural katas workshop several small group people practice discussing designing architecture typically workshop involves several iteration start iteration group get project description constraint get time discover needed ask clarifying question customer often facilitator discus technology option design vision architecture discussing group present result group gather feedback iteration end reflection break information check description architectural kata website practical organization architectural katas organizing good workshop asks proper preparation facilitation might look daunting first hope next section help way learning post extracted organizing event like year evening meetups conference session open fullday session till closed company session audience session varied people people preparation invite invite team member people community interested discussing architecture includes limited developer tester product owner analyst scrum master architect help majority people software development experience affinity strict requirement participant prior experience architect required prefer mixture role background healthy discussion make assumption explicit run architectural katas workshop meetup conference company far understood ted run regular event community kind environment important aspect environment setup provide unlimited modeling space need good modeling space per group often people stop architecting soon allocated space full might miss nice creativity discussion experience large magic whiteboard trump smaller whiteboard trump flipchart trump table flipchart paper trump table smaller size paper room requirement modeling space combination lot people discussing small group ask spacious room several room people need space discus comfortable noise level must remain acceptable people understand larger workshop group prefer use several breakout room material bring enough postits several size form paper good whiteboard permanent marker food drink type workshop exhausting especially keep iterating day long catering sugar dip need drink help people stay focused project kata start brief intentionally vague incomplete project description ted neward effort set website gather share project description http wwwarchitecturalkatascom although ted talk drawing random kata group always preselected project asked group project main motivation allow efficient exchange feedback reflection phase selecting kata important think audience completely new format people architectural background group know embedded development challenge lot complex break others require specialized knowledge audience might like room view starter kata people contact hotel thus basic knowledge domain room view large hotel reservation company want build next generation hotel reservation management system specifically tailored highend resort spa guest view reserve specific room user guest hundred hotel staff le requirement registration made via web mobile phone call walkin guest ability either book type room standard deluxe suite choose specific room stay viewing picture room location hotel system must able maintain room status booked available ready clean etc well room needed next must also stateoftheart housekeeping management functionality cleaning maintenance staff directed various room based priority reservation need using proprietary device supplied reservation company attached cleaning cart standard reservation functionality eg payment registration info etc done leveraging existing reservation system system webbased hosted reservation company additional context peak season quickly approaching system must ready quickly wait next year company also investing heavily cutting edge technology like smart room lock open via cell phone interested highend market sale people tremendous clout organization people often scramble make promise true others often use beginning check work sick stuff cruft another one always keep around challenge world web craft go beyond simple website thus contains lot challenge people additional constraint similar coding katas try adding additional constraint maximize learning effect focus specific learning goal example could use kata learn new modeling language practice constraint exaggerate practice get muscle memory example constraint use model document architectural vision model simple set model visualize static structure software architecture giving people simple standardized way visualize decision architectural model suddenly lot fewer misunderstanding highquality discussion typically use following flip chart add postits one one introduce model explicitly try use existing cloud service build nothing use cloud service component two component talk use event storming identify behavior update constraint might vary different iteration example might give constraint first iteration use constraint second iteration use event storming discover behavior third iteration finally update model represent behavior facilitating workshop agenda timing typical cycle single kata session take min introduction kata constraint team discussion review share architecture reflect share observation experimented longer shorter discussion slot seems sweet spot people still engaged also need trigger new idea reviewing reflecting starting new cycle perfect way bring inspiration depending amount time repeat cycle several time including minute break cycle take thing session single iteration short get lot indepth learning architecture several time would advise running short session convince people try goal conference blogpost maybe repeat weekly could work never tried session two three iteration start interesting people apply thing learned sharing reflection future session two iteration typically change application group provide extra constraint second iteration full day session got lot positive feedback fullday session get typically need take lunch break experience hard iteration day people exhausted example agenda general introduction iteration break iteration lunch iteration break iteration closing role facilitator facilitator architectural kata session easy asks thing facilitation time keeping key factor make architectural katas success facilitation organizing room optimal group discussion coming appropriate challenge constraint use appropriate formate sharing timekeeping help group stuck especially facilitation larger group extra challenging use different technique context feel overwhelmed thought look join another architectural kata session pair experienced facilitator customer question large part facilitator coming answer question towards customer project description purposefully vague short lot room interpretation time make answer fly encourage make assumption make explicit technical experience help technical experience experience architecting especially get group going bit stuck observe help reflect important part iteration reflect happened share learned facilitator ensure everyone get chance share learning end add observation emphasize certain aspect strengthen certain learning see lot expectation towards facilitator event often hard fulfill advise pair rule number facilitating type event handle people prefer pair would add one helper people conclusion architectural katas great way practice discussing architecture modern age agile team supposed define least involved architecture good idea practice modeling architectural discussion skill team hope post help people organize architectural katas company community still doubt anything else help organize architectural kata hesitate contact twitter take challenge go forth organize katas external link see also
2,Lobsters,scaling,Scaling and architecture,Architecture decision record (ADR) examples,https://github.com/joelparkerhenderson/architecture_decision_record,architecture decision record adr example,architecture decision record adr architecture decision record architecture decision record architecture decision architecture decision log architecturallysignificant requirement architecture knowledge management ad adl adr akm asr start using adrs architectural decision start using adrs tool start using adrs git adr file name convention suggestion writing good adrs adr example template information,architecture decision record adr architectural decision record adr document capture important architectural decision made along context consequence content architecture decision record architecture decision record adr document capture important architectural decision made along context consequence architecture decision ad software design choice address significant requirement architecture decision log adl collection adrs created maintained particular project organization architecturallysignificant requirement asr requirement measurable effect software system architecture within topic architecture knowledge management akm goal document provide fast overview adrs create look information abbreviation ad architecture decision adl architecture decision log adr architecture decision record akm architecture knowledge management asr architecturallysignificant requirement start using adrs start using adrs talk teammate area decision identification urgent important ad made wait known personal collective experience well recognized design method practice assist decision identification ideally maintain decision todo list complement product todo list decision making number decision making technique exists general one software architecture specific one instance dialogue mapping group decision making active research topic decision enactment enforcement ad used software design hence communicated accepted stakeholder system fund develop operate architecturally evident coding style code review focus architectural concern decision two related practice ad also considered modernizing software system software evolution decision sharing optional many ad recur across project hence experience past decision good bad valuable reusable asset employing explicit knowledge management strategy group decision making active research topic decision documentation many template tool decision capturing exist see agile community eg nygard adrs see traditional software engineering architecture design process eg table layout suggested ibm umf tyree akerman capitalone decision guidance step adopted wikipedia entry architectural decision number decision making technique exists general one software software architecture specific one instance dialogue mapping start using adrs tool start using adrs tool way want example like using google drive online editing create google doc google sheet like use source code version control git create file adr like using project planning tool atlassian jira use tool planning tracker like using wikis mediawiki create adr wiki start using adrs git like using git version control like start using adrs git typical software project source code create directory adr file adr create text file databasetxt write anything want adr see template repository idea commit adr git repo adr file name convention choose create adrs using typical text file may want come adr file name convention prefer use file name convention specific format example choosedatabasemd formattimestampsmd managepasswordsmd handleexceptionsmd file name convention name present tense imperative verb phrase help readability match commit message format name us lowercase underscore repo balance readability system usability extension markdown useful easy formatting suggestion writing good adrs characteristic good adr point time identify ad made rationality explain reason making particular ad immutable record decision made previously published adr altered specificity adr single ad characteristic good context adr characteristic good consequence adr new adr may take place previous adr ad made replaces invalidates previous adr new adr created adr example template adr example template collected net information introduction template indepth tool example see also remap representation maintenance process knowledge drl decision representation language ibis issuebased information system qoc question option criterion ibm ebusiness reference architecture framework
3,Lobsters,scaling,Scaling and architecture,Unbundling Data Science Workflows with Metaflow and AWS Step Functions,https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280,unbundling data science workflow metaflow aws step function,unbundling data science workflow metaflow aws step function david berg ravi kiran chirravuri romain cledat jason ge savin goyal ferras hamad ville tuulos data science framework netflix opensourced december problem solved people tool metaflow compare workflow scheduler metaflow workflow executed production today releasing first opensource integration scheduler aws step function unbundling dag many framework organize work directed acyclic graph compute step scheduling scientific workflow dag compute layer optimized computer vision scheduler layer architecting data flow state transfer job scheduler topological order foreach construct compute aws batch executing task independent container local scheduler resume command aws step function internal scheduler called meson luigi airflow aws step function high availability zero operational burden sfn costeffective solution highly scalable limited state transition triggering workflow cloudwatch monitoring alerting see documentation hood amazon state language parameter resource retry dependency aws batch next step deploying step function administrator guide metaflow please get touch,unbundling data science workflow metaflow aws step functionsby david berg ravi kiran chirravuri romain cledat jason ge savin goyal ferras hamad ville tuulostl dr today releasing new job scheduler integration aws step function integration allows user metaflow schedule production workflow using highly available scalable maintenancefree service without change existing metaflow codethe idea abstraction layer fundamental way manage complexity computing atom make transistor transistor make functional unit cpu cpu implement instruction set targeted compiler higherlevel language key benefit layer developed independently separate group people coupled together wellscoped interface layer independent life cycle enable higher layer stack maintain semblance stability without hindering innovation layer belowmetaflow data science framework netflix opensourced december designed around idea independent layer already year ago started building metaflow recognized excellent solution available layer typical data science stack stitching layer together challenge data science project wanted metaflow become substrate integrates layer easytouse productivity tool optimized data science use casesin contrast many framework metaflow try abstract away existence separate layer believe problem solved people tool following humancentric usabilitydriven approach data scientist care lower layer stack work believe benefit trying pretend stack exist would problematic especially thing failthis article focus job scheduler layer two layer surround architecture layer defines structure user code compute layer defines code executedsince initial opensource release metaflow heard question metaflow compare workflow scheduler metaflow workflow executed production answer question metaflow designed used conjunction productiongrade job scheduler today releasing first opensource integration scheduler aws step function use execute metaflow workflow scalable highlyavailable mannerbefore going detail aws step function want highlight role job scheduling layer metaflow stackunbundling dagsimilar many framework help manage data science workflow metaflow asks user organize work directed acyclic graph compute step like hypothetical example find dag abstraction natural way think data science workflow instance data scientist might draw dag whiteboard asked want organize modeling pipeline level dag say anything code get executed executed data scientist want structure codethe idea scheduling scientific workflow dag decade old many existing system require tight coupling layer data science stack often necessitated infrastructural limitation predate cloud system user may need specify part modeling code using custom dsl dsl may executed builtin job scheduler tightly coupled compute layer eg hpc cluster defines code executedfor specific use case tight coupling may well justified however since inception metaflow supported hundred different reallife data science use case natural language processing computer vision classical statistic using r make much harder define tightly coupled stack whathowandwhere would work well use case instance user may want use compute layer optimized computer vision certain step workflowmetaflow unbundles dag separate architecture scheduler compute layer user may use language library familiar leveraging rich data science ecosystem r python architect modeling code user code get packaged compute layer metaflow user focus code rather eg writing dockerfiles finally scheduling layer take care executing individual function using compute layer data scientist point view infrastructure work exactly write idiomatic modeling code using familiar abstraction get executed without hassle even massive scalethe scheduler layerbefore dag scheduled user must define metaflow come opinionated syntax set utility crafting data science workflow python r coming soon provide plenty support architecting robust data science code empowers data scientist create operate workflow autonomously even year experience writing scalable system software particular metaflow take care data flow state transfer layer independent schedulermetaflow provides strong guarantee backwards compatibility userfacing api user write code confidently knowing metaflow schedule execute without change even underlying layer evolve time user point view core valueadd metaflow apis rather specific implementation underlying layersonce user specified workflow orchestrating execution dag belongs job scheduler layer scheduling layer need care code executed sole responsibility schedule step topological order making sure step finish successfully successor graph executedwhile may sound deceptively simple consider mean context reallife data science company like netflix graph may almost arbitrarily large especially thanks dynamic fanouts ie foreach construct instance existing metaflow workflow train model every country foreach every model perform hyperparameter search parametrizations foreach result task single workflow hence scalability critical feature schedulerthere arbitrarily many workflow running concurrently instance countrylevel workflow may different variation scheduled simultaneously netflixscale scheduler need able handle hundred thousand active workflowsbesides scale scheduler need highly available responsibility scheduler make sure businesscritical workflow get executed time achieving scalability highavailability system nontrivial engineering challengethe scheduler may provide various way trigger execution workflow workflow may started based time simple cronstyle scheduling external signal may trigger execution netflix workflow triggered based availability upstream data ie ml workflow start whenever fresh data available best organize web workflow deep topic cover detail laterthe scheduler include tool observability alerting convenient monitor execution workflow gui get alerted various mean critical execution failsit important note scheduling layer execute user code execution user code responsibility compute layer box metaflow provides local compute layer executes task local process taking advantage multiple cpu core compute resource needed local scheduler aws step function utilize aws batch executing task independent containerslocal schedulermetaflow come builtin scheduler layer make easy test workflow locally laptop development sandbox builtin scheduler fully functional sense executes step topological order handle workflow ten thousand task lack support highavailability triggering alerting designsince metaflow designed interchangeable layer mind need reinvent wheel building yet another productiongrade dag scheduler instead local scheduler focus providing quick developtestdebug cycle development user deploy workflow productiongrade scheduler aws step function happy result many productiongrade scheduler provide firstclass local development experience local scheduler ensures smooth transition prototyping productionmetaflow recognizes deploying production linear process rather expect user use local scheduler production scheduler parallel instance initial deployment data scientist typically want continue working project locally eventually might want deploy new experimental version production scheduler run parallel production version ab test also thing fail production metaflow allows user reproduce issue occur production scheduler locally simply using resume command continue execution local machineaws step functionsmetaflow builtin local scheduler great solution running isolated workflow testing development quick manual iteration preferred high availability unattended execution netflix data scientist ready deploy metaflow workflow production use internal scheduler called meson internally meson fulfills requirement production scheduler laid abovesince opensourcing metaflow wanted find publicly available replacement meson user metaflow could benefit considered number existing popular opensource workflow scheduler luigi airflow system many benefit found lacking come high availability scalability key requirement production schedulerafter careful consideration chose aws step function sfn target first opensource production scheduler integration found following feature sfn appealing roughly order importance aws proven track record delivering high sla address high availability requirementshigh availability delivered zero operational burden netflix team senior engineer required develop operate internal scheduler expect smaller company want dedicate full team maintain scheduler likely sfn costeffective solutionwe optimistic sfn highly scalable especially term number concurrent workflow today size individual workflow limited state transition enough vast majority use case quite uniquely sfn high limit workflow execution time one year convenient demanding ml workflow may take long time executethere existing mechanism triggering workflow based external event time one leverage functionality build web data ml workflow similar netflix operates internallyone use wellknown aws tool cloudwatch monitoring alertingit remarkable today company size benefit offtheshelf tooling caliber many case negligible cost aligns well vision metaflow use best publicly available infrastructure layer ml stack metaflow take care removing gap stack data scientist point view deploy workflow production simply executingpython myflowpy stepfunctions createfor detail use step function metaflow see documentationunder hoodwhen user executes stepfunctions create metaflow statically analyzes user workflow defined flowspec class parse dag structure compile amazon state language workflow specified aws step functionsbesides compiling dag automatically translate parameter relevant decorator resource retry sfn configuration user code dependency snapshot stored guarantee production workflow impacted external change input datatoday compute layer supported sfn aws batch userdefined code ie metaflow task executed container managed aws batch future workflow scheduled sfn may leverage compute layer wellthis translation ensures user metaflow workflow executed either local scheduler sfn without change code data scientist focus writing modeling code test locally rapid iteration finally deploy code production single command importantly repeat cycle often needed minimal overheadnext stepswith aws step function integration released today user metaflow start leveraging productiongrade workflow scheduler similar setup netflix operating successfully past three yearsif data scientist us plan use metaflow learn deploying step function documentation infrastructure person wanting leverage metaflow step function organization take look brand new administrator guide metaflowwe believe aws step function excellent choice scheduling metaflow workflow production however layer metaflow stack pluggable design good experience another job scheduler could fulfill requirement set need help getting started step function please get touch
5,Lobsters,scaling,Scaling and architecture,Zero Downtime Release: Disruption-free Load Balancing of a Multi-Billion User Website,https://www.dropbox.com/s/g40kvbyxqhn4h72/fbr.pdf?dl=0,zero downtime release disruptionfree load balancing multibillion user website,,
6,Lobsters,scaling,Scaling and architecture,Scaling relational SQL databases,https://stribny.name/blog/2020/07/scaling-relational-sql-databases,scaling relational sql database,update database scale vertically leverage application cache redis memcached use efficient data type data normalization denormalization precompute data leverage materialized view pipelinedb use proper index leverage execution plan query optimization choose correct transaction isolation level bulk insert update compress data storage make alter table work manage concurrent connection add read replica disk partitioning use specialized extension timescaledb postgis sharding mysql cluster citus store everything one table process data outside sql database kafka clickhouse apache spark aware limitation managed sql database final word loading like,many application today still rely traditional sql database like mysql mariadb postgresql data storage data processing growing amount data new workload made database system often find situation need think scaling system come scaling might need think data storage store data becomes expensive slow working themfast insert update writeheavy workloadsmaking select query faster complexity need query huge amount dataconcurrency many client interacting database article present basic idea starting point scaling traditional sql database update database newer version mysql postgresql traditional sql database typically come performance improvement even newer database system faster direct replacement might new feature available take advantage keeping database system date expands option data give u best box performance basic enabler scaling scale vertically intuitive idea scaling use better hardware scale vertically one database server come hardware typically look cpu disk ram number cpu affect many query database run therefore many client servedthe size ram give u space index temporary table cache database store fast memory make system faster due minimizing io access diskdisk io speed highly affect query time especially full scan read index used writeheavy systemsdisk size allows u store data leverage application cache application code control cache data memory store like redis memcached avoid querying database cache database read also use system buffer writes eg collecting analytical data possible delay problem use efficient data type come data start data type word individual piece information physically stored memory choosing appropriate data type always balance efficiency functionality affecting required memory query performance number operation specific data type two basic way think optimal data type logical store general information like telephone number ip address instance store color string enum collection rgb integer store ip address string bytesphysical specific data type choose string integer time since typically multiple option eg date time typically stored timestamp faster limited date range functionality complex datetime developer friendly flexible allowing store timezone etc using appropriate type database column especially important want index column use clause use join lot data data normalization denormalization often trained normalize data relational database order reduce data redundancy improve data integrity generally useful might want reconsider data come scaling generally speaking simpler make data retrieved saved precompute data lot select query optimized data already requested form handy analyzing large amount data instance instead aggregating data every time aggregate beforehand course always possible eg aggregate average aggregate would lose precision leverage materialized view materialized view continuously updated data updated write operation scheduled time seen extension previous point unlike classic view materialized view physically stored need computed needed useful situation select query would take long time produce result example look pipelinedb extension postgresql produce aggregate realtime data allowing u keep aggregated statistic writeheavy system use proper index using right index table huge performance changer typically index column want query data want use perform join however adding index also reduce performance writeheavy workload since index updated every insert update also make sense use index data type finding right balance leverage execution plan query optimization optimizing read need know database query planner execute query use explain statement obtain execution plan sql server use query look database plan use existing index table plan make full table scan give u hint whether change structure data add index rewrite query different way please note however full table scan also faster necessary query black white situation choose correct transaction isolation level common relational database use multiversion concurrency control make locking granular instead locking whole table like myisam storage engine however still need tell database exactly concurrency control behave setting appropriate transaction isolation level basic isolation level standard greatly affect database system behaves bulk insert update writing updating individual row table efficient database like mysql postgresql way insert modify data bulk leverage every time compress data storage many time need query data column table case optimize storage size storing column compressed form especially useful string type binary data database extension might already compressing data hood always make sense look first data stored whether adding compression application layer would bring desired benefit make alter table work amount data workload grows experience slower slower alter table point might finish reasonable time first important thing know typically two way changing structure database table one inplace modifying original table one copy creating new table moving data afterwards database typically try modify table inplace possible might make sense sometimes explicitly ask copy operation adding new column better add end table since inserting column specific place slow changing existing column might good idea create new column first copy data remove column afterwards write operation table make difficult perform alter table make sense stop client writing table first always possible eg collection analytical data postponed using ingestion buffer front database temporarily pause etl job working table last resort simply alter table large writeheavy table opting creating new table store new data instead manage concurrent connection every database server come basic configuration maximum number concurrent connection need typically need reconfigure value increasing value enough though need make sure system actually run well desired number connection add read replica traditional sql database typically scale horizontally write operation adding server still add machine form readonly replica way work write operation done main server propagated machine using write ahead log replica therefore apply operation order underlying storage main server ensures data sync replica used scaling number read query number connected client need read operation disk partitioning partitioning allows u distribute one single table across filesystem store individual partition based specific rule rule chosen well sql query query partition limit search subset cut time query run since whole database table need looked use specialized extension need store work geospatial time series specialized data sometimes use database extension like timescaledb postgis make data processing storage efficient sharding sharding partitioning steroid allows u store part database table shard different server distributed database often built concept shard traditional sql server automatically shard data since work one main server replica investigate whether clustering solution like mysql cluster mysql citus postgresql would solve scaling problem solution built provide horizontally scalable sql database without limitation instance mysql cluster work classic innodb engine write application like used another option shard data manually application layer brings additional complexity application need manage multiple connection different server query database appropriately solution consider store everything one table amount data primary issue table becoming large think whether put data elsewhere need data occasionally need unique index across whole table simply divide data multiple table instance separate data different customer store older data archive table way keep table smaller performant case might even decide okay store data primary database either simply delete move another storage system process data outside sql database sometimes relational sql database enough instance might want create data workflow unfit system might able make change schema change existing program interact database maybe reached limit scaling current data processing need many case best approach move data another system process fortunately utilize write ahead log wal store change insert update alter table database perform programmatically read log stream another system instance stream data kafka using existing database connector allow application read send data directly specialized database eg analytical database clickhouse data lake like hadoop process also move data via mean taking copy database extracting data sql etc want scale data computation without using different database system process data multiple data source simply want organize data processing better use distributed clustercomputing software apache spark aware limitation managed sql database many people pay managed sql database cloud provider like aws azure worry maintaining however need scale traditional sql store many scaling option actually unavailable u include limited ability change database configuration limited hardware option choose limited number replica higher latency write ahead log might inaccessible might able update database version current modern version want might able install extension would improve performance system also debugging performance issue remote system might difficult impossible since might proper access operating system buying dream never worry sql store investigate option managed database offer actually sufficient final word presented option possible starting point give u area think want go extra detail scaling sql database straightforward operation always need think system option making every solution unique loading like
7,Lobsters,scaling,Scaling and architecture,How we migrated Dropbox from Nginx to Envoy,https://dropbox.tech/infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy,migrated dropbox nginx envoy,envoy legacy nginxbased traffic infrastructure next proxy layer written go dropbox traffic infrastructure edge network bandaid new envoybased traffic infrastructure performance architecture opening file even aiowrite thread pool enabled optimizing web server high throughput low latency guideline performance testing guideline benchmarking envoy perf connection accepted behalf currently blocked worker observability fundamental operational need stub status prometheus format admin interface pluggable tracing provider opentracing integration stream access log grpc access log service integration logging syslog protobufs xds querying one xds service universal ata plane api udpa open request cost aggregation orca katranbased ebpfxdp load balancer courier dropbox migration grpc access log service al endpoint discovery service ed secret discovery service sd runtime discovery service rtds route discovery service rds integrate envoy custom service discovery istio gocontrolplane configuration protocgenvalidate protoc plugin example dynamic extensibility protocol buffer extensibility development guide perl javascript ua nginx module openresty library module c community attempt well documented checkout http filter interface abseil static deadlock detection much canonical example http filter module monitoring framework stats also lua support moonjit webassembly webassembly proxy specification rust c extending envoy webassembly webassemblyhub community redefining extensibility proxy introducing webassembly envoy istio building testing shellbased configuration system b azelbuilt monorepo b azelbuilt nginx version integration test adding external dependency copybara prewritten mock integration test framework gtest googletest googlemock requires change unit test coverage azure ci pipeline googlebecnhmark incremental build remote caching distributed buildstests query even augment security one faster variant introducing openbsd new httpd vulnerability exposure addresssanitizer threadsanitizer memorysanitizer fuzzing ossfuzz ossfuzz architecture security advisory p ast year security release policy described great detail postmortem google vulnerability reward program vrp exploiting envoy heap vulnerability ubuntu debian issue stack protector boringssl fips mode feature file serving workinprogress caching brotli ecache multibackend http cache envoy grpc http bridge reverse grpcweb grpc json transcoder dropbox public apis added support http connect method courier grpc library superset grafana community nginxdevel official bug tracker community mailing list community meeting quic implementation recently presented cloudflare came implementation nginx quiche github issue de sign doc help wanted operational improvement performance optimization new grpc transcoding feature load balancing change current state migration configuring envoy edge proxy gated flag commandline option linux kernel support udp acceleration load reporting service lr endpoint discovery service ed around loadbalancing mobile small team mountain view ca,blogpost talk old nginxbased traffic infrastructure pain point benefit gained migrating envoy compare nginx envoy across many software engineering operational dimension also briefly touch migration process current state problem encountered way moved dropbox traffic envoy seamlessly migrate system already handle ten million open connection million request per second terabit bandwidth effectively made u one biggest envoy user world disclaimer although tried remain objective quite comparison specific dropbox way software development work making bet bazel grpc cgolang also note cover open source version nginx commercial version additional feature legacy nginxbased traffic infrastructure nginx configuration mostly static rendered combination yaml change required full redeployment dynamic part upstream management stats exporter written lua sufficiently complex logic moved next proxy layer written go post dropbox traffic infrastructure edge network section legacy nginxbased infrastructure nginx served u well almost decade adapt current development bestpractices internal private external apis gradually migrating rest grpc requires sort transcoding feature proxy protocol buffer became de facto standard service definition configuration software regardless language built tested bazel heavy involvement engineer essential infrastructure project open source community also operationally nginx quite expensive maintain config generation logic flexible split yaml python monitoring mix lua log parsing systembased monitoring increased reliance third party module affected stability performance cost subsequent upgrade nginx deployment process management quite different rest service relied lot system configuration syslog logrotate etc opposed fully separate base system first time year started looking potential replacement nginx frequently mention internally rely heavily golangbased proxy called bandaid great integration dropbox infrastructure access vast ecosystem internal golang library monitoring service discovery rate limiting etc considered migrating nginx bandaid couple issue prevent u golang resource intensive cc low resource usage especially important u edge since easily autoscale deployment cpu overhead mostly come gc http parser tl latter le optimized boringssl used nginxenvoy goroutineperrequest model gc overhead greatly increase memory requirement highconnection service like fips support go tl stack bandaid community outside dropbox mean rely ourself feature development decided start migrating traffic infrastructure envoy instead new envoybased traffic infrastructure let look main development operational dimension one one see think envoy better choice u gained moving nginx envoy performance nginx architecture eventdriven multiprocess support soreuseport epollexclusive workertocpu pinning although eventloop based fully nonblocking mean operation like opening file accesserror logging potentially cause eventloop stall even aio aiowrite thread pool enabled lead increased tail latency cause multisecond delay spinning disk drive envoy similar eventdriven architecture except us thread instead process also soreuseport support bpf filter support relies libevent event loop implementation word fancy epoll feature like epollexclusive envoy blocking io operation event loop even logging implemented nonblocking way cause stall look like theory nginx envoy similar performance characteristic hope strategy first step run diverse set workload test similarly tuned nginx envoy setup interested performance tuning describe standard tuning guideline optimizing web server high throughput low latency involves everything picking hardware o tunables library choice web server configuration test result showed similar performance nginx envoy test workload high request per second rps high bandwidth mixed lowlatencyhighbandwidth grpc proxying arguably hard make good performance test nginx guideline performance testing codified envoy also guideline benchmarking even tooling envoyperf project sadly latter look unmaintained resorted using internal testing tool called hulk reputation smashing service said couple notable difference result nginx showed higher long tail latency mostly due event loop stall heavy io especially used together soreuseport since case connection accepted behalf currently blocked worker nginx performance without stats collection par envoy lua stats collection slowed nginx highrps test factor expected given reliance luashareddict synchronized across worker mutex understand inefficient stats collection considered implementing something akin freebsd counter userspace cpu pinning perworker lockless counter fetching routine loop worker aggregating individual stats gave idea wanted instrument nginx internals eg error condition would mean supporting enormous patch would make subsequent upgrade true hell since envoy suffer either issue migrating able release server previously exclusively occupied nginx observability observability fundamental operational need product especially foundational piece infrastructure proxy even important migration period issue detected monitoring system rather reported frustrated user noncommercial nginx come stub status module stats copy active connection server accepts handled request reading writing waiting definitely enough added simple logbylua handler add perrequest stats based header variable available lua status code size cache hit etc example simple statsemitting function copy function mcachehitstats stat varupstreamcachestatus varupstreamcachestatus hit stat add upstreamcachehit else stat add upstreamcachemiss end end end addition perrequest lua stats also brittle errorlog parser responsible upstream http lua tl error classification top separate exporter gathering nginx internal state time since last reload number worker rssvms size tl certificate age etc typical envoy setup provides u thousand distinct metric prometheus format describing proxied traffic server internal state copy curl http wc l includes myriad stats different aggregation perclusterperupstreampervhost http stats including connection pool info various timing histogram perlistener tcphttptls downstream connection stats various internalruntime stats basic version info uptime memory allocator stats deprecated feature usage counter special shoutout needed envoy admin interface provide additional structured stats cert cluster configdump endpoint also important operational feature ability change error logging fly logging allowed u troubleshoot fairly obscure problem matter minute cpuprofiler heapprofiler contention would surely quite useful inevitable performance troubleshooting runtimemodify endpoint allows u change set configuration parameter without pushing new configuration could used feature gating etc addition stats envoy also support pluggable tracing provider useful traffic team multiple loadbalancing tier also application developer want track request latency endtoend edge app server technically nginx also support tracing thirdparty opentracing integration heavy development last least envoy ability stream access log grpc remove burden supporting syslogtohive bridge traffic team besides way easier secure spin generic grpc service dropbox production add custom tcpudp listener configuration access logging envoy like everything else happens grpc management service access log service al management service standard way integrating envoy data plane various service production brings u next topic integration nginx approach integration best described unixish configuration static heavily relies file eg config file tl certificate ticket allowlistsblocklists etc wellknown industry protocol logging syslog auth subrequests http simplicity backwards compatibility good thing small setup since nginx easily automated couple shell script system scale increase testability standardization become important envoy far opinionated traffic dataplane integrated control plane hence rest infrastructure encourages use protobufs grpc providing stable api commonly referred xds envoy discovers dynamic resource querying one xds service nowadays xds apis evolving beyond envoy universal data plane api udpa ambitious goal becoming de facto standard loadbalancers experience ambition work well already use open request cost aggregation orca internal load testing considering using udpa nonenvoy loadbalancers eg katranbased ebpfxdp load balancer especially good dropbox service internally already interact grpcbased apis implemented version xds control plane integrates envoy configuration management service discovery secret management route information information dropbox rpc please read courier dropbox migration grpc describe detail integrated service discovery secret management stats tracing circuit breaking etc grpc available xds service nginx alternative example use access log service al mentioned let u dynamically configure access log destination encoding format imagine dynamic version nginx logformat accesslog endpoint discovery service ed provides information cluster member analogous dynamically updated list upstream block server entry eg lua would balancerbyluablock nginx config case proxied internal service discovery secret discovery service sd provides various tlsrelated information would cover various ssl directive respectively ssl byluablock adapted interface secret distribution service runtime discovery service rtds providing runtime flag implementation functionality nginx quite hacky based checking existence various file lua approach quickly become inconsistent individual server envoy default implementation also filesystembased instead pointed rtds xds api distributed configuration storage way control whole cluster tool sysctllike interface accidental inconsistency different server route discovery service rds map route virtual host allows additional configuration header filter nginx term would analogous dynamic location block setheaderproxysetheader proxypass lower proxy tier autogenerate directly service definition configs example envoy integration existing production system canonical example integrate envoy custom service discovery also couple open source envoy controlplane implementation istio le complex gocontrolplane homegrown envoy control plane implement increasing number xds apis deployed normal grpc service production act adapter infrastructure building block set common golang library talk internal service expose stable xds apis envoy whole process involve filesystem call signal cron logrotate syslog log parser etc configuration nginx undeniable advantage simple humanreadable configuration win get lost config get complex begin codegenerated mentioned nginx config generated mix yaml may seen even written variation erb pug text template maybe even copy server server server errorpage servererrorpages errorpage errorpagestatusesjoin errorpagefile endfor route serviceroutes routeregex routeprefix routeexactpath location routeregex routeregex elif routeexactpath routeexactpath else routeprefix endif routebrotlilevel brotli brotlicomplevel routebrotlilevel endif approach nginx config generation huge issue language involved config generation allowed substitution andor logic yaml anchor loopsifsmacroses course python turingcomplete without clean data model complexity quickly spread across three problem arguably fixable couple foundational one declarative description config format wanted programmatically generate validate configuration would need invent config syntactically valid could still invalid c code standpoint example bufferrelated variable limitation value restriction alignment interdependency variable semantically validate config needed run nginx envoy hand unified datamodel configs configuration defined protocol buffer solves data modeling problem also add typing information config value given protobufs first class citizen dropbox production common way describingconfiguring service make integration much easier new config generator envoy based protobufs data modeling done proto file logic python example copy import gzip import compressor def defaultgzipconfig compressionlevel gzipcompressionlevelenum gzipcompressionleveldefault gzip return gzip envoy default zdefaultcompression compressionlevelcompressionlevel envoy default bit nginx us maxwbits bit envoy default nginx us maxmemlevel compressorcompressor removeacceptencodingheadertrue contenttypedefaultcompressiblemimetypes still case typechecked protobuf logically invalid example gzip windowbits take value kind restriction easily defined help protocgenvalidate protoc plugin copy windowbits validaterules lte gte finally implicit benefit using formally defined configuration model organically lead documentation collocated configuration definition example gzipproto copy value control amount internal memory used zlib higher value use memory faster produce better compression result default value memorylevel validaterules lte gte thinking using protobufs production system worried may lack schemaless representation good article envoy core developer harvey tuch work around using googleprotobufstruct googleprotobufany dynamic extensibility protocol buffer extensibility extending nginx beyond possible standard configuration usually requires writing c module nginx development guide provides solid introduction available building block said approach relatively heavyweight practice take fairly senior software engineer safely write nginx module term infrastructure available module developer expect basic container like hash tablesqueuesrbtrees nonraii memory management hook phase request processing also couple external library like pcre zlib openssl course libc lightweight feature extension nginx provides perl javascript interface sadly fairly limited ability mostly restricted content phase request processing commonly used extension method adopted community based thirdparty luanginxmodule various openresty library approach hooked pretty much phase request processing used logbylua stats collection balancerbylua dynamic backend reconfiguration theory nginx provides ability develop module c practice lack proper c interfaceswrappers primitive make worthwhile nonetheless community attempt far ready production though envoy main extension mechanism c plugins process well documented nginx case simpler partially due clean wellcommented interface c class act natural extension documentation point example checkout http filter interface language standard library basic language feature like template lambda function typesafe container algorithm general writing modern much different using golang stretch one may even say python feature beyond stdlib provided abseil library include dropin replacement newer c standard mutexes builtin static deadlock detection debug support additionalmore efficient container much specific canonical example http filter module able integrate envoy monitoring framework line code simply implementing envoy stats interface envoy also lua support moonjit luajit fork improved lua support compared nginx lua integration far fewer capability hook make lua envoy far le attractive due cost additional complexity developing testing troubleshooting interpreted code company specialize lua development may disagree case decided avoid use c exclusively envoy extensibility distinguishes envoy rest web server emerging support webassembly wasm fast portable secure extension mechanism wasm meant used directly compilation target generalpurpose programming language envoy implement webassembly proxy specification also includes reference rust c sdks describes boundary wasm code generic proxy separation proxy extension code allows secure sandboxing wasm lowlevel compact binary format allows near native efficiency top envoy proxywasm extension integrated xds allows dynamic update even potential ab testing extending envoy webassembly presentation kubecon remember time nonvirtual conference nice overview wasm envoy potential us also hint performance level native c code wasm service provider get safe efficient way running customer code edge customer get benefit portability extension run cloud implement proxywasm abi additionally allows user use language long compiled webassembly enables use broader set nonc library securely efficiently istio putting lot resource webassembly development already experimental version wasmbased telemetry extension webassemblyhub community sharing extension read detail redefining extensibility proxy introducing webassembly envoy istio currently use webassembly dropbox might change go sdk proxywasm available building testing default nginx built using custom shellbased configuration system makebased build system simple elegant took quite bit effort integrate bazelbuilt monorepo get benefit incremental distributed hermetic reproducible build google opensourced bazelbuilt nginx version consists nginx boringssl pcre zlib brotli librarymodule testingwise nginx set perldriven integration test separate repository unit test given heavy usage lua absence builtin unit testing framework resorted testing using mock configs simple pythonbased test driver copy class protocolcounterstest nginxtestcase classmethod def setupclass cl super protocolcounterstest cl setupclass clsnginxa clsaddnginx nginxconfigpath endpoint upstream clsstartnginxes assertdelta lambda getstat assertdelta lambda getstat def testhttp self r requestsget selfnginxaendpoint url assert rstatuscode requestscodesok top verify syntaxcorrectness generated configs preprocessing eg replacing ip address one switching selfsigned tl cert etc running nginx c result envoy side main build system already bazel integrating monorepo trivial bazel easily allows adding external dependency also use copybara script sync protobufs envoy udpa copybara handy need simple transformation without need forever maintain large patchset envoy flexibility using either unit test based gtestgmock set prewritten mock envoy integration test framework need anymore rely slow endtoend integration test every trivial change gtest fairly wellknown unittest framework used chromium llvm among others want know googletest good intro googletest googlemock open source envoy development requires change unit test coverage test automatically triggered pull request via azure ci pipeline also common practice microbenchmark performancesensitive code googlebecnhmark copy bazel run compilationmodeopt testcommonupstream loadbalancerbenchmark benchmarkfilter leastrequestloadbalancerchoosehost m m switching envoy began rely exclusively unit test internal module development copy testf courierclientidfiltertest identityparsing struct testcase std vector std string uris identity expected std vector testcase test spiffe proddropboxcomservicefoo spiffe proddropboxcomservicefoo foo spiffe proddropboxcomuserboo spiffe proddropboxcomuserboo userboo spiffe proddropboxcomhoststrange spiffe proddropboxcomhoststrange hoststrange spiffe corpdropboxcomuserbadprefix auto test test expectcall ssl urisanpeercertificate willonce testing return testuris expecteq getidentity ssl testexpected subsecond test roundtrips compounding effect productivity empowers u put effort increasing test coverage able choose unit integration test allows u balance coverage speed cost envoy test bazel one best thing ever happened developer experience steep learning curve large upfront investment high return investment incremental build remote caching distributed buildstests etc one le discussed benefit bazel give u ability query even augment dependency graph programmatic interface dependency graph coupled common build system across language powerful feature used foundational building block thing like linters code generation vulnerability tracking deployment system etc security nginx code surface quite small minimal external dependency typical see external dependency resulting binary zlib one faster variant tl library pcre nginx custom implementation protocol parser event library even went far reimplement libc function point nginx considered secure used default web server openbsd later two development community falling lead creation httpd read motivation behind move bsdcon introducing openbsd new httpd minimalism paid practice nginx vulnerability exposure reported year envoy hand way code especially consider c code far dense basic c used nginx also incorporates million line code external dependency everything event notification protocol parser offloaded party library increase attack surface bloat resulting binary counteract envoy relies heavily modern security practice us addresssanitizer threadsanitizer memorysanitizer developer even went beyond adopted fuzzing opensource project critical global infrastructure accepted free platform automated fuzzing learn see ossfuzz architecture practice though precaution fully counteract increased code footprint result envoy security advisory past year envoy security release policy described great detail postmortem selected vulnerability envoy also participant google vulnerability reward program vrp open security researcher vrp provides reward vulnerability discovered reported according rule practical example vulnerability potentially exploited see writeup exploiting envoy heap vulnerability counteract increased vulnerability risk use best binary hardening security practice upstream o vendor ubuntu debian defined special hardened build profile edgeexposed binary includes aslr stack protector symbol table hardening copy build hardened forcepic build hardened coptfstackclashprotection build hardened coptfstackprotectorstrong build hardened linkoptwl z relro z forking webservers like nginx environment issue stack protector since master worker process share stack canary canary verification failure worker process killed canary bruteforced bitbybit try envoy us thread concurrency primitive affected attack also want harden thirdparty dependency use boringssl fips mode includes startup selftests integrity checking binary also considering running asanenabled binary edge canary server feature come opinionated part post brace nginx began web server specialized serving static file minimal resource consumption functionality top line static serving caching including thundering herd protection range caching proxying side though nginx lack feature needed modern infrastructure backends grpc proxying available without connection multiplexing support grpc transcoding top nginx opencore model restricts feature go open source version proxy result critical feature like statistic available community version envoy contrast evolved ingressegress proxy used frequently grpcheavy environment webserving functionality rudimentary file serving still workinprogress caching neither brotli precompression use case still small fallback nginx setup envoy us upstream cluster http cache envoy becomes productionready could move staticserving use case using instead filesystem longterm storage read ecache design see ecache multibackend http cache envoy envoy also native support many grpcrelated capability grpc proxying basic capability allowed u use grpc endtoend application eg dropbox desktop client backends feature allows u greatly reduce number tcp connection traffic tier reducing memory consumption keepalive traffic grpc http bridge reverse allowed u expose legacy application using modern grpc stack grpcweb feature allowed u use grpc endtoend even environment middleboxes firewall id etc yet support grpc json transcoder enables u transcode inbound traffic including dropbox public apis rest grpc addition envoy also used outbound proxy used unify couple use case egress proxy since envoy added support http connect method used dropin replacement squid proxy begun replace outbound squid installation envoy greatly improves visibility also reduces operational toil unifying stack common dataplane observability parsing log stats thirdparty software service discovery relying courier grpc library software instead using envoy service mesh use envoy oneoff case need connect open source service service discovery minimal effort example envoy used service discovery sidecar analytics stack hadoop dynamically discover name journal node superset discover airflow presto hive backends grafana discover mysql database community nginx development quite centralized development happens behind closed door external activity nginxdevel mailing list occasional developmentrelated discussion official bug tracker nginx channel freenode feel free join interactive community conversation envoy development open decentralized coordinated github issuespull request mailing list community meeting also quite bit community activity slack get invite hard quantify development style engineering community let look specific example development nginx quic implementation recently presented code clean zero external dependency development process rather opaque half year cloudflare came implementation nginx result community two separate experimental version nginx envoy case implementation also work progress based chromium quiche quic http etc library project status tracked github issue design doc publicly available way patch completed remaining work would benefit community involvement tagged help wanted see latter structure much transparent greatly encourages collaboration u mean managed upstream lot small medium change operational improvement performance optimization new grpc transcoding feature load balancing change current state migration running nginx envoy sidebyside half year gradually switching traffic one another dns migrated wide variety workload envoy ingres highthroughput service file data dropbox desktop client served via endtoend grpc envoy switching envoy also slightly improved user performance due better connection reuse edge ingres highrps service file metadata dropbox desktop client get benefit endtoend grpc plus removal connection pool mean bounded one request per connection time notification telemetry service handle realtime notification server million http connection one active client notification service implemented via streaming grpc instead expensive longpoll method mixed highthroughputhighrps service api traffic metadata data allows u start thinking public grpc apis may even switch transcoding existing restbased apis right edge egress highthroughput proxy case dropbox aws communication mostly would allow u eventually remove squid proxy production network leaving u single data plane one last thing migrate would wwwdropboxcom migration start decommissioning edge nginx deployment epoch would pas migration flawless course lead notable outage hardest part migration api service lot different device communicate dropbox public curlwgetpowered shell script embedded device custom stack every possible http library nginx battletested defacto industry standard understandably library implicitly depend behavior along number inconsistency nginx envoy behavior api user depend number bug envoy library quickly resolved upstreamed u community help gist unusual nonrfc behavior also worth mentioning common configuration issue encountered circuitbreaking misconfiguration experience running envoy inbound proxy especially mixed environment improperly set circuit breaker cause unexpected downtime traffic spike backend outage consider relaxing using envoy mesh proxy worth mentioning default circuitbreaking limit envoy pretty tight careful buffering nginx allows request buffering disk especially useful environment legacy backends understand chunked transfer encoding nginx could convert request contentlength buffering disk envoy buffer filter without ability store data disk restricted much buffer memory considering using envoy edge proxy would benefit reading configuring envoy edge proxy security resource limit would want exposed part infrastructure getting closer prime time support added popular browser gated flag commandline option envoy support also experimentally available upgrade linux kernel support udp acceleration experiment edge internal xdsbased load balancer outlier detection currently looking using combination load reporting service lr endpoint discovery service ed building block creating common lookaside loadaware loadbalancer envoy grpc wasmbased envoy extension golang proxywasm sdk available start writing envoy extension go give u access wide variety internal golang libs replacement bandaid unifying dropbox proxy layer single dataplane sound compelling happen need migrate lot bandaid feature especially around loadbalancing envoy long way current plan envoy mobile eventually want look using envoy mobile apps compelling traffic perspective support single stack unified monitoring modern capability grpc etc across mobile platform migration truly team effort traffic runtime team spearheading team heavily contributed agata cieplik jeffrey gensler konstantin belyalov louis opter naphat sanguansin nikita v shirokov utsav shah yishu tai course awesome envoy community helped u throughout journey also want explicitly acknowledge tech lead runtime team ruslan nigmatullin whose action envoy evangelist author envoy mvp main driver software engineering side enabled project happen read far good chance actually enjoy digging deep webserversproxies may enjoy working dropbox traffic team dropbox globally distributed edge network terabit traffic million request per second managed small team mountain view ca
8,Lobsters,scaling,Scaling and architecture,What on Earth Is Static Regeneration?,https://jessesibley.com/what-on-earth-is-static-regeneration,earth static regeneration,vercel nextjs stable incremental static regeneration static generation incremental static generation get better register new static page demo explanation big win jamstack nextjs,vercel announced nextjs today gave u stable incremental static regeneration earth familiar static generation process dynamically generating page build time example ecommerce website might generate static html page product catalog oppose traditional approach using server load product information databasethis nice static html file fast serve additionally static html easily served global cdn resulting quick page load wherever worldthis technique also cause le load database server making much cheaper optionthe major drawback traditional static generation however small change made single page entire site must rebuilt redeployed site thousand article product could take long timeincremental static generationto address issue nextjs contributor working giving static page ability revalidated runtimein practise work revalidate option getstaticprops method nextjs page revalidate option specified instruct page regenerated given intervalwhen user visit date page served stale version traffic site cause page rerendered background meaning next user visit page see latest date versionit get better instead define page build time incrementally rebuilding runtime register new static page runtimethis like automatic serverside rendering rendered page served statically also updated demandhere demo explanation nt believe really crazy yearbig winsin opinion approach static generation defining moment jamstack could make far adoptable beforeyou ca nt beat speed pure htmlif build complex dynamic site simplicity tool like nextjs use anything else
9,Lobsters,scaling,Scaling and architecture,Introduction to architecting systems for scale (2011),https://lethain.com//introduction-to-architecting-systems-for-scale/,introduction architecting system scale,working pain growing product yahoo digg modwsgi rabbitmq redis load balancing smart client hardware load balancer citrix netscaler software load balancer haproxy caching memcache postgresql cassandra application v database caching least recently used caching algorithm inmemory cache memcached redis order magnitude least recently used content distribution network nginx cache invalidation solr offline processing message queue rabbitmq scobleizer scheduling periodic task cron puppet mapreduce mapreduce hadoop hive hbase platform layer,computer science software development program attempt teach building block scalable system instead system architecture usually picked job working pain growing product working engineer already learned suffering process post attempt document scalability architecture lesson learned working system yahoo digg attempted maintain color convention diagram green external request external client http request browser etc blue code running container django app running modwsgi python script listening rabbitmq etc red piece infrastructure mysql redis rabbitmq etc load balancing ideal system increase capacity linearly adding hardware system one machine add another capacity would double three add another capacity would increase let call horizontal scalability failure side ideal system nt disrupted loss server losing server simply decrease system capacity amount increased overall capacity added let call redundancy horizontal scalability redundancy usually achieved via load balancing article wo nt address vertical scalability usually undesirable property large system inevitably point becomes cheaper add capacity form additional machine rather additional resource one machine redundancy vertical scaling odds oneanother load balancing process spreading request across multiple resource according metric random roundrobin random weighting machine capacity etc current status available request responding elevated error rate etc load need balanced user request web server must also balanced every stage achieve full scalability redundancy system moderately large system may balance load three layer user web server web server internal platform layer internal platform layer database number way implement load balancing smart client adding loadbalancing functionality database cache service etc client usually attractive solution developer attractive simplest solution usually seductive robust sadly alluring easy reuse tragically developer lean towards smart client developer used writing software solve problem smart client software caveat mind smart client client take pool service host balance load across detects downed host avoids sending request way also detect recovered host deal adding new host etc making fun get working decently terror setup hardware load balancer high load balancing buy dedicated hardware load balancer something like citrix netscaler solve remarkable range problem hardware solution remarkably expensive also nontrivial configure generally even large company substantial budget often avoid using dedicated hardware loadbalancing need instead use first point contact user request infrastructure use mechanism smart client hybrid approach discussed next section loadbalancing traffic within network software load balancer want avoid pain creating smart client purchasing dedicated hardware excessive universe kind enough provide hybrid software loadbalancers haproxy great example approach run locally box service want loadbalance locally bound port example might platform machine accessible via database readpool database writepool haproxy manages healthchecks remove return machine pool according configuration well balancing across machine pool well system recommend starting software load balancer moving smart client hardware load balancing deliberate need caching load balancing help scale horizontally across everincreasing number server caching enable make vastly better use resource already well making otherwise unattainable product requirement feasible caching consists precalculating result eg number visit referring domain previous day pregenerating expensive index eg suggested story based user click history storing copy frequently accessed data faster backend eg memcache instead postgresql practice caching important earlier development process loadbalancing starting consistent caching strategy save time later also ensures nt optimize access pattern ca nt replicated caching mechanism access pattern performance becomes unimportant addition caching found many heavily optimized cassandra application challenge cleanly add caching ifwhen database caching strategy ca nt applied access pattern datamodel generally inconsistent cassandra cache application v database caching two primary approach caching application caching database caching system rely heavily application caching requires explicit integration application code usually check value cache retrieve value database write value cache value especially common using cache observes least recently used caching algorithm code typically look like specifically readthrough cache read value database cache missing cache key user userid userblob memcacheget key userblob none user mysqlquery select user userid userid user memcacheset key jsondumps user return user else return jsonloads userblob side coin database caching flip database going get level default configuration provide degree caching performance initial setting optimized generic usecase tweaking system access pattern generally squeeze great deal performance improvement beauty database caching application code get faster free talented dba operational engineer uncover quite bit performance without code changing whit colleague rob coli spent time recently optimizing configuration cassandra row cache succcessful extent spent week harassing u graph showing io load dropping dramatically request latency improving substantially well inmemory cache term raw encounter store entire set data memory memcached redis example inmemory cache caveat redis configured store data disk access ram order magnitude faster disk hand generally far le ram available disk space need strategy keeping hot subset data memory cache straightforward strategy least recently used employed memcache redis configured employ well lru work evicting le commonly used data preference frequently used data almost always appropriate caching strategy content distribution network particular kind cache might argue usage term find fitting come play site serving large amount static medium content distribution network cdns take burden serving static medium application server typically optimzed serving dynamic page rather static medium provide geographic distribution overall static asset load quickly le strain server new strain business expense typical cdn setup request first ask cdn piece static medium cdn serve content locally available http header used configuring cdn cache given piece content nt available cdn query server file cache locally serve requesting user configuration acting readthrough cache site nt yet large enough merit cdn ease future transition serving static medium separate subdomain eg staticexamplecom using lightweight http server like nginx cutover dns server cdn later date cache invalidation caching fantastic require maintain consistency cache source truth ie database risk truly bizarre applicaiton behavior solving problem known cache invalidation dealing single datacenter tends straightforward problem easy introduce error multiple codepaths writing database cache almost always going happen nt go writing application caching strategy already mind high level solution time value change write new value cache called writethrough cache simply delete current value cache allow readthrough cache populate later choosing read write cache depends application detail generally prefer writethrough cache reduce likelihood stampede backend database invalidation becomes meaningfully challenging scenario involving fuzzy query eg trying add application level caching infront fulltext search engine like solr modification unknown number element eg deleting object created week ago scenario consider relying fully database caching adding aggressive expiration cached data reworking application logic avoid issue eg instead delete retrieve item match criterion invalidate corresponding cache row delete row primary key explicitly offline processing system grows complex almost always necessary perform processing ca nt performed inline client request either creates unacceptable latency eg want want propagate user action across social graph need occur periodically eg want create daily rollups analytics message queue processing like perform inline request slow easiest solution create message queue example rabbitmq message queue allow web application quickly publish message queue consumer process perform processing outside scope timeline client request dividing work offline work handled consumer inline work done web application depends entirely interface exposing user generally either perform almost work consumer merely scheduling task inform user task occur offline usually polling mechanism update interface task complete example provisioning new vm slicehost follows pattern perform enough work inline make appear user task completed tie hanging end afterwards posting message twitter facebook likely follow pattern updating tweetmessage timeline updating follower timeline band simple nt feasible update follower scobleizer realtime message queue another benefit allow create separate machine pool performing offline processing rather burdening web application server allows target increase resource current performance throughput bottleneck rather uniformly increasing resource across bottleneck nonbottleneck system scheduling periodic task almost large system require daily hourly task unfortunately seems still problem waiting widely accepted solution easily support redundancy meantime probably still stuck cron could use cronjobs publish message consumer would mean cron machine responsible scheduling rather needing perform processing anyone know recognized tool solve problem seen many homebrew system nothing clean reusable sure store cronjobs puppet config machine make recovering losing machine easy would still require manual recovery likely acceptable perfect mapreduce large scale application dealing large quantity data point likely add support mapreduce probably using hadoop maybe hive hbase adding mapreduce layer make possible perform data andor processing intensive operation reasonable amount time might use calculating suggested user social graph generating analytics report sufficiently small system often get away adhoc query sql database approach may scale trivially quantity data stored writeload requires sharding database usually require dedicated slave purpose performing query point maybe rather use system designed analyzing large quantity data rather fighting database platform layer application start web application communicating directly database approach tends sufficient application compelling reason adding platform layer web application communicate platform layer turn communicates database first separating platform web application allow scale piece independently add new api add platform server without adding unnecessary capacity web application tier generally specializing server role open additional level configuration optimization nt available general purpose machine database machine usually high io load benefit solidstate drive wellconfigured application server probably nt reading disk normal operation might benefit cpu second adding platform layer way reuse infrastructure multiple product interface web application api iphone app etc without writing much redundant boilerplate code dealing cache database etc third sometimes underappreciated aspect platform layer make easier scale organization best platform expose crisp productagnostic interface mask implementation detail done well allows multiple independent team develop utilizing platform capability well another team implementingoptimizing platform intended go moderate detail handling multiple datacenters topic truly deserves post mention cache invalidation data replicationconsistency become rather interesting problem stage sure made controversial statement post hope dear reader argue learn bit thanks reading
10,Lobsters,scaling,Scaling and architecture,Graceful Degradation,https://www.solipsys.co.uk/new/GracefulDegradation.html?tg14lb,graceful degradation,graceful degradation software contact comment,graceful degradation first learned graceful degradation colleague prefaced story saying good people learn mistake best people learn people mistake bit like saying aviation circle good landing one walk away excellent landing use plane digress told learned graceful degradation message clear telling mistake could learn wish people willing make mistake public software industry would certainly benefit could learn everyone mistake story go broad outline forget detail like made comfortable living programming electronic till time programmed assembly language best output could hope debugging see whether opened till difficult work simulator higher level language debugger singlesteppers help really wrote code assembler transferred machine tested see worked sometimes colleague amazing work could get contract time wanted money good worked six month year whatever wanted rest time except christmas one biggest mistake would come back haunt large department store major capital city used till ran program one sophisticated system around till would keep list item sold central computer would interrogate till turn find much sold item would keep track many remained stock would project reordering would necessary reordering automatic staff would alerted stock low item decision could made loved system buffer stock could reduced reducing store overhead making entire store responsive consumer demand except christmas see every christmas system essence would simply stop entire floor till would stop responding refusing accept purchase unpredictable length time suddenly start working apparent reason apparent rhyme nothing could except call colleague get come fix poblem really nothing could either even worked problem problem capacity till would keep list item sold asked would dump list central computer would go back patronising servicing customer requirement asked problem heaviest time year simply much download queue till would fill central computer got back would stop wait download data could restart entire floor till would grind halt waiting download data restart floor would simply stop good busiest shopping time year really good patch run christmas make till run slightly slow still occasional full stopping moment general system would limp along exhibit catastrophic failure entire floor stopping difficulty judging exactly slow make till run also made central computer contact till clever pseudorandom way meant one till stopped floor others would probably still working done well apart faster communication system could deal load placed till could notice queue full start slowing performance system whole would degrade gracefully sale would slow slightly evenly across store come match processing throughput course would better system simply faster overloaded degrading gracefully usually better option simply stopping think next time worry system capacity better halt clear backlog degrade gracefully continue serve customer comment decided longer include comment directly via disqus system instead delighted get email people wish make comment engage discussion comment integrated page appropriate number emailscomments get large handle might return semiautomated system looking increasingly unlikely
11,Lobsters,scaling,Scaling and architecture,The architecture of nginx (2012),http://www.aosabook.org/en/nginx.html,architecture nginx,high concurrency important nt apache suitable proclaimed advantage using nginx overview nginx architecture code structure section figure worker model nginx process role brief overview nginx caching nginx configuration directive nginx internals spdy experimental protocol faster web,nginx pronounced engine x free open source web server written igor sysoev russian software engineer since public launch nginx focused high performance high concurrency low memory usage additional feature top web server functionality like load balancing caching access bandwidth control ability integrate efficiently variety application helped make nginx good choice modern website architecture currently nginx second popular open source web server internet high concurrency important day internet widespread ubiquitous hard imagine nt exactly know decade ago greatly evolved simple html producing clickable text based ncsa apache web server alwayson communication medium used billion user worldwide proliferation permanently connected pc mobile device recently tablet internet landscape rapidly changing entire economy become digitally wired online service become much elaborate clear bias towards instantly available live information entertainment security aspect running online business also significantly changed accordingly website much complex generally require lot engineering effort robust scalable one biggest challenge website architect always concurrency since beginning web service level concurrency continuously growing uncommon popular website serve hundred thousand even million simultaneous user decade ago major cause concurrency slow adsl dialup connection nowadays concurrency caused combination mobile client newer application architecture typically based maintaining persistent connection allows client updated news tweet friend feed another important factor contributing increased concurrency changed behavior modern browser open four six simultaneous connection website improve page load speed illustrate problem slow client imagine simple apachebased web server produce relatively short kb web page text image merely fraction second generate retrieve page take second transmit client bandwidth kbps kb essentially web server would relatively quickly pull kb content would busy second slowly sending content client freeing connection imagine simultaneously connected client requested similar content mb additional memory allocated per client would result mb gb extra memory devoted serving client kb content reality typical web server based apache commonly allocates mb additional memory per connection regrettably ten kbps still often effective speed mobile communication although situation sending content slow client might extent improved increasing size operating system kernel socket buffer general solution problem undesirable side effect persistent connection problem handling concurrency even pronounced avoid latency associated establishing new http connection client would stay connected connected client certain amount memory allocated web server consequently handle increased workload associated growing audience hence higher level able continuously website based number efficient building block part equation hardware cpu memory disk network capacity application data storage architecture obviously important web server software client connection accepted processed thus web server able scale nonlinearly growing number simultaneous connection request per second nt apache suitable apache web server software still largely dominates internet today root beginning originally architecture matched thenexisting operating system hardware also state internet website typically standalone physical server running single instance apache beginning obvious standalone web server model could easily replicated satisfy need growing web service although apache provided solid foundation future development architected spawn copy new connection suitable nonlinear scalability website eventually apache became general purpose web server focusing many different feature variety thirdparty extension universal applicability practically kind web application development however nothing come without price downside rich universal combination tool single piece software le scalability increased cpu memory usage per connection thus server hardware operating system network resource ceased major constraint website growth web developer worldwide started look around efficient mean running web server around ten year ago daniel kegel prominent software engineer proclaimed time web server handle ten thousand client simultaneously predicted call internet cloud service kegel manifest spurred number attempt solve problem web server optimization handle large number client time nginx turned one successful one aimed solving problem simultaneous connection nginx written different architecture much suitable nonlinear scalability number simultaneous connection request per second nginx eventbased follow apache style spawning new process thread web page request end result even load increase memory cpu usage remain manageable nginx deliver ten thousand concurrent connection server typical hardware first version nginx released meant deployed alongside apache static content like html cs javascript image handled nginx offload concurrency latency processing apachebased application server course development nginx added integration application use fastcgi uswgi scgi protocol distributed memory object caching system like memcached useful functionality like reverse proxy load balancing caching added well additional feature shaped nginx efficient combination tool build scalable web infrastructure upon february apache branch released public although latest release apache added new multiprocessing core module new proxy module aimed enhancing scalability performance soon tell performance concurrency resource utilization par better pure eventdriven web server would nice see apache application server scale better new version though could potentially alleviate bottleneck backend side still often remain unsolved typical nginxplusapache web configuration advantage using nginx handling high concurrency high performance efficiency always key benefit deploying nginx however even interesting benefit last year web architect embraced idea decoupling separating application infrastructure web server however would previously exist form lamp linux apache mysql php python perl based website might become merely lempbased one e standing engine x often exercise pushing web server edge infrastructure integrating revamped set application database tool around different way nginx well suited provides key feature necessary conveniently offload concurrency latency processing ssl secure socket layer static content compression caching connection request throttling even http medium streaming application layer much efficient edge web server layer also allows integrating directly memcachedredis nosql solution boost performance serving large number concurrent user recent flavor development kit programming language gaining wide use company changing application development deployment habit nginx become one important component changing paradigm already helped many company start develop web service quickly within budget first line nginx written released public twoclause bsd license number nginx user growing ever since contributing idea submitting bug report suggestion observation immensely helpful beneficial entire community nginx codebase original written entirely scratch c programming language nginx ported many architecture operating system including linux freebsd solaris mac o x aix microsoft window nginx library standard module use much beyond system c library except zlib pcre openssl optionally excluded build needed potential license conflict word window version nginx nginx work window environment window version nginx like proofofconcept rather fully functional port certain limitation nginx window kernel architecture interact well time known issue nginx version window include much lower number concurrent connection decreased performance caching bandwidth policing future version nginx window match mainstream functionality closely overview nginx architecture traditional process threadbased model handling concurrent connection involve handling connection separate process thread blocking network inputoutput operation depending application inefficient term memory cpu consumption spawning separate process thread requires preparation new runtime environment including allocation heap stack memory creation new execution context additional cpu time also spent creating item eventually lead poor performance due thread thrashing excessive context switching complication manifest older web server architecture like apache tradeoff offering rich set generally applicable feature optimized usage server resource beginning nginx meant specialized tool achieve performance density economical use server resource enabling dynamic growth website followed different model actually inspired ongoing development advanced eventbased mechanism variety operating system resulted modular eventdriven asynchronous singlethreaded nonblocking architecture became foundation nginx code nginx us multiplexing event notification heavily dedicates specific task separate process connection processed highly efficient runloop limited number singlethreaded process called worker within worker nginx handle many thousand concurrent connection request per second code structure nginx worker code includes core functional module core nginx responsible maintaining tight runloop executing appropriate section module code stage request processing module constitute presentation application layer functionality module read write network storage transform content outbound filtering apply serverside include action pas request upstream server proxying activated nginx modular architecture generally allows developer extend set web server feature without modifying nginx core nginx module come slightly different incarnation namely core module event module phase handler protocol variable handler filter upstreams load balancer time nginx nt support dynamically loaded module ie module compiled along core build stage however support loadable module abi planned future major release detailed information role different module found section handling variety action associated accepting processing managing network connection content retrieval nginx us event notification mechanism number disk io performance enhancement linux solaris bsdbased operating system like kqueue epoll event port goal provide many hint operating system possible regard obtaining timely asynchronous feedback inbound outbound traffic disk operation reading writing socket timeouts usage different method multiplexing advanced io operation heavily optimized every unixbased operating system nginx run highlevel overview nginx architecture presented figure figure diagram nginx architecture worker model previously mentioned nginx nt spawn process thread every connection instead worker process accept new request shared listen socket execute highly efficient runloop inside worker process thousand connection per worker specialized arbitration distribution connection worker nginx work done o kernel mechanism upon startup initial set listening socket created worker continuously accept read write socket processing http request response runloop complicated part nginx worker code includes comprehensive inner call relies heavily idea asynchronous task handling asynchronous operation implemented modularity event notification extensive use callback function finetuned timer overall key principle nonblocking possible situation nginx still block enough disk storage performance worker process nginx fork process thread per connection memory usage conservative extremely efficient vast majority case nginx conserve cpu cycle well ongoing createdestroy pattern process thread nginx check state network storage initialize new connection add runloop process asynchronously completion point connection deallocated removed runloop combined careful use syscalls accurate implementation supporting interface like pool slab memory allocator nginx typically achieves moderatetolow cpu usage even extreme workload nginx spawn several worker handle connection scale well across multiple core generally separate worker per core allows full utilization multicore architecture prevents thread thrashing lockup resource starvation resource controlling mechanism isolated within singlethreaded worker process model also allows scalability across physical storage device facilitates disk utilization avoids blocking disk io result server resource utilized efficiently workload shared across several worker disk use cpu load pattern number nginx worker adjusted rule somewhat basic system administrator try couple configuration workload general recommendation might following load pattern cpu instance handling lot tcpip ssl number nginx worker match number cpu core load mostly disk io instance serving different set content storage heavy number worker might one half two time number core engineer choose number worker based number individual storage unit instead though efficiency approach depends type configuration disk storage one major problem developer nginx solving upcoming version avoid blocking disk io moment enough storage performance serve disk operation generated particular worker worker may still block readingwriting disk number mechanism configuration file directive exist mitigate disk io blocking scenario notably combination option like sendfile aio typically produce lot headroom disk performance nginx installation planned based data set amount memory available nginx underlying storage architecture another problem existing worker model related limited support embedded scripting one standard nginx distribution embedding perl script supported simple explanation key problem possibility embedded script block operation exit unexpectedly type behavior would immediately lead situation worker hung affecting many thousand connection work planned make embedded scripting nginx simpler reliable suitable broader range application nginx process role nginx run several process memory single master process several worker process also couple special purpose process specifically cache loader cache manager process singlethreaded version nginx process primarily use sharedmemory mechanism interprocess communication master process run root user cache loader cache manager worker run unprivileged user master process responsible following task reading validating configuration creating binding closing socket starting terminating maintaining configured number worker process reconfiguring without service interruption controlling nonstop binary upgrade starting new binary rolling back necessary reopening log file compiling embedded perl script worker process accept handle process connection client provide reverse proxying filtering functionality almost everything else nginx capable regard monitoring behavior nginx instance system administrator keep eye worker process reflecting actual daytoday operation web server cache loader process responsible checking ondisk cache item populating nginx inmemory database cache metadata essentially cache loader prepares nginx instance work file already stored disk specially allocated directory structure traverse directory check cache content metadata update relevant entry shared memory exit everything clean ready use cache manager mostly responsible cache expiration invalidation stay memory normal nginx operation restarted master process case failure brief overview nginx caching caching nginx implemented form hierarchical data storage filesystem cache key configurable different requestspecific parameter used control get cache cache key cache metadata stored shared memory segment cache loader cache manager worker access currently inmemory caching file optimization implied operating system virtual filesystem mechanism cached response placed different file filesystem hierarchy level naming detail controlled nginx configuration directive response written cache directory structure path name file derived hash proxy url process placing content cache follows nginx read response upstream server content first written temporary file outside cache directory structure nginx finish processing request renames temporary file move cache directory temporary file directory proxying another file system file copied thus recommended keep temporary cache directory file system also quite safe delete file cache directory structure need explicitly purged thirdparty extension nginx make possible control cached content remotely work planned integrate functionality main distribution nginx configuration nginx configuration system inspired igor sysoev experience apache main insight scalable configuration system essential web server main scaling problem encountered maintaining large complicated configuration lot virtual server directory location datasets relatively big web setup nightmare done properly application level system engineer result nginx configuration designed simplify daytoday operation provide easy mean expansion web server configuration nginx configuration kept number plain text file typically reside usrlocaletcnginx etcnginx main configuration file usually called nginxconf keep uncluttered part configuration put separate file automatically included main one however noted nginx currently support apachestyle distributed configuration ie htaccess file configuration relevant nginx web server behavior reside centralized set configuration file configuration file initially read verified master process compiled readonly form nginx configuration available worker process forked master process configuration structure automatically shared usual virtual memory management mechanism nginx configuration several different context main http server upstream location also mail mail proxy block directive context never overlap instance thing putting location block main block directive also avoid unnecessary ambiguity nt anything like global web server configuration nginx configuration meant clean logical allowing user maintain complicated configuration file comprise thousand directive private conversation sysoev said location directory block global server configuration feature never liked apache reason never implemented nginx configuration syntax formatting definition follow socalled cstyle convention particular approach making configuration file already used variety open source commercial software application design cstyle configuration wellsuited nested description logical easy create read maintain liked many engineer cstyle configuration nginx also easily automated nginx directive resemble certain part apache configuration setting nginx instance quite different experience instance rewrite rule supported nginx though would require administrator manually adapt legacy apache rewrite configuration match nginx style implementation rewrite engine differs general nginx setting also provide support several original mechanism useful part lean web server configuration make sense briefly mention variable tryfiles directive somewhat unique nginx variable nginx developed provide additional evenmorepowerful mechanism control runtime configuration web server variable optimized quick evaluation internally precompiled index evaluation done demand ie value variable typically calculated cached lifetime particular request variable used different configuration directive providing additional flexibility describing conditional request processing behavior tryfiles directive initially meant gradually replace conditional configuration statement proper way designed quickly efficiently trymatch different uritocontent mapping overall tryfiles directive work well extremely efficient useful recommended reader thoroughly check tryfiles directive adopt use whenever applicable nginx internals mentioned nginx codebase consists core number module core nginx responsible providing foundation web server web mail reverse proxy functionality enables use underlying network protocol build necessary runtime environment ensures seamless interaction different module however protocol applicationspecific feature done nginx module core internally nginx process connection pipeline chain module word every operation module relevant work eg compression modifying content executing serverside includes communicating upstream application server fastcgi uwsgi protocol talking memcached couple nginx module sit somewhere core real functional module module http mail two module provide additional level abstraction core lowerlevel component module handling sequence event associated respective application layer protocol like http smtp imap implemented combination nginx core upperlevel module responsible maintaining right order call respective functional module http protocol currently implemented part http module plan separate functional module future due need support protocol like spdy see spdy experimental protocol faster web functional module divided event module phase handler output filter variable handler protocol upstreams load balancer module complement http functionality nginx though event module protocol also used mail event module provide particular osdependent event notification mechanism like kqueue epoll event module nginx us depends operating system capability build configuration protocol module allow nginx communicate http tlsssl smtp imap typical http request processing cycle look like following client sends http request nginx core chooses appropriate phase handler based configured location matching request configured load balancer pick upstream server proxying phase handler job pass output buffer first filter first filter pass output second filter second filter pass output third final response sent client nginx module invocation extremely customizable performed series callback using pointer executable function however downside may place big burden programmer would like write module must define exactly module run nginx api developer documentation improved made available alleviate example module attach configuration file read processed configuration directive location server appears main configuration initialized server ie hostport initialized server configuration merged main configuration location configuration initialized merged parent server configuration master process start exit new worker process start exit handling request filtering response header body picking initiating reinitiating request upstream server processing response upstream server finishing interaction upstream server inside worker sequence action leading runloop response generated look like following begin ngxworkerprocesscycle process event o specific mechanism epoll kqueue accept event dispatch relevant action processproxy request header body generate response content header body stream client finalize request reinitialize timer event runloop step ensures incremental generation response streaming client detailed view processing http request might look like initialize request processing process header process body call associated handler run processing phase brings u phase nginx handle http request pass number processing phase phase handler call general phase handler process request produce relevant output phase handler attached location defined configuration file phase handler typically four thing get location configuration generate appropriate response send header send body handler one argument specific structure describing request request structure lot useful information client request request method uri header http request header read nginx lookup associated virtual server configuration virtual server found request go six phase server rewrite phase location phase location rewrite phase bring request back previous phase access control phase tryfiles phase log phase attempt generate necessary content response request nginx pass request suitable content handler depending exact location configuration nginx may try socalled unconditional handler first like perl proxypass flv etc request match content handler picked one following handler exact order random index index autoindex gzipstatic static indexing module detail found nginx documentation module handle request trailing slash specialized module like autoindex nt appropriate content considered file directory disk static served static content handler directory would automatically rewrite uri trailing slash always issue http redirect content handler content passed filter filter also attached location several filter configured location filter task manipulating output produced handler order filter execution determined compile time outofthebox filter predefined thirdparty filter configured build stage existing nginx implementation filter outbound change currently mechanism write attach filter input content transformation input filtering appear future version nginx filter follow particular design pattern filter get called start working call next filter final filter chain called nginx finalizes response filter nt wait previous filter finish next filter chain start work soon input previous one available functionally much like unix pipeline turn output response generated passed client entire response upstream server received header filter body filter nginx feed header body response associated filter separately header filter consists three basic step decide whether operate response operate response call next filter body filter transform generated content example body filter include serverside includes xslt filtering image filtering instance resizing image fly charset modification gzip compression chunked encoding filter chain response passed writer along writer couple additional special purpose filter namely copy filter postpone filter copy filter responsible filling memory buffer relevant response content might stored proxy temporary directory postpone filter used subrequests subrequests important mechanism requestresponse processing subrequests also one powerful aspect nginx subrequests nginx return result different url one client originally requested web framework call internal redirect however nginx go filter perform multiple subrequests combine output single response subrequests also nested hierarchical subrequest perform subsubrequest subsubrequest initiate subsubsubrequests subrequests map file hard disk handler upstream server subrequests useful inserting additional content based data original response example ssi serverside include module us filter parse content returned document replaces include directive content specified url example making filter treat entire content document url retrieved appends new document url upstream load balancer also worth describing briefly upstreams used implement identified content handler reverse proxy proxypass handler upstream module mostly prepare request sent upstream server backend receive response upstream server call output filter upstream module exactly set callback invoked upstream server ready written read callback implementing following functionality exist crafting request buffer chain sent upstream server reinitializingresetting connection upstream server happens right creating request processing first bit upstream response saving pointer payload received upstream server aborting request happens client terminates prematurely finalizing request nginx finish reading upstream server trimming response body eg removing trailer load balancer module attach proxypass handler provide ability choose upstream server one upstream server eligible load balancer register enabling configuration file directive provides additional upstream initialization function resolve upstream name dns etc initializes connection structure decides route request update stats information currently nginx support two standard discipline load balancing upstream server roundrobin iphash upstream load balancing handling mechanism include algorithm detect failed upstream server reroute new request remaining lot additional work planned enhance functionality general work load balancer planned next version nginx mechanism distributing load across different upstream server well health check greatly improved also couple interesting module provide additional set variable use configuration file variable nginx created updated across different module two module entirely dedicated variable geo map geo module used facilitate tracking client based ip address module create arbitrary variable depend client ip address module map allows creation variable variable essentially providing ability flexible mapping hostnames runtime variable kind module may called variable handler memory allocation mechanism implemented inside single nginx worker extent inspired apache highlevel description nginx memory management would following connection necessary memory buffer dynamically allocated linked used storing manipulating header body request response freed upon connection release important note nginx try avoid copying data memory much possible data passed along pointer value calling memcpy going bit deeper response generated module retrieved content put memory buffer added buffer chain link subsequent processing work buffer chain link well buffer chain quite complicated nginx several processing scenario differ depending module type instance quite tricky manage buffer precisely implementing body filter module module operate one buffer chain link time must decide whether overwrite input buffer replace buffer newly allocated buffer insert new buffer buffer question complicate thing sometimes module receive several buffer incomplete buffer chain must operate however time nginx provides lowlevel api manipulating buffer chain actual implementation thirdparty module developer become really fluent arcane part nginx note approach memory buffer allocated entire life connection thus longlived connection extra memory kept time idle keepalive connection nginx spends byte memory possible optimization future release nginx would reuse share memory buffer longlived connection task managing memory allocation done nginx pool allocator shared memory area used accept mutex cache metadata ssl session cache information associated bandwidth policing management limit slab allocator implemented nginx manage shared memory allocation allow simultaneous safe use shared memory number locking mechanism available mutexes semaphore order organize complex data structure nginx also provides redblack tree implementation redblack tree used keep cache metadata shared memory track nonregex location definition couple task unfortunately never described consistent simple manner making job developing thirdparty extension nginx quite complicated although good document nginx internals instance produced evan document required huge reverse engineering effort implementation nginx module still black art many despite certain difficulty associated thirdparty module development nginx user community recently saw lot useful thirdparty module instance embedded lua interpreter module nginx additional module load balancing full webdav support advanced cache control interesting thirdparty work author chapter encourage support future
12,Lobsters,scaling,Scaling and architecture,"A Terrible, Horrible, No-Good, Very Bad Day at Slack",https://slack.engineering/a-terrible-horrible-no-good-very-bad-day-at-slack-dfe05b485f82,terrible horrible nogood bad day slack,hand deck published summary incident impact http consultemplate figure highlevel view slack ingres loadbalancing architecture haproxy runtime api integrate figure set webapp backends managed single slack haproxy server server template figure slot haproxy process excess webapp instance receiving traffic figure haproxy state grown stale time reference mainly deprovisioned host envoy proxy,story describes technical detail problem caused slack downtime may learn process behind incident response outage read ryan katkov post hand deck may slack first significant outage long time published summary incident shortly story interesting one like go detail technical issue around uservisible outage began pacific time story really begin around morning database reliability engineering team alerted significant load increase part database infrastructure time traffic team received alert failing api request increased load database due rollout configuration change triggered longstanding performance bug change quickly pinpointed rolled back feature flag performed percentagebased rollout fast process customer impact lasted three minute user still able send message successfully throughout brief morning incident one incident effect significant scaleup main webapp tier ceo stewart butterfield written impact lockdown stayathome order slack usage result pandemic running significantly higher number instance webapp tier longago day february autoscale quickly worker become saturated happened worker waiting much longer database request complete leading higher utilization increased instance count incident ending highest number webapp host ever run date everything seemed fine next eight hour alerted serving http error normal spun new incident response channel oncall engineer webapp tier manually scaled webapp fleet initial mitigation unusually help quickly noticed subset webapp fleet heavy load rest webapp instance multiple strand investigation began looking webapp performance loadbalancer tier minute later identified problem use fleet haproxy instance behind layer loadbalancer distribute request webapp tier use consul service discovery consultemplate render list healthy webapp backends haproxy route request figure highlevel view slack ingres loadbalancing architecture render webapp host list directly haproxy configuration file however reason updating host list via configuration file requires reloading haproxy process reloading haproxy involves creating brandnew haproxy process keeping old one around finished dealing inflight request frequent reloads could lead many running haproxy process poor performance constraint tension goal autoscaling webapp tier get new instance service quickly possible therefore use haproxy runtime api manipulate haproxy server state without reload time web tier backend come go service worth noting haproxy integrate consul dns interface add lag due dns ttl limit ability use consul tag managing large dns response often seems lead hitting painful edgecases bug figure set webapp backends managed single slack haproxy server define haproxy server template haproxy state effectively slot webapp backends occupy new webapp instance provisioned old one becomes unhealthy consul service catalog updated consultemplate render new version host list separate program developed slack haproxyserverstatemanagement read host list us haproxy runtime api update haproxy state run parallel pool haproxy instance webapp instance pool aws availability zone haproxy configured n slot webapp backends az giving total n backends routed across az month ago total ample headroom never needed run anything even approaching number instance webapp tier however morning database incident running slightly n instance webapp think haproxy slot giant game musical chair webapp instance left without seat problem enough serving capacity figure slot haproxy process excess webapp instance receiving traffic however course day problem developed program synced host list generated consul template haproxy server state bug always attempted find slot new webapp instance freed slot taken old webapp instance longer running program began fail exit early unable find empty slot meaning running haproxy instance getting state updated day passed webapp autoscaling group scaled list backends haproxy state became stale pacific haproxy instance able send request set webapp backends since morning set older webapp backends minority fleet regularly provision new haproxy instance would fresh haproxy instance correct configuration eight hour old therefore stuck full stale backend state outage eventually triggered end business day u begin scale webapp tier traffic drop autoscaling preferentially terminate older instance meant longer enough older webapp instance remaining haproxy server state serve demand figure haproxy state grown stale time reference mainly deprovisioned host knew cause failure resolved quickly rolling restart haproxy fleet incident mitigated first question asked monitoring catch problem alerting place precise situation unfortunately working intendedthe broken monitoring noticed partly system worked long time require change wider haproxy deployment part also relatively static low rate change fewer engineer interacting monitoring alerting infrastructure reason significant work haproxy stack moving towards envoy proxy ingres loadbalancing recently moved websockets traffic onto envoy haproxy served u well reliably many year also operational sharp edge exactly kind highlighted incident complex pipeline use manipulate haproxy server state replaced envoy native integration xds control plane endpoint discovery recent version haproxy since release also solve many operational pain point however envoy proxy choice internal service mesh project time make move envoy ingres loadbalancing attractive initial testing envoy xds scale exciting migration improve performance availability going forward new loadbalancing service discovery architecture susceptible problem caused outage strive keep slack available reliable case failed know slack critical tool user aim learn much every incident whether customer visible apologize inconvenience outage caused continue use lesson learned drive improvement system process
14,Lobsters,scaling,Scaling and architecture,What is a High Traffic Website?,https://theartofmachinery.com/2020/04/21/what_is_high_traffic.html,high traffic website,scalability v performance internal cache versus external cache server response time webpage tend expand complexity looking single worker request arrive random balance nicely want keep response time control target usage theoretical total capacity three traffic level going level scaling scaling problem thing scale boring relational database many major website much different,term like hazardous designing online service salesperson business analyst engineer different perspective mean talking say highstakes online poker room business side low compared technical side however people meeting room together making decision using word mean different thing obvious lead bad sometimes expensive choice lot day job talking business stakeholder figuring technical solution need problem deal got purely technical way think traffic level online service scalability v performance first clear two concept come lot online service design online service performance well usually fast system handle single request unit work scalability volume size work handled online service scalability usually number user request handled within timeframe batch job typically care size dataset process sometimes want system capacity grow shrink based demand sometimes care long handle full range workload expect scalability performance often get confused commonly work together example suppose online service using slow algorithm improve perfomance replacing algorithm another job le work primarily performance gain long new algorithm use memory something able handle request time counterexample discussed internal cache versus external cache server using external cache server like redis mean app make network call cache lookup performance overhead hand app replicated across multiple machine shared external cache effective perapp inmemory cache external cache scalable response time designing system helpful start thinking latency response time requirement even make revise later adding ram cache machine disk solve lot problem latency problem tend fundamental system design example suppose designing online game want latency user straight away speed light limit mean one central server supporting global game regardless whatever algorithm hardware throw problem another reason useful focus server response time practice simple singlefunction website mortgage calculator response time estimated based technical thing like hardware spec code quality typical online service built online service industry tends emphasise adding feature le development cost mean webpage tend expand complexity using easiest code possible get optimised become slow user churn even typical mortgage calculator site end bloated advertising tracking functionality response time website day job depends mostly budget priority technical factor regardless whether ecommerce site cuttingedge data application looking single worker okay imagine simple web app get one request hour take process ignoring static asset bottleneck app obvious performance problem many user give scaling problem server practically never hit capacity limit drop request even traffic rise performance problem bottleneck take priority hypothetical scalability problem simple insight take let say target per request simple web app process request one time serially ie scaling second day calculation say handle request per day scaling problem course real world simple first slower faster request request arrive random balance nicely come cluster fill queue backlog cause large spike response time handy rule say want keep response time control target usage theoretical total capacity diurnal variation local website get nearly traffic business hour third day even global website easily time traffic peak trough population distributed evenly around world lot internet user live east asia northsouth america huge pacific ocean actual ratio depends many factor hard predict even easily get exact capacity estimate simple model justification splitting website three traffic level three traffic level first level site get well dynamic request day website level lot stay way totally useful successful also pose complex technical challenge performance functionality lot work site size true scalability problem opposed problem solved purely improving performance website get bigger get level roughly around dynamic request day scalability problem start appear often site least bit scalable thanks eg async multithreaded programming web developer scaling site level keep discovering new surprise pain point thing smaller site get away start turning real problem level biggest challenge actually sociotechnical team build manages site needing learn think new way next level leaving dynamic request day boundary behind think site level high traffic let stress technical line value judgment ego statement biggest website order magnitude bigger useful website smaller line matter simply run site scale without treating like high traffic site get away low traffic level fumble growing pain level high traffic level work differently coincidentally around traffic level make sense talk request per second request per day way focus much exact traffic level rough honestly picked convenient round number happen reasonable typical website real value depend target response time factor course trying explain level exist exist expect trying grow online service going level happens site get even bigger problem one set bottleneck fixed site scale hit new set bottleneck either application changed significantly large increase traffic example application server scalable next scaling bottleneck could database read followed database writes however basic idea applied different part system working hightraffic site lot le working plain hightraffic site simply major problem need solved get high traffic level first place scaling scaling problem developer try make online service scalable long scalability problem horizon usually adding exotic database broker server something particular startup founder often especially concerned technical asset might scale meet business ambition understandable dangerous trap couple reason one paul classic thing scale essay applies technology stack beat bigger company scale competitive advantage choose solve scalability problem bigger company forced every step take make smaller company agile startup worry much scalability big enterprise without big back problem premature scalability solution easily backfire real scalability problem test solution hard sure correctly solving real problem fact rapidly growing service tend change requirement rapidly risk scalability turning technical debt high keep trying add scalability part system already scalable enough chance next scaling bottleneck appear somewhere else anyway architecture err side simple easier scale long run architecture complex concrete personally think lowtraffic online service worked implemented cleanly enough using simple monolithic web app whatever popular language front boring relational database maybe search index like xapian elasticsearch many major website much different valid architecture triedandtested one said sometimes lowtraffic site need thing sometimes sold scalability solution example replicating app behind load balancer help deploy whenever want without downtime one fintech service worked split credit card code service making pci ds compliance simpler case clear problem scalability solved avoids overengineering often wish systematic way figure technical requirement online service head real world complicated messy sometimes practical way sure experiment however every piece software start idea think scalability online service idea early design phase
15,Lobsters,scaling,Scaling and architecture,Guilds Road to Event-Driven Architecture,https://medium.com/extra-credit-by-guild/get-on-the-bus-adec3a826d58,guild road eventdriven architecture,get bus guild road eventdriven architecture rich haase johnny coster direct invocation api gateway lambda kinesis json schema unix philosophy,get bus guild road eventdriven architecture technology landscape started like lot startup monolith time took proverbial scalpel carved chunk monolith creating standalone service domain like academic program catalog student application process employee registration etc also built mvp system lessthanideal way service short delivery timeline often ended needing move system onto different infrastructure codebases etcas built initial mvp engineering org matured often ran problem two system direct integration went change replace one system also number discrete service guild ecosystem grew kept running problem communicating one system many system direct integration model would mean n n eg n service direct integration service send message service clearly way livewhat tangled web weavedenter eventdriven architecture panacea thing communicating event happen different system centralized event bus architecture team landed would allow u flexible scale communication much effectively architecture course unique guild testing various architectural concept like event stream event consumer would used eventdriven world furthermore already stood web service leveraged technology would become bedrock event bussample diagram usertriggered event flow onto event stream available consumer listenersworking input many engineer across number team one principal engineer rich haase spearheaded bringing event bus life created engineering requirement document put stake ground would build hand another engineer johnny coster began work would mvpover course two month built lean nofrills system one thing well validate published event send event stream provided three main feature interface publishing event supported http direct invocationa event schema registry event validation systemevent stream allowed multiple consumer event oneeventtomanystreams mapping configuration meaning event published event bus would sent stream stream b stream c ruthlessly focused building something simple short time frame also knew system would relied core piece infrastructure many year come needed able scale business technology usage guild active student number continue increase system rely event bus communicate information consumer guild ecosystem end relied aws technology team already experience production built scale api gateway http interface lambda running service code kinesis event stream service managed aws provides layer abstraction mean really deal problem like many compute resource event bus using much network traffic load hitting event bus streamlined let take look system work perspective userverification event published event bus event important one guild ecosystem allows user confirm identity based information received employer effect first touch point guild platform user identity trusted many different system start taking action set data feature support user interaction guild platformfirst system publishes event via http post request event bus api gateway url api gateway receives request pass payload request transparently event bus lambda function alternatively system skip api gateway step invoke lambda directly important piece configuration determine happens next based name event optionally version schema chosen validate shape content event example configuration mapping event example event bus config connects event name schema event stream destinationsto validate event sent event bus rely json schema standard number schema definition technology considered json schema accessible wide range engineer happened already json schema definition use production broke fix snippet user verification event schema definition used validate corresponding event example json schema definition user verification eventvalidation schema ensures event json necessary field value expected type length validation process important downstream event consumer programmed defensively trust shape content event consume expectedif event pas validation detailed error part invalid sent back publisher assuming pas validation final step put event kinesis stream specified configuration file shown case user verification event production environment event put kinesis stream called userprofileserviceprod certainly detail system glossed really get gist search making reliable core piece infrastructure able use many year engineered simple spirit unix philosophy one thing well recent month new feature added event bus sure update also hope share used event bus communicate system well pattern adopted event consumption lookout update extra credit topic
16,Lobsters,scaling,Scaling and architecture,Plugin Architecture in Python (aka Py3EE),https://www.dsouzaman.net/python_plugin_architecture.html,plugin architecture python aka,plugin architecture python aka edward dsouza section example problem basic solution enterprise solution highlevel policy enterprise implementation,plugin architecture python aka edward dsouza june came python java reveled terseness flexiblity however reading clean architecture robert c martin seeing enterprisey idea java actually quite useful artcle look dependency direction interface used create plugin architecture python toy example might look overengineered let u explore idea would valuable realistic context multiple people working together example many concept taken clean architecture section example problem example problem use writing encrypter program take character stdin encrypt using translation table write output stdout first easy part functional datamanipulation component simply translates character encrypted form using shift cipher def translate char str shift letter stringasciilowercase char letter return letter lettersindex char shift len letter return char basic solution straightforward solution full problem probably would write default faced requirement import sys translate import translate def encrypt data sysstdinreadlines line data char line print translate char encrypt code beautiful terseness potential issue main problem mixing together several responsibilies getting character translation outputting character multiple people wanted modify part time would mess getting everyone work together tight space highlevel policy also nt depending directly lowlevel detail like print function one strategy use fix problem make code work generic interface specific implementation interface plugged policy create working system policy decoupled lowlevel detail code organized input output concern nicely separated enterprise solution welcome highlevel policy import abc translate import translate class charreader abcabc abcabstractmethod def readchar self pas class charwriter abcabc abcabstractmethod def writechar self char str pas def encrypter reader charreader writer charwriter def encrypt true try char readerreadchar except stopiteration break encryptedchar translate char writerwritechar encryptedchar return encrypt clearly explicitly spelled highlevel policy reading character encrypting translate function writing encryped character algorithm know care stdin module print function remains open reused different context different input source output channel enterprise implementation admittedly like lot enterprisey code policy fairly verbose nt actually anything make useful need implement charreader charwriter interface plug policy function import sys enterprise import charreader charwriter encrypter class mycharreader charreader def init self def getcharacters data sysstdinreadlines line data char line yield char selfiter getcharacters def readchar self return next selfiter class mycharwriter charwriter def writechar self char str print char myencrypter encrypter mycharreader mycharwriter myencrypter cool messiness converting stdin stream character contained within mycharreader one else burdened
17,Lobsters,scaling,Scaling and architecture,Scaling to 100k Users,https://alexpareto.com/scalability/systems/2020/02/03/scaling-100k.html,scaling user,user machine user split database layer user split client splitting user add load balancer user cdn user scaling data layer caching read replica beyond reference favorite post high scalability translation chinese korean,many startup feel like legion new user signing account every day engineering team scrambling keep thing running good problem information take web app hundred thousand user scarce usually solution come either massive fire popping identifying bottleneck often time said noticed many main pattern taking side project something highly scalable relatively formulaic attempt distill basic around formula writing going take new photo sharing website graminsta user user machine nearly every application website mobile app three key component api database client usually app website database store persistent data api serf request around data client render data user found modern application development thinking client completely separate entity api make much easier reason scaling application first start building application alright three thing run one server way resembles development environment one engineer run database api client computer theory could deploy cloud single digitalocean droplet aws instance like said expect graminsta used person almost always make sense split database layer user split database layer splitting database managed service like amazon rds digital ocean managed database serve u well long time slightly expensive selfhosting single machine instance service get lot easy add ons box come handy line multiregion redundancy read replica automated backup graminsta system look like user split client lucky u first user love graminsta traffic starting get steady time split client one thing note splitting entity key aspect building scalable application one part system get traffic split handle scaling service based specific traffic pattern like think client separate api make easy reason building multiple platform web mobile web io android desktop apps third party service etc client consuming api vein biggest feedback getting user want graminsta phone going launch mobile app system look like user add load balancer thing picking graminsta user uploading photo left right starting get sign ups lonely api instance trouble keeping traffic need compute power load balancer powerful key idea place load balancer front api route traffic instance service allows horizontal scaling increasing amount request handle adding server running code going place separate load balancer front web client api mean multiple instance running api web client code load balancer route request whichever instance least traffic also get redundancy one instance go maybe get overloaded crash instance still respond incoming request instead whole system going load balancer also enables autoscaling set load balancer increase number instance superbowl everyone online decrease number instance user asleep load balancer api layer scale practically infinity keep adding instance get request side note point far similar paas company like heroku aws elastic beanstalk provide box popular heroku put database separate host manages load balancer autoscaling let host web client separately api great reason use service like heroku project early stage startup necessary basic come box user cdn probably done beginning moving fast graminsta serving uploading image starting put way much load server using cloud storage service host static content point think image video aws digital ocean space general api avoid handling thing like serving image image uploads thing get cloud storage service cdn aws addon called cloudfront many cloud storage service offer box cdn automatically cache image different data center throughout world main data center may hosted ohio someone request image japan cloud provider make copy store data center japan next person request image japan recieve much faster important need serve larger file size like image video take long time load send across world user scaling data layer cdn helped u lot thing booming graminsta youtube celebrity mavid mobrick signed posted u story api cpu memory usage low across board thanks load balancer adding api instance environment starting get lot timeouts everything taking long digging see database cpu hovering maxed scaling data layer probably trickiest part equation api server serving stateless request merely add instance true database system case going explore popular relational database system postgresql mysql etc caching one easiest way get database introducing new component system cache layer common way implement cache using inmemory key value store like redis memcached cloud managed version service elasticache aws memorystore google cloud cache come handy service making lot repeated call database information essentially hit database save information cache never touch database example graminsta every time someone go mavid mobrick profile page api layer request mavid mobrick profile information database happening since mavid mobrick profile information changing every request info great candidate cache cache result database redis key user id expiration time second someone go mavid mobrick profile check redis first serve data straight redis exists despite mavid mobrick popular site requesting profile put hardly load database plus cache service scale easier database redis built redis cluster mode similar way load balancer let u distribute redis cache across multiple machine thousand one plea nearly highly scaled application take ample advantage caching absolutely integral part making fast api better query performant code part equation without cache none sufficient scale million user read replica thing database started get hit quite bit add read replica using database management system managed service done oneclick read replica stay date master db able used select statement system beyond app continues scale going want focus splitting service scaled independently example start make use websockets would make sense pull websocket handling code put new instaces behind load balancer scale based many websocket connection opened closed independently many http request coming also going continue bump limitation data layer going want start looking partitioning sharding database require overhead effectively allow data layer scale infinitely want make sure monitoring installed using service like new relic datadog ensure understand request slow improvement need made scale want focused finding bottleneck fixing often taking advantage idea previous section hopefully point people team help well reference post inspired one favorite post high scalability wanted flesh article bit early stage make bit cloud agnostic definitely check interested kind thing translation reader kindly translated post chinese korean
18,Lobsters,scaling,Scaling and architecture,OAuth2 authorization patterns and microservices (2019),https://medium.com/capgemini-norway/oauth2-authorization-patterns-and-microservices-45ffc67a8541,authorization pattern microservices,authorization pattern microservices charos pix oauth road hell microservice pattern password grant type access token confidential client password grant type password grant type separation concern api gateway microservice basic access authentication network security policy password grant type authorization code grant client credential grant authorization code grant identity provider client credential grant client credential grant authorization code grant client credential grant client credential grant authorization code grant,authorization pattern microservicesattribution charos pixone topic led contention project microservice architecture use authorizationthe challenge well known described colourfully one oauth exauthors eran hammer blog post oauth road hell sum eran hammer say best clear oauth hand developer deep understanding web security likely result secure implementation however hand developer experience past two year likely produce insecure implementationsand experience well really baffled though lack knowledge experience even senior developer architect subject even book read concerning microservice architecture example used way best described creative writing read microservice pattern chris richardson blog post necessary clear misunderstanding might around use microservice architecture take following drawing based fig chris richardson bookfirstly nothing wrong technically something like fully possible question example chris richardson using involves using called password grant type concept user give username password client application us hisher credential obtain socalled access token access token used client application gain access server behalf user first issue grant type involves client asking user hisher password secure long client application exists security zone authorization server call confidential client technical term mean need client application backend clientserver application make use mtls client application api gateway without security mechanism pattern secure information security perspective second issue conceptually wrong password grant type never meant used like password grant type done correctly client make direct request authorization server obtain access token make second request access token api gateway api gateway proxy forward request access token microservice well make sure authorize client application use resource server behalf user happening example authorize api gateway using microservice make sense read post separation concern api gateway microservice understand security point might well use basic access authentication use network security policy servicetoservice communication paas caas provider simple way configuring thisso use password grant type use two grant type look authorization code grant client credential grant one use depends context requirement customeruse authorization code grant providing api client application given customer single signon solution want use dealing multiple organization make sure use type identity provider integration customer use different type identity provider quickly turn become project itselfuse client credential grant service service communication dealing multiple organization reaching consensus among regard agreeing common solution providing identity authorization far fetched considering time resource available always migrate client credential grant authorization code grant later advice start setting client credential grant first consequence though fex resource server completely unaware client application something called user client credential grant care client application care right user given client application mean resource server trust client application given proper consent accessing resource resource server still since authorization code grant alone solve challenge related information security fineanswering use way secure information security perspective require separate article perhaps series article perhaps come back next post
19,Lobsters,scaling,Scaling and architecture,Evolution of the precise code intel backend,https://about.sourcegraph.com/blog/evolution-of-the-precise-code-intel-backend,evolution precise code intel backend,sourcegraphcom private instance sourcegraph sourcegraph browser extension motivation srclib srclib lsif mvp express sourcegraph extension optimizing query sqlite dgraph badgerdb change processing uploads asynchronously noderesque resque ipc pr storing crossrepository data postgresql dblink eventually reverted queue bull redis bull search within job payload text matching query queue postgresql adding graphql resolvers sourcegraphgo sourcegraphtypescript campaign code insight diagnostics introducing multiple worker container prefork sourcegraphcom headofline blocking introducing bundle manager rewriting go strangler fig optimizing code intelligence backend removing lsifserver looking forward,jumping definition symbol cursor finding reference two basic mental mechanic software engineering fast code navigation accelerates rate build mental model code available likely use hundred thousand time per day code navigation core sourcegraph help understand part universe code relevant important code navigation also present difficult technical challenge especially want provide code navigation outside ide variety application developer trying understand code webbased code search engine like sourcegraphcom private instance sourcegraph code host like github gitlab bitbucket phabricator sourcegraph browser extension order provide compileraccurate code navigation ides work lot magic behind scene involves static analysis incremental compilation build execution lot lot caching much assumes read write exec permission local filesystem sourcegraph provide precise code navigation user within millisecond without access answer language server index format lsif elsif post share technical journey lsif including chose adopt foundation precise code navigation challenge faced scaling lsifbased backend see thing going motivation journey lsif began first version sourcegraph precise code navigation firstorder concern even early version application time code navigation feature available editor certain language editor depending whether plugin added support specific language specific editor enable code navigation webbased interface eye toward enabling across language editor sourcegraph created srclib first opensource crosslanguage code analysis toolchain indexing format project like language server protocol kythe still year away released srclib worked quite well early version sourcegraph however adding srclib indexer every language turned quite undertaking many product feature demanded time attention support new language slow develop meantime language server protocol emerged new open protocol providing code navigation across many editor lsp piggybacked growing traction v code many implementation lsp emerged major language growing ecosystem sourcegraph saw opportunity take advantage also give back emerging opensource community dedicated making code intelligence ubiquitous every language contributed language server implementation go typescript javascript python java incorporated language server application architecture could deployed sourcegraph provide precise code intelligence many language able provide srclib limited developerhours language server served user well number year eventually amount code sourcegraphcom grew large organization began adopt sourcegraph big internal codebases scaling performance became issue fast forward early largest customer began regularly reporting language server outage related high usage volume large codebase size began looking way improve performance scale started thinking combine richness lsp ecosystem performance indexingbased approach like srclib february surprise delight dirk bumer one creator lsp announced language server index format lsif code intelligence indexing format similar spirit lsp lsif lsif provides crosslanguage serialization format describes data needed quickly resolve action like gotodefinition findreferences raw lsif data json look like id type vertex label definitionresult id type edge label textdocumentdefinition outv inv id type edge label item outv invs document id type vertex label packageinformation name githubcomsourcegraphsourcegraph manager gomod version sourcegraph accepts useruploaded lsif data generated lsif indexer implementation local checkout code common scenario step repository ci pipeline sourcegraph receives data transforms internal format optimized scale query speed sourcegraph us data power local repository crossrepository code navigation action like gotodefintion findreferences today sourcegraph lsif backend multiple component handle aspect uploading parsing transforming reading writing manipulating lsif data serve user request story system grew evolved time story span commits line code mvp lsif backend began life simple express server written typescript proxied outside world endpoint sourcegraph frontend api server accepted lsif uploads wrote directly disk query request server would read lsif data current repository disk memory parse structured representation walk graph vertex edge construct appropriate response client side wrote simple sourcegraph extension query lsifserver api chose typescript express fastest way get something running given experience team primary goal solicit feedback user ergonomics upload api user would configure ci pipeline generate lsif basic user experience code navigation backed lsif performance scalability robustness code quality primary concern optimizing query serve query mvp implementation would simply read raw lsif data disk memory parse structured representation walk graph compute query result lsif index however large larger size codebase given revision larger codebases added significant latency serving user request would often also call oom crash needed make following operation fast get definition identifier given source position get reference identifier given source position get hover text identifier given source position order decided store lsif data different format optimized operation several design discussion reduced choice two option sqlite embedded filebased sql database engine dgraph distributed graph database built top lsmtreebased database badgerdb built two version backend benchmark showed upload performance sqlite dgraph backends proportional input size sqlite factor dgraph factor relatively inexperienced dgraph relatively slow performance could explained lack operational experience bad choice graph schema dgraph backend implementation found time might experimented dgraph decided go sqlite based higher initial performance familiarity using past fact would easier deploy operationally many deployment environment customer change query needed read disk document containing target source range instead lsif dump entire repository processing uploads asynchronously mvp lsif upload process synchronous acceptable mvp reading lsif data network writing directly disk adopting sqlite backend store precise code navigation added additional transformation step lsif upload process rather simply read lsif data network write directly disk parse data convert sqlite bundle increased lsif upload response time significantly began bumping timeouts enforced various client server proxy within around sourcegraph also started see frequent oom error lsifserver process larger uploads data conversion process also increased memory usage process handled uploads query uploading large file multiple small one parallel could ooming process also take code navigation query address issue decided separate work converting lsif sqlite bundle separate background process lsifserver process would continue handle uploads query uploads handler would similar mvp implementation transparently writing raw data disk rather converting synchronously lsifworker process would consume queue lsif dump process converting sqlite bundle background coordinate work lsif upload handler lsifworker process needed queue used noderesque nodejs port popular rail library resque library store job data redis already component stack also considered using postresql accessing existing postgresql instance came certain restriction due concern uptime performance sort local ipc would prevented scaling lsifserver lsifworker independently using amqp server would required introducing new major service architecture implemented splitting lsifserver lsifworker pr storing crossrepository data postgresql uploaded lsif index became single sqlite database ondisk singlerepository database would sufficient support local within repository code navigation order make code navigation experience truly seamless magical wanted support crossrepository code navigation word wanted user click reference function defined dependency arrive directly definition function source repository pretty neat enable added additional sqlite database xrepodb enabled u look repository index versioned package provided version package depended fine long one instance lsifserver lsifworker lived docker container however order support lsif across large multirepository codebases needed scale meant multiple lsifserver lsifworker instance would running different docker container perhaps different machine could longer rely share access singlewriteratatime sqlite database moved xrepodb data postgresql potential volume additional writes continued concern u lsif use grew would cause operational issue postgresql instance would affect performance unrelated part application safe kept table space lsif data disjoint prefixed table name foreign key existing table data also tried migrating lsif table second postgresql instance however required nasty trickery dblink order run migration found quite painful eventually reverted backoftheenvelope calculation suggested lsifrelated load would nt overwhelm single shared postgresql instance calculation largely held time queue bull redis saw operational issue related stuck worker lost job traced back queueing library noderesque motivated switch bull also additional feature allowed u schedule job similar spirit cron list job particular state search within job payload text matching query using redis eval command relevant pr queue postgresql adoption bull resolved issue queue implementation others remained particular problem enforcing logical transaction across data stored partially redis partially postgresql particular data could written postgresql indicating successful completion lsif bundle conversion corresponding job queue stored redis could updated within transaction sometimes got sync furthermore redis treated part sourcegraph ephemeral truncatable cache site administrator aware felt could safely wipe redis data would wreak havoc lsif processing queue address issue moved queue data redis postgresql reduced lot complexity turned custom lua script reached redis data could reduced sql query could also use postgresql transaction enforce allornothing atomicity lsifrelated update adding graphql resolvers lsifserver accessible undocumented proxy sourcegraph frontend service proxy accepted uploads served code navigation query consumer api firstparty sourcegraph extension like sourcegraphgo sourcegraphtypescript adding graphql api enabled lsif backend used part sourcegraph nascent campaign feature currently inprogress code insight also thirdparty sourcegraph extension author thirdparty api consumer functionality lsif backend continues grow recently added support diagnostics possibility user api introducing multiple worker lsifserver lsifworker still run together container easy way enable multiple worker without changing container orchestration decided prefork worker rudimentary way scale overall resource use still constrained single container isolation worker process meaning runaway memory use one starve others container worked well enough time change also helped u resolve issue lsif processing sourcegraphcom suffering headofline blocking introducing bundle manager needed enable lsifserver lsifworker instance scale horizontally time instance shared access persistent storage would store lsif data used serve query naively thought might possible simply attaching shared disk separate instance practice however issue kubernetes persistent volume default mounted readwritemany across multiple node volume plugins cephfs glusterfs nfs enable reliability performance shared access filesystems issue furthermore unsupported gcp sourcegraphcom deployed solution factor responsibility managing shared storage separate service bundle manager originally called lsifdumpmanager today known precisecodeintelbundlemanager made lsifserver lsifworker stateless freeing scale horizontally scaling bundle manager requires sharding scheme similar already used gitserver service responsible serving git data sourcegraph backend rewriting go original lsif backend written typescript easiest prototype also matched technical skillset original team time performance became consideration experience team shifted toward go particular familiar variety technique optimize program written go le wellversed technique application written typescript performance becoming firstorder consideration decided rewrite thing go initially decided adopt strangler fig model refactoring extracting first cpubound work highperformance go process would called existing worker code eventually time logic would ported go process however thing nt always go according plan knew in out typescript code simply rewrote three service go single pas resulting code nt particularly idiomatic since wanted focus bringing new system life quickly possible could sunset old one continuing refactors made code idiomatic time rewrite unlocked large number performance improvement opportunity result described optimizing code intelligence backend removing lsifserver rewriting lsif backend go lsif api server lsifserver completely stateless longer talk process bundle manager written different language taking step back realized api server process little purpose moved remained lsif api server logic server handler client used part sourcegraph external http graphql apis query lsif data dropped lsif api server looking forward journey nt stopping made significant performance improvement lsif backend continue adding feature investing optimization support massive scale lsif thing looking forward automatic zeroconfig lsif indexing repository support monorepos large repository high commit frequency creating public index opensource code connecting private code enable seamless jumping proprietary code opensource dependency keep eye update
20,Lobsters,scaling,Scaling and architecture,Kicking Node into high gear for data processing or how to hire 100 cats to catch a mouse,https://sgolem.com/kicking-node-into-high-gear-for-data-processing-or-how-to-hire-100-cats-to/,kicking node high gear data processing hire cat catch mouse,callback hell promise tokio asyncstd wait something time sequential lot time promisified concurrent per thing spawn multiple thread boost processing,recently came across problem work required light processing across hundred directory initial run nt take long figured whatever could speed would pay long run anticipated need run dozen hundred time got right asked year ago exactly node event loop work say idea today happy say still nt know work least detail meantime learned good chunk rust asyncawait internals pushed right direction understanding asyncawait faster node even though singlethreaded shortest introduction asyncawait syntax imagine remember callback diabolical mess could make okay maybe u probably significant percentage nesting callback could easily produce code hard follow sometimes hard understand even rereading people call phenomenon callback hell came promise promise clean readable code even dealing async process straightforward combine promise chain sequentially process data despite people still tended nest promise logic understandable chosen one would usually last month understanding would decay logical next step rewrite period time someone idea could write async code way write sync code asyncawait born great idea dug tokio asyncstd idea worked speed something run single thread seems code mostly nothing specific wait something time sure everybody fired http request numerous time playing node read file transformed content logged wrote result file appears thing like take considerable amount time waiting o right task network connection might slow congested drive might slow dying network destination might slow far let see happens fire lot sequential network request start request waiting something getting data end request oooo ooo oo ooooooooooooooo lot nothing going request running time mean useful work waiting oooo oo oo oo ooooooooooo oooo oooo take closer look see new data processed single request time request run faster even single thread course adding thread could result much better time could slow whole thing later might realize constantly jumping one request therefore must sort overhead honest nt know much impact either way realworld task would lot waiting data decrease overhead minimum like everything else need reason use asyncawait reading file script might well use sync worry use case async might shine hundred file read network call nodejs quite limited come make super easy run thing sequentially one one time tool workload let see kind api would suitable case use promisified function keep code clean const work filepath const content await fsreadfile filepath const json jsonparse content const url jsondetailsdocumentation const re await fetch url return resbody know need done probably safe assume filepaths array matter obtained const filepaths simplest form api think const documentationpages await pareach work filepaths might also accept option object third parameter let leave later basic shell pareach function could go something like const pareach async work allargs option todo option object contain many configurables let focus number concurrent parallel worker run const batchsize option need variable see much work done know stop code inside pareach function let callwhendone let workerspawned let nextargsindex let write part actual work const spawn const args allargs nexargsindex arrayisarray args work args else work args workersspawned nextargsindex straightforward see happening spawn function get right argument check one multiple start work update variable accordingly probably noticed function let see one look const workersspawned const allargsused nextargsindex allargslength const allworkerswork workersspawned batchsize const noworkerswork workersspawned allworkerswork allargsused spawn noworkerswork allargsused callwhendone also update workersspawned variable work done checking see good time work wrap finish everything one thing left write kickstarting let batchsize spawn return new promise resolve callwhendone resolve fire worker defined batchsize variable save resolve function unblock awaiting pareach function called already published package play check pareach npm spent lot time searching name taken one mixture parallel foreach almost sound like something per thing whatever spawn multiple thread boost processing even check speedup sequential took second concurrent took second parallelized took second produced waiting random sub second timeouts firing hundred request yield similar ratio like know something like useful please clone repo try running test see much pareach version faster sequential processing
21,Lobsters,scaling,Scaling and architecture,Spark Joy by Running Fewer Tests,https://engineering.shopify.com/blogs/engineering/spark-joy-by-running-fewer-tests,spark joy running fewer test,dynamic analysis problem test shopify solving problem test dynamic analysis rotoscope tracepoint use dynamic analysis downside running dynamic analysis ruby rail slow mapping lag behind head untraceable file backend class metaprogramming obfuscates call path failing test caught dynamic analysis rolled failure recall speed improvement compute time approach explored static analysis machine learning research paper pdf machine test benefit running fewer test skepticism dynamic analysistest selection average developer request running full test suite pull request software development shopify expression interest career posting,developer write test ensure correctness allow future change made safely however number feature grows number test test doubleedged sword one hand wellwritten one catch bug maintain program stability code base grows high number test impedes scalability take long time run increase likelihood intermittently failing test software project often require test pas merging main branch add overhead developer intermittently failing test worsen problem cause intermittently failing test timing instability database http connectionsmockings random generator test leak state test test pass every single time fails test depending order unfortunately one fully eradicate intermittently failing test likelihood occurring increase codebase grows make already slow test suite even slower retry pas implying one write test benefit quality assurance performance monitoring speeding development catching bug early instead production outweigh downside however improvement made team thus embarked journey making continuous integration ci stable faster share dynamic analysis system select test implemented followed approach explored decided test selection spark joy life wish bring joy problem test shopify test impede developer productivity test suite monolithic repository test growing size annually take min run hundred docker container parallel pull request requires test pas developer either wait test pay price context switching biannual survey build stability speed recurring topic problem clearly felt developer solving problem test abundance blog postsarticlesresearch paper optimizing code unfortunately test fact learned unrealistic optimize test sheer quantity growth also learned uncharted territory many company research progressed became apparent right solution run test related code change challenging large dynamically typed ruby codebase make ample use language flexibility furthermore difficulty exacerbated metaprogramming rail well nonruby file code base affect application behaves example yaml json javascript dynamic analysis dynamic analysis essence logging every single method call run test track file call graph call graph create test mapping every file find test file call graph looking file modified added removed renamed look test need run check rotoscope tracepoint record call graph ruby application use dynamic analysis ruby dynamically typed language retrieve dependency graph using static analysis thus know corresponding test code downside running dynamic analysis ruby rail slow computationally intensive generate call graph run every single pr instead run dynamic analysis every deployed commit mapping lag behind head generated mapping lag behind head main run asynchronously solve problem run test satisfy least one following criterion added modified current branch added modified head last generated mapping current branch head mapped test per current branch code change untraceable file nonruby file yaml json etc traced call graph added custom patch rail trace example patched backend class trace translation file yaml change file traced run every single test metaprogramming obfuscates call path added existing metaprogramming known directory added glob rule file path determine test run discourage new metaprogramming sorbet linters failing test caught generated mapping dynamic analysis get date latest main sometimes failing test get selected get merged main circumvent issue run full test suite every deploy automatically disable failing test developer blocked shipping code full test suite run asynchronously pull request get merged full test suite completes automatic disabling failing test sound counterintuitive many people observed percentage pull request failing test merged main branch also mechanism mitigate risk canary deploys type checking using sorbet code owner failing test notified expect developer fix remove failure without blocking future deploys dynamic analysis rolled experimentation phase new dynamic analysis system testselection pipeline ran parallel pipeline run full test suite new pr recall new test selection pipeline measured failing test measured new pipeline selects failing test care test pas failing test cause trouble measured result using three metric failure recall define recall percentage legitimately failing test excluding intermittently failing test system selected want close possible hard measure accurately occurrence intermittently failing test instead approximate recall looking number consistently failing test merged main two month project active commits merged failed detect five failing test landed main also managed resolve root cause missed failure problem repeat future achieved recall speed improvement overall selection rate ratio selected test total number test percentage selected test file per build build run fewer test show many developer significantly benefit test selection system percentage build selected fewer test compute time total saving compute time measured adding time spent preparing running test every docker container build averaging across build decrease significant chunk computing time still used setting container database pulling cache note also adding compute time running dynamic analysis every deployed commit main estimate undo infrastructure cost saving approach explored prior choosing dynamic analysis explored approach ultimately ruled static analysis determine dependency graph briefly explored using sorbet would possible entire code base converted strict sorbet type unfortunately code base partially strict sorbet type big team convert rest machine learning possible use machine learning find dependency graph facebook excellent research paper pdf chose dynamic analysis shopify sure enough data make prediction want choose approach deterministic reproducible machine test tried adding machine test suite unfortunately performance increase linearly scaled horizontally fact test average take longer increase number machine past certain number increasing machine reduce intermittently failing test increase possibility failing connection sidecar thus increasing test retries benefit running fewer test three major benefit selectively running test developer get faster feedback test likelihood encountering intermittently failing test decrease thus increase speed developer ci cost le skepticism dynamic analysistest selection feature rolled many developer skeptical would work frankly one many people voiced doubt privately openly however much surprise went live people silent average developer request running full test suite pull requestsif similar situation hope experience help hard developer embrace idea test run importance test ingrained head sound like kind problem enjoy solving come work u check software development shopify expression interest career posting apply specifying interest developer acceleration
22,Lobsters,scaling,Scaling and architecture,A bunch of rants about cloud-induced damage,https://rachelbythebay.com/w/2020/05/06/scale/,bunch rant cloudinduced damage,bunch rant cloudinduced damage exactly,bunch rant cloudinduced damage bunch random complaint thing gotten late cloud business seems like far many people think running virtual machine people hardware way anything nt absolve actually good stuff frequently complicates matter never truly shine say front prefer physical hardware know exactly working spooky stuff going try track problem disk slow find network odd find mysterious host system life someone else world beyond reach want satisfying answer life mystery production environment need able get bottom thing every time introduce bit virtualization reduced likelihood finding answer get even worse cross domain another company vendor whatever point drop standard accept uncertainty badness outage strange behavior live fact thing may never truly get better element right chase folk either join peace never join first place mention introducing troubleshooting singularity reasoning exist solve problem also creates great opportunity charlatan fraud general incompetence easy every time something bad happens blame something one known trouble spot people love scapegoat another fun aspect virtual machine cloud elastic scaling business incredible money sink created seems like everyone set kind rule keep eye cpu utilization helpfully stand instance time go past certain point number terrible behavior enables incredible seen believed mean ah yes come little trip memory lane worked place bunch physical machine also whole group nothing plan allocate capacity word group made sure people got machine appropriate asked also nt ignorant beancounters either real driving force behind team allocation effort badass engineer friend mine person alone probably saved company multiple building worth machine almost decade tenure work easy every time team wanted whole whack new hardware friend went took look see using stuff already nt cpu utilization either friend wanted know using cpu stupid inefficient thing would come wash ran analysis could see stuff spending time profiling thing nice effect forcing team honest forthcoming need try wasteful think hardware would save nt get spend company money friend team represented kept thing becoming freeforall cloud place though nt kind oversight build system autoscale soon dozen hundred even thousand crazy thing write bad code footprint go every day nt even realize odds team idea many machine vms reality quickly changing equivalent friend stand tell first optimize terrible code get hardware provisioning deploy system give away usually happens someone periodically run report find something mindblowing like oh wow service x run machine happen easy nobody stopped needing machine current setup people wring hand promise better still stupid instance sit burning cash day night also fun direction someone might set rule scale job fall cpu utilization maybe tell scaler lower notch every time utilization something like suppose happens first time quiet one instance one instance manages come said autoscaler lower zero right yep without minimum size limiter place get amazing apparently happen immediately fix think would notice need capacity run thing get tugofwar going night long personally think whatever measuring cpu utilization probably divided zero instance running could nt run past point dealing kind scenario seems par course whole neat situation happen talked elsewhere sake completeness give summary service dy traffic everything else drop cpu utilization drop service scale point essential service start working traffic flood back uh oh service far small ca nt handle load worse still written fragile way imagine ca nt even ignore extra traffic stupidly try take everything coming notion rejecting stuff hope handling completely saturated getting nothing done cpu running like crazy autoscaling stuff notice might good minute get instance running joke seen happen pretty basically able point equivalent flamethrower service keep going nt accept extra request one would capable anyway ignore reject rest appropriate way least start absorbing load instead becoming part problem think one instance service fall crash presented enough load rebalance load onto others one boil go rebalancing load onto even fewer instance continue whole thing molten lump slag tell situation like rig horrible thing like turn load balancer restart service maybe set network stuff drop traffic catch ever running incredibly fragile stuff likewise ever worry starting everything otherwise first one get boiled crash second third let first break already bad place side note sound familiar customer year ago absolutely refused fix terrible code wanted better tooling start stop stuff parallel people side nt looking argument gave exactly happy nt think writing better code want stuff many way situation like one temporarily stop checking listening socket new connection mean stop calling accept listen queue back something interesting happen assuming linux probably stop emitting synacks connection attempt point might magic make actively rst connection client fail fast go somewhere else right away whatever thing calm maybe start looking new connection maybe something neat process newest one first since oldest one probably burned timeout already wo nt stick around finish job anyway meant selective lifo post month back kind thing people tend think rpc mindset web mindset want really rough example throttling sendmail mail daemon made life interesting u far long thing kept tab machine load average got first notch would still accept connection would wave error got past second notch would actually stop listening network right would actually close listening socket forcibly reject connection attempt streamofconsciousness intended needed get odds come back point spin tale tell team company already dug hole spend time care stuff resource nt free
23,Lobsters,scaling,Scaling and architecture,Evolutionary Software Architecture,https://venam.nixers.net/blog/programming/2020/06/07/evolutionary-software-arch.html,evolutionary software architecture,post architectus oryzus kpi security code archunit chaos engineering social code analysis github scientist architectural decision record strategy lean methodology disposable software erase darling reactive architecture email irc,previous post underlined philosophy behind domain driven design ddd like move practical approach handle real issue software development architecture requirement constantly change model never precise never current andor never using best technology available one solution problem build evolutionary architecture able discussion understand part software architecture play straight forward considering many definition redefinition note particularly fascinating one architecture software system given point time organization structure significant component interacting interface component composed successively smaller component interface ieee definition software architecture successful software project expert developer working project shared understanding design shared understanding called understanding includes system divided component component interact interface ralph johnson going towards abstract definition following architecture stuff hard change later little stuff possible martin fowler architecture important stuff whatever martin fowler stuff hard change later neal ford definition barely overlap still vague essence joining extract say architecture concerned important decision software project object decision shared knowledge reason view evolutionary architecture standpoint best architecture one decision flexible easily replaceable reversible deferred late possible substituted alternative recent experience shown superior architecture decisionmaking inherently tied concept technical debt compromise trading time design perfect keep mind debt accumulates often lead architectural decay change keep coming entropy increase similarly due vague definition architecture role architect hard describe whether completely separate role whether everyone team act one ambiguous vociferous software architecture evangelist martin fowler prefers term architectus oryzus referring architect also active contributor project thus getting direct insight involvement software architecture thought process applied two broad level application level enterprise level application architecture describing structure application fit together usually using design pattern enterprise architecture organizational level software issue practice information flow methodology standard release mechanism personnel related activity technology stack enforced etc design relates well known development design pattern refactoring technique usage framework bundle component together daily concern evolutionary architecture preferable emergent design instead one set front give u good idea software architecture current state need solution building evolutionary architecture usual way develop software today fighting incoming change want incorporate current architecture software development dynamic equilibrium currently find software constantly unbalanced unstable state whenever change included even though like right thing right time predict decision predictability almost impossible example predict disruptive technology exist yet software age juggle change new requirement room experimentation respond stakeholder want software fulfill architecture significant requirement also known kpi auditability performance security scalability privacy legality productivity portability stability etc expect degrade hence find leastworst tradeoff blindly introduce anything could hinder hard businessdriven change new feature new customer new market etc ecosystem change advance technology library upgrade framework operating system etc recent year seen rise agile development methodology meant replace waterfall approach apt facing challenge create iterative dynamic way control change process call evolutionary architecture start idea embracing change constant feedback want apply across whole architecture spectrum multiple dimension strongest survive one responsive change evolutionary software architecture evolutionary architecture metaarchitecture way thinking software evolutionary term guide first derivative dictating design principle promote change first citizen neal rebecca patrick definition book evolutionary dissect evolutionary architecture support guided incremental change across multiple dimension separate system world continuum draw boundary around system depends purpose discussion donella h meadow agile methodology concerned people process evolutionary architecture encompasses whole spectrum including technical data domain security organizational operational aspect want different perspective evolvable dimension evolutionary mindset surround holistic view software system add new requirement call evolvability dimension help measure easily change dimension evolve architecture easily included dynamic equilibrium example big ball mud architecture extreme coupling architectural rotting dimension evolvability change dimension daunting layered architecture onedimensional structural evolvability change one layer ripple lower one however domain dimension evolvability often domain concept smeared coupled across layer boundary thus domain change requires major refactoring ripple layer microservice style architecture hinge postdevops agile revolution structural domain dimension evolvability n n number isolated service running service microservice architecture represents domain bounded context changed independently others boundary world evolutionary architecture call disjunct piece quantum architectural quantum independently deployable component high functional cohesion includes structural element required system function properly monolith architecture whole monolith quantum however temporal coupling perspective dimension transaction may resonate multiple service microservice architecture thus evolvability transactional dimension enough measure easy change applied also need continually incrementally applies team build software agile methodology software deployed thing continuous integration continuous delivery continuous verificationvalidation rely good devops practice let take back control complex system automated deployment pipeline automated machine provisioning good monitoring gradual migration new service controlling route using database migration tool using chaos engineering facilitate management service experiment without hassle trivially reversibly evolvability across multiple dimension incremental change start evolutionary process need guide push using experiment main stressor architecture direction want call selector evolutionary fitness function similar language used genetic algorithm optimization function architectural fitness function provides objective integrity assessment architectural characteristic fitness function metric cover one multiple dimension care want optimize wide range function evolutionary architecture shine encourages testing hypothesis gathering data manner possible see metric evolve software along experimentation hypothesisdriven development superpower evolutionary architecture deliver limited usual test unit static analysis extends way beyond simple code quality metric could automated global continuous dynamic domain specific etc mention interesting technique used experimentation facilitated ab testing canary release aka phased rollout tdd find emergent design security code especially deployment pipeline architecture code also deployment pipeline test framework archunit license code surprisingly work test production instrumentation metric direct interaction user feature flagsfeature toggle toggle behavior chaos engineering example using simian army continuous fitness function facilitation experiment uncover systemic social code analysis find hotspot code github scientist test hypothesis production keeping normal behavior decides whether run try block randomizes order use try block run measure duration behavior compare result try result use swallow record exception raised try publishes information benefit experimentation soon seen creating real interactive feedback loop user buffet option dynamic equilibrium take care fewer surprise enabled team building evolutionary architecture like ddd law applies shape organization directly reflected software affect architecture without affecting people build far seen team embrace devops agile development given additionally team cocoon evolution experimentation making crossfunctional every role expertise found responsible single project remove bottleneck organization need team resembles architectural quantum small team one fed two pizza twopizzas team avoids separation decides need done decides going done everyone decides together talk team charge product rather project size team also allows information flow seamlessly share architectural domain knowledge method used usual documentation architectural decision record pair programming even mob programming nice team single working unit taking best decision project also important limit boundary many company prefer giving loose recommendation software stack team use instead letting silo specialized centralized knowledge face dynamic equilibrium time team level parallel enterprise architecture called strategy human governance team restrictive would make hard move however team guided fitness function automatic architectural governance continuous verification delivery pipeline act guardrail mechanism two big principle kept mind applying responsible pain together effect making team member le hesitant prone experiment last responsible moment idea lean methodology postponing decision immediately required find time gather much information possible let best possible choice emerge especially useful come structural design technological decision insight clear context appear late help avoid potential cost technical debt useless abstraction vendorlocking code framework direct opposition classical way software architecture decision taken upfront bringing pain forward idea inspired extreme programming methodology facing difficult long painful task instead postponing often encounter know insandouts incited automate pain away dynamic equilibrium pain v time pain increase exponentially wait encouraged thing like test production apply technique chaos engineering rebooting often garbage collecting service merging code often using database migration tool etc eventually knownunknowns become knownknowns common predictable pain gone world software keep getting complex building evolutionary architecture lead topic building robust resilient adaptive rugged system software intimate part life rely real world effect could get inspired aerospace world take look checklist manifesto could embrace statelessness throwabledisposable architecture disposable software erase darling maybe go way flexible reactive architecture selfhealing mechanism anything possible post given overview way perceive evolutionary software architecture place software architecture whole clearly step forward typical static view architecture offer novel organic approach name implies none described necessarily novel putting method thinking together want indepth explanation take look book evolutionary neal ford rebecca parson patrick kua hope article kickstarts journey reference want depth discussion always available email irc discus argue like dislike new idea consider opinion etc nt feel like discussion intimidated email simply say something small comment section andor share friend
24,Lobsters,scaling,Scaling and architecture,Step by Step Guide for Micro-Services Authentication using Istio and JWT,https://medium.com/intelligentmachines/istio-jwt-step-by-step-guide-for-micro-services-authentication-690b170348fc,step step guide microservices authentication using istio jwt,istio fit whole equation istio istio step implement jwt istio prerequisite follow istio documentation setup demo application install demo application install new gateway install virtual service test setup step generate publicprivate key pair using python github step b generate publicprivate key pair using openssl note i sub aud step create jwks json web key set representation public key step tell istio find jwks using crd issued jwt token public key stored jwks json web key set jwksuri json web key set uri jwks json web key set jwksuri json web key set uri http test whole shebang token unauthorized error unauthorized error nice important link jwt public key signature jwt detail google jwksuri jwt token validation http troubleshooting,private key stay secured one place inside authentication service generate jwt token need private key validate token public key good enoughwhere istio fit whole equation understanding design consideration discus istio help implement publicprivate key pair based jwt authentication scaleable secure choice microservices architectureso basically publicprivate key pair based jwt authentication system one dedicated microservice take care login task take username password generate jwt token using private key send back clientand client try communicate service using jwt token service use public key validate token scenario every service system implement application logic toget public key public url memoryvalidate token using public keyimplement case valid tokenimplement case invalid tokenthis may really easy straight forward implement one microservice imagine hundred microservices variety programming language framework every application implement manage thing different language framework implementation thing might consistentistio distio take care task validating jwt token incoming user request implement istio jwt authentication feature application code need bother jwt token validation istio jwt token generation istio generate token authentication microservice generates tokenhow istio istio jwt authentication defined request authentication feature custom resource definition crd named requestauthentication used tell control plane jwt public key found issued jwt tokenafter define configure requestauthentication crd jwksuri jwks issuer information explain meaning detail later section istiod master control plane tell envoy proxy jwt authentication policy informationnow request come service mesh reach application go envoy proxy sidecar container sits right beside application container envoy proxy validate jwt token incoming request carrying using public key available jwksjwksuri issuer information token validated using public key also issuer information match i mentioned jwt token payload envoy forward request directly application container application container handle request without bothering validity request right using istio offload lot application level security concernsteps implement jwt istiolet see implement thing sample project follow istio docswhich fairly straight forward easy understand step need follow see effect jwt authentication policy followsprerequisitesyou already kubernetes cluster setup using gke clusteryou installed latest stable release istio demo profile demo using gke istio although gke come option install istio cluster creation time clicking checkbox would encourage use istioctl install latest stable version istio follow istio documentationyou understanding step needed integrate istio existing kubernetes application new istio please refer story mineit might help get speedsetup demo applicationin step setup demo application using test istio jwt featureswe use httpbin application form istio docsthe istio doc give show mtls jwt authentication application setup demonstrating jwt feature consolidated command needed setup demo copypaste command work like charminstall demo applicationinstall new gatewayinstall virtual servicetest setupexport ingresshost kubectl n istiosystem get service istioingressgateway jsonpath statusloadbalanceringress ip curl ingresshostheaders devnull w httpcode n demo application set setup jwt authentication demo application make secureyou setup jwt authentication language stack python nodejs java goit matter need aware need setup rsa publicprivate key pair based jwt authentication system jwt authentication system us secret text variable file generate validate jwt token like traditional monolithic application jwt tutorial show work istio istio take public key url public key validate jwt token istio worksnow give away jwt token validation responsibility istio follow stepsgenerate publicprivate key paircreate jwks json web key set representation public keytell istio find jwks using requestauthentication crdtest new featurelet get businessstep generate publicprivate key pair using pythonthis script generates rsa publicprivate key pair using python us key generate jwt token run script following instruction github repo readmemd get public key private key valid jwt token generated using private key validated using public keystep b generate publicprivate key pair using opensslin approach use opensource tool named openssl generate token also use sshkeygen generates effectively thing different public key representation use opensslmkdir keyscd keysopenssl genrsa privatekeypem rsa privatekeypem pubout publickeypemafter generating key run following script generate validate tokennote generating token payload reserved key like i sub aud several others mandatory define working jwt system general working istio authn system need put field payloadstep create jwks json web key set representation public keyjust creating raw public key enough istio accepts public key jwks format give istio array public key istio use validate incoming tokenif public key need convert json format pem format example public key given e aqab kty rsa n need encapsulate key item following array like key e aqab kty rsa n valid jwksstep tell istio find jwks using requestauthentication crdto tell istio validate jwt token incoming request define crd named requestauthentication enable istio capability validate token need provide crucial information istio requestauthentication crdwho issued jwt token basically jwt authentication microservice generate jwt token specify i field payload i field match issuer field requestauthentication crdwhere public key stored specify value using either two jwtrules optionsjwks json web key set orjwksuri json web key set uri jwks json web key set using jwks effectively embed jwks created previous step directly value like belowjwksuri json web key set uri using jwksuri put jwks created previous step public url demo using public github repository using store source code blog using url raw github filehttps specify url jwksuri field like belowtest whole shebangif come far havethe demo app running kubernetes clustera gatewaya virtualservicea requestauthentication crd anda jwt token printed consolenow need thing first put jwt token inside environment variable named tokenexport token generated jwt token get ingres ip addressexport ingresshost kubectl n istiosystem get service istioingressgateway jsonpath statusloadbalanceringress ip checked start testing istio jwt feature send request demo application using curl invalid token valid token token request headerif send request invalid token see happenscurl header authorization bearer blablabla ingresshostheaders devnull w httpcode n get unauthorized error demo app set authentication system sort get unauthorized error istio taking care validating incoming jwt token user request nice send request valid tokencurl header authorization bearer token ingresshostheaders devnull w httpcode n get successful request http response code console let send request header allcurl ingresshostheaders devnull w httpcode n get console default istio run authentication policy check permissive mode meaning send request provide valid token provide token help gradual migration process moving istio based system blocking entire operation strictto reject request without valid token add authorization policy rule specifying deny action request without request principal shown notrequestprincipals following example request principal available valid jwt token provided rule therefore denies request without valid tokenslet test send three request invalid valid tokenscurl header authorization bearer blablabla ingresshostheaders devnull w httpcode n response tokencurl header authorization bearer token ingresshostheaders devnull w httpcode n response token curl ingresshostheaders devnull w httpcode n response allowing request valid tokenseffectively script using generate jwt token mimicking role authentication microservice authentication microservice give jwt token valid username password python script give valid jwt token execute authentication microservice implementation logic python script providedso decoupled jwt token generation validation step authentication microservice jwt token generation istio handle token validationhope able drive point home know long point may already forgotten point p read title might make sense nowthis effort document learned hope help wellimportant linkswhen starting aware lot related concept well google quite lot understand put together implement real project link provided belowjwt public key signaturejwt detailsgoogles jwksuri jwt token validationthe link show google store jwks way demohttps
25,Lobsters,scaling,Scaling and architecture,Back of the envelope estimation hacks,https://robertovitillo.com/back-of-the-envelope-estimation-hacks/,back envelope estimation hack,back envelope estimation hack enrico fermi know number approximate power rule rule little law little law remember system design book,back envelope estimation hacksmay two type engineer one quickly estimate one people smarter enrico fermi master estimationback envelope estimation skill learned practice becomes easier get familiar trick tradeknow numbershow fast read data disk quickly network familiar ballpark performance figure component important exact number per se relative difference term order magnitudeapproximate powersthe goal estimation get correct answer get one right ballpark using power make multiplication easy logarithmic space multiplication become additionsfor example let say user watching video uhd mbps machine content delivery network egress gbps many machine need approximating mbps mbps bps subtracting much easier trying get exact mbps difference really matter really precise answer exact anyway user vary time network bandwidth constant etcthe rule rule method estimate long take quantity double grows certain percentage frac rate example let say traffic web service increasing approximately weekly long take double frac take approximately week traffic doubleif combine rule power two quickly find long take quantity increase several order magnitudeslittle lawa queue modeled three parameter average rate new item arrive lambda average time item spends queue www average number item queue llllots thing modeled queue web service seen queue example request rate rate new item arrive time take request processed time item spends queue finally number concurrently processed request number item queuewouldn great could derive one three parameter two turn law relates three quantity one another called little law lambda say average number item queue equal average rate new item arrive multiplied average time item spends queuelet try let say service take average m process request currently receiving two million request per second rps many request processed concurrently service processing request concurrently request cpu heavy requires thread need thread using core machine keep need machinesremember thisestimation vital skill engineer something get better practicing using hack presented post get familiar component performance numbersapproximate power transform multiplication additionsuse rule find long take quantity double given growth ratemodel system queue leverage little lawcheck system design book want learn designing large scale system
26,Lobsters,scaling,Scaling and architecture,Organizing architectural katas,https://nelis.boucke.be/post/architectural-katas/,organizing architectural katas,architectural katas ted neward architectural katas poster architetural katas code katas architectural katas poster step architectural kata description architectural kata website practical organization architectural katas preparation invite kind environment project http wwwarchitecturalkatascom room view additional constraint model facilitating workshop agenda timing role facilitator conclusion twitter external link see also,architectural katas great way spice architectural skill team community practice productive discussion architecture always receive great feedback co organizing architectural katas regularly get question organizing others matteo pierro run session xpdays benelux organize katas sharing experience encourage others organizing kind katas architectural katas ivory tower architect defining architecture team bygone era general trend development team define software architecture least team strongly involved defining architecture people learn architecture well thin air need way learn practice also main motivation ted neward coming architectural katas illustrated following two quote poster architetural katas get great designer great designer design course fred brook supposed get great architect get chance architect fewer halfdozen time career ted neward ted searching way practice defining architecture inspiration code katas came concept architectural katas architectural katas poster step architectural kata architectural katas workshop several small group people practice discussing designing architecture typically workshop involves several iteration start iteration group get project description constraint get time discover needed ask clarifying question customer often facilitator discus technology option design vision architecture discussing group present result group gather feedback iteration end reflection break information check description architectural kata website practical organization architectural katas organizing good workshop asks proper preparation facilitation might look daunting first hope next section help way learning post extracted organizing event like year evening meetups conference session open fullday session till closed company session audience session varied people people preparation invite invite team member people community interested discussing architecture includes limited developer tester product owner analyst scrum master architect help majority people software development experience affinity strict requirement participant prior experience architect required prefer mixture role background healthy discussion make assumption explicit run architectural katas workshop meetup conference company far understood ted run regular event community kind environment important aspect environment setup provide unlimited modeling space need good modeling space per group often people stop architecting soon allocated space full might miss nice creativity discussion experience large magic whiteboard trump smaller whiteboard trump flipchart trump table flipchart paper trump table smaller size paper room requirement modeling space combination lot people discussing small group ask spacious room several room people need space discus comfortable noise level must remain acceptable people understand larger workshop group prefer use several breakout room material bring enough postits several size form paper good whiteboard permanent marker food drink type workshop exhausting especially keep iterating day long catering sugar dip need drink help people stay focused project kata start brief intentionally vague incomplete project description ted neward effort set website gather share project description http wwwarchitecturalkatascom although ted talk drawing random kata group always preselected project asked group project main motivation allow efficient exchange feedback reflection phase selecting kata important think audience completely new format people architectural background group know embedded development challenge lot complex break others require specialized knowledge audience might like room view starter kata people contact hotel thus basic knowledge domain room view large hotel reservation company want build next generation hotel reservation management system specifically tailored highend resort spa guest view reserve specific room user guest hundred hotel staff le requirement registration made via web mobile phone call walkin guest ability either book type room standard deluxe suite choose specific room stay viewing picture room location hotel system must able maintain room status booked available ready clean etc well room needed next must also stateoftheart housekeeping management functionality cleaning maintenance staff directed various room based priority reservation need using proprietary device supplied reservation company attached cleaning cart standard reservation functionality eg payment registration info etc done leveraging existing reservation system system webbased hosted reservation company additional context peak season quickly approaching system must ready quickly wait next year company also investing heavily cutting edge technology like smart room lock open via cell phone interested highend market sale people tremendous clout organization people often scramble make promise true others often use beginning check work sick stuff cruft another one always keep around challenge world web craft go beyond simple website thus contains lot challenge people additional constraint similar coding katas try adding additional constraint maximize learning effect focus specific learning goal example could use kata learn new modeling language practice constraint exaggerate practice get muscle memory example constraint use model document architectural vision model simple set model visualize static structure software architecture giving people simple standardized way visualize decision architectural model suddenly lot fewer misunderstanding highquality discussion typically use following flip chart add postits one one introduce model explicitly try use existing cloud service build nothing use cloud service component two component talk use event storming identify behavior update constraint might vary different iteration example might give constraint first iteration use constraint second iteration use event storming discover behavior third iteration finally update model represent behavior facilitating workshop agenda timing typical cycle single kata session take min introduction kata constraint team discussion review share architecture reflect share observation experimented longer shorter discussion slot seems sweet spot people still engaged also need trigger new idea reviewing reflecting starting new cycle perfect way bring inspiration depending amount time repeat cycle several time including minute break cycle take thing session single iteration short get lot indepth learning architecture several time would advise running short session convince people try goal conference blogpost maybe repeat weekly could work never tried session two three iteration start interesting people apply thing learned sharing reflection future session two iteration typically change application group provide extra constraint second iteration full day session got lot positive feedback fullday session get typically need take lunch break experience hard iteration day people exhausted example agenda general introduction iteration break iteration lunch iteration break iteration closing role facilitator facilitator architectural kata session easy asks thing facilitation time keeping key factor make architectural katas success facilitation organizing room optimal group discussion coming appropriate challenge constraint use appropriate formate sharing timekeeping help group stuck especially facilitation larger group extra challenging use different technique context feel overwhelmed thought look join another architectural kata session pair experienced facilitator customer question large part facilitator coming answer question towards customer project description purposefully vague short lot room interpretation time make answer fly encourage make assumption make explicit technical experience help technical experience experience architecting especially get group going bit stuck observe help reflect important part iteration reflect happened share learned facilitator ensure everyone get chance share learning end add observation emphasize certain aspect strengthen certain learning see lot expectation towards facilitator event often hard fulfill advise pair rule number facilitating type event handle people prefer pair would add one helper people conclusion architectural katas great way practice discussing architecture modern age agile team supposed define least involved architecture good idea practice modeling architectural discussion skill team hope post help people organize architectural katas company community still doubt anything else help organize architectural kata hesitate contact twitter take challenge go forth organize katas external link see also
27,Lobsters,scaling,Scaling and architecture,Architecture decision record (ADR) examples,https://github.com/joelparkerhenderson/architecture_decision_record,architecture decision record adr example,architecture decision record adr architecture decision record architecture decision record architecture decision architecture decision log architecturallysignificant requirement architecture knowledge management ad adl adr akm asr start using adrs architectural decision start using adrs tool start using adrs git adr file name convention suggestion writing good adrs adr example template information,architecture decision record adr architectural decision record adr document capture important architectural decision made along context consequence content architecture decision record architecture decision record adr document capture important architectural decision made along context consequence architecture decision ad software design choice address significant requirement architecture decision log adl collection adrs created maintained particular project organization architecturallysignificant requirement asr requirement measurable effect software system architecture within topic architecture knowledge management akm goal document provide fast overview adrs create look information abbreviation ad architecture decision adl architecture decision log adr architecture decision record akm architecture knowledge management asr architecturallysignificant requirement start using adrs start using adrs talk teammate area decision identification urgent important ad made wait known personal collective experience well recognized design method practice assist decision identification ideally maintain decision todo list complement product todo list decision making number decision making technique exists general one software architecture specific one instance dialogue mapping group decision making active research topic decision enactment enforcement ad used software design hence communicated accepted stakeholder system fund develop operate architecturally evident coding style code review focus architectural concern decision two related practice ad also considered modernizing software system software evolution decision sharing optional many ad recur across project hence experience past decision good bad valuable reusable asset employing explicit knowledge management strategy group decision making active research topic decision documentation many template tool decision capturing exist see agile community eg nygard adrs see traditional software engineering architecture design process eg table layout suggested ibm umf tyree akerman capitalone decision guidance step adopted wikipedia entry architectural decision number decision making technique exists general one software software architecture specific one instance dialogue mapping start using adrs tool start using adrs tool way want example like using google drive online editing create google doc google sheet like use source code version control git create file adr like using project planning tool atlassian jira use tool planning tracker like using wikis mediawiki create adr wiki start using adrs git like using git version control like start using adrs git typical software project source code create directory adr file adr create text file databasetxt write anything want adr see template repository idea commit adr git repo adr file name convention choose create adrs using typical text file may want come adr file name convention prefer use file name convention specific format example choosedatabasemd formattimestampsmd managepasswordsmd handleexceptionsmd file name convention name present tense imperative verb phrase help readability match commit message format name us lowercase underscore repo balance readability system usability extension markdown useful easy formatting suggestion writing good adrs characteristic good adr point time identify ad made rationality explain reason making particular ad immutable record decision made previously published adr altered specificity adr single ad characteristic good context adr characteristic good consequence adr new adr may take place previous adr ad made replaces invalidates previous adr new adr created adr example template adr example template collected net information introduction template indepth tool example see also remap representation maintenance process knowledge drl decision representation language ibis issuebased information system qoc question option criterion ibm ebusiness reference architecture framework
28,Lobsters,scaling,Scaling and architecture,Unbundling Data Science Workflows with Metaflow and AWS Step Functions,https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280,unbundling data science workflow metaflow aws step function,unbundling data science workflow metaflow aws step function david berg ravi kiran chirravuri romain cledat jason ge savin goyal ferras hamad ville tuulos data science framework netflix opensourced december problem solved people tool metaflow compare workflow scheduler metaflow workflow executed production today releasing first opensource integration scheduler aws step function unbundling dag many framework organize work directed acyclic graph compute step scheduling scientific workflow dag compute layer optimized computer vision scheduler layer architecting data flow state transfer job scheduler topological order foreach construct compute aws batch executing task independent container local scheduler resume command aws step function internal scheduler called meson luigi airflow aws step function high availability zero operational burden sfn costeffective solution highly scalable limited state transition triggering workflow cloudwatch monitoring alerting see documentation hood amazon state language parameter resource retry dependency aws batch next step deploying step function administrator guide metaflow please get touch,unbundling data science workflow metaflow aws step functionsby david berg ravi kiran chirravuri romain cledat jason ge savin goyal ferras hamad ville tuulostl dr today releasing new job scheduler integration aws step function integration allows user metaflow schedule production workflow using highly available scalable maintenancefree service without change existing metaflow codethe idea abstraction layer fundamental way manage complexity computing atom make transistor transistor make functional unit cpu cpu implement instruction set targeted compiler higherlevel language key benefit layer developed independently separate group people coupled together wellscoped interface layer independent life cycle enable higher layer stack maintain semblance stability without hindering innovation layer belowmetaflow data science framework netflix opensourced december designed around idea independent layer already year ago started building metaflow recognized excellent solution available layer typical data science stack stitching layer together challenge data science project wanted metaflow become substrate integrates layer easytouse productivity tool optimized data science use casesin contrast many framework metaflow try abstract away existence separate layer believe problem solved people tool following humancentric usabilitydriven approach data scientist care lower layer stack work believe benefit trying pretend stack exist would problematic especially thing failthis article focus job scheduler layer two layer surround architecture layer defines structure user code compute layer defines code executedsince initial opensource release metaflow heard question metaflow compare workflow scheduler metaflow workflow executed production answer question metaflow designed used conjunction productiongrade job scheduler today releasing first opensource integration scheduler aws step function use execute metaflow workflow scalable highlyavailable mannerbefore going detail aws step function want highlight role job scheduling layer metaflow stackunbundling dagsimilar many framework help manage data science workflow metaflow asks user organize work directed acyclic graph compute step like hypothetical example find dag abstraction natural way think data science workflow instance data scientist might draw dag whiteboard asked want organize modeling pipeline level dag say anything code get executed executed data scientist want structure codethe idea scheduling scientific workflow dag decade old many existing system require tight coupling layer data science stack often necessitated infrastructural limitation predate cloud system user may need specify part modeling code using custom dsl dsl may executed builtin job scheduler tightly coupled compute layer eg hpc cluster defines code executedfor specific use case tight coupling may well justified however since inception metaflow supported hundred different reallife data science use case natural language processing computer vision classical statistic using r make much harder define tightly coupled stack whathowandwhere would work well use case instance user may want use compute layer optimized computer vision certain step workflowmetaflow unbundles dag separate architecture scheduler compute layer user may use language library familiar leveraging rich data science ecosystem r python architect modeling code user code get packaged compute layer metaflow user focus code rather eg writing dockerfiles finally scheduling layer take care executing individual function using compute layer data scientist point view infrastructure work exactly write idiomatic modeling code using familiar abstraction get executed without hassle even massive scalethe scheduler layerbefore dag scheduled user must define metaflow come opinionated syntax set utility crafting data science workflow python r coming soon provide plenty support architecting robust data science code empowers data scientist create operate workflow autonomously even year experience writing scalable system software particular metaflow take care data flow state transfer layer independent schedulermetaflow provides strong guarantee backwards compatibility userfacing api user write code confidently knowing metaflow schedule execute without change even underlying layer evolve time user point view core valueadd metaflow apis rather specific implementation underlying layersonce user specified workflow orchestrating execution dag belongs job scheduler layer scheduling layer need care code executed sole responsibility schedule step topological order making sure step finish successfully successor graph executedwhile may sound deceptively simple consider mean context reallife data science company like netflix graph may almost arbitrarily large especially thanks dynamic fanouts ie foreach construct instance existing metaflow workflow train model every country foreach every model perform hyperparameter search parametrizations foreach result task single workflow hence scalability critical feature schedulerthere arbitrarily many workflow running concurrently instance countrylevel workflow may different variation scheduled simultaneously netflixscale scheduler need able handle hundred thousand active workflowsbesides scale scheduler need highly available responsibility scheduler make sure businesscritical workflow get executed time achieving scalability highavailability system nontrivial engineering challengethe scheduler may provide various way trigger execution workflow workflow may started based time simple cronstyle scheduling external signal may trigger execution netflix workflow triggered based availability upstream data ie ml workflow start whenever fresh data available best organize web workflow deep topic cover detail laterthe scheduler include tool observability alerting convenient monitor execution workflow gui get alerted various mean critical execution failsit important note scheduling layer execute user code execution user code responsibility compute layer box metaflow provides local compute layer executes task local process taking advantage multiple cpu core compute resource needed local scheduler aws step function utilize aws batch executing task independent containerslocal schedulermetaflow come builtin scheduler layer make easy test workflow locally laptop development sandbox builtin scheduler fully functional sense executes step topological order handle workflow ten thousand task lack support highavailability triggering alerting designsince metaflow designed interchangeable layer mind need reinvent wheel building yet another productiongrade dag scheduler instead local scheduler focus providing quick developtestdebug cycle development user deploy workflow productiongrade scheduler aws step function happy result many productiongrade scheduler provide firstclass local development experience local scheduler ensures smooth transition prototyping productionmetaflow recognizes deploying production linear process rather expect user use local scheduler production scheduler parallel instance initial deployment data scientist typically want continue working project locally eventually might want deploy new experimental version production scheduler run parallel production version ab test also thing fail production metaflow allows user reproduce issue occur production scheduler locally simply using resume command continue execution local machineaws step functionsmetaflow builtin local scheduler great solution running isolated workflow testing development quick manual iteration preferred high availability unattended execution netflix data scientist ready deploy metaflow workflow production use internal scheduler called meson internally meson fulfills requirement production scheduler laid abovesince opensourcing metaflow wanted find publicly available replacement meson user metaflow could benefit considered number existing popular opensource workflow scheduler luigi airflow system many benefit found lacking come high availability scalability key requirement production schedulerafter careful consideration chose aws step function sfn target first opensource production scheduler integration found following feature sfn appealing roughly order importance aws proven track record delivering high sla address high availability requirementshigh availability delivered zero operational burden netflix team senior engineer required develop operate internal scheduler expect smaller company want dedicate full team maintain scheduler likely sfn costeffective solutionwe optimistic sfn highly scalable especially term number concurrent workflow today size individual workflow limited state transition enough vast majority use case quite uniquely sfn high limit workflow execution time one year convenient demanding ml workflow may take long time executethere existing mechanism triggering workflow based external event time one leverage functionality build web data ml workflow similar netflix operates internallyone use wellknown aws tool cloudwatch monitoring alertingit remarkable today company size benefit offtheshelf tooling caliber many case negligible cost aligns well vision metaflow use best publicly available infrastructure layer ml stack metaflow take care removing gap stack data scientist point view deploy workflow production simply executingpython myflowpy stepfunctions createfor detail use step function metaflow see documentationunder hoodwhen user executes stepfunctions create metaflow statically analyzes user workflow defined flowspec class parse dag structure compile amazon state language workflow specified aws step functionsbesides compiling dag automatically translate parameter relevant decorator resource retry sfn configuration user code dependency snapshot stored guarantee production workflow impacted external change input datatoday compute layer supported sfn aws batch userdefined code ie metaflow task executed container managed aws batch future workflow scheduled sfn may leverage compute layer wellthis translation ensures user metaflow workflow executed either local scheduler sfn without change code data scientist focus writing modeling code test locally rapid iteration finally deploy code production single command importantly repeat cycle often needed minimal overheadnext stepswith aws step function integration released today user metaflow start leveraging productiongrade workflow scheduler similar setup netflix operating successfully past three yearsif data scientist us plan use metaflow learn deploying step function documentation infrastructure person wanting leverage metaflow step function organization take look brand new administrator guide metaflowwe believe aws step function excellent choice scheduling metaflow workflow production however layer metaflow stack pluggable design good experience another job scheduler could fulfill requirement set need help getting started step function please get touch
30,Lobsters,scaling,Scaling and architecture,Zero Downtime Release: Disruption-free Load Balancing of a Multi-Billion User Website,https://www.dropbox.com/s/g40kvbyxqhn4h72/fbr.pdf?dl=0,zero downtime release disruptionfree load balancing multibillion user website,,
31,Lobsters,scaling,Scaling and architecture,Scaling relational SQL databases,https://stribny.name/blog/2020/07/scaling-relational-sql-databases,scaling relational sql database,update database scale vertically leverage application cache redis memcached use efficient data type data normalization denormalization precompute data leverage materialized view pipelinedb use proper index leverage execution plan query optimization choose correct transaction isolation level bulk insert update compress data storage make alter table work manage concurrent connection add read replica disk partitioning use specialized extension timescaledb postgis sharding mysql cluster citus store everything one table process data outside sql database kafka clickhouse apache spark aware limitation managed sql database final word loading like,many application today still rely traditional sql database like mysql mariadb postgresql data storage data processing growing amount data new workload made database system often find situation need think scaling system come scaling might need think data storage store data becomes expensive slow working themfast insert update writeheavy workloadsmaking select query faster complexity need query huge amount dataconcurrency many client interacting database article present basic idea starting point scaling traditional sql database update database newer version mysql postgresql traditional sql database typically come performance improvement even newer database system faster direct replacement might new feature available take advantage keeping database system date expands option data give u best box performance basic enabler scaling scale vertically intuitive idea scaling use better hardware scale vertically one database server come hardware typically look cpu disk ram number cpu affect many query database run therefore many client servedthe size ram give u space index temporary table cache database store fast memory make system faster due minimizing io access diskdisk io speed highly affect query time especially full scan read index used writeheavy systemsdisk size allows u store data leverage application cache application code control cache data memory store like redis memcached avoid querying database cache database read also use system buffer writes eg collecting analytical data possible delay problem use efficient data type come data start data type word individual piece information physically stored memory choosing appropriate data type always balance efficiency functionality affecting required memory query performance number operation specific data type two basic way think optimal data type logical store general information like telephone number ip address instance store color string enum collection rgb integer store ip address string bytesphysical specific data type choose string integer time since typically multiple option eg date time typically stored timestamp faster limited date range functionality complex datetime developer friendly flexible allowing store timezone etc using appropriate type database column especially important want index column use clause use join lot data data normalization denormalization often trained normalize data relational database order reduce data redundancy improve data integrity generally useful might want reconsider data come scaling generally speaking simpler make data retrieved saved precompute data lot select query optimized data already requested form handy analyzing large amount data instance instead aggregating data every time aggregate beforehand course always possible eg aggregate average aggregate would lose precision leverage materialized view materialized view continuously updated data updated write operation scheduled time seen extension previous point unlike classic view materialized view physically stored need computed needed useful situation select query would take long time produce result example look pipelinedb extension postgresql produce aggregate realtime data allowing u keep aggregated statistic writeheavy system use proper index using right index table huge performance changer typically index column want query data want use perform join however adding index also reduce performance writeheavy workload since index updated every insert update also make sense use index data type finding right balance leverage execution plan query optimization optimizing read need know database query planner execute query use explain statement obtain execution plan sql server use query look database plan use existing index table plan make full table scan give u hint whether change structure data add index rewrite query different way please note however full table scan also faster necessary query black white situation choose correct transaction isolation level common relational database use multiversion concurrency control make locking granular instead locking whole table like myisam storage engine however still need tell database exactly concurrency control behave setting appropriate transaction isolation level basic isolation level standard greatly affect database system behaves bulk insert update writing updating individual row table efficient database like mysql postgresql way insert modify data bulk leverage every time compress data storage many time need query data column table case optimize storage size storing column compressed form especially useful string type binary data database extension might already compressing data hood always make sense look first data stored whether adding compression application layer would bring desired benefit make alter table work amount data workload grows experience slower slower alter table point might finish reasonable time first important thing know typically two way changing structure database table one inplace modifying original table one copy creating new table moving data afterwards database typically try modify table inplace possible might make sense sometimes explicitly ask copy operation adding new column better add end table since inserting column specific place slow changing existing column might good idea create new column first copy data remove column afterwards write operation table make difficult perform alter table make sense stop client writing table first always possible eg collection analytical data postponed using ingestion buffer front database temporarily pause etl job working table last resort simply alter table large writeheavy table opting creating new table store new data instead manage concurrent connection every database server come basic configuration maximum number concurrent connection need typically need reconfigure value increasing value enough though need make sure system actually run well desired number connection add read replica traditional sql database typically scale horizontally write operation adding server still add machine form readonly replica way work write operation done main server propagated machine using write ahead log replica therefore apply operation order underlying storage main server ensures data sync replica used scaling number read query number connected client need read operation disk partitioning partitioning allows u distribute one single table across filesystem store individual partition based specific rule rule chosen well sql query query partition limit search subset cut time query run since whole database table need looked use specialized extension need store work geospatial time series specialized data sometimes use database extension like timescaledb postgis make data processing storage efficient sharding sharding partitioning steroid allows u store part database table shard different server distributed database often built concept shard traditional sql server automatically shard data since work one main server replica investigate whether clustering solution like mysql cluster mysql citus postgresql would solve scaling problem solution built provide horizontally scalable sql database without limitation instance mysql cluster work classic innodb engine write application like used another option shard data manually application layer brings additional complexity application need manage multiple connection different server query database appropriately solution consider store everything one table amount data primary issue table becoming large think whether put data elsewhere need data occasionally need unique index across whole table simply divide data multiple table instance separate data different customer store older data archive table way keep table smaller performant case might even decide okay store data primary database either simply delete move another storage system process data outside sql database sometimes relational sql database enough instance might want create data workflow unfit system might able make change schema change existing program interact database maybe reached limit scaling current data processing need many case best approach move data another system process fortunately utilize write ahead log wal store change insert update alter table database perform programmatically read log stream another system instance stream data kafka using existing database connector allow application read send data directly specialized database eg analytical database clickhouse data lake like hadoop process also move data via mean taking copy database extracting data sql etc want scale data computation without using different database system process data multiple data source simply want organize data processing better use distributed clustercomputing software apache spark aware limitation managed sql database many people pay managed sql database cloud provider like aws azure worry maintaining however need scale traditional sql store many scaling option actually unavailable u include limited ability change database configuration limited hardware option choose limited number replica higher latency write ahead log might inaccessible might able update database version current modern version want might able install extension would improve performance system also debugging performance issue remote system might difficult impossible since might proper access operating system buying dream never worry sql store investigate option managed database offer actually sufficient final word presented option possible starting point give u area think want go extra detail scaling sql database straightforward operation always need think system option making every solution unique loading like
32,Lobsters,scaling,Scaling and architecture,How we migrated Dropbox from Nginx to Envoy,https://dropbox.tech/infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy,migrated dropbox nginx envoy,envoy legacy nginxbased traffic infrastructure next proxy layer written go dropbox traffic infrastructure edge network bandaid new envoybased traffic infrastructure performance architecture opening file even aiowrite thread pool enabled optimizing web server high throughput low latency guideline performance testing guideline benchmarking envoy perf connection accepted behalf currently blocked worker observability fundamental operational need stub status prometheus format admin interface pluggable tracing provider opentracing integration stream access log grpc access log service integration logging syslog protobufs xds querying one xds service universal ata plane api udpa open request cost aggregation orca katranbased ebpfxdp load balancer courier dropbox migration grpc access log service al endpoint discovery service ed secret discovery service sd runtime discovery service rtds route discovery service rds integrate envoy custom service discovery istio gocontrolplane configuration protocgenvalidate protoc plugin example dynamic extensibility protocol buffer extensibility development guide perl javascript ua nginx module openresty library module c community attempt well documented checkout http filter interface abseil static deadlock detection much canonical example http filter module monitoring framework stats also lua support moonjit webassembly webassembly proxy specification rust c extending envoy webassembly webassemblyhub community redefining extensibility proxy introducing webassembly envoy istio building testing shellbased configuration system b azelbuilt monorepo b azelbuilt nginx version integration test adding external dependency copybara prewritten mock integration test framework gtest googletest googlemock requires change unit test coverage azure ci pipeline googlebecnhmark incremental build remote caching distributed buildstests query even augment security one faster variant introducing openbsd new httpd vulnerability exposure addresssanitizer threadsanitizer memorysanitizer fuzzing ossfuzz ossfuzz architecture security advisory p ast year security release policy described great detail postmortem google vulnerability reward program vrp exploiting envoy heap vulnerability ubuntu debian issue stack protector boringssl fips mode feature file serving workinprogress caching brotli ecache multibackend http cache envoy grpc http bridge reverse grpcweb grpc json transcoder dropbox public apis added support http connect method courier grpc library superset grafana community nginxdevel official bug tracker community mailing list community meeting quic implementation recently presented cloudflare came implementation nginx quiche github issue de sign doc help wanted operational improvement performance optimization new grpc transcoding feature load balancing change current state migration configuring envoy edge proxy gated flag commandline option linux kernel support udp acceleration load reporting service lr endpoint discovery service ed around loadbalancing mobile small team mountain view ca,blogpost talk old nginxbased traffic infrastructure pain point benefit gained migrating envoy compare nginx envoy across many software engineering operational dimension also briefly touch migration process current state problem encountered way moved dropbox traffic envoy seamlessly migrate system already handle ten million open connection million request per second terabit bandwidth effectively made u one biggest envoy user world disclaimer although tried remain objective quite comparison specific dropbox way software development work making bet bazel grpc cgolang also note cover open source version nginx commercial version additional feature legacy nginxbased traffic infrastructure nginx configuration mostly static rendered combination yaml change required full redeployment dynamic part upstream management stats exporter written lua sufficiently complex logic moved next proxy layer written go post dropbox traffic infrastructure edge network section legacy nginxbased infrastructure nginx served u well almost decade adapt current development bestpractices internal private external apis gradually migrating rest grpc requires sort transcoding feature proxy protocol buffer became de facto standard service definition configuration software regardless language built tested bazel heavy involvement engineer essential infrastructure project open source community also operationally nginx quite expensive maintain config generation logic flexible split yaml python monitoring mix lua log parsing systembased monitoring increased reliance third party module affected stability performance cost subsequent upgrade nginx deployment process management quite different rest service relied lot system configuration syslog logrotate etc opposed fully separate base system first time year started looking potential replacement nginx frequently mention internally rely heavily golangbased proxy called bandaid great integration dropbox infrastructure access vast ecosystem internal golang library monitoring service discovery rate limiting etc considered migrating nginx bandaid couple issue prevent u golang resource intensive cc low resource usage especially important u edge since easily autoscale deployment cpu overhead mostly come gc http parser tl latter le optimized boringssl used nginxenvoy goroutineperrequest model gc overhead greatly increase memory requirement highconnection service like fips support go tl stack bandaid community outside dropbox mean rely ourself feature development decided start migrating traffic infrastructure envoy instead new envoybased traffic infrastructure let look main development operational dimension one one see think envoy better choice u gained moving nginx envoy performance nginx architecture eventdriven multiprocess support soreuseport epollexclusive workertocpu pinning although eventloop based fully nonblocking mean operation like opening file accesserror logging potentially cause eventloop stall even aio aiowrite thread pool enabled lead increased tail latency cause multisecond delay spinning disk drive envoy similar eventdriven architecture except us thread instead process also soreuseport support bpf filter support relies libevent event loop implementation word fancy epoll feature like epollexclusive envoy blocking io operation event loop even logging implemented nonblocking way cause stall look like theory nginx envoy similar performance characteristic hope strategy first step run diverse set workload test similarly tuned nginx envoy setup interested performance tuning describe standard tuning guideline optimizing web server high throughput low latency involves everything picking hardware o tunables library choice web server configuration test result showed similar performance nginx envoy test workload high request per second rps high bandwidth mixed lowlatencyhighbandwidth grpc proxying arguably hard make good performance test nginx guideline performance testing codified envoy also guideline benchmarking even tooling envoyperf project sadly latter look unmaintained resorted using internal testing tool called hulk reputation smashing service said couple notable difference result nginx showed higher long tail latency mostly due event loop stall heavy io especially used together soreuseport since case connection accepted behalf currently blocked worker nginx performance without stats collection par envoy lua stats collection slowed nginx highrps test factor expected given reliance luashareddict synchronized across worker mutex understand inefficient stats collection considered implementing something akin freebsd counter userspace cpu pinning perworker lockless counter fetching routine loop worker aggregating individual stats gave idea wanted instrument nginx internals eg error condition would mean supporting enormous patch would make subsequent upgrade true hell since envoy suffer either issue migrating able release server previously exclusively occupied nginx observability observability fundamental operational need product especially foundational piece infrastructure proxy even important migration period issue detected monitoring system rather reported frustrated user noncommercial nginx come stub status module stats copy active connection server accepts handled request reading writing waiting definitely enough added simple logbylua handler add perrequest stats based header variable available lua status code size cache hit etc example simple statsemitting function copy function mcachehitstats stat varupstreamcachestatus varupstreamcachestatus hit stat add upstreamcachehit else stat add upstreamcachemiss end end end addition perrequest lua stats also brittle errorlog parser responsible upstream http lua tl error classification top separate exporter gathering nginx internal state time since last reload number worker rssvms size tl certificate age etc typical envoy setup provides u thousand distinct metric prometheus format describing proxied traffic server internal state copy curl http wc l includes myriad stats different aggregation perclusterperupstreampervhost http stats including connection pool info various timing histogram perlistener tcphttptls downstream connection stats various internalruntime stats basic version info uptime memory allocator stats deprecated feature usage counter special shoutout needed envoy admin interface provide additional structured stats cert cluster configdump endpoint also important operational feature ability change error logging fly logging allowed u troubleshoot fairly obscure problem matter minute cpuprofiler heapprofiler contention would surely quite useful inevitable performance troubleshooting runtimemodify endpoint allows u change set configuration parameter without pushing new configuration could used feature gating etc addition stats envoy also support pluggable tracing provider useful traffic team multiple loadbalancing tier also application developer want track request latency endtoend edge app server technically nginx also support tracing thirdparty opentracing integration heavy development last least envoy ability stream access log grpc remove burden supporting syslogtohive bridge traffic team besides way easier secure spin generic grpc service dropbox production add custom tcpudp listener configuration access logging envoy like everything else happens grpc management service access log service al management service standard way integrating envoy data plane various service production brings u next topic integration nginx approach integration best described unixish configuration static heavily relies file eg config file tl certificate ticket allowlistsblocklists etc wellknown industry protocol logging syslog auth subrequests http simplicity backwards compatibility good thing small setup since nginx easily automated couple shell script system scale increase testability standardization become important envoy far opinionated traffic dataplane integrated control plane hence rest infrastructure encourages use protobufs grpc providing stable api commonly referred xds envoy discovers dynamic resource querying one xds service nowadays xds apis evolving beyond envoy universal data plane api udpa ambitious goal becoming de facto standard loadbalancers experience ambition work well already use open request cost aggregation orca internal load testing considering using udpa nonenvoy loadbalancers eg katranbased ebpfxdp load balancer especially good dropbox service internally already interact grpcbased apis implemented version xds control plane integrates envoy configuration management service discovery secret management route information information dropbox rpc please read courier dropbox migration grpc describe detail integrated service discovery secret management stats tracing circuit breaking etc grpc available xds service nginx alternative example use access log service al mentioned let u dynamically configure access log destination encoding format imagine dynamic version nginx logformat accesslog endpoint discovery service ed provides information cluster member analogous dynamically updated list upstream block server entry eg lua would balancerbyluablock nginx config case proxied internal service discovery secret discovery service sd provides various tlsrelated information would cover various ssl directive respectively ssl byluablock adapted interface secret distribution service runtime discovery service rtds providing runtime flag implementation functionality nginx quite hacky based checking existence various file lua approach quickly become inconsistent individual server envoy default implementation also filesystembased instead pointed rtds xds api distributed configuration storage way control whole cluster tool sysctllike interface accidental inconsistency different server route discovery service rds map route virtual host allows additional configuration header filter nginx term would analogous dynamic location block setheaderproxysetheader proxypass lower proxy tier autogenerate directly service definition configs example envoy integration existing production system canonical example integrate envoy custom service discovery also couple open source envoy controlplane implementation istio le complex gocontrolplane homegrown envoy control plane implement increasing number xds apis deployed normal grpc service production act adapter infrastructure building block set common golang library talk internal service expose stable xds apis envoy whole process involve filesystem call signal cron logrotate syslog log parser etc configuration nginx undeniable advantage simple humanreadable configuration win get lost config get complex begin codegenerated mentioned nginx config generated mix yaml may seen even written variation erb pug text template maybe even copy server server server errorpage servererrorpages errorpage errorpagestatusesjoin errorpagefile endfor route serviceroutes routeregex routeprefix routeexactpath location routeregex routeregex elif routeexactpath routeexactpath else routeprefix endif routebrotlilevel brotli brotlicomplevel routebrotlilevel endif approach nginx config generation huge issue language involved config generation allowed substitution andor logic yaml anchor loopsifsmacroses course python turingcomplete without clean data model complexity quickly spread across three problem arguably fixable couple foundational one declarative description config format wanted programmatically generate validate configuration would need invent config syntactically valid could still invalid c code standpoint example bufferrelated variable limitation value restriction alignment interdependency variable semantically validate config needed run nginx envoy hand unified datamodel configs configuration defined protocol buffer solves data modeling problem also add typing information config value given protobufs first class citizen dropbox production common way describingconfiguring service make integration much easier new config generator envoy based protobufs data modeling done proto file logic python example copy import gzip import compressor def defaultgzipconfig compressionlevel gzipcompressionlevelenum gzipcompressionleveldefault gzip return gzip envoy default zdefaultcompression compressionlevelcompressionlevel envoy default bit nginx us maxwbits bit envoy default nginx us maxmemlevel compressorcompressor removeacceptencodingheadertrue contenttypedefaultcompressiblemimetypes still case typechecked protobuf logically invalid example gzip windowbits take value kind restriction easily defined help protocgenvalidate protoc plugin copy windowbits validaterules lte gte finally implicit benefit using formally defined configuration model organically lead documentation collocated configuration definition example gzipproto copy value control amount internal memory used zlib higher value use memory faster produce better compression result default value memorylevel validaterules lte gte thinking using protobufs production system worried may lack schemaless representation good article envoy core developer harvey tuch work around using googleprotobufstruct googleprotobufany dynamic extensibility protocol buffer extensibility extending nginx beyond possible standard configuration usually requires writing c module nginx development guide provides solid introduction available building block said approach relatively heavyweight practice take fairly senior software engineer safely write nginx module term infrastructure available module developer expect basic container like hash tablesqueuesrbtrees nonraii memory management hook phase request processing also couple external library like pcre zlib openssl course libc lightweight feature extension nginx provides perl javascript interface sadly fairly limited ability mostly restricted content phase request processing commonly used extension method adopted community based thirdparty luanginxmodule various openresty library approach hooked pretty much phase request processing used logbylua stats collection balancerbylua dynamic backend reconfiguration theory nginx provides ability develop module c practice lack proper c interfaceswrappers primitive make worthwhile nonetheless community attempt far ready production though envoy main extension mechanism c plugins process well documented nginx case simpler partially due clean wellcommented interface c class act natural extension documentation point example checkout http filter interface language standard library basic language feature like template lambda function typesafe container algorithm general writing modern much different using golang stretch one may even say python feature beyond stdlib provided abseil library include dropin replacement newer c standard mutexes builtin static deadlock detection debug support additionalmore efficient container much specific canonical example http filter module able integrate envoy monitoring framework line code simply implementing envoy stats interface envoy also lua support moonjit luajit fork improved lua support compared nginx lua integration far fewer capability hook make lua envoy far le attractive due cost additional complexity developing testing troubleshooting interpreted code company specialize lua development may disagree case decided avoid use c exclusively envoy extensibility distinguishes envoy rest web server emerging support webassembly wasm fast portable secure extension mechanism wasm meant used directly compilation target generalpurpose programming language envoy implement webassembly proxy specification also includes reference rust c sdks describes boundary wasm code generic proxy separation proxy extension code allows secure sandboxing wasm lowlevel compact binary format allows near native efficiency top envoy proxywasm extension integrated xds allows dynamic update even potential ab testing extending envoy webassembly presentation kubecon remember time nonvirtual conference nice overview wasm envoy potential us also hint performance level native c code wasm service provider get safe efficient way running customer code edge customer get benefit portability extension run cloud implement proxywasm abi additionally allows user use language long compiled webassembly enables use broader set nonc library securely efficiently istio putting lot resource webassembly development already experimental version wasmbased telemetry extension webassemblyhub community sharing extension read detail redefining extensibility proxy introducing webassembly envoy istio currently use webassembly dropbox might change go sdk proxywasm available building testing default nginx built using custom shellbased configuration system makebased build system simple elegant took quite bit effort integrate bazelbuilt monorepo get benefit incremental distributed hermetic reproducible build google opensourced bazelbuilt nginx version consists nginx boringssl pcre zlib brotli librarymodule testingwise nginx set perldriven integration test separate repository unit test given heavy usage lua absence builtin unit testing framework resorted testing using mock configs simple pythonbased test driver copy class protocolcounterstest nginxtestcase classmethod def setupclass cl super protocolcounterstest cl setupclass clsnginxa clsaddnginx nginxconfigpath endpoint upstream clsstartnginxes assertdelta lambda getstat assertdelta lambda getstat def testhttp self r requestsget selfnginxaendpoint url assert rstatuscode requestscodesok top verify syntaxcorrectness generated configs preprocessing eg replacing ip address one switching selfsigned tl cert etc running nginx c result envoy side main build system already bazel integrating monorepo trivial bazel easily allows adding external dependency also use copybara script sync protobufs envoy udpa copybara handy need simple transformation without need forever maintain large patchset envoy flexibility using either unit test based gtestgmock set prewritten mock envoy integration test framework need anymore rely slow endtoend integration test every trivial change gtest fairly wellknown unittest framework used chromium llvm among others want know googletest good intro googletest googlemock open source envoy development requires change unit test coverage test automatically triggered pull request via azure ci pipeline also common practice microbenchmark performancesensitive code googlebecnhmark copy bazel run compilationmodeopt testcommonupstream loadbalancerbenchmark benchmarkfilter leastrequestloadbalancerchoosehost m m switching envoy began rely exclusively unit test internal module development copy testf courierclientidfiltertest identityparsing struct testcase std vector std string uris identity expected std vector testcase test spiffe proddropboxcomservicefoo spiffe proddropboxcomservicefoo foo spiffe proddropboxcomuserboo spiffe proddropboxcomuserboo userboo spiffe proddropboxcomhoststrange spiffe proddropboxcomhoststrange hoststrange spiffe corpdropboxcomuserbadprefix auto test test expectcall ssl urisanpeercertificate willonce testing return testuris expecteq getidentity ssl testexpected subsecond test roundtrips compounding effect productivity empowers u put effort increasing test coverage able choose unit integration test allows u balance coverage speed cost envoy test bazel one best thing ever happened developer experience steep learning curve large upfront investment high return investment incremental build remote caching distributed buildstests etc one le discussed benefit bazel give u ability query even augment dependency graph programmatic interface dependency graph coupled common build system across language powerful feature used foundational building block thing like linters code generation vulnerability tracking deployment system etc security nginx code surface quite small minimal external dependency typical see external dependency resulting binary zlib one faster variant tl library pcre nginx custom implementation protocol parser event library even went far reimplement libc function point nginx considered secure used default web server openbsd later two development community falling lead creation httpd read motivation behind move bsdcon introducing openbsd new httpd minimalism paid practice nginx vulnerability exposure reported year envoy hand way code especially consider c code far dense basic c used nginx also incorporates million line code external dependency everything event notification protocol parser offloaded party library increase attack surface bloat resulting binary counteract envoy relies heavily modern security practice us addresssanitizer threadsanitizer memorysanitizer developer even went beyond adopted fuzzing opensource project critical global infrastructure accepted free platform automated fuzzing learn see ossfuzz architecture practice though precaution fully counteract increased code footprint result envoy security advisory past year envoy security release policy described great detail postmortem selected vulnerability envoy also participant google vulnerability reward program vrp open security researcher vrp provides reward vulnerability discovered reported according rule practical example vulnerability potentially exploited see writeup exploiting envoy heap vulnerability counteract increased vulnerability risk use best binary hardening security practice upstream o vendor ubuntu debian defined special hardened build profile edgeexposed binary includes aslr stack protector symbol table hardening copy build hardened forcepic build hardened coptfstackclashprotection build hardened coptfstackprotectorstrong build hardened linkoptwl z relro z forking webservers like nginx environment issue stack protector since master worker process share stack canary canary verification failure worker process killed canary bruteforced bitbybit try envoy us thread concurrency primitive affected attack also want harden thirdparty dependency use boringssl fips mode includes startup selftests integrity checking binary also considering running asanenabled binary edge canary server feature come opinionated part post brace nginx began web server specialized serving static file minimal resource consumption functionality top line static serving caching including thundering herd protection range caching proxying side though nginx lack feature needed modern infrastructure backends grpc proxying available without connection multiplexing support grpc transcoding top nginx opencore model restricts feature go open source version proxy result critical feature like statistic available community version envoy contrast evolved ingressegress proxy used frequently grpcheavy environment webserving functionality rudimentary file serving still workinprogress caching neither brotli precompression use case still small fallback nginx setup envoy us upstream cluster http cache envoy becomes productionready could move staticserving use case using instead filesystem longterm storage read ecache design see ecache multibackend http cache envoy envoy also native support many grpcrelated capability grpc proxying basic capability allowed u use grpc endtoend application eg dropbox desktop client backends feature allows u greatly reduce number tcp connection traffic tier reducing memory consumption keepalive traffic grpc http bridge reverse allowed u expose legacy application using modern grpc stack grpcweb feature allowed u use grpc endtoend even environment middleboxes firewall id etc yet support grpc json transcoder enables u transcode inbound traffic including dropbox public apis rest grpc addition envoy also used outbound proxy used unify couple use case egress proxy since envoy added support http connect method used dropin replacement squid proxy begun replace outbound squid installation envoy greatly improves visibility also reduces operational toil unifying stack common dataplane observability parsing log stats thirdparty software service discovery relying courier grpc library software instead using envoy service mesh use envoy oneoff case need connect open source service service discovery minimal effort example envoy used service discovery sidecar analytics stack hadoop dynamically discover name journal node superset discover airflow presto hive backends grafana discover mysql database community nginx development quite centralized development happens behind closed door external activity nginxdevel mailing list occasional developmentrelated discussion official bug tracker nginx channel freenode feel free join interactive community conversation envoy development open decentralized coordinated github issuespull request mailing list community meeting also quite bit community activity slack get invite hard quantify development style engineering community let look specific example development nginx quic implementation recently presented code clean zero external dependency development process rather opaque half year cloudflare came implementation nginx result community two separate experimental version nginx envoy case implementation also work progress based chromium quiche quic http etc library project status tracked github issue design doc publicly available way patch completed remaining work would benefit community involvement tagged help wanted see latter structure much transparent greatly encourages collaboration u mean managed upstream lot small medium change operational improvement performance optimization new grpc transcoding feature load balancing change current state migration running nginx envoy sidebyside half year gradually switching traffic one another dns migrated wide variety workload envoy ingres highthroughput service file data dropbox desktop client served via endtoend grpc envoy switching envoy also slightly improved user performance due better connection reuse edge ingres highrps service file metadata dropbox desktop client get benefit endtoend grpc plus removal connection pool mean bounded one request per connection time notification telemetry service handle realtime notification server million http connection one active client notification service implemented via streaming grpc instead expensive longpoll method mixed highthroughputhighrps service api traffic metadata data allows u start thinking public grpc apis may even switch transcoding existing restbased apis right edge egress highthroughput proxy case dropbox aws communication mostly would allow u eventually remove squid proxy production network leaving u single data plane one last thing migrate would wwwdropboxcom migration start decommissioning edge nginx deployment epoch would pas migration flawless course lead notable outage hardest part migration api service lot different device communicate dropbox public curlwgetpowered shell script embedded device custom stack every possible http library nginx battletested defacto industry standard understandably library implicitly depend behavior along number inconsistency nginx envoy behavior api user depend number bug envoy library quickly resolved upstreamed u community help gist unusual nonrfc behavior also worth mentioning common configuration issue encountered circuitbreaking misconfiguration experience running envoy inbound proxy especially mixed environment improperly set circuit breaker cause unexpected downtime traffic spike backend outage consider relaxing using envoy mesh proxy worth mentioning default circuitbreaking limit envoy pretty tight careful buffering nginx allows request buffering disk especially useful environment legacy backends understand chunked transfer encoding nginx could convert request contentlength buffering disk envoy buffer filter without ability store data disk restricted much buffer memory considering using envoy edge proxy would benefit reading configuring envoy edge proxy security resource limit would want exposed part infrastructure getting closer prime time support added popular browser gated flag commandline option envoy support also experimentally available upgrade linux kernel support udp acceleration experiment edge internal xdsbased load balancer outlier detection currently looking using combination load reporting service lr endpoint discovery service ed building block creating common lookaside loadaware loadbalancer envoy grpc wasmbased envoy extension golang proxywasm sdk available start writing envoy extension go give u access wide variety internal golang libs replacement bandaid unifying dropbox proxy layer single dataplane sound compelling happen need migrate lot bandaid feature especially around loadbalancing envoy long way current plan envoy mobile eventually want look using envoy mobile apps compelling traffic perspective support single stack unified monitoring modern capability grpc etc across mobile platform migration truly team effort traffic runtime team spearheading team heavily contributed agata cieplik jeffrey gensler konstantin belyalov louis opter naphat sanguansin nikita v shirokov utsav shah yishu tai course awesome envoy community helped u throughout journey also want explicitly acknowledge tech lead runtime team ruslan nigmatullin whose action envoy evangelist author envoy mvp main driver software engineering side enabled project happen read far good chance actually enjoy digging deep webserversproxies may enjoy working dropbox traffic team dropbox globally distributed edge network terabit traffic million request per second managed small team mountain view ca
33,Lobsters,scaling,Scaling and architecture,What on Earth Is Static Regeneration?,https://jessesibley.com/what-on-earth-is-static-regeneration,earth static regeneration,vercel nextjs stable incremental static regeneration static generation incremental static generation get better register new static page demo explanation big win jamstack nextjs,vercel announced nextjs today gave u stable incremental static regeneration earth familiar static generation process dynamically generating page build time example ecommerce website might generate static html page product catalog oppose traditional approach using server load product information databasethis nice static html file fast serve additionally static html easily served global cdn resulting quick page load wherever worldthis technique also cause le load database server making much cheaper optionthe major drawback traditional static generation however small change made single page entire site must rebuilt redeployed site thousand article product could take long timeincremental static generationto address issue nextjs contributor working giving static page ability revalidated runtimein practise work revalidate option getstaticprops method nextjs page revalidate option specified instruct page regenerated given intervalwhen user visit date page served stale version traffic site cause page rerendered background meaning next user visit page see latest date versionit get better instead define page build time incrementally rebuilding runtime register new static page runtimethis like automatic serverside rendering rendered page served statically also updated demandhere demo explanation nt believe really crazy yearbig winsin opinion approach static generation defining moment jamstack could make far adoptable beforeyou ca nt beat speed pure htmlif build complex dynamic site simplicity tool like nextjs use anything else
34,Lobsters,scaling,Scaling and architecture,Introduction to architecting systems for scale (2011),https://lethain.com//introduction-to-architecting-systems-for-scale/,introduction architecting system scale,working pain growing product yahoo digg modwsgi rabbitmq redis load balancing smart client hardware load balancer citrix netscaler software load balancer haproxy caching memcache postgresql cassandra application v database caching least recently used caching algorithm inmemory cache memcached redis order magnitude least recently used content distribution network nginx cache invalidation solr offline processing message queue rabbitmq scobleizer scheduling periodic task cron puppet mapreduce mapreduce hadoop hive hbase platform layer,computer science software development program attempt teach building block scalable system instead system architecture usually picked job working pain growing product working engineer already learned suffering process post attempt document scalability architecture lesson learned working system yahoo digg attempted maintain color convention diagram green external request external client http request browser etc blue code running container django app running modwsgi python script listening rabbitmq etc red piece infrastructure mysql redis rabbitmq etc load balancing ideal system increase capacity linearly adding hardware system one machine add another capacity would double three add another capacity would increase let call horizontal scalability failure side ideal system nt disrupted loss server losing server simply decrease system capacity amount increased overall capacity added let call redundancy horizontal scalability redundancy usually achieved via load balancing article wo nt address vertical scalability usually undesirable property large system inevitably point becomes cheaper add capacity form additional machine rather additional resource one machine redundancy vertical scaling odds oneanother load balancing process spreading request across multiple resource according metric random roundrobin random weighting machine capacity etc current status available request responding elevated error rate etc load need balanced user request web server must also balanced every stage achieve full scalability redundancy system moderately large system may balance load three layer user web server web server internal platform layer internal platform layer database number way implement load balancing smart client adding loadbalancing functionality database cache service etc client usually attractive solution developer attractive simplest solution usually seductive robust sadly alluring easy reuse tragically developer lean towards smart client developer used writing software solve problem smart client software caveat mind smart client client take pool service host balance load across detects downed host avoids sending request way also detect recovered host deal adding new host etc making fun get working decently terror setup hardware load balancer high load balancing buy dedicated hardware load balancer something like citrix netscaler solve remarkable range problem hardware solution remarkably expensive also nontrivial configure generally even large company substantial budget often avoid using dedicated hardware loadbalancing need instead use first point contact user request infrastructure use mechanism smart client hybrid approach discussed next section loadbalancing traffic within network software load balancer want avoid pain creating smart client purchasing dedicated hardware excessive universe kind enough provide hybrid software loadbalancers haproxy great example approach run locally box service want loadbalance locally bound port example might platform machine accessible via database readpool database writepool haproxy manages healthchecks remove return machine pool according configuration well balancing across machine pool well system recommend starting software load balancer moving smart client hardware load balancing deliberate need caching load balancing help scale horizontally across everincreasing number server caching enable make vastly better use resource already well making otherwise unattainable product requirement feasible caching consists precalculating result eg number visit referring domain previous day pregenerating expensive index eg suggested story based user click history storing copy frequently accessed data faster backend eg memcache instead postgresql practice caching important earlier development process loadbalancing starting consistent caching strategy save time later also ensures nt optimize access pattern ca nt replicated caching mechanism access pattern performance becomes unimportant addition caching found many heavily optimized cassandra application challenge cleanly add caching ifwhen database caching strategy ca nt applied access pattern datamodel generally inconsistent cassandra cache application v database caching two primary approach caching application caching database caching system rely heavily application caching requires explicit integration application code usually check value cache retrieve value database write value cache value especially common using cache observes least recently used caching algorithm code typically look like specifically readthrough cache read value database cache missing cache key user userid userblob memcacheget key userblob none user mysqlquery select user userid userid user memcacheset key jsondumps user return user else return jsonloads userblob side coin database caching flip database going get level default configuration provide degree caching performance initial setting optimized generic usecase tweaking system access pattern generally squeeze great deal performance improvement beauty database caching application code get faster free talented dba operational engineer uncover quite bit performance without code changing whit colleague rob coli spent time recently optimizing configuration cassandra row cache succcessful extent spent week harassing u graph showing io load dropping dramatically request latency improving substantially well inmemory cache term raw encounter store entire set data memory memcached redis example inmemory cache caveat redis configured store data disk access ram order magnitude faster disk hand generally far le ram available disk space need strategy keeping hot subset data memory cache straightforward strategy least recently used employed memcache redis configured employ well lru work evicting le commonly used data preference frequently used data almost always appropriate caching strategy content distribution network particular kind cache might argue usage term find fitting come play site serving large amount static medium content distribution network cdns take burden serving static medium application server typically optimzed serving dynamic page rather static medium provide geographic distribution overall static asset load quickly le strain server new strain business expense typical cdn setup request first ask cdn piece static medium cdn serve content locally available http header used configuring cdn cache given piece content nt available cdn query server file cache locally serve requesting user configuration acting readthrough cache site nt yet large enough merit cdn ease future transition serving static medium separate subdomain eg staticexamplecom using lightweight http server like nginx cutover dns server cdn later date cache invalidation caching fantastic require maintain consistency cache source truth ie database risk truly bizarre applicaiton behavior solving problem known cache invalidation dealing single datacenter tends straightforward problem easy introduce error multiple codepaths writing database cache almost always going happen nt go writing application caching strategy already mind high level solution time value change write new value cache called writethrough cache simply delete current value cache allow readthrough cache populate later choosing read write cache depends application detail generally prefer writethrough cache reduce likelihood stampede backend database invalidation becomes meaningfully challenging scenario involving fuzzy query eg trying add application level caching infront fulltext search engine like solr modification unknown number element eg deleting object created week ago scenario consider relying fully database caching adding aggressive expiration cached data reworking application logic avoid issue eg instead delete retrieve item match criterion invalidate corresponding cache row delete row primary key explicitly offline processing system grows complex almost always necessary perform processing ca nt performed inline client request either creates unacceptable latency eg want want propagate user action across social graph need occur periodically eg want create daily rollups analytics message queue processing like perform inline request slow easiest solution create message queue example rabbitmq message queue allow web application quickly publish message queue consumer process perform processing outside scope timeline client request dividing work offline work handled consumer inline work done web application depends entirely interface exposing user generally either perform almost work consumer merely scheduling task inform user task occur offline usually polling mechanism update interface task complete example provisioning new vm slicehost follows pattern perform enough work inline make appear user task completed tie hanging end afterwards posting message twitter facebook likely follow pattern updating tweetmessage timeline updating follower timeline band simple nt feasible update follower scobleizer realtime message queue another benefit allow create separate machine pool performing offline processing rather burdening web application server allows target increase resource current performance throughput bottleneck rather uniformly increasing resource across bottleneck nonbottleneck system scheduling periodic task almost large system require daily hourly task unfortunately seems still problem waiting widely accepted solution easily support redundancy meantime probably still stuck cron could use cronjobs publish message consumer would mean cron machine responsible scheduling rather needing perform processing anyone know recognized tool solve problem seen many homebrew system nothing clean reusable sure store cronjobs puppet config machine make recovering losing machine easy would still require manual recovery likely acceptable perfect mapreduce large scale application dealing large quantity data point likely add support mapreduce probably using hadoop maybe hive hbase adding mapreduce layer make possible perform data andor processing intensive operation reasonable amount time might use calculating suggested user social graph generating analytics report sufficiently small system often get away adhoc query sql database approach may scale trivially quantity data stored writeload requires sharding database usually require dedicated slave purpose performing query point maybe rather use system designed analyzing large quantity data rather fighting database platform layer application start web application communicating directly database approach tends sufficient application compelling reason adding platform layer web application communicate platform layer turn communicates database first separating platform web application allow scale piece independently add new api add platform server without adding unnecessary capacity web application tier generally specializing server role open additional level configuration optimization nt available general purpose machine database machine usually high io load benefit solidstate drive wellconfigured application server probably nt reading disk normal operation might benefit cpu second adding platform layer way reuse infrastructure multiple product interface web application api iphone app etc without writing much redundant boilerplate code dealing cache database etc third sometimes underappreciated aspect platform layer make easier scale organization best platform expose crisp productagnostic interface mask implementation detail done well allows multiple independent team develop utilizing platform capability well another team implementingoptimizing platform intended go moderate detail handling multiple datacenters topic truly deserves post mention cache invalidation data replicationconsistency become rather interesting problem stage sure made controversial statement post hope dear reader argue learn bit thanks reading
35,Lobsters,scaling,Scaling and architecture,Graceful Degradation,https://www.solipsys.co.uk/new/GracefulDegradation.html?tg14lb,graceful degradation,graceful degradation software contact comment,graceful degradation first learned graceful degradation colleague prefaced story saying good people learn mistake best people learn people mistake bit like saying aviation circle good landing one walk away excellent landing use plane digress told learned graceful degradation message clear telling mistake could learn wish people willing make mistake public software industry would certainly benefit could learn everyone mistake story go broad outline forget detail like made comfortable living programming electronic till time programmed assembly language best output could hope debugging see whether opened till difficult work simulator higher level language debugger singlesteppers help really wrote code assembler transferred machine tested see worked sometimes colleague amazing work could get contract time wanted money good worked six month year whatever wanted rest time except christmas one biggest mistake would come back haunt large department store major capital city used till ran program one sophisticated system around till would keep list item sold central computer would interrogate till turn find much sold item would keep track many remained stock would project reordering would necessary reordering automatic staff would alerted stock low item decision could made loved system buffer stock could reduced reducing store overhead making entire store responsive consumer demand except christmas see every christmas system essence would simply stop entire floor till would stop responding refusing accept purchase unpredictable length time suddenly start working apparent reason apparent rhyme nothing could except call colleague get come fix poblem really nothing could either even worked problem problem capacity till would keep list item sold asked would dump list central computer would go back patronising servicing customer requirement asked problem heaviest time year simply much download queue till would fill central computer got back would stop wait download data could restart entire floor till would grind halt waiting download data restart floor would simply stop good busiest shopping time year really good patch run christmas make till run slightly slow still occasional full stopping moment general system would limp along exhibit catastrophic failure entire floor stopping difficulty judging exactly slow make till run also made central computer contact till clever pseudorandom way meant one till stopped floor others would probably still working done well apart faster communication system could deal load placed till could notice queue full start slowing performance system whole would degrade gracefully sale would slow slightly evenly across store come match processing throughput course would better system simply faster overloaded degrading gracefully usually better option simply stopping think next time worry system capacity better halt clear backlog degrade gracefully continue serve customer comment decided longer include comment directly via disqus system instead delighted get email people wish make comment engage discussion comment integrated page appropriate number emailscomments get large handle might return semiautomated system looking increasingly unlikely
36,Lobsters,scaling,Scaling and architecture,The architecture of nginx (2012),http://www.aosabook.org/en/nginx.html,architecture nginx,high concurrency important nt apache suitable proclaimed advantage using nginx overview nginx architecture code structure section figure worker model nginx process role brief overview nginx caching nginx configuration directive nginx internals spdy experimental protocol faster web,nginx pronounced engine x free open source web server written igor sysoev russian software engineer since public launch nginx focused high performance high concurrency low memory usage additional feature top web server functionality like load balancing caching access bandwidth control ability integrate efficiently variety application helped make nginx good choice modern website architecture currently nginx second popular open source web server internet high concurrency important day internet widespread ubiquitous hard imagine nt exactly know decade ago greatly evolved simple html producing clickable text based ncsa apache web server alwayson communication medium used billion user worldwide proliferation permanently connected pc mobile device recently tablet internet landscape rapidly changing entire economy become digitally wired online service become much elaborate clear bias towards instantly available live information entertainment security aspect running online business also significantly changed accordingly website much complex generally require lot engineering effort robust scalable one biggest challenge website architect always concurrency since beginning web service level concurrency continuously growing uncommon popular website serve hundred thousand even million simultaneous user decade ago major cause concurrency slow adsl dialup connection nowadays concurrency caused combination mobile client newer application architecture typically based maintaining persistent connection allows client updated news tweet friend feed another important factor contributing increased concurrency changed behavior modern browser open four six simultaneous connection website improve page load speed illustrate problem slow client imagine simple apachebased web server produce relatively short kb web page text image merely fraction second generate retrieve page take second transmit client bandwidth kbps kb essentially web server would relatively quickly pull kb content would busy second slowly sending content client freeing connection imagine simultaneously connected client requested similar content mb additional memory allocated per client would result mb gb extra memory devoted serving client kb content reality typical web server based apache commonly allocates mb additional memory per connection regrettably ten kbps still often effective speed mobile communication although situation sending content slow client might extent improved increasing size operating system kernel socket buffer general solution problem undesirable side effect persistent connection problem handling concurrency even pronounced avoid latency associated establishing new http connection client would stay connected connected client certain amount memory allocated web server consequently handle increased workload associated growing audience hence higher level able continuously website based number efficient building block part equation hardware cpu memory disk network capacity application data storage architecture obviously important web server software client connection accepted processed thus web server able scale nonlinearly growing number simultaneous connection request per second nt apache suitable apache web server software still largely dominates internet today root beginning originally architecture matched thenexisting operating system hardware also state internet website typically standalone physical server running single instance apache beginning obvious standalone web server model could easily replicated satisfy need growing web service although apache provided solid foundation future development architected spawn copy new connection suitable nonlinear scalability website eventually apache became general purpose web server focusing many different feature variety thirdparty extension universal applicability practically kind web application development however nothing come without price downside rich universal combination tool single piece software le scalability increased cpu memory usage per connection thus server hardware operating system network resource ceased major constraint website growth web developer worldwide started look around efficient mean running web server around ten year ago daniel kegel prominent software engineer proclaimed time web server handle ten thousand client simultaneously predicted call internet cloud service kegel manifest spurred number attempt solve problem web server optimization handle large number client time nginx turned one successful one aimed solving problem simultaneous connection nginx written different architecture much suitable nonlinear scalability number simultaneous connection request per second nginx eventbased follow apache style spawning new process thread web page request end result even load increase memory cpu usage remain manageable nginx deliver ten thousand concurrent connection server typical hardware first version nginx released meant deployed alongside apache static content like html cs javascript image handled nginx offload concurrency latency processing apachebased application server course development nginx added integration application use fastcgi uswgi scgi protocol distributed memory object caching system like memcached useful functionality like reverse proxy load balancing caching added well additional feature shaped nginx efficient combination tool build scalable web infrastructure upon february apache branch released public although latest release apache added new multiprocessing core module new proxy module aimed enhancing scalability performance soon tell performance concurrency resource utilization par better pure eventdriven web server would nice see apache application server scale better new version though could potentially alleviate bottleneck backend side still often remain unsolved typical nginxplusapache web configuration advantage using nginx handling high concurrency high performance efficiency always key benefit deploying nginx however even interesting benefit last year web architect embraced idea decoupling separating application infrastructure web server however would previously exist form lamp linux apache mysql php python perl based website might become merely lempbased one e standing engine x often exercise pushing web server edge infrastructure integrating revamped set application database tool around different way nginx well suited provides key feature necessary conveniently offload concurrency latency processing ssl secure socket layer static content compression caching connection request throttling even http medium streaming application layer much efficient edge web server layer also allows integrating directly memcachedredis nosql solution boost performance serving large number concurrent user recent flavor development kit programming language gaining wide use company changing application development deployment habit nginx become one important component changing paradigm already helped many company start develop web service quickly within budget first line nginx written released public twoclause bsd license number nginx user growing ever since contributing idea submitting bug report suggestion observation immensely helpful beneficial entire community nginx codebase original written entirely scratch c programming language nginx ported many architecture operating system including linux freebsd solaris mac o x aix microsoft window nginx library standard module use much beyond system c library except zlib pcre openssl optionally excluded build needed potential license conflict word window version nginx nginx work window environment window version nginx like proofofconcept rather fully functional port certain limitation nginx window kernel architecture interact well time known issue nginx version window include much lower number concurrent connection decreased performance caching bandwidth policing future version nginx window match mainstream functionality closely overview nginx architecture traditional process threadbased model handling concurrent connection involve handling connection separate process thread blocking network inputoutput operation depending application inefficient term memory cpu consumption spawning separate process thread requires preparation new runtime environment including allocation heap stack memory creation new execution context additional cpu time also spent creating item eventually lead poor performance due thread thrashing excessive context switching complication manifest older web server architecture like apache tradeoff offering rich set generally applicable feature optimized usage server resource beginning nginx meant specialized tool achieve performance density economical use server resource enabling dynamic growth website followed different model actually inspired ongoing development advanced eventbased mechanism variety operating system resulted modular eventdriven asynchronous singlethreaded nonblocking architecture became foundation nginx code nginx us multiplexing event notification heavily dedicates specific task separate process connection processed highly efficient runloop limited number singlethreaded process called worker within worker nginx handle many thousand concurrent connection request per second code structure nginx worker code includes core functional module core nginx responsible maintaining tight runloop executing appropriate section module code stage request processing module constitute presentation application layer functionality module read write network storage transform content outbound filtering apply serverside include action pas request upstream server proxying activated nginx modular architecture generally allows developer extend set web server feature without modifying nginx core nginx module come slightly different incarnation namely core module event module phase handler protocol variable handler filter upstreams load balancer time nginx nt support dynamically loaded module ie module compiled along core build stage however support loadable module abi planned future major release detailed information role different module found section handling variety action associated accepting processing managing network connection content retrieval nginx us event notification mechanism number disk io performance enhancement linux solaris bsdbased operating system like kqueue epoll event port goal provide many hint operating system possible regard obtaining timely asynchronous feedback inbound outbound traffic disk operation reading writing socket timeouts usage different method multiplexing advanced io operation heavily optimized every unixbased operating system nginx run highlevel overview nginx architecture presented figure figure diagram nginx architecture worker model previously mentioned nginx nt spawn process thread every connection instead worker process accept new request shared listen socket execute highly efficient runloop inside worker process thousand connection per worker specialized arbitration distribution connection worker nginx work done o kernel mechanism upon startup initial set listening socket created worker continuously accept read write socket processing http request response runloop complicated part nginx worker code includes comprehensive inner call relies heavily idea asynchronous task handling asynchronous operation implemented modularity event notification extensive use callback function finetuned timer overall key principle nonblocking possible situation nginx still block enough disk storage performance worker process nginx fork process thread per connection memory usage conservative extremely efficient vast majority case nginx conserve cpu cycle well ongoing createdestroy pattern process thread nginx check state network storage initialize new connection add runloop process asynchronously completion point connection deallocated removed runloop combined careful use syscalls accurate implementation supporting interface like pool slab memory allocator nginx typically achieves moderatetolow cpu usage even extreme workload nginx spawn several worker handle connection scale well across multiple core generally separate worker per core allows full utilization multicore architecture prevents thread thrashing lockup resource starvation resource controlling mechanism isolated within singlethreaded worker process model also allows scalability across physical storage device facilitates disk utilization avoids blocking disk io result server resource utilized efficiently workload shared across several worker disk use cpu load pattern number nginx worker adjusted rule somewhat basic system administrator try couple configuration workload general recommendation might following load pattern cpu instance handling lot tcpip ssl number nginx worker match number cpu core load mostly disk io instance serving different set content storage heavy number worker might one half two time number core engineer choose number worker based number individual storage unit instead though efficiency approach depends type configuration disk storage one major problem developer nginx solving upcoming version avoid blocking disk io moment enough storage performance serve disk operation generated particular worker worker may still block readingwriting disk number mechanism configuration file directive exist mitigate disk io blocking scenario notably combination option like sendfile aio typically produce lot headroom disk performance nginx installation planned based data set amount memory available nginx underlying storage architecture another problem existing worker model related limited support embedded scripting one standard nginx distribution embedding perl script supported simple explanation key problem possibility embedded script block operation exit unexpectedly type behavior would immediately lead situation worker hung affecting many thousand connection work planned make embedded scripting nginx simpler reliable suitable broader range application nginx process role nginx run several process memory single master process several worker process also couple special purpose process specifically cache loader cache manager process singlethreaded version nginx process primarily use sharedmemory mechanism interprocess communication master process run root user cache loader cache manager worker run unprivileged user master process responsible following task reading validating configuration creating binding closing socket starting terminating maintaining configured number worker process reconfiguring without service interruption controlling nonstop binary upgrade starting new binary rolling back necessary reopening log file compiling embedded perl script worker process accept handle process connection client provide reverse proxying filtering functionality almost everything else nginx capable regard monitoring behavior nginx instance system administrator keep eye worker process reflecting actual daytoday operation web server cache loader process responsible checking ondisk cache item populating nginx inmemory database cache metadata essentially cache loader prepares nginx instance work file already stored disk specially allocated directory structure traverse directory check cache content metadata update relevant entry shared memory exit everything clean ready use cache manager mostly responsible cache expiration invalidation stay memory normal nginx operation restarted master process case failure brief overview nginx caching caching nginx implemented form hierarchical data storage filesystem cache key configurable different requestspecific parameter used control get cache cache key cache metadata stored shared memory segment cache loader cache manager worker access currently inmemory caching file optimization implied operating system virtual filesystem mechanism cached response placed different file filesystem hierarchy level naming detail controlled nginx configuration directive response written cache directory structure path name file derived hash proxy url process placing content cache follows nginx read response upstream server content first written temporary file outside cache directory structure nginx finish processing request renames temporary file move cache directory temporary file directory proxying another file system file copied thus recommended keep temporary cache directory file system also quite safe delete file cache directory structure need explicitly purged thirdparty extension nginx make possible control cached content remotely work planned integrate functionality main distribution nginx configuration nginx configuration system inspired igor sysoev experience apache main insight scalable configuration system essential web server main scaling problem encountered maintaining large complicated configuration lot virtual server directory location datasets relatively big web setup nightmare done properly application level system engineer result nginx configuration designed simplify daytoday operation provide easy mean expansion web server configuration nginx configuration kept number plain text file typically reside usrlocaletcnginx etcnginx main configuration file usually called nginxconf keep uncluttered part configuration put separate file automatically included main one however noted nginx currently support apachestyle distributed configuration ie htaccess file configuration relevant nginx web server behavior reside centralized set configuration file configuration file initially read verified master process compiled readonly form nginx configuration available worker process forked master process configuration structure automatically shared usual virtual memory management mechanism nginx configuration several different context main http server upstream location also mail mail proxy block directive context never overlap instance thing putting location block main block directive also avoid unnecessary ambiguity nt anything like global web server configuration nginx configuration meant clean logical allowing user maintain complicated configuration file comprise thousand directive private conversation sysoev said location directory block global server configuration feature never liked apache reason never implemented nginx configuration syntax formatting definition follow socalled cstyle convention particular approach making configuration file already used variety open source commercial software application design cstyle configuration wellsuited nested description logical easy create read maintain liked many engineer cstyle configuration nginx also easily automated nginx directive resemble certain part apache configuration setting nginx instance quite different experience instance rewrite rule supported nginx though would require administrator manually adapt legacy apache rewrite configuration match nginx style implementation rewrite engine differs general nginx setting also provide support several original mechanism useful part lean web server configuration make sense briefly mention variable tryfiles directive somewhat unique nginx variable nginx developed provide additional evenmorepowerful mechanism control runtime configuration web server variable optimized quick evaluation internally precompiled index evaluation done demand ie value variable typically calculated cached lifetime particular request variable used different configuration directive providing additional flexibility describing conditional request processing behavior tryfiles directive initially meant gradually replace conditional configuration statement proper way designed quickly efficiently trymatch different uritocontent mapping overall tryfiles directive work well extremely efficient useful recommended reader thoroughly check tryfiles directive adopt use whenever applicable nginx internals mentioned nginx codebase consists core number module core nginx responsible providing foundation web server web mail reverse proxy functionality enables use underlying network protocol build necessary runtime environment ensures seamless interaction different module however protocol applicationspecific feature done nginx module core internally nginx process connection pipeline chain module word every operation module relevant work eg compression modifying content executing serverside includes communicating upstream application server fastcgi uwsgi protocol talking memcached couple nginx module sit somewhere core real functional module module http mail two module provide additional level abstraction core lowerlevel component module handling sequence event associated respective application layer protocol like http smtp imap implemented combination nginx core upperlevel module responsible maintaining right order call respective functional module http protocol currently implemented part http module plan separate functional module future due need support protocol like spdy see spdy experimental protocol faster web functional module divided event module phase handler output filter variable handler protocol upstreams load balancer module complement http functionality nginx though event module protocol also used mail event module provide particular osdependent event notification mechanism like kqueue epoll event module nginx us depends operating system capability build configuration protocol module allow nginx communicate http tlsssl smtp imap typical http request processing cycle look like following client sends http request nginx core chooses appropriate phase handler based configured location matching request configured load balancer pick upstream server proxying phase handler job pass output buffer first filter first filter pass output second filter second filter pass output third final response sent client nginx module invocation extremely customizable performed series callback using pointer executable function however downside may place big burden programmer would like write module must define exactly module run nginx api developer documentation improved made available alleviate example module attach configuration file read processed configuration directive location server appears main configuration initialized server ie hostport initialized server configuration merged main configuration location configuration initialized merged parent server configuration master process start exit new worker process start exit handling request filtering response header body picking initiating reinitiating request upstream server processing response upstream server finishing interaction upstream server inside worker sequence action leading runloop response generated look like following begin ngxworkerprocesscycle process event o specific mechanism epoll kqueue accept event dispatch relevant action processproxy request header body generate response content header body stream client finalize request reinitialize timer event runloop step ensures incremental generation response streaming client detailed view processing http request might look like initialize request processing process header process body call associated handler run processing phase brings u phase nginx handle http request pass number processing phase phase handler call general phase handler process request produce relevant output phase handler attached location defined configuration file phase handler typically four thing get location configuration generate appropriate response send header send body handler one argument specific structure describing request request structure lot useful information client request request method uri header http request header read nginx lookup associated virtual server configuration virtual server found request go six phase server rewrite phase location phase location rewrite phase bring request back previous phase access control phase tryfiles phase log phase attempt generate necessary content response request nginx pass request suitable content handler depending exact location configuration nginx may try socalled unconditional handler first like perl proxypass flv etc request match content handler picked one following handler exact order random index index autoindex gzipstatic static indexing module detail found nginx documentation module handle request trailing slash specialized module like autoindex nt appropriate content considered file directory disk static served static content handler directory would automatically rewrite uri trailing slash always issue http redirect content handler content passed filter filter also attached location several filter configured location filter task manipulating output produced handler order filter execution determined compile time outofthebox filter predefined thirdparty filter configured build stage existing nginx implementation filter outbound change currently mechanism write attach filter input content transformation input filtering appear future version nginx filter follow particular design pattern filter get called start working call next filter final filter chain called nginx finalizes response filter nt wait previous filter finish next filter chain start work soon input previous one available functionally much like unix pipeline turn output response generated passed client entire response upstream server received header filter body filter nginx feed header body response associated filter separately header filter consists three basic step decide whether operate response operate response call next filter body filter transform generated content example body filter include serverside includes xslt filtering image filtering instance resizing image fly charset modification gzip compression chunked encoding filter chain response passed writer along writer couple additional special purpose filter namely copy filter postpone filter copy filter responsible filling memory buffer relevant response content might stored proxy temporary directory postpone filter used subrequests subrequests important mechanism requestresponse processing subrequests also one powerful aspect nginx subrequests nginx return result different url one client originally requested web framework call internal redirect however nginx go filter perform multiple subrequests combine output single response subrequests also nested hierarchical subrequest perform subsubrequest subsubrequest initiate subsubsubrequests subrequests map file hard disk handler upstream server subrequests useful inserting additional content based data original response example ssi serverside include module us filter parse content returned document replaces include directive content specified url example making filter treat entire content document url retrieved appends new document url upstream load balancer also worth describing briefly upstreams used implement identified content handler reverse proxy proxypass handler upstream module mostly prepare request sent upstream server backend receive response upstream server call output filter upstream module exactly set callback invoked upstream server ready written read callback implementing following functionality exist crafting request buffer chain sent upstream server reinitializingresetting connection upstream server happens right creating request processing first bit upstream response saving pointer payload received upstream server aborting request happens client terminates prematurely finalizing request nginx finish reading upstream server trimming response body eg removing trailer load balancer module attach proxypass handler provide ability choose upstream server one upstream server eligible load balancer register enabling configuration file directive provides additional upstream initialization function resolve upstream name dns etc initializes connection structure decides route request update stats information currently nginx support two standard discipline load balancing upstream server roundrobin iphash upstream load balancing handling mechanism include algorithm detect failed upstream server reroute new request remaining lot additional work planned enhance functionality general work load balancer planned next version nginx mechanism distributing load across different upstream server well health check greatly improved also couple interesting module provide additional set variable use configuration file variable nginx created updated across different module two module entirely dedicated variable geo map geo module used facilitate tracking client based ip address module create arbitrary variable depend client ip address module map allows creation variable variable essentially providing ability flexible mapping hostnames runtime variable kind module may called variable handler memory allocation mechanism implemented inside single nginx worker extent inspired apache highlevel description nginx memory management would following connection necessary memory buffer dynamically allocated linked used storing manipulating header body request response freed upon connection release important note nginx try avoid copying data memory much possible data passed along pointer value calling memcpy going bit deeper response generated module retrieved content put memory buffer added buffer chain link subsequent processing work buffer chain link well buffer chain quite complicated nginx several processing scenario differ depending module type instance quite tricky manage buffer precisely implementing body filter module module operate one buffer chain link time must decide whether overwrite input buffer replace buffer newly allocated buffer insert new buffer buffer question complicate thing sometimes module receive several buffer incomplete buffer chain must operate however time nginx provides lowlevel api manipulating buffer chain actual implementation thirdparty module developer become really fluent arcane part nginx note approach memory buffer allocated entire life connection thus longlived connection extra memory kept time idle keepalive connection nginx spends byte memory possible optimization future release nginx would reuse share memory buffer longlived connection task managing memory allocation done nginx pool allocator shared memory area used accept mutex cache metadata ssl session cache information associated bandwidth policing management limit slab allocator implemented nginx manage shared memory allocation allow simultaneous safe use shared memory number locking mechanism available mutexes semaphore order organize complex data structure nginx also provides redblack tree implementation redblack tree used keep cache metadata shared memory track nonregex location definition couple task unfortunately never described consistent simple manner making job developing thirdparty extension nginx quite complicated although good document nginx internals instance produced evan document required huge reverse engineering effort implementation nginx module still black art many despite certain difficulty associated thirdparty module development nginx user community recently saw lot useful thirdparty module instance embedded lua interpreter module nginx additional module load balancing full webdav support advanced cache control interesting thirdparty work author chapter encourage support future
37,Lobsters,scaling,Scaling and architecture,"A Terrible, Horrible, No-Good, Very Bad Day at Slack",https://slack.engineering/a-terrible-horrible-no-good-very-bad-day-at-slack-dfe05b485f82,terrible horrible nogood bad day slack,hand deck published summary incident impact http consultemplate figure highlevel view slack ingres loadbalancing architecture haproxy runtime api integrate figure set webapp backends managed single slack haproxy server server template figure slot haproxy process excess webapp instance receiving traffic figure haproxy state grown stale time reference mainly deprovisioned host envoy proxy,story describes technical detail problem caused slack downtime may learn process behind incident response outage read ryan katkov post hand deck may slack first significant outage long time published summary incident shortly story interesting one like go detail technical issue around uservisible outage began pacific time story really begin around morning database reliability engineering team alerted significant load increase part database infrastructure time traffic team received alert failing api request increased load database due rollout configuration change triggered longstanding performance bug change quickly pinpointed rolled back feature flag performed percentagebased rollout fast process customer impact lasted three minute user still able send message successfully throughout brief morning incident one incident effect significant scaleup main webapp tier ceo stewart butterfield written impact lockdown stayathome order slack usage result pandemic running significantly higher number instance webapp tier longago day february autoscale quickly worker become saturated happened worker waiting much longer database request complete leading higher utilization increased instance count incident ending highest number webapp host ever run date everything seemed fine next eight hour alerted serving http error normal spun new incident response channel oncall engineer webapp tier manually scaled webapp fleet initial mitigation unusually help quickly noticed subset webapp fleet heavy load rest webapp instance multiple strand investigation began looking webapp performance loadbalancer tier minute later identified problem use fleet haproxy instance behind layer loadbalancer distribute request webapp tier use consul service discovery consultemplate render list healthy webapp backends haproxy route request figure highlevel view slack ingres loadbalancing architecture render webapp host list directly haproxy configuration file however reason updating host list via configuration file requires reloading haproxy process reloading haproxy involves creating brandnew haproxy process keeping old one around finished dealing inflight request frequent reloads could lead many running haproxy process poor performance constraint tension goal autoscaling webapp tier get new instance service quickly possible therefore use haproxy runtime api manipulate haproxy server state without reload time web tier backend come go service worth noting haproxy integrate consul dns interface add lag due dns ttl limit ability use consul tag managing large dns response often seems lead hitting painful edgecases bug figure set webapp backends managed single slack haproxy server define haproxy server template haproxy state effectively slot webapp backends occupy new webapp instance provisioned old one becomes unhealthy consul service catalog updated consultemplate render new version host list separate program developed slack haproxyserverstatemanagement read host list us haproxy runtime api update haproxy state run parallel pool haproxy instance webapp instance pool aws availability zone haproxy configured n slot webapp backends az giving total n backends routed across az month ago total ample headroom never needed run anything even approaching number instance webapp tier however morning database incident running slightly n instance webapp think haproxy slot giant game musical chair webapp instance left without seat problem enough serving capacity figure slot haproxy process excess webapp instance receiving traffic however course day problem developed program synced host list generated consul template haproxy server state bug always attempted find slot new webapp instance freed slot taken old webapp instance longer running program began fail exit early unable find empty slot meaning running haproxy instance getting state updated day passed webapp autoscaling group scaled list backends haproxy state became stale pacific haproxy instance able send request set webapp backends since morning set older webapp backends minority fleet regularly provision new haproxy instance would fresh haproxy instance correct configuration eight hour old therefore stuck full stale backend state outage eventually triggered end business day u begin scale webapp tier traffic drop autoscaling preferentially terminate older instance meant longer enough older webapp instance remaining haproxy server state serve demand figure haproxy state grown stale time reference mainly deprovisioned host knew cause failure resolved quickly rolling restart haproxy fleet incident mitigated first question asked monitoring catch problem alerting place precise situation unfortunately working intendedthe broken monitoring noticed partly system worked long time require change wider haproxy deployment part also relatively static low rate change fewer engineer interacting monitoring alerting infrastructure reason significant work haproxy stack moving towards envoy proxy ingres loadbalancing recently moved websockets traffic onto envoy haproxy served u well reliably many year also operational sharp edge exactly kind highlighted incident complex pipeline use manipulate haproxy server state replaced envoy native integration xds control plane endpoint discovery recent version haproxy since release also solve many operational pain point however envoy proxy choice internal service mesh project time make move envoy ingres loadbalancing attractive initial testing envoy xds scale exciting migration improve performance availability going forward new loadbalancing service discovery architecture susceptible problem caused outage strive keep slack available reliable case failed know slack critical tool user aim learn much every incident whether customer visible apologize inconvenience outage caused continue use lesson learned drive improvement system process
39,Lobsters,scaling,Scaling and architecture,What is a High Traffic Website?,https://theartofmachinery.com/2020/04/21/what_is_high_traffic.html,high traffic website,scalability v performance internal cache versus external cache server response time webpage tend expand complexity looking single worker request arrive random balance nicely want keep response time control target usage theoretical total capacity three traffic level going level scaling scaling problem thing scale boring relational database many major website much different,term like hazardous designing online service salesperson business analyst engineer different perspective mean talking say highstakes online poker room business side low compared technical side however people meeting room together making decision using word mean different thing obvious lead bad sometimes expensive choice lot day job talking business stakeholder figuring technical solution need problem deal got purely technical way think traffic level online service scalability v performance first clear two concept come lot online service design online service performance well usually fast system handle single request unit work scalability volume size work handled online service scalability usually number user request handled within timeframe batch job typically care size dataset process sometimes want system capacity grow shrink based demand sometimes care long handle full range workload expect scalability performance often get confused commonly work together example suppose online service using slow algorithm improve perfomance replacing algorithm another job le work primarily performance gain long new algorithm use memory something able handle request time counterexample discussed internal cache versus external cache server using external cache server like redis mean app make network call cache lookup performance overhead hand app replicated across multiple machine shared external cache effective perapp inmemory cache external cache scalable response time designing system helpful start thinking latency response time requirement even make revise later adding ram cache machine disk solve lot problem latency problem tend fundamental system design example suppose designing online game want latency user straight away speed light limit mean one central server supporting global game regardless whatever algorithm hardware throw problem another reason useful focus server response time practice simple singlefunction website mortgage calculator response time estimated based technical thing like hardware spec code quality typical online service built online service industry tends emphasise adding feature le development cost mean webpage tend expand complexity using easiest code possible get optimised become slow user churn even typical mortgage calculator site end bloated advertising tracking functionality response time website day job depends mostly budget priority technical factor regardless whether ecommerce site cuttingedge data application looking single worker okay imagine simple web app get one request hour take process ignoring static asset bottleneck app obvious performance problem many user give scaling problem server practically never hit capacity limit drop request even traffic rise performance problem bottleneck take priority hypothetical scalability problem simple insight take let say target per request simple web app process request one time serially ie scaling second day calculation say handle request per day scaling problem course real world simple first slower faster request request arrive random balance nicely come cluster fill queue backlog cause large spike response time handy rule say want keep response time control target usage theoretical total capacity diurnal variation local website get nearly traffic business hour third day even global website easily time traffic peak trough population distributed evenly around world lot internet user live east asia northsouth america huge pacific ocean actual ratio depends many factor hard predict even easily get exact capacity estimate simple model justification splitting website three traffic level three traffic level first level site get well dynamic request day website level lot stay way totally useful successful also pose complex technical challenge performance functionality lot work site size true scalability problem opposed problem solved purely improving performance website get bigger get level roughly around dynamic request day scalability problem start appear often site least bit scalable thanks eg async multithreaded programming web developer scaling site level keep discovering new surprise pain point thing smaller site get away start turning real problem level biggest challenge actually sociotechnical team build manages site needing learn think new way next level leaving dynamic request day boundary behind think site level high traffic let stress technical line value judgment ego statement biggest website order magnitude bigger useful website smaller line matter simply run site scale without treating like high traffic site get away low traffic level fumble growing pain level high traffic level work differently coincidentally around traffic level make sense talk request per second request per day way focus much exact traffic level rough honestly picked convenient round number happen reasonable typical website real value depend target response time factor course trying explain level exist exist expect trying grow online service going level happens site get even bigger problem one set bottleneck fixed site scale hit new set bottleneck either application changed significantly large increase traffic example application server scalable next scaling bottleneck could database read followed database writes however basic idea applied different part system working hightraffic site lot le working plain hightraffic site simply major problem need solved get high traffic level first place scaling scaling problem developer try make online service scalable long scalability problem horizon usually adding exotic database broker server something particular startup founder often especially concerned technical asset might scale meet business ambition understandable dangerous trap couple reason one paul classic thing scale essay applies technology stack beat bigger company scale competitive advantage choose solve scalability problem bigger company forced every step take make smaller company agile startup worry much scalability big enterprise without big back problem premature scalability solution easily backfire real scalability problem test solution hard sure correctly solving real problem fact rapidly growing service tend change requirement rapidly risk scalability turning technical debt high keep trying add scalability part system already scalable enough chance next scaling bottleneck appear somewhere else anyway architecture err side simple easier scale long run architecture complex concrete personally think lowtraffic online service worked implemented cleanly enough using simple monolithic web app whatever popular language front boring relational database maybe search index like xapian elasticsearch many major website much different valid architecture triedandtested one said sometimes lowtraffic site need thing sometimes sold scalability solution example replicating app behind load balancer help deploy whenever want without downtime one fintech service worked split credit card code service making pci ds compliance simpler case clear problem scalability solved avoids overengineering often wish systematic way figure technical requirement online service head real world complicated messy sometimes practical way sure experiment however every piece software start idea think scalability online service idea early design phase
40,Lobsters,scaling,Scaling and architecture,Guilds Road to Event-Driven Architecture,https://medium.com/extra-credit-by-guild/get-on-the-bus-adec3a826d58,guild road eventdriven architecture,get bus guild road eventdriven architecture rich haase johnny coster direct invocation api gateway lambda kinesis json schema unix philosophy,get bus guild road eventdriven architecture technology landscape started like lot startup monolith time took proverbial scalpel carved chunk monolith creating standalone service domain like academic program catalog student application process employee registration etc also built mvp system lessthanideal way service short delivery timeline often ended needing move system onto different infrastructure codebases etcas built initial mvp engineering org matured often ran problem two system direct integration went change replace one system also number discrete service guild ecosystem grew kept running problem communicating one system many system direct integration model would mean n n eg n service direct integration service send message service clearly way livewhat tangled web weavedenter eventdriven architecture panacea thing communicating event happen different system centralized event bus architecture team landed would allow u flexible scale communication much effectively architecture course unique guild testing various architectural concept like event stream event consumer would used eventdriven world furthermore already stood web service leveraged technology would become bedrock event bussample diagram usertriggered event flow onto event stream available consumer listenersworking input many engineer across number team one principal engineer rich haase spearheaded bringing event bus life created engineering requirement document put stake ground would build hand another engineer johnny coster began work would mvpover course two month built lean nofrills system one thing well validate published event send event stream provided three main feature interface publishing event supported http direct invocationa event schema registry event validation systemevent stream allowed multiple consumer event oneeventtomanystreams mapping configuration meaning event published event bus would sent stream stream b stream c ruthlessly focused building something simple short time frame also knew system would relied core piece infrastructure many year come needed able scale business technology usage guild active student number continue increase system rely event bus communicate information consumer guild ecosystem end relied aws technology team already experience production built scale api gateway http interface lambda running service code kinesis event stream service managed aws provides layer abstraction mean really deal problem like many compute resource event bus using much network traffic load hitting event bus streamlined let take look system work perspective userverification event published event bus event important one guild ecosystem allows user confirm identity based information received employer effect first touch point guild platform user identity trusted many different system start taking action set data feature support user interaction guild platformfirst system publishes event via http post request event bus api gateway url api gateway receives request pass payload request transparently event bus lambda function alternatively system skip api gateway step invoke lambda directly important piece configuration determine happens next based name event optionally version schema chosen validate shape content event example configuration mapping event example event bus config connects event name schema event stream destinationsto validate event sent event bus rely json schema standard number schema definition technology considered json schema accessible wide range engineer happened already json schema definition use production broke fix snippet user verification event schema definition used validate corresponding event example json schema definition user verification eventvalidation schema ensures event json necessary field value expected type length validation process important downstream event consumer programmed defensively trust shape content event consume expectedif event pas validation detailed error part invalid sent back publisher assuming pas validation final step put event kinesis stream specified configuration file shown case user verification event production environment event put kinesis stream called userprofileserviceprod certainly detail system glossed really get gist search making reliable core piece infrastructure able use many year engineered simple spirit unix philosophy one thing well recent month new feature added event bus sure update also hope share used event bus communicate system well pattern adopted event consumption lookout update extra credit topic
41,Lobsters,scaling,Scaling and architecture,Plugin Architecture in Python (aka Py3EE),https://www.dsouzaman.net/python_plugin_architecture.html,plugin architecture python aka,plugin architecture python aka edward dsouza section example problem basic solution enterprise solution highlevel policy enterprise implementation,plugin architecture python aka edward dsouza june came python java reveled terseness flexiblity however reading clean architecture robert c martin seeing enterprisey idea java actually quite useful artcle look dependency direction interface used create plugin architecture python toy example might look overengineered let u explore idea would valuable realistic context multiple people working together example many concept taken clean architecture section example problem example problem use writing encrypter program take character stdin encrypt using translation table write output stdout first easy part functional datamanipulation component simply translates character encrypted form using shift cipher def translate char str shift letter stringasciilowercase char letter return letter lettersindex char shift len letter return char basic solution straightforward solution full problem probably would write default faced requirement import sys translate import translate def encrypt data sysstdinreadlines line data char line print translate char encrypt code beautiful terseness potential issue main problem mixing together several responsibilies getting character translation outputting character multiple people wanted modify part time would mess getting everyone work together tight space highlevel policy also nt depending directly lowlevel detail like print function one strategy use fix problem make code work generic interface specific implementation interface plugged policy create working system policy decoupled lowlevel detail code organized input output concern nicely separated enterprise solution welcome highlevel policy import abc translate import translate class charreader abcabc abcabstractmethod def readchar self pas class charwriter abcabc abcabstractmethod def writechar self char str pas def encrypter reader charreader writer charwriter def encrypt true try char readerreadchar except stopiteration break encryptedchar translate char writerwritechar encryptedchar return encrypt clearly explicitly spelled highlevel policy reading character encrypting translate function writing encryped character algorithm know care stdin module print function remains open reused different context different input source output channel enterprise implementation admittedly like lot enterprisey code policy fairly verbose nt actually anything make useful need implement charreader charwriter interface plug policy function import sys enterprise import charreader charwriter encrypter class mycharreader charreader def init self def getcharacters data sysstdinreadlines line data char line yield char selfiter getcharacters def readchar self return next selfiter class mycharwriter charwriter def writechar self char str print char myencrypter encrypter mycharreader mycharwriter myencrypter cool messiness converting stdin stream character contained within mycharreader one else burdened
42,Lobsters,scaling,Scaling and architecture,Scaling to 100k Users,https://alexpareto.com/scalability/systems/2020/02/03/scaling-100k.html,scaling user,user machine user split database layer user split client splitting user add load balancer user cdn user scaling data layer caching read replica beyond reference favorite post high scalability translation chinese korean,many startup feel like legion new user signing account every day engineering team scrambling keep thing running good problem information take web app hundred thousand user scarce usually solution come either massive fire popping identifying bottleneck often time said noticed many main pattern taking side project something highly scalable relatively formulaic attempt distill basic around formula writing going take new photo sharing website graminsta user user machine nearly every application website mobile app three key component api database client usually app website database store persistent data api serf request around data client render data user found modern application development thinking client completely separate entity api make much easier reason scaling application first start building application alright three thing run one server way resembles development environment one engineer run database api client computer theory could deploy cloud single digitalocean droplet aws instance like said expect graminsta used person almost always make sense split database layer user split database layer splitting database managed service like amazon rds digital ocean managed database serve u well long time slightly expensive selfhosting single machine instance service get lot easy add ons box come handy line multiregion redundancy read replica automated backup graminsta system look like user split client lucky u first user love graminsta traffic starting get steady time split client one thing note splitting entity key aspect building scalable application one part system get traffic split handle scaling service based specific traffic pattern like think client separate api make easy reason building multiple platform web mobile web io android desktop apps third party service etc client consuming api vein biggest feedback getting user want graminsta phone going launch mobile app system look like user add load balancer thing picking graminsta user uploading photo left right starting get sign ups lonely api instance trouble keeping traffic need compute power load balancer powerful key idea place load balancer front api route traffic instance service allows horizontal scaling increasing amount request handle adding server running code going place separate load balancer front web client api mean multiple instance running api web client code load balancer route request whichever instance least traffic also get redundancy one instance go maybe get overloaded crash instance still respond incoming request instead whole system going load balancer also enables autoscaling set load balancer increase number instance superbowl everyone online decrease number instance user asleep load balancer api layer scale practically infinity keep adding instance get request side note point far similar paas company like heroku aws elastic beanstalk provide box popular heroku put database separate host manages load balancer autoscaling let host web client separately api great reason use service like heroku project early stage startup necessary basic come box user cdn probably done beginning moving fast graminsta serving uploading image starting put way much load server using cloud storage service host static content point think image video aws digital ocean space general api avoid handling thing like serving image image uploads thing get cloud storage service cdn aws addon called cloudfront many cloud storage service offer box cdn automatically cache image different data center throughout world main data center may hosted ohio someone request image japan cloud provider make copy store data center japan next person request image japan recieve much faster important need serve larger file size like image video take long time load send across world user scaling data layer cdn helped u lot thing booming graminsta youtube celebrity mavid mobrick signed posted u story api cpu memory usage low across board thanks load balancer adding api instance environment starting get lot timeouts everything taking long digging see database cpu hovering maxed scaling data layer probably trickiest part equation api server serving stateless request merely add instance true database system case going explore popular relational database system postgresql mysql etc caching one easiest way get database introducing new component system cache layer common way implement cache using inmemory key value store like redis memcached cloud managed version service elasticache aws memorystore google cloud cache come handy service making lot repeated call database information essentially hit database save information cache never touch database example graminsta every time someone go mavid mobrick profile page api layer request mavid mobrick profile information database happening since mavid mobrick profile information changing every request info great candidate cache cache result database redis key user id expiration time second someone go mavid mobrick profile check redis first serve data straight redis exists despite mavid mobrick popular site requesting profile put hardly load database plus cache service scale easier database redis built redis cluster mode similar way load balancer let u distribute redis cache across multiple machine thousand one plea nearly highly scaled application take ample advantage caching absolutely integral part making fast api better query performant code part equation without cache none sufficient scale million user read replica thing database started get hit quite bit add read replica using database management system managed service done oneclick read replica stay date master db able used select statement system beyond app continues scale going want focus splitting service scaled independently example start make use websockets would make sense pull websocket handling code put new instaces behind load balancer scale based many websocket connection opened closed independently many http request coming also going continue bump limitation data layer going want start looking partitioning sharding database require overhead effectively allow data layer scale infinitely want make sure monitoring installed using service like new relic datadog ensure understand request slow improvement need made scale want focused finding bottleneck fixing often taking advantage idea previous section hopefully point people team help well reference post inspired one favorite post high scalability wanted flesh article bit early stage make bit cloud agnostic definitely check interested kind thing translation reader kindly translated post chinese korean
43,Lobsters,scaling,Scaling and architecture,OAuth2 authorization patterns and microservices (2019),https://medium.com/capgemini-norway/oauth2-authorization-patterns-and-microservices-45ffc67a8541,authorization pattern microservices,authorization pattern microservices charos pix oauth road hell microservice pattern password grant type access token confidential client password grant type password grant type separation concern api gateway microservice basic access authentication network security policy password grant type authorization code grant client credential grant authorization code grant identity provider client credential grant client credential grant authorization code grant client credential grant client credential grant authorization code grant,authorization pattern microservicesattribution charos pixone topic led contention project microservice architecture use authorizationthe challenge well known described colourfully one oauth exauthors eran hammer blog post oauth road hell sum eran hammer say best clear oauth hand developer deep understanding web security likely result secure implementation however hand developer experience past two year likely produce insecure implementationsand experience well really baffled though lack knowledge experience even senior developer architect subject even book read concerning microservice architecture example used way best described creative writing read microservice pattern chris richardson blog post necessary clear misunderstanding might around use microservice architecture take following drawing based fig chris richardson bookfirstly nothing wrong technically something like fully possible question example chris richardson using involves using called password grant type concept user give username password client application us hisher credential obtain socalled access token access token used client application gain access server behalf user first issue grant type involves client asking user hisher password secure long client application exists security zone authorization server call confidential client technical term mean need client application backend clientserver application make use mtls client application api gateway without security mechanism pattern secure information security perspective second issue conceptually wrong password grant type never meant used like password grant type done correctly client make direct request authorization server obtain access token make second request access token api gateway api gateway proxy forward request access token microservice well make sure authorize client application use resource server behalf user happening example authorize api gateway using microservice make sense read post separation concern api gateway microservice understand security point might well use basic access authentication use network security policy servicetoservice communication paas caas provider simple way configuring thisso use password grant type use two grant type look authorization code grant client credential grant one use depends context requirement customeruse authorization code grant providing api client application given customer single signon solution want use dealing multiple organization make sure use type identity provider integration customer use different type identity provider quickly turn become project itselfuse client credential grant service service communication dealing multiple organization reaching consensus among regard agreeing common solution providing identity authorization far fetched considering time resource available always migrate client credential grant authorization code grant later advice start setting client credential grant first consequence though fex resource server completely unaware client application something called user client credential grant care client application care right user given client application mean resource server trust client application given proper consent accessing resource resource server still since authorization code grant alone solve challenge related information security fineanswering use way secure information security perspective require separate article perhaps series article perhaps come back next post
44,Lobsters,scaling,Scaling and architecture,Evolution of the precise code intel backend,https://about.sourcegraph.com/blog/evolution-of-the-precise-code-intel-backend,evolution precise code intel backend,sourcegraphcom private instance sourcegraph sourcegraph browser extension motivation srclib srclib lsif mvp express sourcegraph extension optimizing query sqlite dgraph badgerdb change processing uploads asynchronously noderesque resque ipc pr storing crossrepository data postgresql dblink eventually reverted queue bull redis bull search within job payload text matching query queue postgresql adding graphql resolvers sourcegraphgo sourcegraphtypescript campaign code insight diagnostics introducing multiple worker container prefork sourcegraphcom headofline blocking introducing bundle manager rewriting go strangler fig optimizing code intelligence backend removing lsifserver looking forward,jumping definition symbol cursor finding reference two basic mental mechanic software engineering fast code navigation accelerates rate build mental model code available likely use hundred thousand time per day code navigation core sourcegraph help understand part universe code relevant important code navigation also present difficult technical challenge especially want provide code navigation outside ide variety application developer trying understand code webbased code search engine like sourcegraphcom private instance sourcegraph code host like github gitlab bitbucket phabricator sourcegraph browser extension order provide compileraccurate code navigation ides work lot magic behind scene involves static analysis incremental compilation build execution lot lot caching much assumes read write exec permission local filesystem sourcegraph provide precise code navigation user within millisecond without access answer language server index format lsif elsif post share technical journey lsif including chose adopt foundation precise code navigation challenge faced scaling lsifbased backend see thing going motivation journey lsif began first version sourcegraph precise code navigation firstorder concern even early version application time code navigation feature available editor certain language editor depending whether plugin added support specific language specific editor enable code navigation webbased interface eye toward enabling across language editor sourcegraph created srclib first opensource crosslanguage code analysis toolchain indexing format project like language server protocol kythe still year away released srclib worked quite well early version sourcegraph however adding srclib indexer every language turned quite undertaking many product feature demanded time attention support new language slow develop meantime language server protocol emerged new open protocol providing code navigation across many editor lsp piggybacked growing traction v code many implementation lsp emerged major language growing ecosystem sourcegraph saw opportunity take advantage also give back emerging opensource community dedicated making code intelligence ubiquitous every language contributed language server implementation go typescript javascript python java incorporated language server application architecture could deployed sourcegraph provide precise code intelligence many language able provide srclib limited developerhours language server served user well number year eventually amount code sourcegraphcom grew large organization began adopt sourcegraph big internal codebases scaling performance became issue fast forward early largest customer began regularly reporting language server outage related high usage volume large codebase size began looking way improve performance scale started thinking combine richness lsp ecosystem performance indexingbased approach like srclib february surprise delight dirk bumer one creator lsp announced language server index format lsif code intelligence indexing format similar spirit lsp lsif lsif provides crosslanguage serialization format describes data needed quickly resolve action like gotodefinition findreferences raw lsif data json look like id type vertex label definitionresult id type edge label textdocumentdefinition outv inv id type edge label item outv invs document id type vertex label packageinformation name githubcomsourcegraphsourcegraph manager gomod version sourcegraph accepts useruploaded lsif data generated lsif indexer implementation local checkout code common scenario step repository ci pipeline sourcegraph receives data transforms internal format optimized scale query speed sourcegraph us data power local repository crossrepository code navigation action like gotodefintion findreferences today sourcegraph lsif backend multiple component handle aspect uploading parsing transforming reading writing manipulating lsif data serve user request story system grew evolved time story span commits line code mvp lsif backend began life simple express server written typescript proxied outside world endpoint sourcegraph frontend api server accepted lsif uploads wrote directly disk query request server would read lsif data current repository disk memory parse structured representation walk graph vertex edge construct appropriate response client side wrote simple sourcegraph extension query lsifserver api chose typescript express fastest way get something running given experience team primary goal solicit feedback user ergonomics upload api user would configure ci pipeline generate lsif basic user experience code navigation backed lsif performance scalability robustness code quality primary concern optimizing query serve query mvp implementation would simply read raw lsif data disk memory parse structured representation walk graph compute query result lsif index however large larger size codebase given revision larger codebases added significant latency serving user request would often also call oom crash needed make following operation fast get definition identifier given source position get reference identifier given source position get hover text identifier given source position order decided store lsif data different format optimized operation several design discussion reduced choice two option sqlite embedded filebased sql database engine dgraph distributed graph database built top lsmtreebased database badgerdb built two version backend benchmark showed upload performance sqlite dgraph backends proportional input size sqlite factor dgraph factor relatively inexperienced dgraph relatively slow performance could explained lack operational experience bad choice graph schema dgraph backend implementation found time might experimented dgraph decided go sqlite based higher initial performance familiarity using past fact would easier deploy operationally many deployment environment customer change query needed read disk document containing target source range instead lsif dump entire repository processing uploads asynchronously mvp lsif upload process synchronous acceptable mvp reading lsif data network writing directly disk adopting sqlite backend store precise code navigation added additional transformation step lsif upload process rather simply read lsif data network write directly disk parse data convert sqlite bundle increased lsif upload response time significantly began bumping timeouts enforced various client server proxy within around sourcegraph also started see frequent oom error lsifserver process larger uploads data conversion process also increased memory usage process handled uploads query uploading large file multiple small one parallel could ooming process also take code navigation query address issue decided separate work converting lsif sqlite bundle separate background process lsifserver process would continue handle uploads query uploads handler would similar mvp implementation transparently writing raw data disk rather converting synchronously lsifworker process would consume queue lsif dump process converting sqlite bundle background coordinate work lsif upload handler lsifworker process needed queue used noderesque nodejs port popular rail library resque library store job data redis already component stack also considered using postresql accessing existing postgresql instance came certain restriction due concern uptime performance sort local ipc would prevented scaling lsifserver lsifworker independently using amqp server would required introducing new major service architecture implemented splitting lsifserver lsifworker pr storing crossrepository data postgresql uploaded lsif index became single sqlite database ondisk singlerepository database would sufficient support local within repository code navigation order make code navigation experience truly seamless magical wanted support crossrepository code navigation word wanted user click reference function defined dependency arrive directly definition function source repository pretty neat enable added additional sqlite database xrepodb enabled u look repository index versioned package provided version package depended fine long one instance lsifserver lsifworker lived docker container however order support lsif across large multirepository codebases needed scale meant multiple lsifserver lsifworker instance would running different docker container perhaps different machine could longer rely share access singlewriteratatime sqlite database moved xrepodb data postgresql potential volume additional writes continued concern u lsif use grew would cause operational issue postgresql instance would affect performance unrelated part application safe kept table space lsif data disjoint prefixed table name foreign key existing table data also tried migrating lsif table second postgresql instance however required nasty trickery dblink order run migration found quite painful eventually reverted backoftheenvelope calculation suggested lsifrelated load would nt overwhelm single shared postgresql instance calculation largely held time queue bull redis saw operational issue related stuck worker lost job traced back queueing library noderesque motivated switch bull also additional feature allowed u schedule job similar spirit cron list job particular state search within job payload text matching query using redis eval command relevant pr queue postgresql adoption bull resolved issue queue implementation others remained particular problem enforcing logical transaction across data stored partially redis partially postgresql particular data could written postgresql indicating successful completion lsif bundle conversion corresponding job queue stored redis could updated within transaction sometimes got sync furthermore redis treated part sourcegraph ephemeral truncatable cache site administrator aware felt could safely wipe redis data would wreak havoc lsif processing queue address issue moved queue data redis postgresql reduced lot complexity turned custom lua script reached redis data could reduced sql query could also use postgresql transaction enforce allornothing atomicity lsifrelated update adding graphql resolvers lsifserver accessible undocumented proxy sourcegraph frontend service proxy accepted uploads served code navigation query consumer api firstparty sourcegraph extension like sourcegraphgo sourcegraphtypescript adding graphql api enabled lsif backend used part sourcegraph nascent campaign feature currently inprogress code insight also thirdparty sourcegraph extension author thirdparty api consumer functionality lsif backend continues grow recently added support diagnostics possibility user api introducing multiple worker lsifserver lsifworker still run together container easy way enable multiple worker without changing container orchestration decided prefork worker rudimentary way scale overall resource use still constrained single container isolation worker process meaning runaway memory use one starve others container worked well enough time change also helped u resolve issue lsif processing sourcegraphcom suffering headofline blocking introducing bundle manager needed enable lsifserver lsifworker instance scale horizontally time instance shared access persistent storage would store lsif data used serve query naively thought might possible simply attaching shared disk separate instance practice however issue kubernetes persistent volume default mounted readwritemany across multiple node volume plugins cephfs glusterfs nfs enable reliability performance shared access filesystems issue furthermore unsupported gcp sourcegraphcom deployed solution factor responsibility managing shared storage separate service bundle manager originally called lsifdumpmanager today known precisecodeintelbundlemanager made lsifserver lsifworker stateless freeing scale horizontally scaling bundle manager requires sharding scheme similar already used gitserver service responsible serving git data sourcegraph backend rewriting go original lsif backend written typescript easiest prototype also matched technical skillset original team time performance became consideration experience team shifted toward go particular familiar variety technique optimize program written go le wellversed technique application written typescript performance becoming firstorder consideration decided rewrite thing go initially decided adopt strangler fig model refactoring extracting first cpubound work highperformance go process would called existing worker code eventually time logic would ported go process however thing nt always go according plan knew in out typescript code simply rewrote three service go single pas resulting code nt particularly idiomatic since wanted focus bringing new system life quickly possible could sunset old one continuing refactors made code idiomatic time rewrite unlocked large number performance improvement opportunity result described optimizing code intelligence backend removing lsifserver rewriting lsif backend go lsif api server lsifserver completely stateless longer talk process bundle manager written different language taking step back realized api server process little purpose moved remained lsif api server logic server handler client used part sourcegraph external http graphql apis query lsif data dropped lsif api server looking forward journey nt stopping made significant performance improvement lsif backend continue adding feature investing optimization support massive scale lsif thing looking forward automatic zeroconfig lsif indexing repository support monorepos large repository high commit frequency creating public index opensource code connecting private code enable seamless jumping proprietary code opensource dependency keep eye update
45,Lobsters,scaling,Scaling and architecture,Kicking Node into high gear for data processing or how to hire 100 cats to catch a mouse,https://sgolem.com/kicking-node-into-high-gear-for-data-processing-or-how-to-hire-100-cats-to/,kicking node high gear data processing hire cat catch mouse,callback hell promise tokio asyncstd wait something time sequential lot time promisified concurrent per thing spawn multiple thread boost processing,recently came across problem work required light processing across hundred directory initial run nt take long figured whatever could speed would pay long run anticipated need run dozen hundred time got right asked year ago exactly node event loop work say idea today happy say still nt know work least detail meantime learned good chunk rust asyncawait internals pushed right direction understanding asyncawait faster node even though singlethreaded shortest introduction asyncawait syntax imagine remember callback diabolical mess could make okay maybe u probably significant percentage nesting callback could easily produce code hard follow sometimes hard understand even rereading people call phenomenon callback hell came promise promise clean readable code even dealing async process straightforward combine promise chain sequentially process data despite people still tended nest promise logic understandable chosen one would usually last month understanding would decay logical next step rewrite period time someone idea could write async code way write sync code asyncawait born great idea dug tokio asyncstd idea worked speed something run single thread seems code mostly nothing specific wait something time sure everybody fired http request numerous time playing node read file transformed content logged wrote result file appears thing like take considerable amount time waiting o right task network connection might slow congested drive might slow dying network destination might slow far let see happens fire lot sequential network request start request waiting something getting data end request oooo ooo oo ooooooooooooooo lot nothing going request running time mean useful work waiting oooo oo oo oo ooooooooooo oooo oooo take closer look see new data processed single request time request run faster even single thread course adding thread could result much better time could slow whole thing later might realize constantly jumping one request therefore must sort overhead honest nt know much impact either way realworld task would lot waiting data decrease overhead minimum like everything else need reason use asyncawait reading file script might well use sync worry use case async might shine hundred file read network call nodejs quite limited come make super easy run thing sequentially one one time tool workload let see kind api would suitable case use promisified function keep code clean const work filepath const content await fsreadfile filepath const json jsonparse content const url jsondetailsdocumentation const re await fetch url return resbody know need done probably safe assume filepaths array matter obtained const filepaths simplest form api think const documentationpages await pareach work filepaths might also accept option object third parameter let leave later basic shell pareach function could go something like const pareach async work allargs option todo option object contain many configurables let focus number concurrent parallel worker run const batchsize option need variable see much work done know stop code inside pareach function let callwhendone let workerspawned let nextargsindex let write part actual work const spawn const args allargs nexargsindex arrayisarray args work args else work args workersspawned nextargsindex straightforward see happening spawn function get right argument check one multiple start work update variable accordingly probably noticed function let see one look const workersspawned const allargsused nextargsindex allargslength const allworkerswork workersspawned batchsize const noworkerswork workersspawned allworkerswork allargsused spawn noworkerswork allargsused callwhendone also update workersspawned variable work done checking see good time work wrap finish everything one thing left write kickstarting let batchsize spawn return new promise resolve callwhendone resolve fire worker defined batchsize variable save resolve function unblock awaiting pareach function called already published package play check pareach npm spent lot time searching name taken one mixture parallel foreach almost sound like something per thing whatever spawn multiple thread boost processing even check speedup sequential took second concurrent took second parallelized took second produced waiting random sub second timeouts firing hundred request yield similar ratio like know something like useful please clone repo try running test see much pareach version faster sequential processing
46,Lobsters,scaling,Scaling and architecture,Spark Joy by Running Fewer Tests,https://engineering.shopify.com/blogs/engineering/spark-joy-by-running-fewer-tests,spark joy running fewer test,dynamic analysis problem test shopify solving problem test dynamic analysis rotoscope tracepoint use dynamic analysis downside running dynamic analysis ruby rail slow mapping lag behind head untraceable file backend class metaprogramming obfuscates call path failing test caught dynamic analysis rolled failure recall speed improvement compute time approach explored static analysis machine learning research paper pdf machine test benefit running fewer test skepticism dynamic analysistest selection average developer request running full test suite pull request software development shopify expression interest career posting,developer write test ensure correctness allow future change made safely however number feature grows number test test doubleedged sword one hand wellwritten one catch bug maintain program stability code base grows high number test impedes scalability take long time run increase likelihood intermittently failing test software project often require test pas merging main branch add overhead developer intermittently failing test worsen problem cause intermittently failing test timing instability database http connectionsmockings random generator test leak state test test pass every single time fails test depending order unfortunately one fully eradicate intermittently failing test likelihood occurring increase codebase grows make already slow test suite even slower retry pas implying one write test benefit quality assurance performance monitoring speeding development catching bug early instead production outweigh downside however improvement made team thus embarked journey making continuous integration ci stable faster share dynamic analysis system select test implemented followed approach explored decided test selection spark joy life wish bring joy problem test shopify test impede developer productivity test suite monolithic repository test growing size annually take min run hundred docker container parallel pull request requires test pas developer either wait test pay price context switching biannual survey build stability speed recurring topic problem clearly felt developer solving problem test abundance blog postsarticlesresearch paper optimizing code unfortunately test fact learned unrealistic optimize test sheer quantity growth also learned uncharted territory many company research progressed became apparent right solution run test related code change challenging large dynamically typed ruby codebase make ample use language flexibility furthermore difficulty exacerbated metaprogramming rail well nonruby file code base affect application behaves example yaml json javascript dynamic analysis dynamic analysis essence logging every single method call run test track file call graph call graph create test mapping every file find test file call graph looking file modified added removed renamed look test need run check rotoscope tracepoint record call graph ruby application use dynamic analysis ruby dynamically typed language retrieve dependency graph using static analysis thus know corresponding test code downside running dynamic analysis ruby rail slow computationally intensive generate call graph run every single pr instead run dynamic analysis every deployed commit mapping lag behind head generated mapping lag behind head main run asynchronously solve problem run test satisfy least one following criterion added modified current branch added modified head last generated mapping current branch head mapped test per current branch code change untraceable file nonruby file yaml json etc traced call graph added custom patch rail trace example patched backend class trace translation file yaml change file traced run every single test metaprogramming obfuscates call path added existing metaprogramming known directory added glob rule file path determine test run discourage new metaprogramming sorbet linters failing test caught generated mapping dynamic analysis get date latest main sometimes failing test get selected get merged main circumvent issue run full test suite every deploy automatically disable failing test developer blocked shipping code full test suite run asynchronously pull request get merged full test suite completes automatic disabling failing test sound counterintuitive many people observed percentage pull request failing test merged main branch also mechanism mitigate risk canary deploys type checking using sorbet code owner failing test notified expect developer fix remove failure without blocking future deploys dynamic analysis rolled experimentation phase new dynamic analysis system testselection pipeline ran parallel pipeline run full test suite new pr recall new test selection pipeline measured failing test measured new pipeline selects failing test care test pas failing test cause trouble measured result using three metric failure recall define recall percentage legitimately failing test excluding intermittently failing test system selected want close possible hard measure accurately occurrence intermittently failing test instead approximate recall looking number consistently failing test merged main two month project active commits merged failed detect five failing test landed main also managed resolve root cause missed failure problem repeat future achieved recall speed improvement overall selection rate ratio selected test total number test percentage selected test file per build build run fewer test show many developer significantly benefit test selection system percentage build selected fewer test compute time total saving compute time measured adding time spent preparing running test every docker container build averaging across build decrease significant chunk computing time still used setting container database pulling cache note also adding compute time running dynamic analysis every deployed commit main estimate undo infrastructure cost saving approach explored prior choosing dynamic analysis explored approach ultimately ruled static analysis determine dependency graph briefly explored using sorbet would possible entire code base converted strict sorbet type unfortunately code base partially strict sorbet type big team convert rest machine learning possible use machine learning find dependency graph facebook excellent research paper pdf chose dynamic analysis shopify sure enough data make prediction want choose approach deterministic reproducible machine test tried adding machine test suite unfortunately performance increase linearly scaled horizontally fact test average take longer increase number machine past certain number increasing machine reduce intermittently failing test increase possibility failing connection sidecar thus increasing test retries benefit running fewer test three major benefit selectively running test developer get faster feedback test likelihood encountering intermittently failing test decrease thus increase speed developer ci cost le skepticism dynamic analysistest selection feature rolled many developer skeptical would work frankly one many people voiced doubt privately openly however much surprise went live people silent average developer request running full test suite pull requestsif similar situation hope experience help hard developer embrace idea test run importance test ingrained head sound like kind problem enjoy solving come work u check software development shopify expression interest career posting apply specifying interest developer acceleration
47,Lobsters,scaling,Scaling and architecture,A bunch of rants about cloud-induced damage,https://rachelbythebay.com/w/2020/05/06/scale/,bunch rant cloudinduced damage,bunch rant cloudinduced damage exactly,bunch rant cloudinduced damage bunch random complaint thing gotten late cloud business seems like far many people think running virtual machine people hardware way anything nt absolve actually good stuff frequently complicates matter never truly shine say front prefer physical hardware know exactly working spooky stuff going try track problem disk slow find network odd find mysterious host system life someone else world beyond reach want satisfying answer life mystery production environment need able get bottom thing every time introduce bit virtualization reduced likelihood finding answer get even worse cross domain another company vendor whatever point drop standard accept uncertainty badness outage strange behavior live fact thing may never truly get better element right chase folk either join peace never join first place mention introducing troubleshooting singularity reasoning exist solve problem also creates great opportunity charlatan fraud general incompetence easy every time something bad happens blame something one known trouble spot people love scapegoat another fun aspect virtual machine cloud elastic scaling business incredible money sink created seems like everyone set kind rule keep eye cpu utilization helpfully stand instance time go past certain point number terrible behavior enables incredible seen believed mean ah yes come little trip memory lane worked place bunch physical machine also whole group nothing plan allocate capacity word group made sure people got machine appropriate asked also nt ignorant beancounters either real driving force behind team allocation effort badass engineer friend mine person alone probably saved company multiple building worth machine almost decade tenure work easy every time team wanted whole whack new hardware friend went took look see using stuff already nt cpu utilization either friend wanted know using cpu stupid inefficient thing would come wash ran analysis could see stuff spending time profiling thing nice effect forcing team honest forthcoming need try wasteful think hardware would save nt get spend company money friend team represented kept thing becoming freeforall cloud place though nt kind oversight build system autoscale soon dozen hundred even thousand crazy thing write bad code footprint go every day nt even realize odds team idea many machine vms reality quickly changing equivalent friend stand tell first optimize terrible code get hardware provisioning deploy system give away usually happens someone periodically run report find something mindblowing like oh wow service x run machine happen easy nobody stopped needing machine current setup people wring hand promise better still stupid instance sit burning cash day night also fun direction someone might set rule scale job fall cpu utilization maybe tell scaler lower notch every time utilization something like suppose happens first time quiet one instance one instance manages come said autoscaler lower zero right yep without minimum size limiter place get amazing apparently happen immediately fix think would notice need capacity run thing get tugofwar going night long personally think whatever measuring cpu utilization probably divided zero instance running could nt run past point dealing kind scenario seems par course whole neat situation happen talked elsewhere sake completeness give summary service dy traffic everything else drop cpu utilization drop service scale point essential service start working traffic flood back uh oh service far small ca nt handle load worse still written fragile way imagine ca nt even ignore extra traffic stupidly try take everything coming notion rejecting stuff hope handling completely saturated getting nothing done cpu running like crazy autoscaling stuff notice might good minute get instance running joke seen happen pretty basically able point equivalent flamethrower service keep going nt accept extra request one would capable anyway ignore reject rest appropriate way least start absorbing load instead becoming part problem think one instance service fall crash presented enough load rebalance load onto others one boil go rebalancing load onto even fewer instance continue whole thing molten lump slag tell situation like rig horrible thing like turn load balancer restart service maybe set network stuff drop traffic catch ever running incredibly fragile stuff likewise ever worry starting everything otherwise first one get boiled crash second third let first break already bad place side note sound familiar customer year ago absolutely refused fix terrible code wanted better tooling start stop stuff parallel people side nt looking argument gave exactly happy nt think writing better code want stuff many way situation like one temporarily stop checking listening socket new connection mean stop calling accept listen queue back something interesting happen assuming linux probably stop emitting synacks connection attempt point might magic make actively rst connection client fail fast go somewhere else right away whatever thing calm maybe start looking new connection maybe something neat process newest one first since oldest one probably burned timeout already wo nt stick around finish job anyway meant selective lifo post month back kind thing people tend think rpc mindset web mindset want really rough example throttling sendmail mail daemon made life interesting u far long thing kept tab machine load average got first notch would still accept connection would wave error got past second notch would actually stop listening network right would actually close listening socket forcibly reject connection attempt streamofconsciousness intended needed get odds come back point spin tale tell team company already dug hole spend time care stuff resource nt free
48,Lobsters,scaling,Scaling and architecture,Evolutionary Software Architecture,https://venam.nixers.net/blog/programming/2020/06/07/evolutionary-software-arch.html,evolutionary software architecture,post architectus oryzus kpi security code archunit chaos engineering social code analysis github scientist architectural decision record strategy lean methodology disposable software erase darling reactive architecture email irc,previous post underlined philosophy behind domain driven design ddd like move practical approach handle real issue software development architecture requirement constantly change model never precise never current andor never using best technology available one solution problem build evolutionary architecture able discussion understand part software architecture play straight forward considering many definition redefinition note particularly fascinating one architecture software system given point time organization structure significant component interacting interface component composed successively smaller component interface ieee definition software architecture successful software project expert developer working project shared understanding design shared understanding called understanding includes system divided component component interact interface ralph johnson going towards abstract definition following architecture stuff hard change later little stuff possible martin fowler architecture important stuff whatever martin fowler stuff hard change later neal ford definition barely overlap still vague essence joining extract say architecture concerned important decision software project object decision shared knowledge reason view evolutionary architecture standpoint best architecture one decision flexible easily replaceable reversible deferred late possible substituted alternative recent experience shown superior architecture decisionmaking inherently tied concept technical debt compromise trading time design perfect keep mind debt accumulates often lead architectural decay change keep coming entropy increase similarly due vague definition architecture role architect hard describe whether completely separate role whether everyone team act one ambiguous vociferous software architecture evangelist martin fowler prefers term architectus oryzus referring architect also active contributor project thus getting direct insight involvement software architecture thought process applied two broad level application level enterprise level application architecture describing structure application fit together usually using design pattern enterprise architecture organizational level software issue practice information flow methodology standard release mechanism personnel related activity technology stack enforced etc design relates well known development design pattern refactoring technique usage framework bundle component together daily concern evolutionary architecture preferable emergent design instead one set front give u good idea software architecture current state need solution building evolutionary architecture usual way develop software today fighting incoming change want incorporate current architecture software development dynamic equilibrium currently find software constantly unbalanced unstable state whenever change included even though like right thing right time predict decision predictability almost impossible example predict disruptive technology exist yet software age juggle change new requirement room experimentation respond stakeholder want software fulfill architecture significant requirement also known kpi auditability performance security scalability privacy legality productivity portability stability etc expect degrade hence find leastworst tradeoff blindly introduce anything could hinder hard businessdriven change new feature new customer new market etc ecosystem change advance technology library upgrade framework operating system etc recent year seen rise agile development methodology meant replace waterfall approach apt facing challenge create iterative dynamic way control change process call evolutionary architecture start idea embracing change constant feedback want apply across whole architecture spectrum multiple dimension strongest survive one responsive change evolutionary software architecture evolutionary architecture metaarchitecture way thinking software evolutionary term guide first derivative dictating design principle promote change first citizen neal rebecca patrick definition book evolutionary dissect evolutionary architecture support guided incremental change across multiple dimension separate system world continuum draw boundary around system depends purpose discussion donella h meadow agile methodology concerned people process evolutionary architecture encompasses whole spectrum including technical data domain security organizational operational aspect want different perspective evolvable dimension evolutionary mindset surround holistic view software system add new requirement call evolvability dimension help measure easily change dimension evolve architecture easily included dynamic equilibrium example big ball mud architecture extreme coupling architectural rotting dimension evolvability change dimension daunting layered architecture onedimensional structural evolvability change one layer ripple lower one however domain dimension evolvability often domain concept smeared coupled across layer boundary thus domain change requires major refactoring ripple layer microservice style architecture hinge postdevops agile revolution structural domain dimension evolvability n n number isolated service running service microservice architecture represents domain bounded context changed independently others boundary world evolutionary architecture call disjunct piece quantum architectural quantum independently deployable component high functional cohesion includes structural element required system function properly monolith architecture whole monolith quantum however temporal coupling perspective dimension transaction may resonate multiple service microservice architecture thus evolvability transactional dimension enough measure easy change applied also need continually incrementally applies team build software agile methodology software deployed thing continuous integration continuous delivery continuous verificationvalidation rely good devops practice let take back control complex system automated deployment pipeline automated machine provisioning good monitoring gradual migration new service controlling route using database migration tool using chaos engineering facilitate management service experiment without hassle trivially reversibly evolvability across multiple dimension incremental change start evolutionary process need guide push using experiment main stressor architecture direction want call selector evolutionary fitness function similar language used genetic algorithm optimization function architectural fitness function provides objective integrity assessment architectural characteristic fitness function metric cover one multiple dimension care want optimize wide range function evolutionary architecture shine encourages testing hypothesis gathering data manner possible see metric evolve software along experimentation hypothesisdriven development superpower evolutionary architecture deliver limited usual test unit static analysis extends way beyond simple code quality metric could automated global continuous dynamic domain specific etc mention interesting technique used experimentation facilitated ab testing canary release aka phased rollout tdd find emergent design security code especially deployment pipeline architecture code also deployment pipeline test framework archunit license code surprisingly work test production instrumentation metric direct interaction user feature flagsfeature toggle toggle behavior chaos engineering example using simian army continuous fitness function facilitation experiment uncover systemic social code analysis find hotspot code github scientist test hypothesis production keeping normal behavior decides whether run try block randomizes order use try block run measure duration behavior compare result try result use swallow record exception raised try publishes information benefit experimentation soon seen creating real interactive feedback loop user buffet option dynamic equilibrium take care fewer surprise enabled team building evolutionary architecture like ddd law applies shape organization directly reflected software affect architecture without affecting people build far seen team embrace devops agile development given additionally team cocoon evolution experimentation making crossfunctional every role expertise found responsible single project remove bottleneck organization need team resembles architectural quantum small team one fed two pizza twopizzas team avoids separation decides need done decides going done everyone decides together talk team charge product rather project size team also allows information flow seamlessly share architectural domain knowledge method used usual documentation architectural decision record pair programming even mob programming nice team single working unit taking best decision project also important limit boundary many company prefer giving loose recommendation software stack team use instead letting silo specialized centralized knowledge face dynamic equilibrium time team level parallel enterprise architecture called strategy human governance team restrictive would make hard move however team guided fitness function automatic architectural governance continuous verification delivery pipeline act guardrail mechanism two big principle kept mind applying responsible pain together effect making team member le hesitant prone experiment last responsible moment idea lean methodology postponing decision immediately required find time gather much information possible let best possible choice emerge especially useful come structural design technological decision insight clear context appear late help avoid potential cost technical debt useless abstraction vendorlocking code framework direct opposition classical way software architecture decision taken upfront bringing pain forward idea inspired extreme programming methodology facing difficult long painful task instead postponing often encounter know insandouts incited automate pain away dynamic equilibrium pain v time pain increase exponentially wait encouraged thing like test production apply technique chaos engineering rebooting often garbage collecting service merging code often using database migration tool etc eventually knownunknowns become knownknowns common predictable pain gone world software keep getting complex building evolutionary architecture lead topic building robust resilient adaptive rugged system software intimate part life rely real world effect could get inspired aerospace world take look checklist manifesto could embrace statelessness throwabledisposable architecture disposable software erase darling maybe go way flexible reactive architecture selfhealing mechanism anything possible post given overview way perceive evolutionary software architecture place software architecture whole clearly step forward typical static view architecture offer novel organic approach name implies none described necessarily novel putting method thinking together want indepth explanation take look book evolutionary neal ford rebecca parson patrick kua hope article kickstarts journey reference want depth discussion always available email irc discus argue like dislike new idea consider opinion etc nt feel like discussion intimidated email simply say something small comment section andor share friend
49,Lobsters,scaling,Scaling and architecture,Step by Step Guide for Micro-Services Authentication using Istio and JWT,https://medium.com/intelligentmachines/istio-jwt-step-by-step-guide-for-micro-services-authentication-690b170348fc,step step guide microservices authentication using istio jwt,istio fit whole equation istio istio step implement jwt istio prerequisite follow istio documentation setup demo application install demo application install new gateway install virtual service test setup step generate publicprivate key pair using python github step b generate publicprivate key pair using openssl note i sub aud step create jwks json web key set representation public key step tell istio find jwks using crd issued jwt token public key stored jwks json web key set jwksuri json web key set uri jwks json web key set jwksuri json web key set uri http test whole shebang token unauthorized error unauthorized error nice important link jwt public key signature jwt detail google jwksuri jwt token validation http troubleshooting,private key stay secured one place inside authentication service generate jwt token need private key validate token public key good enoughwhere istio fit whole equation understanding design consideration discus istio help implement publicprivate key pair based jwt authentication scaleable secure choice microservices architectureso basically publicprivate key pair based jwt authentication system one dedicated microservice take care login task take username password generate jwt token using private key send back clientand client try communicate service using jwt token service use public key validate token scenario every service system implement application logic toget public key public url memoryvalidate token using public keyimplement case valid tokenimplement case invalid tokenthis may really easy straight forward implement one microservice imagine hundred microservices variety programming language framework every application implement manage thing different language framework implementation thing might consistentistio distio take care task validating jwt token incoming user request implement istio jwt authentication feature application code need bother jwt token validation istio jwt token generation istio generate token authentication microservice generates tokenhow istio istio jwt authentication defined request authentication feature custom resource definition crd named requestauthentication used tell control plane jwt public key found issued jwt tokenafter define configure requestauthentication crd jwksuri jwks issuer information explain meaning detail later section istiod master control plane tell envoy proxy jwt authentication policy informationnow request come service mesh reach application go envoy proxy sidecar container sits right beside application container envoy proxy validate jwt token incoming request carrying using public key available jwksjwksuri issuer information token validated using public key also issuer information match i mentioned jwt token payload envoy forward request directly application container application container handle request without bothering validity request right using istio offload lot application level security concernsteps implement jwt istiolet see implement thing sample project follow istio docswhich fairly straight forward easy understand step need follow see effect jwt authentication policy followsprerequisitesyou already kubernetes cluster setup using gke clusteryou installed latest stable release istio demo profile demo using gke istio although gke come option install istio cluster creation time clicking checkbox would encourage use istioctl install latest stable version istio follow istio documentationyou understanding step needed integrate istio existing kubernetes application new istio please refer story mineit might help get speedsetup demo applicationin step setup demo application using test istio jwt featureswe use httpbin application form istio docsthe istio doc give show mtls jwt authentication application setup demonstrating jwt feature consolidated command needed setup demo copypaste command work like charminstall demo applicationinstall new gatewayinstall virtual servicetest setupexport ingresshost kubectl n istiosystem get service istioingressgateway jsonpath statusloadbalanceringress ip curl ingresshostheaders devnull w httpcode n demo application set setup jwt authentication demo application make secureyou setup jwt authentication language stack python nodejs java goit matter need aware need setup rsa publicprivate key pair based jwt authentication system jwt authentication system us secret text variable file generate validate jwt token like traditional monolithic application jwt tutorial show work istio istio take public key url public key validate jwt token istio worksnow give away jwt token validation responsibility istio follow stepsgenerate publicprivate key paircreate jwks json web key set representation public keytell istio find jwks using requestauthentication crdtest new featurelet get businessstep generate publicprivate key pair using pythonthis script generates rsa publicprivate key pair using python us key generate jwt token run script following instruction github repo readmemd get public key private key valid jwt token generated using private key validated using public keystep b generate publicprivate key pair using opensslin approach use opensource tool named openssl generate token also use sshkeygen generates effectively thing different public key representation use opensslmkdir keyscd keysopenssl genrsa privatekeypem rsa privatekeypem pubout publickeypemafter generating key run following script generate validate tokennote generating token payload reserved key like i sub aud several others mandatory define working jwt system general working istio authn system need put field payloadstep create jwks json web key set representation public keyjust creating raw public key enough istio accepts public key jwks format give istio array public key istio use validate incoming tokenif public key need convert json format pem format example public key given e aqab kty rsa n need encapsulate key item following array like key e aqab kty rsa n valid jwksstep tell istio find jwks using requestauthentication crdto tell istio validate jwt token incoming request define crd named requestauthentication enable istio capability validate token need provide crucial information istio requestauthentication crdwho issued jwt token basically jwt authentication microservice generate jwt token specify i field payload i field match issuer field requestauthentication crdwhere public key stored specify value using either two jwtrules optionsjwks json web key set orjwksuri json web key set uri jwks json web key set using jwks effectively embed jwks created previous step directly value like belowjwksuri json web key set uri using jwksuri put jwks created previous step public url demo using public github repository using store source code blog using url raw github filehttps specify url jwksuri field like belowtest whole shebangif come far havethe demo app running kubernetes clustera gatewaya virtualservicea requestauthentication crd anda jwt token printed consolenow need thing first put jwt token inside environment variable named tokenexport token generated jwt token get ingres ip addressexport ingresshost kubectl n istiosystem get service istioingressgateway jsonpath statusloadbalanceringress ip checked start testing istio jwt feature send request demo application using curl invalid token valid token token request headerif send request invalid token see happenscurl header authorization bearer blablabla ingresshostheaders devnull w httpcode n get unauthorized error demo app set authentication system sort get unauthorized error istio taking care validating incoming jwt token user request nice send request valid tokencurl header authorization bearer token ingresshostheaders devnull w httpcode n get successful request http response code console let send request header allcurl ingresshostheaders devnull w httpcode n get console default istio run authentication policy check permissive mode meaning send request provide valid token provide token help gradual migration process moving istio based system blocking entire operation strictto reject request without valid token add authorization policy rule specifying deny action request without request principal shown notrequestprincipals following example request principal available valid jwt token provided rule therefore denies request without valid tokenslet test send three request invalid valid tokenscurl header authorization bearer blablabla ingresshostheaders devnull w httpcode n response tokencurl header authorization bearer token ingresshostheaders devnull w httpcode n response token curl ingresshostheaders devnull w httpcode n response allowing request valid tokenseffectively script using generate jwt token mimicking role authentication microservice authentication microservice give jwt token valid username password python script give valid jwt token execute authentication microservice implementation logic python script providedso decoupled jwt token generation validation step authentication microservice jwt token generation istio handle token validationhope able drive point home know long point may already forgotten point p read title might make sense nowthis effort document learned hope help wellimportant linkswhen starting aware lot related concept well google quite lot understand put together implement real project link provided belowjwt public key signaturejwt detailsgoogles jwksuri jwt token validationthe link show google store jwks way demohttps
50,Lobsters,scaling,Scaling and architecture,Back of the envelope estimation hacks,https://robertovitillo.com/back-of-the-envelope-estimation-hacks/,back envelope estimation hack,back envelope estimation hack enrico fermi know number approximate power rule rule little law little law remember system design book,back envelope estimation hacksmay two type engineer one quickly estimate one people smarter enrico fermi master estimationback envelope estimation skill learned practice becomes easier get familiar trick tradeknow numbershow fast read data disk quickly network familiar ballpark performance figure component important exact number per se relative difference term order magnitudeapproximate powersthe goal estimation get correct answer get one right ballpark using power make multiplication easy logarithmic space multiplication become additionsfor example let say user watching video uhd mbps machine content delivery network egress gbps many machine need approximating mbps mbps bps subtracting much easier trying get exact mbps difference really matter really precise answer exact anyway user vary time network bandwidth constant etcthe rule rule method estimate long take quantity double grows certain percentage frac rate example let say traffic web service increasing approximately weekly long take double frac take approximately week traffic doubleif combine rule power two quickly find long take quantity increase several order magnitudeslittle lawa queue modeled three parameter average rate new item arrive lambda average time item spends queue www average number item queue llllots thing modeled queue web service seen queue example request rate rate new item arrive time take request processed time item spends queue finally number concurrently processed request number item queuewouldn great could derive one three parameter two turn law relates three quantity one another called little law lambda say average number item queue equal average rate new item arrive multiplied average time item spends queuelet try let say service take average m process request currently receiving two million request per second rps many request processed concurrently service processing request concurrently request cpu heavy requires thread need thread using core machine keep need machinesremember thisestimation vital skill engineer something get better practicing using hack presented post get familiar component performance numbersapproximate power transform multiplication additionsuse rule find long take quantity double given growth ratemodel system queue leverage little lawcheck system design book want learn designing large scale system
51,Lobsters,scaling,Scaling and architecture,"Build platforms that flexibly mix SQL, batch, and stream processing paradigms",https://github.com/gazette/core,build platform flexibly mix sql batch stream processing paradigm,overview start,overview gazette make easy build platform flexibly mix sql batch millisecondlatency streaming processing paradigm enables team application analyst work common catalog data way convenient gazette core abstraction journal streaming append log represented using regular file blob store ie magic representation journal simultaneously lowlatency data stream collection immutable organized file cloud storage aka data lake collection directly integrated familiar processing tool sql engine atop journal broker service gazette offer powerful consumer framework building streaming application go gazette served production use case nearly five year deployment scaled million streamed record per second start
52,Lobsters,scaling,Scaling and architecture,The Direction and Magnitude of SiFive Intelligence,https://www.youtube.com/watch?v=7NpGmerzSVI,direction magnitude sifive intelligence,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature sifive direction magnitude sifive intelligence youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature sifive direction magnitude sifive intelligence youtube
54,Lobsters,scaling,Scaling and architecture,"Bryan Cantrill talks Sun Microsystems, DTrace, and Shouting in the Data Center",https://www.youtube.com/watch?v=_IYzD_NR0W4,bryan cantrill talk sun microsystems dtrace shouting data center,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature bryan cantrill talk sun microsystems dtrace shouting data center youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature bryan cantrill talk sun microsystems dtrace shouting data center youtube
55,Lobsters,scaling,Scaling and architecture,C10K Problem - strategies for handling many connections,http://www.kegel.com/c10k.html,problem strategy handling many connection,help save best linux news source web subscribe linux weekly news offered several isps content fast unix server web page presentation slashdot unix network programming networking apis socket xti volume thundering herd jeff darcy note highperformance server design building scalable web site ace asio libevent nice graph time handle one event poller benchmark compare performance various apis rn paper cory nelson scale library serve many client thread use nonblocking io readiness notification serve many client thread use nonblocking io readiness notification serve many client server thread use asynchronous io serve one client server thread use blocking io build server code kernel edge triggered bsdcon paper kqueue flash usenix freebsd single unix specification thanks chuck lever november freebsdhackers list vivek pei et al reported pollerselect cc h implemented benchmarked niels provo pollerpoll cc h benchmark see patchid solaris according sun pollerdevpoll cc h benchmark see leveltriggered banga mogul drusha netbsdcurrent oct kqueue kevent jonathan lemon page bsdcon paper kqueue pollerkqueue cc h benchmark devepoll wwwxmailserverorglinuxpatchesnioimprovehtml unifying epoll aio event source polyakov kevent news flash feb july evgeniy polyakov posted patch seem unify epoll aio goal support network aio drepper new network interface sigwaitinfo flush signal queue temporarily changing signal handler sigdfl pollersigio cc h zach brown phhttpd provo lever tweedie behave appropriately vitaly luban announced patch implementing scheme may wwwlubanorggplgplhtml dkftpbench pollersigfd cc h aio interface aiosuspend dafs api approach aio red hat note evgeniy polyakov keventbased aio sgi implemented highspeed aio programming real world sunsite linus suggests inside io completion port amazon mspress u patent msdn event bad idea highconcurrency server linuxtheads ngpt announced nptl ulrich drepper glibc ingo molnar jerry cooperstein article march bill abt ngpt team glibc maintainer ulrich drepper others met et al fast userspace mutexes futexes described ulrich initial benchmark wwwunobviouscombsdfreebsdthreadshtml jeff roberson posted freebsdarch robert watson proposed threading implementation become default freebsd implementation scheduler activation netbsd operating system sun multithreaded programming guide sun note java solaris threading volanomark table khttpd tux threaded linux webserver september announcement ftp ftpredhatcompubredhattux zach brown remark netmap sandstorm paper discussing io option measured various strategy source code userver dean gaudet posted editorial article comment earlier draft ingo molnar rebuttal april russinovich comment may rebuttal post linuxkernel kaio sysinternalscom msdn support io completion port simultaneous connection second week posted posted analysis pollselect overhead posted work improving select poll posted bit possible api replace poll select suggested http pointed asserted compared noted posted continued doug royer solaris faq question etcloginconf bodo bauer proc documentation posted oskar post create thread mingo posted patch intel volano detailed instruction raising file thread fdset limit kernel volano detailed benchmark info info tune various system onethreadperclient model jdk javanio release note thread polling api javanbio sandstorm performance connection dean gaudet essay paper jaguar system interfacing java virtual interface architecture volano scalability benchmark http wwwacmecomsoftwarethttpdbenchmarkshtml linux weekly news kernel traffic linuxkernel mailing list mindcraft redux page article mindcraft april benchmark linux scalability project niels provo hinting poll patch thundering herd problem mike jagdis work improving select poll mike post mohit aron aron csriceedu writes ratebased clocking tcp improve http response time slow connection http wwwacmecomsoftwarethttpdbenchmarkshtml old note comparing thttpd apache chuck lever keep reminding u banga druschel paper web server benchmarking java server benchmark nginx book fourth edition scalable network io linux cmogstored chromium source available original announcement zach brown phhttpd siginfo post may translation belorussian translation ucallweconn changelog return wwwkegelcom,help save best linux news source web subscribe linux weekly news time web server handle ten thousand client simultaneously nt think web big place computer big buy machine gigabyte ram ethernet card let see client per client nt take horsepower take four kilobyte disk send network second twenty thousand client work per client way licensing fee operating system charge starting look little heavy hardware longer bottleneck one busiest ftp site cdromcom actually handled client simultaneously gigabit ethernet pipe speed offered several isps expect become increasingly popular large business customer thin client model computing appears coming back style time server internet serving thousand client mind note configure operating system write code support thousand client discussion center around unixlike operating system personal area interest window also covered bit content see nick black execellent fast unix server page look situation october felix von leitner put together excellent web page presentation network scalability complete benchmark comparing various networking system call operating system one observation linux kernel really beat kernel many many good graph give o developer food thought time see also slashdot comment interesting see whether anyone followup benchmark improving felix result nt read already go get copy unix network programming networking apis socket xti volume late w richard stevens describes many io strategy pitfall related writing highperformance server even talk thundering herd problem go read jeff darcy note highperformance server design another book might helpful using rather writing web server building scalable web site cal henderson prepackaged library available abstract technique presented insulating code operating system making portable ace heavyweight c io framework contains objectoriented implementation io strategy many useful thing particular reactor oo way nonblocking io proactor oo way asynchronous io asio c io framework becoming part boost library like ace updated stl era libevent lightweight c io framework niels provo support kqueue select soon support poll epoll leveltriggered think good bad side niels nice graph time handle one event function number connection show kqueue sysepoll clear winner attempt lightweight framework sadly kept date poller lightweight c io framework implement leveltriggered readiness api using whatever underlying readiness api want poll select devpoll kqueue sigio useful benchmark compare performance various apis document link poller subclass illustrate readiness apis used rn lightweight c io framework second try poller lgpl easier use commercial apps c easier use nonc apps used commercial product matt welsh wrote paper april balance use worker thread eventdriven technique building scalable server paper describes part sandstorm io framework cory nelson scale library async socket file pipe io library window designer networking software many option whether issue multiple io call single thread nt use blockingsynchronous call throughout possibly use multiple thread process achieve concurrency use nonblocking call eg write socket set ononblock start io readiness notification eg poll devpoll know ok start next io channel generally usable network io disk io use asynchronous call eg aiowrite start io completion notification eg signal completion port know io finish good network disk io control code servicing client one process client classic unix approach used since one oslevel thread handle many client client controlled userlevel thread eg gnu state thread classic java green thread state machine bit esoteric popular circle favorite continuation bit esoteric popular circle one oslevel thread client eg classic java native thread one oslevel thread active client eg tomcat apache front end nt completion port thread pool whether use standard o service put code kernel eg custom driver kernel module vxd following five combination seem popular serve many client thread use nonblocking io leveltriggered readiness notification serve many client thread use nonblocking io readiness change notification serve many client server thread use asynchronous io serve one client server thread use blocking io build server code kernel set nonblocking mode network handle use select poll tell network handle data waiting traditional favorite scheme kernel tell whether file descriptor ready whether done anything file descriptor since last time kernel told name level triggered come computer hardware design opposite edge triggered jonathon lemon introduced term bsdcon paper kqueue note particularly important remember readiness notification kernel hint file descriptor might ready anymore try read important use nonblocking mode using readiness notification important bottleneck method read sendfile disk block page core moment setting nonblocking mode disk file handle effect thing go memorymapped disk file first time server need disk io process block client must wait raw nonthreaded performance go waste asynchronous io system lack aio worker thread process disk io also get around bottleneck one approach use memorymapped file mincore indicates io needed ask worker io continue handling network traffic jef poskanzer mention pai druschel zwaenepoel flash web server us trick gave talk usenix look like mincore available bsdderived unix like freebsd solaris part single unix specification available part linux kernel thanks chuck lever november freebsdhackers list vivek pei et al reported good result using systemwide profiling flash web server attack bottleneck one bottleneck found mincore guess nt good idea another fact sendfile block disk access improved performance introducing modified sendfile return something like ewouldblock disk page fetching yet core sure tell user page resident seems really needed aiosendfile end result optimization score freebsd box better anything file specorg several way single thread tell set nonblocking socket ready io traditional select unfortunately select limited fdsetsize handle limit compiled standard library user program version c library let raise limit user app compile time see pollerselect cc h example use select interchangeably readiness notification scheme traditional poll hardcoded limit number file descriptor poll handle get slow thousand since file descriptor idle one time scanning thousand file descriptor take time o eg solaris speed poll et al use technique like poll hinting implemented benchmarked niels provo linux see pollerpoll cc h benchmark example use poll interchangeably readiness notification scheme devpoll recommended poll replacement solaris idea behind devpoll take advantage fact often poll called many time argument devpoll get open handle devpoll tell o file interested writing handle read set currently ready file descriptor handle appeared quietly solaris see patchid first public appearance solaris according sun client overhead poll various implementation devpoll tried linux none perform well epoll never really completed devpoll use linux recommended see pollerdevpoll cc h benchmark example use devpoll interchangeably many readiness notification scheme caution example linux devpoll might work right solaris kqueue recommended poll replacement freebsd soon netbsd see kqueue specify either edge triggering level triggering readiness change notification edgetriggered readiness notification mean give kernel file descriptor later descriptor transition ready ready kernel notifies somehow assumes know file descriptor ready send readiness notification type file descriptor something cause file descriptor longer ready eg receive ewouldblock error send recv accept call send recv transfer le requested number byte use readiness change notification must prepared spurious event since one common implementation signal readiness whenever packet received regardless whether file descriptor already ready opposite leveltriggered readiness notification bit le forgiving programming mistake since miss one event connection event get stuck forever nevertheless found edgetriggered readiness notification made programming nonblocking client openssl easier worth trying banga mogul drusha described kind scheme several apis let application retrieve file descriptor became ready notification kqueue recommended edgetriggered poll replacement freebsd soon netbsd freebsd later netbsdcurrent oct support generalized alternative poll called kqueue kevent support edgetriggering leveltriggering see also jonathan lemon page bsdcon paper kqueue like devpoll allocate listening object rather opening file devpoll call kqueue allocate one change event listening get list current event call kevent descriptor returned kqueue listen socket readiness also plain file readiness signal even io completion note october threading library freebsd interact well kqueue evidently kqueue block entire process block calling thread see pollerkqueue cc h benchmark example use kqueue interchangeably many readiness notification scheme example library using kqueue epoll recommended edgetriggered poll replacement linux kernel july davide libenzi proposed alternative realtime signal patch provides call devepoll wwwxmailserverorglinuxpatchesnioimprovehtml like realtime signal readiness notification coalesces redundant event efficient scheme bulk event retrieval epoll merged kernel tree interface changed special file dev system call sysepoll patch older version epoll available kernel lengthy debate unifying epoll aio event source linuxkernel mailing list around halloween may yet happen davide concentrating firming epoll general first polyakov kevent linux news flash feb july evgeniy polyakov posted patch seem unify epoll aio goal support network aio see drepper new network interface proposal linux ols ulrich drepper proposed new highspeed asynchronous networking api see realtime signal recommended edgetriggered poll replacement linux kernel linux kernel deliver socket readiness event via particular realtime signal turn behavior mask sigio signal want use sigemptyset sigset sigaddset sigset signum sigaddset sigset sigio sigprocmask sigblock msigset null file descriptor invoke fsetown fsetsig set oasync fcntl fd fsetown int getpid fcntl fd fsetsig signum flag fcntl fd fgetfl flag ononblockoasync fcntl fd fsetfl flag sends signal normal io function like read write completes use write normal poll outer loop inside handled fd noticed poll loop calling sigwaitinfo sigwaitinfo sigtimedwait return realtime signal siginfosifd siginfosiband give almost information pollfdfd pollfdrevents would call poll handle io continue calling sigwaitinfo sigwaitinfo return traditional sigio signal queue overflowed flush signal queue temporarily changing signal handler sigdfl break back outer poll loop see pollersigio cc h example use rtsignals interchangeably many readiness notification scheme see zach brown phhttpd example code us feature directly nt phhttpd bit hard figure provo lever tweedie describes recent benchmark phhttpd using variant sigtimedwait let retrieve multiple signal one call interestingly chief benefit seemed allowed app gauge system overload could behave appropriately note poll provides measure system overload signalperfd chandra mosberger proposed modification realtime signal approach called signalperfd reduces eliminates realtime signal queue overflow coalescing redundant event nt outperform epoll though paper compare performance scheme select devpoll vitaly luban announced patch implementing scheme may patch life wwwlubanorggplgplhtml note sept may still stability problem patch heavy load dkftpbench user may able trigger oops see pollersigfd cc h example use signalperfd interchangeably many readiness notification scheme yet become popular unix probably operating system support asynchronous io also possibly like nonblocking io requires rethinking application standard unix asynchronous io provided aio interface scroll link asynchronous input output associate signal value io operation signal value queued delivered efficiently user process posix realtime extension also single unix specification version aio normally used edgetriggered completion notification ie signal queued operation complete also used level triggered completion notification calling aiosuspend though suspect people glibc later provide generic implementation written standard compliance rather performance ben lahaise implementation linux aio merged main linux kernel nt use kernel thread efficient underlying api nt yet support socket also aio patch kernel implementation somewhat different info suparna also suggests look dafs api approach aio red hat suse sle provide highperformance implementation kernel related completely identical kernel implementation february new attempt made provide network aio see note evgeniy polyakov keventbased aio sgi implemented highspeed aio linux version said work well disk io socket seems use kernel thread still useful people ca nt wait ben aio support socket oreilly book programming real world said include good introduction aio tutorial earlier nonstandard aio implementation solaris online sunsite probably worth look keep mind need mentally convert aioread aioread etc note aio nt provide way open file without blocking disk io care sleep caused opening disk file linus suggests simply open different thread rather wishing aioopen system call window asynchronous io associated term overlapped io iocp io completion port microsoft iocp combine technique prior art like asynchronous io like aiowrite queued completion notification like using aiosigevent field aiowrite new idea holding back request try keep number running thread associated single iocp constant information see inside io completion port mark russinovich sysinternalscom jeffrey richter book programming serverside application microsoft window amazon mspress u patent msdn let read write block disadvantage using whole stack frame client cost memory many o also trouble handling hundred thread thread get stack uncommon default value run virtual memory thread bit machine useraccessible vm like say linux normally shipped work around giving thread smaller stack since thread library nt allow growing thread stack created mean designing program minimize stack use also work around moving bit processor thread support linux freebsd solaris improving bit processor around corner even mainstream user perhaps nottoodistant future prefer using one thread per client able use paradigm even client nevertheless current time actually want support many client probably better using paradigm unabashedly prothread viewpoint see event bad idea highconcurrency server von behren condit brewer ucb presented hotos ix anyone antithread camp care point paper rebuts one linuxtheads name standard linux thread library integrated glibc since mostly posixcompliant le stellar performance signal support ngpt project started ibm bring good posixcompliant thread support linux stable version work well ngpt team announced putting ngpt codebase supportonly mode feel best way support community long term ngpt team continue working improve linux thread support focused improving nptl kudos ngpt team good work graceful way conceded nptl nptl project ulrich drepper benevolent dicthhhhmaintainer glibc ingo molnar bring worldclass posix threading support linux october nptl merged glibc cv tree addon directory like linuxthreads almost certainly released along next release glibc first major distribution include early snapshot nptl red hat bit inconvenient user somebody break ice nptl link try describing history nptl see also jerry cooperstein article march bill abt ngpt team glibc maintainer ulrich drepper others met figure linuxthreads one idea came meeting improve mutex performance rusty russell et al subsequently implemented fast userspace mutexes futexes used ngpt nptl attendee figured ngpt merged glibc ulrich drepper though nt like ngpt figured could better ever tried contribute patch glibc may come big surprise next month ulrich drepper ingo molnar others contributed glibc kernel change make something called native posix thread library nptl nptl us kernel enhancement designed ngpt take advantage new one ingo molnar described kernel enhancement follows nptl us three kernel feature introduced ngpt getpid return pid clonethread futexes nptl also us relies much wider set new kernel feature developed part project item ngpt introduced kernel around got modified cleaned extended thread group handling clonethread clonethread change impacted ngpt compatibility got synced ngpt folk make sure ngpt break unacceptable way kernel feature developed used nptl described design whitepaper http peopleredhatcomdreppernptldesignpdf short list tl support various clone extension clonesettls clonesettid clonecleartid posix threadsignal handling sysexit extension release tid futex upon vmrelease sysexitgroup systemcall sysexecve enhancement support detached thread also work put extending pid space eg procfs crashed due pid assumption maxpid pid allocation scalability work plus number performanceonly improvement done well essence new feature nocompromises approach threading kernel help everything improve threading precisely minimally necessary set context switch kernel call every basic threading primitive one big difference two nptl threading model whereas ngpt n threading model see spite ulrich initial benchmark seem show nptl indeed much faster ngpt ngpt team looking forward seeing ulrich benchmark code verify result freebsd support linuxthreads userspace threading library also n implementation called kse introduced freebsd one overview see wwwunobviouscombsdfreebsdthreadshtml mar jeff roberson posted freebsdarch thanks foundation provided julian david xu mini dan eischen everyone else participated kse libpthread development mini developed threading implementation code work parallel kse break way actually help bring n threading closer testing shared bit july robert watson proposed threading implementation become default freebsd know discussed past figured trundling forward time think benchmark many common application scenario libthr demonstrates significantly better performance libpthread libthr also implemented across larger number platform already libpthread several first recommendation make mysql heavy thread user switch libthr suggestive also strawman proposal make libthr default threading library according note noriyuki soda kernel supported n thread library based scheduler activation model merged netbsdcurrent jan detail see implementation scheduler activation netbsd operating system nathan j williams wasabi system inc presented freenix thread support solaris evolving solaris solaris default threading library used n model solaris default model thread support see sun multithreaded programming guide sun note java solaris threading well known java support method handling network connection one thread per client volanomark good microbenchmark measure throughput messsages per second various number simultaneous connection may jdk implementation various vendor fact able handle ten thousand simultaneous connection albeit significant performance degradation see table idea jvms handle connection performance suffers number connection increase choice implementing threading library either put threading support kernel called threading model move fair bit userspace called n threading model one point n thought higher performance complex hard get right people moving away novell microsoft said done various time least one nfs implementation khttpd linux static web page tux threaded linux webserver blindingly fast flexible kernelspace http server ingo molnar linux ingo september announcement say alpha version tux downloaded ftp ftpredhatcompubredhattux explains join mailing list info linuxkernel list discussing pro con approach consensus seems instead moving web server kernel kernel smallest possible hook added improve web server performance way kind server benefit see eg zach brown remark userland v kernel http server appears linux kernel provides sufficient power user program server run fast tux nt use kernel modification see instance netmap packet io framework sandstorm proofofconcept web server based richard gooch written paper discussing io option tim brecht mmichal ostrowski measured various strategy simple selectbased server data worth look tim brecht posted source code userver small web server put together several server written abhishek chandra david mosberger david pariag michal ostrowski use select poll epoll sigio back march dean gaudet posted keep getting asked nt guy use selectevent based model like zeus clearly fastest reason boiled really hard payoff nt clear within month though became clear people willing work mark russinovich wrote editorial article discussing io strategy issue linux kernel worth reading even seems misinformed point particular seems think linux asynchronous io see fsetsig nt notify user process data ready new connection arrive seems like bizarre misunderstanding see also comment earlier draft ingo molnar rebuttal april russinovich comment may rebuttal alan cox various post linuxkernel suspect trying say linux nt support asynchronous disk io used true sgi implemented kaio true anymore see page sysinternalscom msdn information completion port said unique nt nutshell overlapped io turned low level convenient completion port wrapper provides queue completion event plus scheduling magic try keep number running thread constant allowing thread pick completion event thread picked completion event port sleeping perhaps blocking io see also support io completion port interesting discussion linuxkernel september titled simultaneous connection second week thread highlight ed hall posted note experience achieved connectssecond running solaris code used small pool thread per cpu managing large number client using eventbased model mike jagdis posted analysis pollselect overhead said current selectpoll implementation improved significantly especially blocking case overhead still increase number descriptor selectpoll remember descriptor interesting would easy fix new api suggestion welcome mike posted work improving select poll mike posted bit possible api replace poll select device like api write pollfd like structs device listens event delivers pollfd like structs representing read rogier wolff suggested using api digital guy suggested http joerg pommnitz pointed new api along line able wait file descriptor event also signal maybe sysvipc synchronization primitive certainly able waitformultipleobjects least stephen tweedie asserted combination fsetsig queued realtime signal sigwaitinfo superset api proposed http also mention keep signal blocked time interested performance instead signal delivered asynchronously process grab next one queue sigwaitinfo jayson nordwick compared completion port fsetsig synchronous event model concluded pretty similar alan cox noted older rev sct sigio patch included jordan mendelson posted example code showing use fsetsig stephen c tweedie continued comparison completion port fsetsig noted signal dequeuing mechanism application going get signal destined various library component library using mechanism library set signal handler nt affect program much doug royer noted gotten connection solaris working sun calendar server others chimed estimate much ram would require linux bottleneck would hit interesting reading unix limit set ulimit setrlimit solaris see solaris faq question thereabouts renumber question periodically freebsd edit bootloaderconf add line set kernmaxfilesxxxx xxxx desired system limit file descriptor reboot thanks anonymous reader wrote say achieved far connection freebsd say fwiw ca nt actually tune maximum number connection freebsd trivially via sysctl bootloaderconf file reason zalloci call initializing socket tcpcb structure zone occurs early system startup order zone type stable swappable also need set number mbufs much higher since unmodified kernel chew one mbuf per connection tcptempl structure used implement keepalive another reader say freebsd tcptempl structure longer allocated longer worry one mbuf chewed per connection see also openbsd reader say openbsd additional tweak required increase number open filehandles available per process openfilescur parameter etcloginconf need increased change kernmaxfiles either sysctl w sysctlconf effect matter shipped loginconf limit quite low nonprivileged process privileged linux see bodo bauer proc documentation kernel echo procsysfsfilemax increase system limit open file ulimit n increase current process limit kernel echo procsysfsfilemax echo procsysfsinodemax increase system limit open file ulimit n increase current process limit verified process red hat plus patch open least file descriptor way another fellow verified process open least file descriptor way appropriate limit upper bound seems available memory stephen c tweedie posted set ulimit limit globally peruser boot time using initscript pamlimit older kernel though number open file per process still limited even change see also oskar post talk perprocess systemwide limit file descriptor kernel architecture may need reduce amount stack space allocated thread avoid running virtual memory set runtime pthreadattrinit using pthreads solaris support many thread fit memory hear linux kernel nptl procsysvmmaxmapcount may need increased go thread need use small stack thread get anywhere near number thread though unless bit processor see nptl mailing list eg thread subject create thread info linux procsyskernelthreadsmax max number thread default red hat system set increase usual echoing new value file eg echo procsyskernelthreadsmax linux even kernel limit number thread least intel nt know limit architecture mingo posted patch intel removed limit appears integrated see also volano detailed instruction raising file thread fdset limit kernel wow document step lot stuff would hard figure somewhat dated java see volano detailed benchmark info plus info tune various system handle lot thread jdk java standard networking library mostly offered onethreadperclient model way nonblocking read way nonblocking writes may jdk introduced package javanio provide full support nonblocking io goody see release note caveat try give sun feedback hp java also includes thread polling api matt welsh implemented nonblocking socket java performance benchmark show advantage blocking socket server handling many connection class library called javanbio part sandstorm project benchmark showing performance connection available see also dean gaudet essay subject java network io thread paper matt welsh event v worker thread nio several proposal improving java networking apis matt welsh jaguar system proposes preserialized object new java bytecodes memory management change allow use asynchronous io java interfacing java virtual interface architecture cc chang von eicken proposes memory management change allow use asynchronous io java sun project came javanio package matt welsh participated say sun nt listen old system library might use bit variable hold file handle cause trouble handle ok many system use bit variable hold process thread id would interesting port volano scalability benchmark c see upper limit number thread various operating system much threadlocal memory preallocated operating system thread get total vm space creates upper limit thread look performance comparison graph bottom http wwwacmecomsoftwarethttpdbenchmarkshtml notice various server trouble connection even solaris anyone figure let know note tcp stack bug cause short delay syn fin time linux o http daemon hard limit number connection open would expect exactly behavior may cause linux look like kernel bottleneck fixed constantly see linux weekly news kernel traffic linuxkernel mailing list mindcraft redux page march microsoft sponsored benchmark comparing nt linux serving large number http smb client failed see good result linux see also article mindcraft april benchmark info see also linux scalability project interesting work including niels provo hinting poll patch work thundering herd problem see also mike jagdis work improving select poll mike post mohit aron aron csriceedu writes ratebased clocking tcp improve http response time slow connection two test particular simple interesting hard raw connection per second many byte file per second serve total transfer rate large file many slow client many modem client simultaneously download server performance go pot jef poskanzer published benchmark comparing many web server see http wwwacmecomsoftwarethttpdbenchmarkshtml result also old note comparing thttpd apache may interest beginner chuck lever keep reminding u banga druschel paper web server benchmarking worth read ibm excellent paper titled java server benchmark baylor et al worth read nginx web server us whatever highefficiency network event mechanism available target o getting popular even book since page originally written many including fourth edition book n provo c lever scalable network io linux may freenix track proc usenix san diego california june describes version thttpd modified support devpoll performance compared phhttpd cmogstored us epollkqueue networking thread disk chromium us kernel sigio feature together sendfile tcpcork reportedly achieves higher speed even tux source available community source open source license see original announcement fabio riccardi zach brown phhttpd quick web server written showcase sigiosiginfo event model consider code highly experimental highly mental try use production environment us siginfo feature later includes needed patch earlier kernel rumored even faster khttpd see post may note translation belorussian translation provided patric conrad ucallweconn changelog added nginxorg log v revision dank added asio revision dank link cal henderson book revision dank listify polyakov link add drepper new proposal note freebsd might move revision dank link scale library updated polyakov link revision dank link polyakov patch revision dank link linus message deprecating idea aioopen revision dank link userver revision dank link vivek pei new flash paper mention great score copyright dan kegel dank kegelcom last updated february minor correction may return wwwkegelcom
56,Lobsters,scaling,Scaling and architecture,Why are services slow sometimes?,https://dev.to/aws/why-are-services-slow-sometimes-mn3,service slow sometimes,zipkin aws xray blog post dr neil gunther distribution known pareto hyperbolic useful work jim brady neil gunther paper jim brady problematic measurement misleading linux includes operating system thread blocked waiting disk io well minute minute minute talked depth past added new post subject manage maximum response time neil gunther,built service call something return result long take take longer user would like time post start basic gradually introduce standardized terminology thing make answer question complicated highlighting key point know start need way measure long take understand two fundamentally different point view measure experience outside user calling service measure long take respond instrument code measure request start finish run measuring service lead first key point people get sloppy terminology often nt clear got measurement careful measure response time user service time service real world example many step process step take time time step residence time consists wait time service time example user launch app iphone call web service authenticate user might slow sometimes amount actual service time work required generate request phone transmit web service lookup user return result display next step pretty much every time variation response time driven waiting line queueing resource also processing request network transmission iphone authentication server many hop hop queue packet waiting line sent queue empty short response quick queue long response slow request arrives server also queue waiting cpu start processing request database lookup needed another queue get done waiting queue main reason response time increase instrumentation get monitoring tool measure often something completed call throughput case also measure incoming work arrival rate simple case like web service steady state workload one request result one response however retries error increase arrival without increasing throughput rapidly changing workload long request like batch job see temporary imbalance arrival completed throughput possible construct complex request pattern throughput number completed successful request look see arrival rate different sure know actually measured possible measure single request flowing system using tracing mechanism like zipkin aws xray discussion think effect large number request interact average behavior measured fixed time interval could second minute hour day need enough data average together without going theory rule thumb least data point average infrequent request pick time period average least request together get useful measurement time period coarse hide variation workload example video conferencing system measuring hourly call rate miss fact call start around first minute hour easy get peak overload system persecond measurement appropriate spiky workload use high resolution onesecond average measurement monitoring tool vary however rare get direct measurement long line various queue also always obvious much concurrency available process queue network one packet time transit cpu core vcpu work parallel process run queue database often fixed maximum number connection client limit concurrency step processing request record estimate concurrency used process think system running steady state stable average throughput response time estimate queue length simply multiplying throughput residence time known little law simple often used monitoring tool generate queue length estimate true steady state average randomly arriving work little law average queue average throughput average residence understand work nt important understand work arrives service determines gap request running simple performance test loop gap request constant little law nt apply queue short test nt realistic unless trying simulate conveyor belt like situation common mistake kind test successfully put service production watch get slow fall much lower throughput test workload constant rate loop test nt generate queue simulate conveyor belt real world internet traffic many independent user nt coordinating make single request interval request random want load generator randomizes wait time request way system us uniformly distributed random distribution better conveyor belt nt correct simulate web traffic little law apply need use use negative exponential distribution described blog post dr neil gunther proper random think time calculation needed generate realistic queue however get worse turn network traffic randomly distributed come burst burst come clump think actually happens user start iphone app nt make one request make burst request addition user taking part flash sale synchronized visit app around time causing clump burst traffic distribution known pareto hyperbolic addition network reconfigure traffic delayed queue build flood downstream system thundering herd update useful work jim brady neil gunther configure load testing tool realistic paper jim brady measuring well test load behaving truly real world workload bursty higher queue long tail response time common load test tool generate default expect queue response time vary long tail extremely slow request even best time average utilization low happens one step process start get busy processing step nt available concurrency like network transmission utilization increase probability request contend increase residence time rule thumb network gradually start get slow around utilization plan keep network utilization good latency utilization also problematic measurement misleading defined proportion time something busy cpu execution happening parallel slow happens higher utilization kick harder surprise make intuitive sense think last available cpu point contention example vcpus last cpu last capacity residence time kick sharply around utilization vcpu system kick around utilization formula approximates behavior randomly arriving request steady state condition apply little law r inflation average residence time utilization increase reduced multiprocessor system hit wall harder unpicking take average utilization proportion percentage raise power number processor subtract divide average service time get estimate average residence time average utilization low dividing number near mean average residence time nearly average service time network concurrency average utilization mean dividing average residence time three time higher low utilization rule thumb keep inflation average residence time throughout system maintain good average user visible response time vcpu system average utilization dividing roughly double average residence time average utilization dividing average residence time go acceptable slow increase average utilization problem running multiprocessor system high average utilization small change workload level increasingly large impact standard unixlinux metric called load average poorly understood several problem unix system including solarisaixhpux record number operating system thread running waiting run cpu linux includes operating system thread blocked waiting disk io well maintains three time decayed value minute minute minute first thing understand metric date back single cpu system would always divide load average value number vcpus get measure comparable across system second measuring across fixed time interval like metric kind average build delayed response third linux implementation bug become institutionalized feature inflates result terrible metric monitor alert feed autoscaling algorithm load average nt measure load nt average best ignored system overrun work arrives processed reach utilization formula divide zero predicts infinite residence time practice worse system becomes slow first thing happens upstream user system retry request magnifies amount work done cause retry storm system long queue work go catatonic state responding system hit sustained average utilization become unresponsive build large queue work look timeouts retries configured often find many retries timeouts far long increase work amplification make retry storm likely talked depth past added new post subject best strategy short timeout single retry possible different connection go different instance service timeouts never set across system need much longer front end edge much shorter deep inside system usual operator response clear overloaded queue rebooting system well designed system limit queue shed work silently dropping generating fastfail response incoming request database service fixed maximum number connection behave like ca nt get another connection make request get fast fail response connection limit set low database reject work capacity process set high database slow much reject incoming work think shed incoming work system reach average utilization configuration limit set best way maintain good response time extreme condition fail fast shed incoming load even normal condition real world system long tail slow response time however right measurement manage anticipate problem careful design testing possible build system manage maximum response time user request dig deeper topic lot good information neil gunther blog book training class worked neil develop summer school performance training class hosted stanford late running event ever since copresenting neil formative experience deep dive queuing theory really solidified mental model system behave picture line waiting barrel taken adrian wine cellar bordeaux
57,Lobsters,scaling,Scaling and architecture,To Microservices and Back Again - Why Segment Went Back to a Monolith,https://www.infoq.com/news/2020/04/microservices-back-again/,microservices back segment went back monolith,qcon london alexandra noonan segment centrifuge microservices still worth,almost every engineering team considered moving microservices point advantage bring come serious tradeoff qcon london alexandra noonan told segment broke monolith microservices year later went back monolithic architecture noonan word microservices implemented incorrectly used bandaid without addressing root flaw system unable new product development drowning complexity microservices first introduced address limited fault isolation segment monolith however company became successful integrated external service operational overhead supporting microservices became much bear decision move back monolith came new architecture considered pain point around scaling related company growth making sacrifice modularity environmental isolation visibility monolith addressed major issue operational overhead allowed engineering team get back new feature development noonan explained several key point evolution segment architecture problem faced decision made time sounded familiar experienced software engineer advantage hindsight clear decision could better noonan explained major decision point timeline noted pro con state system architecture segment started monolithic architecture provided low operational overhead lacked environmental isolation segment functionality based around integrating data many different provider monolith problem connecting one provider destination could adverse effect destination entire system lack isolation within monolith addressed moving microservices one worker service per destination microservices also improved modularity visibility throughout system allowing team easily see queue length identify problem worker noonan pointed visibility built monolith got free microservices however microservices came increased operational overhead problem around code reuse period hypergrowth segment around added new destination three per month code repository service manageable handful destination worker became problem scale increased shared library created provide behavior similar worker however created new bottleneck change shared code could require week developer effort mostly due testing constraint creating version shared library made code change quicker implement reversed benefit shared code intended provide noonan pointed limitation onesizefitsall approach microservices much effort required add new service implementation customized one autoscaling rule applied service despite vastly different load cpu resource need also proper solution true fault isolation would one microservice per queue per customer would required microservices decision move back monolith considered tradeoff including comfortable losing benefit microservices resulting architecture named centrifuge able handle billion message per day sent dozen public apis single code repository destination worker use version shared library larger worker better able handle spike load adding new destination longer add operational overhead deployment take minute important business able start building new product team felt benefit worth reduced modularity environmental isolation visibility came free microservices qcon attendee discussing presentation sounded like typical engineer joining project long history quick remark well obviously nt countered voice experience pointing decision made based best information available time one key takeaway spending day week analysis could avoid situation take year correct editor note updated qcon noonan also participated panel discussion microservices still worth available watch infoq
58,Lobsters,scaling,Scaling and architecture,Comparison of instruction set architectures,https://en.wikipedia.org/wiki/Comparison_of_instruction_set_architectures,comparison instruction set architecture,instruction set architecture computer performance interface software hardware binary compatibility computing machine language programmer data type main memory register memory consistency addressing mode machine instruction inputoutput binary decimal ternary computer architecture bit instruction set ibm ibm model edit edit byte risc byte edit power two register file register window register renaming operand register branch endian ness cisc avx mmx sse pae bmi avx aes fma xop alpha risc arc jazelle trustzone jazelle trustzone avr java virtual machine blackfin cdc risc crusoe elbrus mmx sse avx dlx esirisc itanium epic mips mdmx mmix openrisc parisc max cisc power powerpc power isa altivec vsx cell riscv rx score sparc vi superh zarchitecture transputer stack machine misc vax operand register branch endian ness edit edit ibm naval ordnance research calculator russian virtual computer museum hall fame nikolay petrovich brusentsov isbn http http wwwsynopsyscomdesignwareipprocessorsolutionsarcprocessorshtml technology preview original arm go new chip architecture architecture document atmel blackfin manual blackfin processor architecture overview analog device blackfin memory architecture analog device original crusoe exposed transmeta architecture technology behind crusoe processor architecture lattice semiconductor original open source licensing lattice semiconductor original architecture programmer release architecture programmer release mips open openrisc architecture revision user handbook power isa version riscv isa specification oracle sparc processor documentation sparc architecture license,instruction set architecture isa abstract model computer also referred architecture computer architecture realization isa called implementation isa permit multiple implementation may vary performance physical size monetary cost among thing isa serf interface software hardware software written isa run different implementation isa enabled binary compatibility different generation computer easily achieved development computer family development helped lower cost computer increase applicability reason isa one important abstraction computing today isa defines everything machine language programmer need know order program computer isa defines differs isas general isas define supported data type state main memory register semantics memory consistency addressing mode instruction set set machine instruction comprises computer machine language inputoutput model early decade computing computer used binary decimal even ternary contemporary computer almost exclusively binary computer architecture often described nbit architecture today n often size used including actually strong simplification computer architecture often le natural datasizes instruction set hardware implementation may different many instruction set architecture instruction implementation instruction set architecture operate half andor twice size processor major internal datapaths example well many others type implementation twice wide operation typically also take around twice many clock cycle case high performance implementation instance mean instead clock tick particular chip may described architecture implementation ibm instruction set architecture several model series ibm model internal data path external databus width also used determine width architecture basically chip different external data bus bus used register width address may may different width data early microprocessor often address processor operand edit number operand one factor may give indication performance instruction set threeoperand architecture allow b c computed one instruction twooperand architecture allow b computed one instruction two instruction need executed simulate single threeoperand instruction b c endianness edit architecture may use big little endianness configurable use either little endian processor order byte memory least significant byte multibyte value lowestnumbered memory location big endian architecture instead order significant byte lowestnumbered address architecture well several architecture little endian risc architecture sparc power powerpc mips originally big endian arm little endian many including arm configurable endianness applies processor allow individual addressing unit data byte smaller basic addressable machine word instruction set edit usually number register power two eg case hardwiredtozero pseudoregister included part register file architecture mostly simplify indexing mode table count integer register usable general instruction moment architecture always include specialpurpose register program pointer pc counted unless mentioned note architecture sparc register window architecture count indicates many register available within register window also nonarchitected register register renaming counted note common type architecture loadstore synonym register register meaning instruction access memory except special load register store register possible exception atomic memory operation locking table compare basic information instruction set implemented cpu architecture architecture bit version introduced max operand type design register excluding fpvector instruction encoding branch evaluation endianness extension open royaltyfree register memory cisc variable condition register little register memory cisc variable condition register big register memory cisc data address variable condition register big register memory cisc variable bit condition register little register register cisc variable byte compare branch little integer avx register memory cisc segment reg segment reg gscs variable variable byte w mmu intel sdk byte prefix pentium onward byte prefix byte prefix third party emulation byte wo prefix mmu ssemmx byte w prefix avx byte w prefix condition code little mmx sse pae bmi avx aes fma xop alpha register register risc including zero fixed condition register bi arc register register risc including sp user increase variable compare branch bi apex userdefined instruction register register risc fixed condition code bi neon jazelle trustzone lpae register register risc thumb instruction thumb fixed variable condition code bi neon jazelle trustzone lpae register register risc including stack pointer zero register fixed condition code bi none nonoptional avr register register risc reduced architecture variable mostly four instruction condition register skip conditionedon io orgeneral purposeregister bit compare skip little rev risc variable big java virtual machine blackfin register register risc accumulator data register pointer register index register buffer register variable condition code little cdc register memory risc address index operand reg variable compare branch na comparemove unit additionalperipheral processing unit crusoe native vliw register register vliw native push stack emulation mode rename integer shadow debug native vliwmode variable native mode byte emulation condition code little elbrus native vliw register register vliw condition code little justintime dynamic translation mmx sse avx dlx risc fixed big esirisc register register risc variable compare branchand condition register bi userdefined instruction itanium register register epic fixed bit bundle bit template tagand instruction bit long condition register bi selectable intel virtualization technology register register risc variable condition register bi register register risc fixed compare branch big userdefined instruction yes yes mips register register risc including zero fixed condition register bi mdmx mmix register register risc fixed big yes yes memory memory cisc variable huffman coded byte long condition code little bitblt instruction openrisc register register risc fixed yes yes parisc hppa register register risc fixed compare branch big bi max register memory cisc accumulator multiplier quotient register fixed condition register test branch eae extended arithmetic element memory memory cisc includes stack pointer though register canact stack pointer fixed condition code little floating point commercial instruction set power powerpc power isa register register risc fixed variable condition code bigbi altivec apu vsx cell yes yes riscv register register risc including zero variable compare branch little yes yes rx memory memory cisc integer address variable compare branch little score risc little sparc register register risc including zero fixed condition code big bi vi yes yes superh sh register registerregister memory risc fixed variable condition code single bit bi fma distinctoperand facility vector inst register memorymemory memoryregister register cisc variable condition code compare branch big transputer stack machine misc stack variable byte compare branch little vax memory memory cisc variable compare branch little register memory cisc variable bit condition register little architecture bit version introduced max operand type design register excluding fpvector instruction encoding branch evaluation endianness extension open royaltyfree see also edit reference edit da cruz frank october ibm naval ordnance research calculator columbia university computing history retrieved january russian virtual computer museum hall fame nikolay petrovich brusentsov trogemann georg nitussov alexander ernst wolfgang computing russia history computer device information technology revealed viewegteubner verlag pp isbn lea later imulimmediate later instruction accept three operand instruction base integer isa accept two operand http http wwwsynopsyscomdesignwareipprocessorsolutionsarcprocessorshtml technology preview pdf archived original pdf retrieved arm go new chip architecture retrieved may architecture document pdf atmel retrieved blackfin manual pdf analogcom blackfin processor architecture overview analog device retrieved blackfin memory architecture analog device archived original retrieved since memory array word mean access subunit big endian v little endian make sense optional cmu unit us big endian semantics b c e f crusoe exposed transmeta architecture real world technology b c alexander klaiber january technology behind crusoe processor pdf transmeta corporation retrieved december architecture lattice semiconductor archived original june open source licensing lattice semiconductor archived original june architecture programmer release architecture programmer release mips open openrisc architecture revision user handbook pdf bitsaversorg power isa version openpowerfoundationorg retrieved riscv isa specification retrieved june oracle sparc processor documentation sparc architecture license
59,Lobsters,scaling,Scaling and architecture,The Elephant in the Architecture,https://martinfowler.com/articles/value-architectural-attribute.html,elephant architecture,examine architecture value stream mapping consider business value impact failure crossfunctional requirement justified business value business value implies cfrs vary component use monitoring ass business value qa production moving cloud take valuable bit business value vital inconstant business knowledge part technical career path,colleague often called perform architectural assessment client architect involved system talk performance system resilient fault designed evolve easily support new capability elephant rarely come however different system contribute business value value interacts architectural attribute without understanding value many architectural decision become much harder make one example asked adding fault tolerance trading system cost via hot failover another server range ten thousand dollar could cost justified approach ask value trade passed system much revenue provided client looked saw system handling million dollar day trade adding fault tolerance trivially easy justify important thing story nt fact client team neglected ask value part decisionmaking although certainly lesson point architecture team nt know value system different component contributed broader business performance find common gap thinking assessment usually ask talk relevant people finance department usually greeted would want talk another example asked insurance company help break monolithic system microservices identified reasonably wanted separate different product line home personal car etc nt know proportion company profit came product line important element deciding priority breakup like first item separate probably nt important one value term since first separation carry many risk something first time team practiced separation get valuable product line separated make easier modify scale examine architecture value stream mapping good first step assessing business value architecture value stream mapping focus various system component landscape business often kind valuestream mapping business process examining part customer journey impact revenue margin often reach mapping stop attempt made map value stream various system architect build allocating similar business measure system support business process extending mapping system well financial measure important nonfinancial measure also looked extent customer ability check claim status online affect customer retention measure usually difficult make thinking measure often yield important insight recent client conducted exercise starting customer journey described customer interacted client company put step top wall team room tied step system component client portfolio could ass system contributed step customer journey effect failure might consider business value impact failure first example suggested particular importance come assessing consequence failure want take measure improve resilience system good express term value risk fail one retailer struggling justify testing backup restoration process inventory database large backlog businessvisible feature needed christmas season hard prioritize technical task suggestion ask business felt database crash black friday would cost quickly would want system back question led businessled decision around dressrehearsals database restores reducing time recovery otherwise would buried work queue assessment value would risk worth approaching two complimentary way one route go topdown looking business function identifying software system support function reverse starting software system considering ramification failure would important part analyzing much value risk recognizing since different failure differently severe consequence software component need level resilience imagine failure stock management system boxing day british equivalent black friday mean ca nt check stock level either confirm order ca nt fill nt take order cause significant loss however failure fulfillment system might order sitting queue waiting process may lead delayed delivery business leader may consider latter lesser concern thus prepared live lesser resilience system whatever particular key level resilience business decision similarly data consistency looking distributed database architect often balk relaxing consistency availability business cost doublebooking hotel room may much le worrying taking booking trade consistency availability enshrined cap theorem business decision technical one crossfunctional requirement justified business value theme nt assessing value feature also system characteristic often categorized crossfunctional requirement preferred name nonfunctional requirement ilities want system adhere technical standard need understand value lost failing communicate value nontechnical stakeholder assessing value cfrs often difficult many technologist shy away resulting debate failing harm risk overinvesting lowvalue technology also erects real barrier collaboration technologist user understanding value inform decision flexibility component one client payment processing component programmed work particular payment processing company wanted easily configurable could adjust quickly payment processor change two broad option existed one would hardcode interaction payment provider system payment processing component option made interaction configurable configurable option would allow payment provider changed altering configuration file day usually case however configurablity would add complexity component code would increase cost change common tradeoff configurablity ease change configurable bit cost harder change elsewhere vital piece business context hard part changing payment provider actually negotiating new legal contract exercise usually took year configuring provider information nt worth modifying nonconfigurable component still much quicker legal negotiation business value implies cfrs vary component last two example illustrate one size fit approach thing like resilience flexability unhelpful wasteful several year ago worked organisation decided impose blanket five nine availability requirement amazingly even applied sandwich ordering system used staff reserve favourite lunchtime snack found agreeing different tier service business useful example loss service immediately impact customer experince revenue afford hour database restored another issue highlighted understanding system support business value single component support multiple different value flow level reliablilty common monolithic component multiple different business process supported important motivation breaking thing apart example allowing u pay premium high availability justified corresponding business value use monitoring ass business value big fan using rich monitoring get better understanding system behave something particularly important growing complexity increasingly distributed system monitoring often focused system health supporting qa production also use kind monitoring ass contribution system business value determining much sale revenue pass particular component one retail client found monitoring queue length message rate mainframe could good proxy measure revenue taken shop accurate making available business stakeholder let spot issue monitoring purely technical perspective might missed another client able derive accurate revenue measure every transaction web site displayed minute minute screen around office time far attention paid screen showing cpu memory technical measure one might argue gathering data nt part understanding system behavior would assert vital intelligence understanding contribution software component make business system get hosted cloud see bill generated individual faa function get cost granularity also strive gather data benefit regular monitoring data inform investment decision came across government agency used web provide service citizen cost adding new feature web application significantly increased cost support older browser considered mandatory citizen could nt upgrade looking traffic revealed citizen used older browser rather supporting old browser would cheaper give new computer bunch flower moving cloud take valuable bit rise cloud system seeing many organization wishing move software system cloud hosting parallel long history system replacement company look replace existing system something support capability run modern hopefully efficient infrastructure several decade seeing kind effort career seen easy mistake make perhaps common idea simplest way system replacement featureparity replacement seek exactly mimic existing functionality new platform likeforlike approach miss fact much existing functionality little value used actively interfering optimal business process featureparity replacement harder people usually think taking time avoid copying littleused function pay easily one organisation worked kicked migration effort moving logistics handling code new system took several month involved many development staff talked business future plan explained planned drop support many kind packaging due high cost dealing edge case around packaging turned thing consumed time migration yet business would happy without understanding contribution business value thus significant help identifying better replatforming effort existing component nt contributing much value nt copied new platform common case service company service follow common case unusual offering come rarely unusual case often involve special software support marginal case always reviewed reimplemented new platform business expects stop oferring service next six month something understood part reimplementation plan business value vital inconstant anything life software architecture assessment value nt constant worked insurance company competitive advantage rating model software seen one company crown jewel time big shift insurance quote online straightthrough processing crown jewel required lot parameter could reasonably captured preonline era agent meeting customer complicated form unappealing web shift value crown jewel withered away well getting understanding current value software asset worth trying rough forecast value affected change technical business environment another case retailer whose catalog management system coped comfortably updating twice year could nt deal shift rapid online change opportunity cost lost revenue never easy thing quantify need considered deciding place investment restructuring rewriting component business knowledge part technical career path people look growth technical leader attention tends go hard technical subject training course certified le abound various software platform technical skill development advocate training focus core principle rather today popular platform wise skill development realizes much difficult area soft skill note ironyquotes become increasingly important folk rise leadership ladder something endorse valuable though thing also think vital ensure technical leader firm understanding business operate value generated various player domain usually something come training course rather something come regular interaction business leader social interaction begin early technologist career notion separating staff business staff cause untold ill profession like software development whose value rooted much software deeply entwined activity enterprise support developer need learn early constant contact user customer normal learn well year contact reap great reward become leader familiar business grown barrier communication business one sadly enduring theme long career software development architect disconnected understanding flow business value raise cost wasted technical effort loss opportunity presented change environment software leader need put attention interplay business activity software decision making ensure part career development process technical staff
60,Lobsters,scaling,Scaling and architecture,Running Kleroteria for free by (ab)using free tiers,https://www.simonmweber.com/2018/07/09/running-kleroteria-for-free-by-abusing-free-tiers.html,running kleroteria free ab using free tier,running kleroteria free ab using free tier kleroteria autoresponder repominder dynamo math throttling service limit everything else whew go join kleroteria justify effort,running kleroteria free ab using free tier july kleroteria email lottery periodically random subscriber chosen write everyone else built replace listserve recently shut providing year advice hundred personal story one impromptu picnic fun personal corner internet missed immediately suspect creator listserve got tired running first high maintenance lottery post sending done hand second expensive powered mailchimp would cost hundred dollar per month given ten thousand subscriber need address replacement figured could easily run little cost sending email could host entire thing free sure figuring quickly became personal challenge far long digging pricing doc came serverless setup run aws indefinitely free tier illustrative much amazon give away also quickly serverless architecture get hand post lay ridiculous thirteenservice setup inspire future free tier shenanigan side project run unceremoniously cheap virtual private server autoresponder repominder example run deal lowendbox comparable free offering google instance though concerned bottlenecking vcpu signup spike started looking serverless option since free tier seemed generous gcp looked promising google free email offering limited thousand sendsmonth considered azure well turned lack free hosted database aws ended providing roughly piece google benefit free sendsmonth s thing starting take shape could use lambda backend dynamodb storage rds free year s email service limit issue never need storage dynamo throughput concern measured capacity unit boil writessecond eventuallyconsistent readssecond needed make sure could stay limit dynamo math kleroteria store two thing post subscriber decided use random id prevent hot partition full schema ended looking like pendingposts table key randomlygenerated id value content status subscriber table key randomlygenerated id value email address post content capped row le subscriber row hand average row size important since capacity unit measured chunk writes chunk read example consider sending post requires full scan subscriber table table row read perfect chunk sustained scan one read capacity unit would require second using rcus take second similarly subscriber could handled second rcus selecting lottery winner roughly thing full scan random selection originally avoided scan using composite primary key involved twolevel index partitioned first character id sorted full id could select random item two query turned worth trouble though since distribution biased aws limit rcus allows fast enough scan operation straightforward subscribing unsubscribing example cost wcu processed synchronously though could cause issue beyond signupssecond start getting throttled worse charged needed way spread operation spike throttling service limit kleroteria luxury entirely asynchronous connected everything sqs queue allows precise throttling adjusting queue polling rate example subscription sent queue consumed messagesecond never exceed wcu dynamo capacity unit limit play though sqs lambda allow one million free requestsmonth lambda also constrains runtime memory usage s sending rate limit fit within currently running scheduled lambda execution minute second lambda execution message sqs poll second wait sqs poll put executionsmonth pollsmonth wcu emailsecond pretty conservative plan increase message limit live putting together piece involved subscription handling look like browser aws sdk cognito sqs v cloudwatch event lambda dynamo s note frontend enqueues directly sqs avoids api gateway free perrequest lambda execution come cost though javascript must enabled sign give user feedback serverside validation hoping matter much practice everything else astute reader count six service far promised thirteen seventh iam used internal access control always free since essential part aws remaining six service aws one netlify used host frontend asset found comparable github page usual choice though noticed surprisingly flaky uptime according new relic synthetic external monitoring tool remaining service play supporting role require much commentary sentry error reporting frontend lambda google analytics frontend analytics bitbucket private git repo cloudflare dns cdn disabled since netlify provides one whew thirteen service ten thousand subscriber per month worth probably least context side project reading pricing doc planning dynamo capacity setting local environment added day weekend project said fun challenge result robust usual vps setup go join kleroteria justify effort interested hearing getting notified open source consider subscribing link
61,Lobsters,scaling,Scaling and architecture,When scaling your workload is a matter of saving lives - Modeling the spread of COVID-19,https://www.allthingsdistributed.com/2020/04/scaling-covid19-model.html,scaling workload matter saving life modeling spread,john hopkins bloomberg school public health opensource project end result http wwwusdigitalresponseorg,march pm received urgent email friend dj patil former white house chief data scientist head technology devoted health senior fellow belfer center harvard kennedy school advisor venrock partner get many title name unless pretty good something dj math computer science dj writing california crisis command center explained working governor across country model potential impact scenario planning wanted help answer critical question like many hospital bed need reduce spread temporarily close place people gather issue shelterinplace order long nobody predict future modeling virus factor know best shot helping leader make informed decision would impact hundred thousand life dj assembled team volunteer consisted brightest mind across silicon valley country though following call arm professional came together personal capacity fight best way knew data good news model model dj team working one primarily developed world renowned john hopkins bloomberg school public health jhsph model opensource project us state county population number along transportation data model number people potentially would exposed infected andor hospitalized model also considers virus spread based variety nonpharmaceutical intervention including shutting school park issuing quarantine order however model running jhsph onpremises infrastructure model pipeline scale run large number scenario simultaneously meet need country potentially world slow get required scale speed dj team needed run model moved onpremises code aws led another challenge code written initially cloud mind fully take advantage scale optimization possible aws result team spent one week porting running single scenario california still fast enough imagine long would take scale pipeline state would require least month work adding multiple scenario different variable would delay even team kind time word dr anthony fauci director national institute allergy infectious disease nt make timeline virus make team needed able onboard model pipeline run full report hour rather week month dj reached wanted help optimizing model pipeline cloud presented opportunity help friend support project could save life immediately said sounded alarm internally something amazing happened people across company volunteered help knew expertise project needed nobody asked responsible customer bandwidth work project everyone sprang action immediately wanted get model pipeline running like jet engine first thing create architecture would fuel model every step way began profiling code recompiling key numerical library next help optimize model pipeline specialist team including high performance computing hpc group stepped professional help organization solve biggest datarelated problem tame largest workload like genomics computational chemistry machine learning autonomous vehicle simulation worked member team jhsph state employee nonstop two weekend optimize model pipeline rearchitecture deployment aws contributed opensource jhsph model enable continuous integration deployment cicd pipeline container deployment amazon elastic container registry also orchestrated scalable deployment strategy aws elastic container service aws batch integrated several service including amazon amazon auto scaling high level coordination aws critical bringing technology together figure architecture scalable covid scenario pipeline using aws batch end result team reduced time onboard model pipeline generate full report one week one scenario hour multiple scenario new model created jhsph rolled aws state internationally help making decision directly impact global spread story begin spread virus global yet grasp full impact human society learn spreading condition virus iterating existing model crucial many sleepless night everyone working initiative continue scale model state country well analyze effect mitigation strategy relevant skill technology communication andor operation please consider joining u digital response team volunteerrun nonpartisan effort help federal state local government technology data design operation communication project management need crisis http wwwusdigitalresponseorg
62,Lobsters,scaling,Scaling and architecture,Cloud Design Patterns,https://docs.microsoft.com/en-us/azure/architecture/patterns/,cloud design pattern,ambassador design implementation management monitoring anticorruption layer design implementation management monitoring asynchronous requestreply messaging backends frontends design implementation bulkhead resiliency cacheaside data management performance scalability choreography messaging performance scalability circuit breaker resiliency claim check messaging compensating transaction resiliency competing consumer messaging compute resource consolidation design implementation cqrs data management design implementation performance scalability deployment stamp availability performance scalability event sourcing data management performance scalability external configuration store design implementation management monitoring federated identity security gatekeeper security gateway aggregation design implementation management monitoring gateway offloading design implementation management monitoring gateway routing design implementation management monitoring geode availability performance scalability health endpoint monitoring availability management monitoring resiliency index table data management performance scalability leader election design implementation resiliency materialized view data management performance scalability pipe filter design implementation messaging priority queue messaging performance scalability publishersubscriber messaging queuebased load leveling availability messaging resiliency performance scalability retry resiliency scheduler agent supervisor messaging resiliency sequential convoy messaging sharding data management performance scalability sidecar design implementation management monitoring static content hosting design implementation data management performance scalability strangler design implementation management monitoring throttling availability performance scalability valet key data management security,ambassador create helper service send network request behalf consumer service application design implementation management monitoring anticorruption layer implement adapter layer modern application legacy system design implementation management monitoring asynchronous requestreply decouple backend processing frontend host backend processing need asynchronous frontend still need clear response messaging backends frontends create separate backend service consumed specific frontend application interface design implementation bulkhead isolate element application pool one fails others continue function resiliency cacheaside load data demand cache data store data management performance scalability choreography let service decide business operation processed instead depending central orchestrator messaging performance scalability circuit breaker handle fault might take variable amount time fix connecting remote service resource resiliency claim check split large message claim check payload avoid overwhelming message bus messaging compensating transaction undo work performed series step together define eventually consistent operation resiliency competing consumer enable multiple concurrent consumer process message received messaging channel messaging compute resource consolidation consolidate multiple task operation single computational unit design implementation cqrs segregate operation read data operation update data using separate interface data management design implementation performance scalability deployment stamp deploy multiple independent copy application component including data store availability performance scalability event sourcing use appendonly store record full series event describe action taken data domain data management performance scalability external configuration store move configuration information application deployment package centralized location design implementation management monitoring federated identity delegate authentication external identity provider security gatekeeper protect application service using dedicated host instance act broker client application service validates sanitizes request pass request data security gateway aggregation use gateway aggregate multiple individual request single request design implementation management monitoring gateway offloading offload shared specialized service functionality gateway proxy design implementation management monitoring gateway routing route request multiple service using single endpoint design implementation management monitoring geode deploy backend service set geographical node service client request region availability performance scalability health endpoint monitoring implement functional check application external tool access exposed endpoint regular interval availability management monitoring resiliency index table create index field data store frequently referenced query data management performance scalability leader election coordinate action performed collection collaborating task instance distributed application electing one instance leader assumes responsibility managing instance design implementation resiliency materialized view generate prepopulated view data one data store data nt ideally formatted required query operation data management performance scalability pipe filter break task performs complex processing series separate element reused design implementation messaging priority queue prioritize request sent service request higher priority received processed quickly lower priority messaging performance scalability publishersubscriber enable application announce event multiple interested consumer asynchronously without coupling sender receiver messaging queuebased load leveling use queue act buffer task service invokes order smooth intermittent heavy load availability messaging resiliency performance scalability retry enable application handle anticipated temporary failure try connect service network resource transparently retrying operation previously failed resiliency scheduler agent supervisor coordinate set action across distributed set service remote resource messaging resiliency sequential convoy process set related message defined order without blocking processing group message messaging sharding divide data store set horizontal partition shard data management performance scalability sidecar deploy component application separate process container provide isolation encapsulation design implementation management monitoring static content hosting deploy static content cloudbased storage service deliver directly client design implementation data management performance scalability strangler incrementally migrate legacy system gradually replacing specific piece functionality new application service design implementation management monitoring throttling control consumption resource used instance application individual tenant entire service availability performance scalability valet key use token key provides client restricted direct access specific resource service data management security
63,Lobsters,scaling,Scaling and architecture,A database clustering system for horizontal scaling of MySQL,https://vitess.io/docs/overview/whatisvitess/,database clustering system horizontal scaling mysql,feature comparison storage option vitess v vanilla mysql vitess v nosql,vitess database solution deploying scaling managing large cluster opensource database instance currently support mysql mariadb architected run effectively public private cloud architecture dedicated hardware combine extends many important sql feature scalability nosql database vitess help following problem scaling sql database allowing shard keeping application change minimummigrating baremetal private public clouddeploying managing large number sql database instancesvitess includes compliant jdbc go database driver using native query protocol additionally implement mysql server protocol compatible virtually languagevitess serving youtube database traffic since adopted many enterprise production needsfeaturescomparisons storage optionsthe following section compare vitess two common alternative vanilla mysql implementation nosql implementationvitess v vanilla mysqlvitess improves vanilla mysql implementation several way vanilla mysqlvitessevery mysql connection memory overhead range almost depending mysql release using user base grows need add ram support additional connection ram contribute faster query addition significant cpu cost associated obtaining connectionsvitess creates lightweight connection vitess connection pooling feature us go concurrency support map lightweight connection small pool mysql connection vitess easily handle thousand connectionspoorly written query set limit negatively impact database performance usersvitess employ sql parser us configurable set rule rewrite query might hurt database performancesharding process partitioning data improve scalability performance mysql lack native sharding support requiring write sharding code embed sharding logic applicationvitess support variety sharding scheme also migrate table different database scale number shard function performed nonintrusively completing data transition second readonly downtimea mysql cluster using replication availability master database replica master fails replica become new master requires manage database lifecycle communicate current system state applicationvitess help manage lifecycle database scenario support automatically handle various scenario including master failover data backupsa mysql cluster custom database configuration different workload like master database writes fast readonly replica web client slower readonly replica batch job forth database horizontal sharding setup repeated shard app need bakedin logic know find right databasevitess us topology backed consistent data store like etcd zookeeper mean cluster view always uptodate consistent different client vitess also provides proxy route query efficiently appropriate mysql instancevitess v nosqlif considering nosql solution primarily concern scalability mysql vitess might appropriate choice application nosql provides great support unstructured data vitess still offer several benefit available nosql datastores nosqlvitessnosql database define relationship database table support subset sql languagevitess simple keyvalue store support complex query semantics clause join aggregation function morenosql datastores usually support transactionsvitess support transactionsnosql solution custom apis leading custom architecture application toolsvitess add little variance mysql database people already accustomed working withnosql solution provide limited support database index compared mysqlvitess allows use mysql indexing functionality optimize query performance
64,Lobsters,scaling,Scaling and architecture,The High Cost of Splitting Related Data,http://kevinmahoney.co.uk/articles/the-high-cost-of-splitting-related-data/,high cost splitting related data,unreliable network microservices availability loss functionality interface explosion problem incorrectness consistency consistently undervalued performance crash,april consider following simple architecture table database related use related loosely could foreign key one table another maybe shared identifier generalise data tends combined queried common antipattern see break related data notice relationship two related table pushed database layer application layer often detrimental reliability performance correctness simplicity flexibility speed development unreliable network consider pattern repeated see network request database server compared two network request single database server original consider individual request independently chance success original top level request accumulated success rate new example success rate get worse every time pattern extended see article microservices availability detailed argument loss functionality approach loses functionality database join filtering ordering aggregation must reimplemented often poorly application layer example two table requires simple join api must fetch result first table via first api find relevant id request second api want avoid query second api must support form multifetch could alternatively implemented denormalising data come cost complexity interface explosion problem change structure data result multiple change dependent apis really slow development cause bug fewer apis data better incorrectness splitting data multiple database loses acid transaction short introducing distributed transaction consistency table lost updated atomically see article consistency consistently undervalued thought performance crash api often http server json interface every step api stack tcp http json serialisation performance cost must paid aggregation filtering join performed application layer also result overfetching database
66,Lobsters,scaling,Scaling and architecture,"""Good Enough"" Architecture",https://www.youtube.com/watch?v=PzEox3szeRc,good enough architecture,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature goto good enough architecture stefan tilkov youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature goto good enough architecture stefan tilkov youtube
67,Lobsters,scaling,Scaling and architecture,Software Engineering Advice from Building Large-Scale Distributed Systems,https://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf,software engineering advice building largescale distributed system,,obj length r filter flatedecode stream x tpv   endstream endobj obj endobj obj type page parent r resource r content r mediabox annots r endobj obj procset pdf text imageb imagec imagei colorspace r r font r xobject r endobj obj r r endobj obj length r type xobject subtype image width height colorspace r interpolate true intent perceptual bitspercomponent filter dctdecode stream jfif h h mm b j   h h adobe photoshop c window h h jfif h h adobecm adobe w z u h h
68,Lobsters,scaling,Scaling and architecture,Azure demand up by 775% in regions with social distancing,https://azure.microsoft.com/en-us/blog/update-2-on-microsoft-cloud-services-continuity/,azure demand region social distancing,shared azure service health microsoft service health continuity xbox live observed last week read team data made change prioritization criterion outlined last week given prioritization criterion impact azure customer service disruption allocation failure azure service health service incident happen communicate customer partner action taking prevent capacity constraint azure service health needed make change team experience xbox live putting strain overall azure capacity inhome broadband use impact service continuity capacity specific work done isps,since last week update global health pandemic continues impact every employee customer serve everyone working tirelessly support customer especially critical health safety organization across globe cloud service needed sustain operation unprecedented time equally hard work providing service support hundred million people rely microsoft stay connected work play remotely satya nadella shared time like remind u u something contribute importance coming together community time great societal disruption steadfast commitment help everyone get week update want share common question hearing customer partner along insight address important inquiry immediate need please refer following resource azure service health tracking issue impacting customer workload understanding azure service health microsoft service health continuity tracking understanding service health xbox live tracking game service status observed last week response health authority emphasizing importance social distancing seen usage increase service support microsoft team window virtual desktop power bi seen percent increase cloud service region enforced social distancing shelter place order seen percent increase team calling meeting monthly user one month period italy social distancing shelter place order enforced seen significant spike team usage million daily user user generated million meeting calling minute team daily single week read team data window virtual desktop usage grown government use public power bi share dashboard citizen surged percent week made change prioritization criterion outlined last week top priority remains support critical health safety organization ensuring remote worker stay running core functionality team specifically providing highest level monitoring time following first responder fire em police dispatch system emergency routing reporting application medical supply management delivery system application alert emergency response team accident fire issue healthbots health screening application website health management application record system given prioritization criterion impact azure customer implementing temporary restriction designed balance best possible experience customer placed limit free offer prioritize capacity existing customer also limit certain resource new subscription soft quota limit customer raise support request increase limit request met immediately recommend customer use alternative region live region may le demand surge manage surge demand expedite creation new capacity appropriate region service disruption despite significant increase demand significant service disruption result surge use last week experienced significant demand region europe north europe west uk south france central asia east india south brazil south observing deployment compute resource type region drop typical percent success rate although majority deployment still succeed encourage customer experiencing allocation failure retry deployment process place ensure customer encounter repeated issue receive relevant mitigation option treat shortterm allocation shortfall service incident send targeted update mitigation guidance impacted customer via azure service per standard process known platform issue service incident happen communicate customer partner standard operating procedure manage mitigation communication impacted customer partner notified service health experience azure portal andor microsoft admin center action taking prevent capacity constraint expediting addition significant new capacity available week ahead concurrently monitor support request needed encourage customer consider alternative region alternative resource type depending timeline requirement implementation effort alleviate demand sufficient customer may experience intermittent deployment related issue happen impacted customer informed via azure service health needed make change team experience best support team customer worldwide accommodate new growth demand made temporary adjustment select nonessential capability often check user presence interval show party typing video resolution adjustment significant impact end user daily experience xbox live putting strain overall azure capacity actively monitoring performance usage trend ensure optimizing service gamers worldwide time taking proactive step plan highusage period includes taking prudent measure publishing partner deliver higherbandwidth activity like game update offpeak hour inhome broadband use impact service continuity capacity specific work done isps regular communication isps across globe actively working augment capacity needed particular discussion several isps taking measure reduce bandwidth video source order enable network performant workday continue provide regular update microsoft azure blog post updated march clarify first bullet
69,Lobsters,scaling,Scaling and architecture,Why We Started Putting Unpopular Assets in Memory,https://blog.cloudflare.com/why-we-started-putting-unpopular-assets-in-memory/,started putting unpopular asset memory,iop nvme ssds unpopular end result cache hit infrastructure faster customer popular putting popular asset memory page cache caching read among technique buffering writes smarter page cache putting unpopular asset memory motivation writes bad slow read operation dramatically reduced impact latency motivation many asset onehitwonders reducing disk writes speed disk read hit tail latency reduced approximately five percent potential implementation approach remember cache bloom filter counting bloom filter countmin sketch specific tradeoff transient cache memory transient cache system real world internet tradeoff transient cache memory footprint competition page cache competition process memory usage production future work broader deployment transient cache type storage conclusion come work cloudflare,part cloudflare service cdn make million internet property faster reliable caching web asset closer browser end userswe make improvement infrastructure make enduser experience faster secure reliable time case study one engineering effort something counterintuitive turned right approachour storage layer serf million cache hit per second globally powered high iop nvme ssdsalthough ssds fast reliable cache hit tail latency within system dominated io capacity ssds moreover flash memory chip wear nonnegligible portion operational cost including cost new device shipment labor downtime spent replacing dead ssdsrecently developed technology reduces hit tail latency reduces wear ssds technology memoryssd hybrid storage system put unpopular asset memorythe end result cache hit infrastructure faster customersyou may thought typo explanation technique work fact colleague thought proposed project internally think meant say popular asset document intuitively put popular asset memory since memory even faster ssdsyou wrong also correct blog post explains whyputting popular asset memoryfirst let explain already use memory speed io popular assetspage cachesince obvious memory speed popular asset linux already much hard work u functionality called page cache file linux system organized page internally hence namethe page cache us system available memory cache read buffer writes durable storage typical operation cloudflare server service process consume physical memory remaining memory used page cachebelow show memory layout typical cloudflare edge server physical memorywe see used memory r top r memory consumption service gb cache service consuming gb bottom see overall page cache usage total size gb cache service using gb serve cached web assetscaching readspages frequently used cached memory speed read linux us least recently used lru algorithm among technique decide keep page cache word popular asset already memorybuffering writesthe page cache also buffer writes writes synchronized disk right away buffered page cache decides later time either periodically due memory pressure therefore also reduces repetitive writes filehowever nature caching workload web asset arriving cache system usually static unlike workload database single value get repeatedly updated caching workload repetitive update asset completely cached therefore page cache help reducing writes disk since page eventually written diskssmarter page cache although linux page cache work great box caching system still outsmart since system know context data accessed content type access frequency timetolive valueinstead improving caching popular asset page cache already next section focus something page cache well putting unpopular asset memoryputting unpopular asset memorymotivation writes badworn ssds solely caused writes read mentioned costly replace ssds since cloudflare operates data center city across world cost shipment replacement significant proportion cost ssd itselfmoreover write operation ssds slow read operation ssd writes require programerase pe operation issued storage chip operation block read chip short writes slower read since page cache already cached popular asset effect highest impact cache hit tail latency previously talked dramatically reduced impact latency meantime directly reducing latency also helpfulmotivation many asset onehitwonders onehitwonders refer cached asset never accessed get accessed cached yet never read addition onehitwonders also include cached asset evicted used due lack popularity performance website harmed even cache asset first placeto quantify many asset onehitwonders test hypothesis reducing disk writes speed disk read conducted simple experiment experiment using representative sample traffic modified caching logic cache asset second time server encountered red line indicates experiment started green line represents experimental group yellow line represents control groupthe result disk writes per second reduced roughly half corresponding disk hit tail latency reduced approximately five percentthis experiment demonstrated cache onehitwonders disk last longer cache hit fasterone benefit caching onehitwonders immediately obvious experiment increase effective capacity cache reduced competing pressure removal unpopular asset turn increase cache hit ratio cache retentionthe next question replicate result scale production without impacting customer experience negatively potential implementation approachesremember cacheone smart way eliminate onehitwonders cache cache asset first miss remember appearance cache system encounter asset repeatedly certain number time system start cache asset basically experiment abovein order remember appearance beside hash table many memory efficient data structure bloom filter counting bloom filter countmin sketch used specific tradeoffssuch approach solves one problem introduces new one every asset missed least twice cached number cache miss amplified compared today approach multiplies cost bandwidth server utilization customer acceptable tradeoff eyestransient cache memorya better idea came put every asset want cache disk memory first memory called transient cache asset transient cache promoted permanent cache backed ssds accessed certain number time indicating popular enough stored persistently promoted eventually evicted transient cache lack popularitythis design make sure disk writes spent asset popular ensuring every asset missed oncethe transient cache system real world internetwe implemented idea deployed production caching system learning data point would like sharetradeoffsour transient cache technology pull performance improvement rabbit hat achieves improved performance customer consuming resource system resource consumption tradeoff must carefully considered deploying system scale cloudflare operatestransient cache memory footprintthe amount memory allocated transient cache matter cache size directly dictate long given asset live cache evicted transient cache small new asset evict old asset old asset receive hit promote disk customer perspective cache retention short unacceptable lead miss commensurate higher cost worse eyeball performancecompetition page cacheas mentioned page cache us system available memory memory allocate transient cache unpopular asset le memory popular asset page cache finding sweet spot tradeoff two depends traffic volume usage pattern specific hardware configuration software running oncompetition process memory usageanother competitor transient cache regular memory used service process unlike page cache used opportunistically process memory usage hard requirement operation software additional wrinkle edge service increase total r memory consumption performing zero downtime upgrade run old new version service parallel short period time experience enough physical memory system overall performance degrade unacceptable level due increased io pressure reduced page cache spacein productiongiven consideration enabled technology conservatively start new hybrid storage system used newer generation server physical memory older generation additionally system used subset asset tuning size transient cache percentage request use able explore sweet spot tradeoff performance cache retention memory resource consumptionthe chart show io usage enabled cohort red line enabling transient cachewe see disk write byte per second reduced peak peak although early tell lifespan ssds extended proportionallymore importantly customer cdn cache hit tail latency measurably decreased future workwe made first step towards smart memoryssd hybrid storage system still lot donebroader deploymentcurrently apply technology specific hardware generation portion traffic decommission older generation server replace newer generation physical memory able apply transient cache larger portion traffic per preliminary experiment certain data center applying transient cache traffic able reduce disk writes decrease end user visible tail latency peak hoursour implemented promotion strategy moving asset transient cache durable storage based simple heuristic number hit information used make better promotion decision example ttl time live asset taken account well one strategy refuse promote asset ttl second left reduce unnecessary disk writesanother aspect algorithm demotion actively passively eviction demote asset persistent cache transient cache based criterion mentioned demotion directly reduce writes persistent cache could help cache hit ratio cache retention done smartlytransient cache type storagewe choose memory host transient cache wear cost low none also case hdds possible build hybrid storage system across memory ssds hdds find right balance cost performance characteristicsconclusionby putting unpopular asset memory able tradeoff memory usage tail hit latency ssd lifetime current configuration able extend ssd life reducing tail hit latency expense available system memory transient cache also open possibility future heuristic storage hierarchy improvementswhether technique described post benefit system depends workload hardware resource constraint system hopefully u sharing counterintuitive idea inspire novel way building faster efficient systemsand finally system work like sound interesting come work cloudflare
70,Lobsters,scaling,Scaling and architecture,Mitigating serverless lock-in fears,https://www.thoughtworks.com/insights/blog/mitigating-serverless-lock-fears,mitigating serverless lockin fear,go,enable javascript browser better experience need know enable go
71,Lobsters,scaling,Scaling and architecture,Ready for changes with Hexagonal Architecture,https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749,ready change hexagonal architecture,ready change hexagonal architecture highly integrated start swappable data source leveraging hexagonal architecture swap data source without impacting business logic hexagonal architecture without significant impact major code rewrite codebase without relying protocol easily change defining core concept entity repository interactors entity repository interactors data source transport layer api layer swapping data source managed transfer read json api graphql data source within hour simple oneline change hexagonal architecture worked u hiding data source detail testing strategy think nice musthave interactors tested detail data source integration spec contract testing delaying decision project paradox,ready change hexagonal architectureby damir svrtan sergii makagonas production netflix original grows year need build apps enable efficiency throughout entire creative process wider studio engineering organization built numerous apps help content progress pitch aka screenplay playback ranging script content acquisition deal negotiation vendor management scheduling streamlining production workflow onhighly integrated startabout year ago studio workflow team started working new app cross multiple domain business interesting challenge hand needed build core app scratch also needed data existed many different systemssome data point needed data movie production date employee shooting location distributed across many service implementing various protocol grpc json api graphql existing data crucial behavior business logic application needed highly integrated startswappable data sourcesone early application bringing visibility production built monolith monolith allowed rapid development quick change knowledge space nonexistent one point developer working well database tablesover time application evolved broad service offering towards highly specialized resulted decision decompose monolith specific service decision geared performance issue setting boundary around different domain enabling dedicated team develop domainspecific service independentlylarge amount data needed new app still provided monolith knew monolith would broken point sure timing breakup knew inevitable needed preparedthus could leverage data monolith first still source truth prepared swap data source new microservices soon came onlineleveraging hexagonal architecturewe needed support ability swap data source without impacting business logic knew needed keep decoupled decided build app based principle behind hexagonal architecturethe idea hexagonal architecture put input output edge design business logic depend whether expose rest graphql api depend get data database microservice api exposed via grpc rest simple csv filethe pattern allows u isolate core logic application outside concern core logic isolated mean easily change data source detail without significant impact major code rewrite codebaseone main advantage also saw app clear boundary testing strategy majority test verify business logic without relying protocol easily changedefining core conceptsleveraged hexagonal architecture three main concept define business logic entity repository interactorsentities domain object eg movie shooting location knowledge stored unlike active record ruby rail java persistence api repository interface getting entity well creating changing keep list method used communicate data source return single entity list entity eg userrepository interactors class orchestrate perform domain action think service object use case object implement complex business rule validation logic specific domain action eg onboarding production three main type object able define business logic without knowledge care data kept business logic triggered outside business logic data source transport layer data source adapter different storage implementationsa data source might adapter sql database active record class rail jpa java elastic search adapter rest api even adapter something simple csv file hash data source implement method defined repository store implementation fetching pushing datatransport layer trigger interactor perform business logic treat input system common transport layer microservices http api layer set controller handle request business logic extracted interactors coupled particular transport layer controller implementation interactors triggered controller also event cron job command linethe dependency graph hexagonal architecture go inwardwith traditional layered architecture would dependency point one direction layer depending layer transport layer would depend interactors interactors would depend persistence layerin hexagonal architecture dependency point inward core business logic know anything transport layer data source still transport layer know use interactors data source know conform repository interfacewith prepared inevitable change studio system whenever need happen task swapping data source easy accomplishswapping data sourcesthe need swap data source came earlier expected suddenly hit read constraint monolith needed switch certain read one entity newer microservice exposed graphql aggregation layer microservice monolith kept sync data reading one service produced resultswe managed transfer read json api graphql data source within hoursthe main reason able pull fast due hexagonal architecture let persistence specific leak business logic created graphql data source implemented repository interface simple oneline change needed start reading different data sourcewith proper abstraction easy change data sourcesat point knew hexagonal architecture worked usthe great part oneline change mitigates risk release easy rollback case downstream microservice failed initial deployment well enables u decouple deployment activation decide data source use configurationhiding data source detailsone great advantage architecture able encapsulate data source implementation detail ran case needed api call yet exist service api fetch single resource bulk fetch implemented talking team providing api realized endpoint would take time deliver decided move forward another solution solve problem endpoint builtwe defined repository method would grab multiple resource given multiple record identifier initial implementation method data source sent multiple concurrent call downstream service knew temporary solution second take data source implementation use bulk api implementedour business logic need aware specific data source limitationsa design like enabled u move forward meeting business need without accruing much technical debt need change business logic afterwardtesting strategywhen started experimenting hexagonal architecture knew needed come testing strategy knew prerequisite great development velocity test suite reliable super fast think nice musthavewe decided test app three different layer test interactors core business logic life independent type persistence transportation leverage dependency injection mock kind repository interaction business logic tested detail test strive ofwe test data source determine integrate correctly service whether conform repository interface check behave upon error try minimize amount testswe integration spec go whole stack transport api layer interactors repository data source hit downstream service spec test whether wired everything correctly data source external api hit endpoint record response store git allowing test suite run fast every subsequent invocation extensive test coverage layer usually one success scenario one failure scenario per domain actionwe test repository simple interface data source implement rarely test entity plain object attribute defined test entity additional method without touching persistence layer room improvement pinging service rely relying contract testing test suite written manner manage run around spec second single processit lovely work test suite easily run machine development team work daily feature without disruptiondelaying decisionswe great position come swapping data source different microservices one key benefit delay decision whether want store data internal application based feature use case even flexibility determine type data store whether relational documentsat beginning project least amount information system building lock architecture uninformed decision leading project paradoxthe decision made make sense need enabled u move fast best part hexagonal architecture keep application flexible future requirement come
72,Lobsters,scaling,Scaling and architecture,Simple Systems Have Less Downtime,https://www.gkogan.co/blog/simple-systems/,simple system le downtime,marketing consultant startup simple system le downtime simple soundpowered phone simplicity lead le downtime proficiency take le time get hit bus troubleshooting take le time alternative solution startup story principle simple system feature justify complexity complex idea lead complex implementation modification addition smooth sailing http get touch,maersk triplee class container ship foot long carry container across mile europe asia entire crew fit inside passenger van former naval architect current marketing consultant startup found principle let crew navigate world largest container ship port halfway around world without breaking also applies startup working towards aggressive growth goal simple system le downtime ship contain simple system easy operate easy understand make easy fix mean le downtime important quality considering downtime ship could mean stranded thousand mile help take ship steering system instance rudder pushed left right metal rod rod moved hydraulic pressure pressure controlled hydraulic pump pump controlled electronic signal wheelhouse signal controlled autopilot require rocket scientist naval architect find cause solution problem autopilot fails steer ship manually wheelhouse electronic signal fail go rudder control room control pump hand talking bridge simple soundpowered phone hydraulics fail use mechanically linked emergency steering wheel mechanical linkage fails hook chain side rudder pull direction want startup like ship afford stall system downtime extended downtime sale marketing web customer support hiring product system may cause irreparable damage growth rate although automation prevalent modern ship affect time take thing attention required monitor everything propulsion auxiliary system simple ever thanks modern diesel electric propulsion system replaced pipeladen steam plant simplicity lead le downtime proficiency take le time person responsible system leaf fall overboard get hit bus get pulled another project another person take without much learning training mean people able step troubleshoot fix issue example analytics dashboard built tableau likely qualified people fix one built patchwork custom script apis nobody pull data scientist product developer away work fix bar chart way write article like every month covering lesson learned growing software startup get email update next one published troubleshooting take le time system behavior component relationship comment easily understood ruling issue finding broken root intuitive example company many downloadable whitepapers website gated behind single opposed custom form need troubleshoot one form one automation workflow whitepaper downloads stop working alternative solution part system clear function alternative easier find example imagine salesforce process us mishmash automation thirdparty tool score filter classify assign new sale lead fails obvious replacement everything put hold process fixed replaced similarly complex solution imagine sale process sale team simply notified new sale lead along pertinent detail letting decide whether follow lead salesforce notification step fails easy come hundred way getting information sale team report slack notification list export manual observation using zapier send alert virtually medium downtime would last minute startup story one client using legacy enterprise marketing automation platform marketo automated process built several year something broke needed tweaking one person among employee could issue took several day even week fix marketing campaign stalled patch overall system grew complex person left company nobody left operate system every passing week new issue would come faster could find fix keep marketing operation coming standstill rushed migrate company marketo hubspot simple platform would easier operate troubleshoot migration took one week along way however another complex system reared head salesforce automated process salesforce combined operation dependent various delicately timed automation marketo took two long understand integrate process new marketing platform overall two complex system marketo salesforce resulted six week downtime marketing team three week downtime sale team counting many week downtime experienced throughout past year many week downtime would experience future overhaul underlying system end system put place fewer process providing capability bug found day later got resolved four minute experience made wonder principle startup adopt avoid pitfall complex system principle simple system ripandreplace project painful disruptive even longterm benefit worth many luxury time resource perform overhaul underway three principle follow evaluating implementing new system feature justify complexity good complicated flight control system ground entire fleet aircraft enterprise marketing platform like marketo nobody run marketing campaign choose tool simple operate promise feature frequent recommendation give startup choose hubspot marketing platform instead enterprise platform like marketo eloqua pardot complex idea lead complex implementation take long explain grasp idea implementation complex take long fix something inevitably break example proposed sale process requires hourlong presentation nightmare maintain regardless clever seems modification addition new requirement come tendency add layer top existing way additional step integration instead see system core modified meet new requirement change may cause planned downtime upfront marketotohubspot migration example le unplanned downtime long term smooth sailing simple thing le liable disordered easier repaired disordered thomas paine common sense question thing break along startup journey surely ship crossing globe however onboard system simple issue leave startup drifting helplessly middle ocean image source http p liked article write one every month covering lesson learned startup growth nt miss next one need help marketing revenue growth get touch
73,Lobsters,scaling,Scaling and architecture,Loading NumPy arrays from disk: mmap() vs. Zarr/HDF5,https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/,loading numpy array disk mmap v,process chunk happens read write disk g drive cache drive cache program cache program g cache program cache program g cache program program cache g cache drive cache drive ing array g clusterprogram clusterarray drive cache drive cache limit data filesystem loading enough data reading writing disk become bottleneck ndimensional array want slice along different ax slice line default structure fast may need read get amount relevant data zarr zarr difference zarr using zarr zarr chunk dimension g compression g clusterprogram drive cache drive cache temporarychunks cache temporarychunks array temporarychunks array zarr,numpy array big fit memory process chunk either transparently explicitly loading one chunk time disk either way need store array disk somehow particular situation two common approach take mmap let treat file disk transparently memory zarr pair similar storage format let load store compressed chunk array demand approach different strength weakness article explain storage system work might want use particular focusing data format optimized running calculation necessarily sharing others happens read write disk read file disk first time operating system copy data process first copy operating system memory storing copy buffer cache useful operating system keep buffer cache around case read data file g drive hard drive cache buffer cache drive cache read program program memory cache program read read second read come ram order magnitude faster g cache buffer cache program program memory cache program read need memory something else buffer cache automatically cleared write data flow opposite get written buffer cache mean writes typically quite fast wait much slower write disk writing ram g cache buffer cache program program memory program cache write eventually data get flushed buffer cache hard drive g cache buffer cache drive hard drive cache drive write mmap ing array purpose mmap let treat file disk inmemory array operating system transparently readwrite either buffer cache disk depending whether particular part array cached ram data buffer cache great access directly data disk access slower think get loaded transparently added bonus case buffer cache file embedded program memory making extra copy memory buffer cache program memory g clusterprogram program memory clusterarray array drive hard drive cache buffer cache drive cache read demand numpy builtin support mmap import numpy np array npmemmap mydatamyarrayarr mode r shape run code array transparently either return memory buffer cache read disk limit mmap mmap work quite well circumstance also limitation data filesystem load data blob store like aws loading enough data reading writing disk become bottleneck remember disk much slower ram disk read writes transparent make faster ndimensional array want slice along different ax slice line default structure fast rest require large number read disk expand last item imagine large array byte integer read disk byte read dimension match layout disk disk read read integer read dimension match layout disk disk read give relevant integer may need read get amount relevant data zarr overcome limit use zarr fairly similar usable python via pytables older restrictive format benefit use multiple programming language zarr much modern flexible moment use python program feeling better choice situation unless need multilanguage support eg much better threading support rest article talk zarr interested indepth talk joe jevnik difference zarr using zarr zarr let store chunk data load memory array write back chunk array might load array zarr import zarr numpy np z zarropen examplezarr mode shape chunk type z class zarrcorearray type z class numpyndarray notice actually slice object get numpyndarray zarrcorearray metadata load disk subset data slice zarr zarr address limit mmap discussed earlier store chunk disk aws really storage system provides keyvalue lookup chunk size shape structure allow efficient read across multiple ax also true chunk compressed also true let go last two point chunk dimension let say storing array want read x ax store chunk following layout practice probably want chunk g load either x dimension efficiently could load chunk could load chunk depending axis relevant code compression chunk compressed mean load data faster disk bandwidth data compression ratio load data fast disk bandwidth minus cpu time spent decompressing data g clusterprogram program memory drive hard drive cache buffer cache drive cache read temporarychunks chunk cache temporarychunks read array array temporarychunks array decompress chunk loaded dropped program memory mmap zarr one use mmap useful multiple process reading part file process able share buffer cache via mmap need one copy memory matter many process running want manually manage memory rely o transparently automatically zarr useful loading data remote source rather local filesystem zarr useful reading disk likely bottleneck compression give effective bandwidth need slice multidimensional data across different ax allow using appropriate chunk shape size first pas would personally default zarr flexibility give
74,Lobsters,scaling,Scaling and architecture,Architecture for generations,https://increment.com/software-architecture/architecture-for-generations/,architecture generation,need small number large file blackboard running perennial temporal database problem original mapreduce paper called pattern previously written took job dropbox encouraged development mypy promoted spread offer formal mentorship program losing ground alternative like spark looking dask,first got started tech data analytics team company worked getting ready implement hadoop move data oracle one team member gathered u conference room hour pas wisdom received fourday cloudera called mapreduce said way processing lot file name node worker node work parallel much powerful relational idea multiple machine controlled single central machine work together seemed important novel receive context hadoop team member new distributed computing spent month recreating star schema laboriously put together oracle countless hour trying get hdfs hive pig previously done relational setting analyst running hive query finding new data hdfs month struggled alone understand new architecture supported equally lost questionaskers stack overflow apache mailing listswe leveraging power hadoop effectively benefit mapreduce paradigm large amount computation quickly need small number large file otherwise parallel process start become inefficient distributed system migrate data relational database also lose something powerful ability query thing quickly get answer optimizing hive query really sql syntax transpiled java method requires different programming paradigm traditional relational sql query order know understand assumption hadoop operates relational database came different also need understand use case eachmy team context struggled get running one point getting good data oracle focused moving hadoop hadoop entirely understand worked yet alone early day numerous company blackboard lured power find spinning wheel get data efficiently process query data already platformmy problem really twofold new entrant tech thrown deep end also many moving adopt hadoop quickly without hiring consulting people experience architecturei still thinking repeatable software pattern mentorship last year husband came across naval commissioning ceremony intrepid museum aircraft carrierturnedexhibition space docked new york city final act newly commissioned officer returned first salute enlisted service member assisted officer candidate process service member often mentor figure helped newly promoted officer training salute returned newly commissioned officer shake hand new subordinate pas silver dollar traditional token respect gratitude date back revolutionary wari struck act similar tradition tech industry wherein developer paired senior team member deploy production first time indeed technologist often enter industry without knowledge work done fact helped breathless pace technological change earlycareer engineer may evaluate redshift versus versus hdfs versus postgres storing data within cadence sprint cycle realizing specific use case redshift postgres driven relational hierarchy hdfs share similar folderlike architecture people choose move relational model long run though stop running perennial temporal database problem come role historical contextual knowledge need design implement effective software architecture developer must instead gather information proactively keeping rotating list architectural pattern background mind head need rework prevent architectural misfitfor example mapreduce architecture work running data simple pattern initial document split multiple part function run part aggregating word count word count added together across document developer mapreduce discover pattern rather looked map reduce method already available functional programming citation listed original mapreduce paper make clear model arose work hundred academic across number different platform example one citation paper prefix published way back map reduce pattern also exists function language like scala scheme clojure hadley wickham called pattern dask spark work principle slightly different implementationsyou could even use pattern without using distributed system previously written running map reduce pattern laptop programmatically efficient example former team struggling understand hadoop spent lot time learning system worked implementing word count canonical world big data later reading programming pearl john bentley saw implemented word count program c program count word existed implemented language old fortran somehow never thought outside context hadoopoften software first encounter system highly practical one something broken need fixed time learn past current state broken build business need new feature result early many people ended solving problem scratch hadoop already solved previous iteration distributed system map reduce paradigmsthis lack institutional memory also resulted repeated rise fall rise fall sql relational database existed since around time hadoop released late company started experimenting nosql pattern storage processing resulting product included nonrelational store like mongodb hdfs cassandra new paradigm could store data without entity mapping relationship index needed solution prided quick writesthis analyst data scientist needed count thing nearly impossible query writing mapreduce script folder folder disaggregated log efficiency sql index logbased file got stuck hdfs purgatory industry built sqlbased retrieval engine presto hadoop ksql aggregation kafka streaming cql cassandrain rare case historical memory get passed benefit clear look experience former python benevolent dictator life guido van rossum took job dropbox initial designer developer python van rossum arguably experience language also institutional memory design change made dropbox encouraged development mypy typechecking library pythonbecause van experience python codebase community recognized desire need type hint larger company ran limitation running python without type checking hired jukka lehtosalo undertake project encouraging building typechecking architecture also promoted spread among various team dropbox van guidance dropbox migrating python latest version language process many large company struggled python slated sunset january cutting across time place even small bit institutional knowledge save company hundred hour example analyst effort learn hadoop greatly accelerated experienced unix admin joined team mentorship also help people become productive developer may otherwise left company stagnated role today organization offer formal mentorship program including airbnb pinterest adobea handful year hadoop gained popularity platform losing ground alternative like spark also based mapreduce cloudbased etl solution take place even looking dask favor spark see similar trend software engineering large company shuttling monolith microservices single onprem server cloud iteration incremental improvement natural feature want device smaller code run quickly however technological iteration also due part lack historical continuity provided formal mentorship practice much time one step say five year perhaps see tech stack change reasonable work stable architecture result
75,Lobsters,scaling,Scaling and architecture,Overthinking it and the value of simple solutions,https://korecki.me/blog/2019/10/8/overthinking-it-and-the-value-of-simple-solutions,overthinking value simple solution,many company open source project scheduler leader run le stuff evaluated strict way possible redlock seems lot issue lockjaw lockjaw postgres advisory lock component,fully distributed team organize adhoc meeting time want therefore adopted rfc request commentschange process author writes document state problem possible solution evaluation attempt pitfallscons conclusion rfc process quite popular many company open source project used great successfor scheduler problem took rfc document testing following approach using redis lock serviceusing zookeeper consul jgroups basis leader election mechanismintroducing new service would centralized distributed scheduler orchestrating job rabbitmq similar google cloud scheduler servicethe last idea rejected introducing much complexity dependency single point failure rest system approach evaluated form internal clojure library called leader provides uniform interface picking cluster leader backed consul zookeeper jgroups course backends pluggable end approach documented built tested rejected themrun le stuffanything introduces new piece infrastructure evaluated strict way possible solving problem hand also answering following question possibly going manage new piece failure mode going manage upgrade suitable scale associated infrastructure cost security implication single purpose tool use purposesonce question taken account basically ruled introducing solution based consul zookeeper similar system deployment setup incredibly simple deploy containerized application dedicated vms backed internal load balancer managed terraform adding another layer state deployment introduced yet another layer complexitythe last choice redis based solution also feel right redis usage light could potentially add one workload without affecting rest system however popular approach redlock seems lot issue comfortable deal potential problem caused clock skew etcback drawing boardlockjawthis lockjaw come small library expose postgres advisory lock pluggable component
77,Lobsters,scaling,Scaling and architecture,Doubling System Read Throughput with Only 26 Lines of Code,https://pingcap.com/blog/doubling-system-read-throughput-with-only-26-lines-of-code/,doubling system read throughput line code,tidb beta tidb follower read backup restore br optimizer hint tikv region raft group leader follower line code raft consensus algorithm tidb architecture follower read follower read feature strongly consistent read snapshot isolation introduced follower read tikv architecture design implementation multiraft region split problem tikv architecture leader deal heavy workload follower maintained cold standby implement follower read algorithm linearizability issue raft algorithm peer solution linearizability issue implementation issue follower read current implementation follower read issue follower read issue linearizability percolator algorithm issue read latency remote procedure call benchmark follower read test environment test result scenario number scan key qps latency tikv latency client scenario number scan key qps latency tikv latency client scenario number scan key qps latency tikv latency client scenario number scan key qps latency tikv latency client next follower read strategy variedheat data online transaction processing tidb internal iii scheduling placement driver local read based follower read mvcc tikv multiversion concurrency control atomicity consistency isolation durability conclusion user document project,dec released tidb beta version tidb introduced two significant feature follower read backup restore br enriched optimizer hintsfor tidb beta follower read highlight opensource feature understand important feature need bit background tidb storage engine tikv store data basic unit called region multiple replica region form raft group read hotspot appears region region leader become read bottleneck entire system situation enabling follower read feature significantly reduce load leader improve read throughput whole system balancing load among multiple followerswe wrote line code implement follower read benchmark test feature enabled could roughly double read throughput entire systemin post guide introduced follower read implement future plan itnote post assumes basic knowledge raft consensus algorithm tidb architecturewhat follower readthe follower read feature let follower replica region serve read request premise strongly consistent readsthis feature improves throughput tidb cluster reduces load raft leader contains series load balancing mechanism offload tikv read load leader replica follower replica regiontikv follower read implementation guarantee linearizability singlerow data reading combined snapshot isolation tidb implementation also provides user strongly consistent readswhy introduced follower readin tikv architecture use raft algorithm ensure data consistency previous mechanism leader region handled heavy workload calculation resource follower put use therefore introduced follower read handle read request follower reduce load leaderthe tikv architecturetikv us raft algorithm guarantee data consistency goal tikv support tb data impossible one raft group therefore need use multiple raft group multiraft see previous post design implementation multiraft implementing raft tikv architecture tikv divide data region default region three replica region replica form raft group data writes increase size region number key reach threshold region split occurs conversely data deleted size region amount key shrink use region merge merge smaller adjacent region relief stress raftstorethe problem tikv architecturethe raft algorithm achieves consensus via elected leader server raft group either leader follower leader unavailable candidate election leader replicates log followersalthough tikv spread region evenly node leader provide external service two follower receive data replicated leader vote elect raft leader failover simply put region level leader deal heavy workload follower maintained cold standbyssometimes hot data resource region leader machine fully occupied although forcibly split region move data another machine operation always lag calculation resource follower usedhere come question handle client read request follower yes relieve load leaderthe solution follower readhow implement follower readthe implementation follower read based readindex algorithm elaborating follower read let introduce readindex firstthe readindex algorithmthis section discus readindex algorithm solves linearizability issuethe linearizability issue raft algorithmhow ensure read latest data follower return data latest committed index follower client answer raft quorumbased algorithm commit log nt need successfully write data replica region also known peer instead log committed majority peer mean successfully written tikv case local data follower might stale violates linearizabilityin fact trivial raft implementation even leader handle load stale data problem may still occur example network partition occurs old leader isolated minority node time majority node elected new leader old leader nt know may return stale data client leader leasereadindex solution linearizability issuethe quorum read mechanism help solve problem might consume lot resource take long improve effectiveness crucial issue old leader sure whether latest leader therefore need method leader confirm leader state method called readindex algorithm work follows current leader process read request system record current leader latest committed indexthe current leader ensures still leader sending heartbeat quorumafter leader confirms leader state return entrythis way linearizability violated although readindex algorithm need network communication majority cluster communication transmits metadata remarkably reduce network io thus increase throughput furthermore tikv go beyond standard readindex algorithm implement leaseread guarantee leader lease shorter election timeout reelecting new leaderimplementation issue follower readthis section show implement follower read feature issuesthe current implementation follower readhow ensure read latest data follower maybe consider common policy request forwarded leader leader return latest committed data follower used proxy idea simple safe implementyou also optimize policy leader need tell follower latest committed index case even follower nt stored log locally log applied locally sooner laterbased thought tidb currently implement follower read feature way client sends read request follower follower request leader committed indexafter follower get leader latest committed index applies index follower return entry clientissues follower readcurrently follower read feature two issue issue linearizabilitytikv us asynchronous apply might violate linearizability although leader tell follower latest committed index leader applies log asynchronously follower may apply log leader result read entry follower may take read leaderalthough ca nt ensure linearizability raft layer follower read guarantee snapshot isolation database distributed transaction layer familiar percolator algorithm get execute point get query use timestamp read data one row data one region accessed guarantee snapshot isolation transactionif data committed timestamp t read follower sql statement within transaction temporarily ca nt read t leader lock inevitably occurs subsequent processing lock guarantee snapshot isolation two fact ensure follower read enabled tidb transaction still implement snapshot isolation thus transaction correctness nt affectedissue read latencyour current follower read implementation still us remote procedure call rpc ask leader committed index therefore even though use follower read read latency remains highfor issue even though solution nt significantly improve latency help improve read throughput reduce load leadertherefore follower read feature fine optimizationbenchmarks follower readwe ran test four scenario arrived following conclusion multiregion architecture follower read enabled client could read data local data center reduced network bandwidth usage data volume reached certain scale follower read remarkably improved system performance see scenario scenario follower read effectively increase read throughput entire system balance hotspot read pressure see scenario scenario read section get detailstest environmentwe used following machine testing ucloud gb gb ssd beijing chinaucloud gb gb ssd shanghai chinawe tested bandwidth latency determined following bandwidth mbitsseclatency ranged mswe used yahoo cloud serving benchmark ycsb import row data tested scan performance different rowswe performed raw keyvalue kv scan test adjusting number scan key got request different data size testingtest resultsscenario description follower client data center dc leader data center another region leader served read requeststest result number scan latency latency msresult analysis latency client tikv increased due sliding window mechanism corresponding tcp connection bandwidth decreased data volume large even observed lot deadlineexceeded logsthe reason behind high latency tikv leader lease expired onethird read request read readindex executedscenario description follower client data center leader data center another region follower served read requeststest result number scan latency latency msresult analysis follower read reduced crossdc traffic thus avoided impact tcp sliding window mechanism highlatency network amount data request small impact crossdc latency big amount data request big impact crossdc latency small case follower read enabled read throughput change muchscenario description leader client node leader served read requeststest result number scan latency latency msresult analysis client higher latency tikv ran netstat found many packet tcp sendq mean tcp sliding window mechanism limited bandwidthscenario description leader client follower data center follower served read requeststest result number scan latency latency msresult analysis amount data read request small request processing time short impact follower read latency big follower read feature included internal remote procedure call rpc rpc operation took large portion read request processing time therefore big influence read throughputwhat next follower readthis feature seems simple really important future use even way improve tidb performancestrategies variedheat datayou might ask question run large query table affect ongoing online transaction processing oltp io priority queue built tikv prioritizes important oltp request still consumes resource machine leader statea corner case small hot table many read operation write operation although hot data cached memory data extremely hot cpu network io bottleneck occursour previous post tidb internal iii scheduling mention use separate component called placement driver pd schedule loadbalance region tikv cluster currently scheduling work limited splitting merging moving region transferring leader near future tidb able dynamically use different replica strategy data different heat degreesfor example find small table extremely hot pd quickly let tikv dynamically create multiple three readonly replica data use follower read feature divert load leader load pressure mitigated readonly replica destroyed region tikv small mb default tidb flexible lightweight thislocal read based follower readcurrently even though tidb deployed across data center distributes data replica among data center leader provides service piece data mean application need close leader possible therefore usually recommend user deploy application single data center make pd focus leader data center process read write request faster raft used achieve high availability across data centersfor read request process request nearby node reduce read latency improve read throughputas mentioned current implementation follower read little reduce read latency get local committed log without asking leader yes casesas discussed previous post mvcc tikv tidb us multiversion concurrency control mvcc control transaction concurrency entry unique monotonically increasing version numbernext combine follower read mvcc version number data latest committed log local node greater transaction initiated client system return data latest committed log local node wo nt violate atomicity consistency isolation durability acid property transactionsin addition scenario data consistency strict requirement make sense directly support read low isolation level future tidb support read low isolation level performance might improve dramaticallyconclusionthe follower read feature us series load balancing mechanism offload read request raft leader onto follower region ensures linearizability singlerow data read offer strongly consistent read combined tidb snapshot isolationfollower read help reduce load region leader substantially enhances throughput entire system like try see user documentwe taken first step craft follower read make continuous effort optimize feature future interested welcome test contribute project
78,Lobsters,scaling,Scaling and architecture,Fractals of Complexity,https://threadreaderapp.com/thread/1224816921268359168.html,fractal complexity,,allowed customer reach app even one server went increased reliability survived future outage single server tragedy struck error one new hire server started running different version app
79,Lobsters,scaling,Scaling and architecture,How Sustainable is a Solar Powered Website?,https://solar.lowtechmagazine.com/2020/01/how-sustainable-is-a-solar-powered-website.html,sustainable solar powered website,diego marmolejo uptime electricity use system efficiency optimal balance uptime sustainability uptime battery size uptime solar panel size embodied energy different size solar panel battery emission sustainable solar powered website possible improvement let scale thing energy use website throughout internet introduction launched new website growing quickly static website weather bad uptime increased energy use server electricity use system efficiency energy use per unique visitor diego marmolejo location diego marmolejo uptime minimum energy storage required keep website online night uptime uptime expected uptime battery type full charge cloud hour sunlight necessary fully charge battery solar panel size embodied energy different component per year operation embodied energy per year different solar setup carbon emission solar powered website solar panel battery produced china embodied energy oil equivalent l year carbon emission kg year different solar setup comparison carbon intensity spanish power grid server single broken implementation system efficiency build better one energy storage smallscale compressed air energy storage system energy source could built wood solar tracker hand foot diego marmolejo solar webhosting company add website spanish french translation combine server lighting also partly powered offgrid solar energy high embodied energy actually exists country low energy use long life expectancy,illustration diego marmolejo selfhosted solarpowered offgrid website running month article present energy uptime data calculate embodied energy configuration based result consider optimal balance sustainability server uptime outline possible improvement uptime electricity use system efficiency optimal balance uptime sustainability uptime battery size uptime solar panel size embodied energy different size solar panel battery sustainable solar powered website possible improvement let scale thing energy use website throughout internet introduction september lowtech magazine launched new website aimed radically reduce energy use carbon emission associated accessing content internet energy use growing quickly account increasing bit rate online content get heavier increased time spent online especially since arrival mobile computing wireless internet solar powered website buck trend drop energy use far average website opted backtobasics web design using static website instead database driven content management system reduce energy use associated production solar panel battery chose minimal setup accepted website go offline weather bad uptime electricity use system efficiency uptime solar powered website go offline weather bad often happen period one year day december november achieved uptime mean offline due bad weather hour ignore last two month uptime downtime hour uptime plummeted last two month software upgrade increased energy use server knocked website offline least hour every night electricity use system efficiency let look electricity used web server operational energy use measurement server solar charge controller comparing value reveals inefficiency system period roughly one year december november electricity use server kilowatthours kwh measured significant loss solar pv system due voltage conversion chargedischarge loss battery solar charge controller showed yearly electricity use kwh meaning system efficiency roughly energy use per unique visitor period study solar powered website received unique visitor including energy loss solar setup electricity use per unique visitor watthour one kilowatthour solar generated electricity thus serve almost unique visitor one watthour electricity serve roughly unique visitor renewable energy direct associated carbon emission one kilowatthour solar generated electricity serve almost unique visitor embodied energy use uptime story often end renewable energy presented solution growing energy use internet researcher examine energy use data center host content accessible internet never take account energy required build maintain infrastructure power data center omission selfhosted website powered offthegrid solar pv installation solar panel battery solar charge controller equally essential part installation server consequently energy use mining resource manufacture component embodied energy must also taken account simple representation system voltage conversion charge controller server battery meter server battery missing illustration diego marmolejo unfortunately energy come fossil fuel either form diesel mining raw material transporting component form electricity generated mainly fossil fuel power plant manufacturing process sizing battery solar panel compromise uptime sustainability embodied energy configuration mainly determined size battery solar panel time size battery solar panel determine often website online uptime consequently sizing battery solar panel compromise uptime sustainability find optimal balance run keep running system different combination solar panel battery uptime embodied energy also determined local weather condition result present valid location balcony author home near barcelona spain illustration diego marmolejo uptime battery size battery storage capacity determines long website run without supply solar power minimum energy storage required get night additional storage compensate certain period low solar power production day battery deteriorate age best start capacity actually needed otherwise battery need replaced rather quickly uptime first let calculate minimum energy storage needed keep website online night provided weather good battery new solar panel large enough charge battery completely average power use web server first year including energy loss solar installation watt shortest night year june need watthour storage capacity longest night year december need wh minimum energy storage required keep website online night month daylight night storage sep wh oct wh nov wh dec wh jan wh feb wh mar wh apr wh may wh jun wh jul wh aug wh leadacid battery discharged half capacity solar powered server requires wh leadacid battery get shortest night solar condition optimal x year ran system slightly larger energy storage wh solar panel achieved mentioned uptime uptime larger battery would keep website running even longer period bad weather provided solar panel large enough charge battery completely compensate day bad weather significant power production need watthour x watt storage capacity december january combined w solar panel watthour battery practical storage capacity watthour enough storage keep website running two night day even though tested configuration darkest period year relatively nice weather achieved uptime however assure uptime period year would require energy storage keep website online four day low power production would need watthour leadacid battery size car battery include configuration represent conventional approach offgrid solar power uptime also made calculation battery large enough get website shortest night year wh wh wh practical storage capacity wh wh wh respectively latter smallest leadacid battery commercially available website go offline evening could interesting option local online publication low anticipated traffic midnight weather good wh leadacid battery keep server running night march september wh lead acidbattery keep website online maximum hour meaning server go offline night year although different hour depending season finally wh battery keep website online four hour solar power even weather good server stop working around summer around pm winter maximum uptime smallest battery would around practice lower due cloud rain website go offline evening could interesting option local online publication low anticipated traffic midnight however since lowtech magazine readership almost equally divided europe usa attractive option website go every night american reader could access morning expected uptime battery type full charge battery uptime website get day bad weather website get day bad weather website get night weather good website go offline many night year website go offline every night website go offline every night uptime solar panel size uptime solar powered website determined battery also solar panel especially relation bad weather larger solar panel quicker charge battery fewer hour sun needed get website night example w solar panel one two hour full sun sufficient completely charge battery except car battery replace w solar panel w solar panel however system need least hour charge wh battery optimal condition w operate server w charge battery solar panel combined larger wh leadacid battery need hour full sun charge battery completely possible february november larger solar panel increase chance website remains online even weather condition optimal cloud larger solar panel equally advantageous cloudy weather cloud lower solar energy production anywhere maximum capacity depending thickness cloud cover watt solar panel produce maximum capacity still enough run server charge battery however w solar panel produce capacity enough power server battery charged ran website w panel january quickly went weather optimal powering website solar panel wh battery w solar panel smallest solar panel commercially available absolute minimum required run solar powered website however optimal condition able power server charge battery could keep website running night day long enough solar panel rarely generate maximum power capacity would result website online sun shine even though combination small solar panel large battery embodied energy combination large solar panel small battery system creates different characteristic general best opt larger solar panel smaller battery combination increase life expectancy battery leadacid battery need fully charged time time lose storage capacity hour sunlight necessary fully charge battery solar panel size battery embodied energy different size battery solar panel take megajoule mj produce watthour leadacid battery capacity mj energy produce one solar panel table present embodied energy different size battery solar panel calculate embodied energy per year based life expectancy year battery year solar panel value converted kilowatthours per year refer primary energy electricity solar powered website also need charge controller course web server embodied energy component remains matter size solar panel battery embodied energy per year based life expectancy year embodied energy different component per year operation battery embodied energy battery kwhyear battery kwhyear battery kwhyear battery kwhyear battery kwhyear battery kwhyear solar panel embodied energy solar panel kwhyear solar panel kwhyear solar panel kwhyear solar panel kwhyear component embodied energy solar charge controller kwhyear server kwhyear data calculate total embodied energy combination solar panel battery result presented table embodied energy varies factor five depending configuration kwh primary energy per year combination smallest solar panel smallest battery wh kwh primary energy per year combination largest solar panel w largest battery embodied energy per year different solar setup solar panel battery kwh kwh na na kwh kwh kwh na kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh kwh divide result number unique visitor per year obtain embodied energy use per unique visitor website original configuration uptime solar panel battery primary energy use per unique visitor wh result would pretty similar configuration lower uptime although embodied energy lower number unique visitor carbon emission sustainable solar powered website carbon emission solar powered website calculated embodied energy different configuration calculate carbon emission compare environmental footprint solar powered website old website hosted elsewhere measure energy use compare solar powered website similar selfhosted configuration run grid power allows u ass un sustainability running website solar power life cycle analysis solar panel useful working component work assumption energy produced panel used necessarily true case larger solar panel waste lot solar power optimal weather condition hosting solar powered lowtech magazine year produced much emission average car driving distance km therefore take another approach convert embodied energy component litre oil litre oil kwh primary energy calculate result based oil litre oil produce kg greenhouse gas including mining refining take account solar panel battery produced china power grid three time carbonintensive le energy efficient europe mean fossil fuel use associated hosting solar powered lowtech magazine first year panel wh battery corresponds litre oil kg carbon emission much average european car driving distance km result configuration embodied energy oil equivalent l year carbon emission kg year different solar setup kg kg na na kg kg kg na kg kg kg kg kg kg kg kg kg kg kg kg kg kg kg kg comparison carbon intensity spanish power grid let calculate hypothetical running selfhosted web server grid power instead solar power case depend spanish power grid happens one least carbon intensive europe due high share renewable nuclear energy respectively last year carbon intensity spanish power grid decreased per kwh electricity comparison average carbon intensity europe around per kwh electricity carbon intensity u chinese power grid respectively per kwh electricity look operational energy use server kwh electricity first year running spanish power grid would produced kg compared kg tested configuration seems indicate solar powered server bad idea even smallest solar panel smallest battery generates carbon emission grid power carbon intensity power grid measured embodied energy renewable power infrastructure taken zero however comparing apple orange calculated emission based embodied energy installation carbon intensity spanish power grid measured embodied energy renewable power infrastructure taken zero calculated carbon intensity way course would zero ignoring embodied carbon emission power infrastructure reasonable grid powered fossil fuel power plant carbon emission build infrastructure small compared carbon emission fuel burned however reverse true renewable power source operational carbon emission almost zero carbon emitted production power plant make fair comparison solar powered server calculation carbon intensity spanish power grid take account emission building maintaining power plant transmission line fossil fuel power plant eventually disappear energy storage course ultimately embodied energy component would depend chosen uptime possible improvement many way sustainability solar powered website could improved maintaining present uptime producing solar panel battery using electricity spanish grid would largest impact term carbon emission carbon footprint configuration would roughly time lower lower operational energy use server improve system efficiency solar pv installation would allow u run server smaller battery solar panel thereby reducing embodied energy could also switch another type energy storage even another type energy source server already made change resulted lower operational energy use server example discovered half total data traffic server tb caused single broken r implementation pulled feed every couple minute difference power use watt add watthour course hour mean website stay online hour longer fixing well change lowered power use server excluding energy loss watt watt gain may seem small difference power use watt add watthour course hour mean website stay online hour longer system efficiency system efficiency first year energy loss experienced charging discharging battery well voltage conversion solar pv system usb connection loss add initial voltage converter built pretty suboptimum solar charge controller builtin usbconnection could build better one switch solar pv setup energy storage increase efficiency energy storage could replace leadacid battery expensive lithiumion battery lower chargedischarge loss lower embodied energy likely eventually switch poetic smallscale compressed air energy storage system caes although low pressure caes system similar efficiency leadacid battery much lower embodied energy due long life expectancy decade instead year energy source another way lower embodied energy switch renewable energy source solar pv power high embodied energy compared alternative wind water human power power source could harvested little generator voltage regulator rest power plant could built wood furthermore waterpowered website require hightech energy storage cold climate could even operate website heat wood stove using thermoelectric generator solar tracker people good supply wind water power could build system lower embodied energy however unless author start powering website hand foot pretty much stuck solar power biggest improvement could make add solar tracker make panel follow sun could increase electricity generation much allow u obtain better uptime smaller panel let scale thing final way improve sustainability system would scale run website server run larger server solar pv system setup would much lower embodied energy oversized system website alone illustration diego marmolejo solar webhosting company fill author balcony solar panel start solar powered webhosting company embodied energy per unique visitor would decrease significantly would need one server multiple website one solar charge controller multiple solar panel voltage conversion would energy efficient solar battery power could shared website brings economy scale course concept data center although ambition start business others could take idea forward towards data center run efficiently data center today powered renewables go offline weather bad add website found capacity server large enough host website already took small step towards economy scale moving spanish french version lowtech magazine solar powered server well translation although move increase operational energy use potentially also embodied energy use also eliminate website hosted elsewhere also keep mind number unique visitor lowtech magazine may grow future need become energy efficient maintain environmental footprint combine server lighting another way achieve economy scale would give whole new twist idea solar powered server part author household also partly powered offgrid solar energy could test different size battery solar panel simply swapping component solar installation running server w panel author running light living room panel often left sitting dark running server w panel way around light household expense lower server uptime weather get bad author could decide use light keep server online way around let say run light server one solar pv system would lower embodied energy system considered one solar charge controller would needed furthermore could result much smaller battery solar panel compared two separate system weather get bad author could decide use light keep server online way around flexibility available server load power use easily manipulated energy use network far know first life cycle analysis website run entirely renewable energy includes embodied energy power energy storage infrastructure however course total energy use associated website also operational embodied energy network infrastructure includes router internet backbone mobile phone network operational embodied energy device visitor use access website smartphones tablet laptop desktop low operational energy use limited lifespan thus high embodied energy energy use network directly related bit rate data traffic run lightweight website efficient communication network server however little influence device people use access website direct advantage design much smaller network example website potential increase life expectancy computer light enough accessed old machine unfortunately website alone make people use computer longer network infrastructure enduse device could reimagined along line solar powered website said network infrastructure enduse device could reimagined along line solar powered website downscaled powered renewable energy source limited energy storage part network infrastructure could go offline local weather bad email may temporarily stored rainstorm km away type network infrastructure actually exists country network partly inspired solar powered website enduse device could low energy use long life expectancy total energy use internet usually measured roughly equally distributed server network enduse device including manufacturing device make rough estimate total energy use website throughout reimagined internet original setup uptime would kwh primary energy corresponds litre oil kg per year improvement outlined earlier could bring number calculation whole internet powered oversized solar pv system balcony kris de decker roel roscam abbing marie otsuka thanks kathy vanhout adriana parra gauthier roussilhe proofread alice essam make comment please send email solar lowtechmagazine dot com
80,Lobsters,scaling,Scaling and architecture,How I write backends,https://github.com/fpereiro/backendlore,write backends,write backends hn discussion public domain approach ac pic core tool ubuntu long term support version architecture nodejs long term support version redis nginx amazon web service s architecture reverse proxy let encrypt http section mvp mongodb postgres file section sticky session spiped dns load balancing aws elb cap theorem consistency availability redis cluster really database partitioning http cculte redis mission critical data acid property rdb persistence example file node one server one service server library iek express bodyparser busboybodyparser cookieparser environment variable configuration notification attentive reader pointed log rotation code structure documentation provisioning mon mongroup notification docker keeping server fresh warning nt db partitioning defined vertical partitioning guarantee making structural change production database deploying server identity security bcryptjs x csrf prevention route whitelisting user admin user master node cluster master task reply function validation autoactivation autoactivation teishi payload style rest caching etags testing hitit separate tool example development cycle vagrant future direction rolled publicly license fpereiro gmailcom,write backends late present writing backends serverside code web application document summarizes many aspect write piece code writing lore three purpose share systematize future reference improvement learn feedback question observation welcome must sting please also nice please accurate like thank everyone joined hn discussion document pointed stimulated many interesting point missed humbled grateful tremendously positive constructive feedback received yall rock public domain take whatever find useful approach approach backends code general iteratively strive simplicity approach share good luck enabled write lean solid maintainable server code relatively small time investment minimal hardware resource see approach work practice see code backend open source service ac pic core tool latest ubuntu long term support version underlying o use cloud see architecture locally latest nodejs long term support version although older even much older version node would suffice latest version recommended security performance improvement latest version redis available package manager current ubuntu latest version nginx available package manager current ubuntu amazon web service particular file s email optionally architecture simplest version architecture local development look like architecture local ubuntu box internet node local filesystem localhost redis aws s simplest version architecture remote environment server connected internet public ip address look like architecture b remote ubuntu box internet nginx node local filesystem redis v aws s nginx work reverse proxy main use provide http support particular http support nginx extremely easy configure let encrypt free automated open certificate http nginx please refer http section architecture run everything single ubuntu instance node redis access local filesystem take surprisingly far definitely sufficient test environment also serve production environment mvp even application moderate use mission critical redis replaced complemented another nosql database mongodb relational database postgres throughout section see redis feel free replace database might want use node stateless rather store state redis f allows easier scaling later see importantly storing state explicitly either redis f overall structure much easier understand debug notice arrow connecting redis aws reflects highly recommended possibility periodically automatically uploading redis dump aws restore database snapshot case issue relying local f optional since also using aws discussion see file section performance simple setup outstanding personally witnessed virtual instance significantly le power comparable dedicated server core ram handling thousand request per second request per day millisecond latency unless application logic cpu intensive performance reason changing architecture unless expect load closer billion request per day two main reason using architecture involving machine resilience outage use one machine single point failure much data redis andor file fit one instance architecture several instance node running course possible case load high andor latency need kept bare minimum case nginx placed machine also filesystem redis placed another instance architecture c node aws s load balancer internet nginx node f server redis notice data server comprising database file seems reflect pattern repeated since beginning computing always two type storage one fast small another one larger slower redis f serve particular incarnation pattern within architecture node refer data server state including session request served node node serf inconsequential sidestep need sticky session architecture c modification necessary instead relying local access filesystem node server must act filesystem server implement route reading writing listing deleting file experience logic done line code must written carefully data server publicly accessible redis must secured either password better spiped f server meanwhile probably use either password header better auth system cooky authorizedeny access security provision necessary data server accessible broader internet use aws vpc mean restrict network access data server unnecessary either case f server also communicate aws handle content highly recommended redis followerslave replica separate server architecture node aws s load balancer internet nginx node f server redis redis replica n node server possible scale horizontally improve performance avoid outage one node server architecture however still two point failure load balancer data server multiple load balancer also possible multiple record dns load balancing using aws elb leaf problem scaling data server f split among several server although undertaking nontrivial case large amount data rely mostly aws use f server mere cache truly difficult thing scale careful vendor statement scaling database scaling database allows partitioning data size fit single server matter much data depending type partitioningsharding use allow downtime data loss event database node failing essential aware cap theorem partition database multiple node connected internet thus susceptible delay error communication choose consistency availability either decide respond request risk serving storing inconsistent data choose remain fully consistent price downtime hard choice interesting one solution including redis cluster favor availability consistency whatever solution use one default way database choice meant scale unless really know architecture e node aws s load balancer internet nginx node f server redis redis replica f server redis redis replica note database partitioning http providing http support node possible experience quite cumbersome using nginx seems reduce overall complexity architecture setup using nginx receive request also advantage node server run nonroot user prevents exploit node giving access attacker o without configuration default port http http used root user system port actually architecture nginx run root listens two port forward traffic node listening port higher question future research node instead nginx running root reduces attack surface application mere cculte concrete security advantage run nginx root merit taking trouble set port forwarding configure http nginx domain domain could either domain mydomaincom subdomain appmydomaincom main record pointing ip ubuntu server control set http sure replace occurrence domain actual domain sudo aptget install pythoncertbotnginx note old version ubuntu might run two command installing certbot sudo addaptrepository ppa certbotcertbot sudo aptget update file etcnginxsitesavailabledefault change servername domain sudo service nginx reload sudo certbot nginx domain forwarding traffic nginx local node use nginx configuration snippet within server block use please replace port port node server listening location proxypass http port proxysetheader xforwardedfor remoteaddr proxysetheader line sends node ip address whoever made request useful security purpose like detecting abnormal geographic location login repeated source malicious request redis redis amazing choice database may best choice highly recommend take look particularly never used redis much keyvalue store server implement data structure memory practice mean access fundamental construct like list hash set data structure implemented amazing quality consistency performance work mission critical data financial transaction healthcare suggest working instead relational database fulfills acid property high probability losing data ever far know redis guarantee aci guarantee extent least running single node stated architecture section highly recommended rdb persistence possibly aof turned highly recommend backing redis dump aws instead locally done node instance redis inmemory database either redis underlying instance restarted almost certainly lose second data data written memory yet committed rdbaof file disk survive restart critical neither redis instance run randomly restarted deeper cause accidental restarts analyzed eliminated seven year using redis never experienced restart coming redis redis bug matter experienced docker o restarting redis memory limit recommend writing readme key structure used application redis example nt like redis cluster partitioningsharding model scaling redis multiple node maintaining consistency understandability overall structure open problem file using aws highly recommended highly secure redundant cheap way store file use node recommend using official sdk awssdk example initialization var new require awssdk apiversion sslenabled true credential accesskeyid secretaccesskey params bucket main method use possible write line code layer provides consistent access method used rest server usually also use local f serve file relying recovery case server disk issue two main reason cost application potentially serf lot data cost significant downloading gb currently cost cost storing entire month make worse hard predict much data may use given month unlimited financial downside ideal service would charge downloads storage speed fast even accounting network delay latency par course compared sub local f serving file server much better speed cost low predictable downside using local f requires extra code management file also requires f server architecture multiple node instance requires multiple server partitioning logic data large fit single f server whether go using local f single f server fullfledged scalable f server idea use ultimate storage file guarantee durability deleting file recommended delete moving separate bucket deletion lifecycle automatically deleted later may help case unwanted deletion also good pattern case object overwritten node application logic life node node serf incoming request http api http api allows u serve code need user facing ui either web native admin also programmatic access third party get three price one long web client userfacing admin perform clientside rendering however application server side rendering adding http api might entail extra work personally embrace clientside rendering approach outside scope backend lore data transmitted client server two type file http image client also upload file request type multipartformdata json data preferred format interaction one server one service experience nontrivial application contains application logic plus set function interacting f redis auth logic written line code eye warrant keep server single centralized concern code hardly called monolith rush embrace microservices paradigm offer following rule thumb two piece information dependent belong single server word natural boundary service natural boundary data server library project maintain code prefer iek although library begging rewrite case use express addons bodyparser busboybodyparser cookieparser environment variable configuration use single environment variable specify name environment typical option dev prod environment provided default dev environment passed argument running server node server env retrieved server code follows var env processargv store configuration server j file usually named configjs case server open source code create separate file secretjs store credential secret file referred henceforth config case secretjs present sure add gitignore file typical thing go config port node listens name cookie session stored redis configuration cookie secret signing cooky aws credential whitelisted usersadmins config simply required var config require configjs specify configuration item per environment rather separate object better split property change environment environment example var env processargv moduleexports port bucket env prod prodbucket devbucket notification find useful single function notify report error warning function act funnel pas event may concern u function variety thing like printing log sending email sending data remote logging service suggest invoking notify function following case master process fails worker process fails database fails unreachable server start client transmission error client replied error code lately relying local log anymore instead sending log separate log server store log permanent file make accessible searchable web admin saner person would probably use established logging service attentive reader pointed redirecting standard output node tmp mean log wo nt preserved restart design nt risk log filling disk something happens way often usually expected enough require another moving part log rotation advantage local log nt ssh different server see going approach however requires important data logged including perhaps foremost uncaught exception error stacktraces code structure adminjs admin client optional clientjs user ui configjs secretjs configuration deploysh script deploying app mongroupconf keeping app running packagejson dependency list provisionsh creating new instance serverjs server code testjs test suite documentation contained single markdown file readmemd usually contain following purpose project general description todo list route including description behavior payload db structure f structure configuration instruction license provisioning provisioning new server upgrade package first ssh host aptget update ssh host debianfrontendnoninteractive aptget upgrade withnewpkgs recommend installing ratelimit ip perform invalid ssh logins ssh host aptget install provision node server merely need install node plus mon mongroup install node ssh host curl sl http sudo e bash ssh host aptget install nodejs install mon mongroup ssh host aptget install buildessential ssh host mkdir tmpmon cd tmpmon curl l http githubcomtjmonarchivemastertargz tar zx strip make install rm rf tmpmon ssh host npm install g mongroup use mongroup keep server running need file mongroupconf look like log tmpmyapplogs pid tmpmyapppids node server env env dev prod place command single provisionsh file create different file provisioning different machine important thing set unambiguous set command run successfully represent entire configuration needed instance unspecified unwritten step provisioning instance want store log application within server please change log path mongroupconf another location tmp highly recommend set log rotation logging please see notification section long fully control remote instancesservers nt see need running application within docker sort virtualization since full environment replicable starting given o run certain number command reach provisioned instance idempotence benefit virtualization running service host everything else simpler recommend virtualization deploying environment nt fully control keeping server fresh running architecture type b nt mind couple minute downtime per week way keep instance fresh run script call refreshsh like every week upgrade software package stop app gracefully stop redis gracefully restart instance whether strictly necessary debatable superficial level inviting ssh instance greets message system restart required even context software written quality mind running process tend become fragile time includes o particularly case linux suspect openbsd might likely run forever without issue restarting process periodically might crude effective way reboot aging runtime internet resilient expects failure instead trying prevent vein system recover reboots feel like step right direction open debate particularly practical experience regard warning showing refreshsh please bear mind nt instance run production database application significant amount traffic data nt data server architecture c also turn node beforehand avoid serving request error triggering alert ignoring alert dangerous practice general coordinated automatic outage sound daunting never implemented nt good answer problem know relying production database serverinstance never updated restarted also fragile approach never rely instance never failing approach far perform operation manually every couple month adrenaline pumping checking backup always entail downtime hope better solution future larger setup multiple node use approach instance running node without experiencing downtime long nt restart node instance time ok enough warning go refreshsh export path usrlocalsbin usrlocalbin usrsbin usrbin sbin bin snapbin aptget update debianfrontendnoninteractive aptget upgrade withnewpkgs aptget autoremove aptget clean cd pathtoyourapp mg stop service redisserver stop shutdown r notice app stopped redis stopped avoid serving request error trigger alert indicating database unreachable running script instance run node omit fourth line since redis work instance need start application automatically another script call startsh needed cd pathtoyourapp mg restart put two entry crontab must replace h minute hour day week want perform refresh also change path indicate script leaving place execute script make sure working intended h pathtorefreshsh reboot pathtostartsh db partitioning splitting data database part called sharding name give chill shard defined piece broken glass pottery especially one found archaeological dig mind eye evokes sight broken glass pretty much last thing want see talking main repository information large application potentially thousand million user reason use term partitioning usually f connotation instead shard speak node offended likely know way distributed database pay attention saying anyway notice partitioning unrelated whether readonly replica database example would nt consider architecture single redis master single redis replica attached architecture partitioned database two redis master database process writes looking partitioned architecture experience partitioned database slim worked context manual type partitioning stored certain type data one database certain type data one called vertical partitioning reason sheer size database redis easy fit memory one server since talk experience unable suggest perform db partitioning especially using relational database time standard solution partitioning intimately linked database using developed promoted database vendor using probably much saner rolling partitioning case adopting partitioning scheme suggest understanding answer following question database partition automatically done manually easy easy reduce scale merge partition create downtime scaling updown perhaps performance degradation chance losing consistency take place data distributed randomly across partition decide store data partitioning compromise usual guarantee database provides atomicity consistency isolation durability render transaction impossible case node failing total writeread failure total write failure partial write failure possible inconsistency later backup recover entire cluster database scratch shut system cleanly recreate somewhere else making structural change production database experience changing structure production database entirely within nosql orbit mostly redis little bit mongodb whenever worked relational database nt one charge performing database migration although written couple relational database quite sturdy mechanism performing update schema table nosql database generally thing go irreversibly bad much easier reason protocol follow modifying structure production database system running generally entail downtime unless change performing affect data consistency case api need turned change made database receiving request protocol probably overkill relational database though sure doubt please go ask someone type thing relational database running update nosql database might come handy write script performs change database case usually single function performs necessary change structure data test local dev environment data get working reasonably well use recent production backup restore either locally dev environment hold data debug script replica database production data created might recreate data several time fine think time dev environment sparing committing mistake production database thousand million depend add automated sanity check end script make sure look good script run make final clean run check script correctly one go without intervention part eschew manual touchups since easy forget later commit script nt done already somewhere repo know exactly script ran script affect consistency data might turn traffic run full backup production database check backup created recreated dev environment created take minute want trust backup take deep breath run script production database script run consider whether adrenaline rush nt entirely unrelated choice working backends script done slow systematic make sure interpret sanity check correctly something failed restore production database backup created nt freak exactly made backup first place nothing failed hurray turn apis make sure traffic flowing smoothly always paranoid process single stressful controlled backend task probably encounter also find exhilarating deploying server normally deploy bash script performs following determines ip depending environment asks extra parameter confirm deploying prod compress entire repo folder ignoring nodemodules folder swp temporary file generated vim text editor copy compressed folder target server uncompresses folder add environment mongroupconf run npm nosave installupdate dependency restarts server command mg restart deletes compressed folder server local computer run server locally installing dependency npm nosave merely run node server eschew automatic code deployment triggered commit certain repository branch deploying manually script take second like see deployment indeed successful moment instead finding email later committing deploying reasonably considered separate process identity security identity managed server without reliance external service user data stored database absolutely critical hash user password recommend using bcryptjs side note wherever possible use pure j module avoid c binding latter sometimes require native package installed experience tend fickle j code need runtime dependency run compilation cooky used identify session session quantity cryptographically hard guess cookie name predetermined configuration example myappname value session session nt contain user information user information stored database recently enabled httponly attribute cookie accessible clientside j main reason app ever victim x malicious user content failed filter user wo nt fully vulnerable otherwise session temporary passwordequivalent would immediately available attacker enable csrf prevention create csrf token bound particular session token sent server successful login also endpoint client request csrf token endpoint also serf way client ask server whether currently logged csrf token sent along every post request actually request could perform change csrf token might necessary supporting older browser discussion alternative see cooky also signed session within cookie easily guessable signing nt make harder guess reason signing however allows u distinguish cookie valid expired invalid cookie word distinguish expired session attack without keep expired session database deal cookiesession lifecycle set cooky expire distant future expires attribute let server decide cookie expired expired session received determined server server reply order browser delete cookie way nt guess cookie expire server retains full control lifecycle session expiry period could n hour n day session nt used period time expires removed since redis inbuilt mechanism expiring key period time happens automatically without extra code every time session used automatically renewed avoids user kicked session using besides httponly expires set path attribute cookie since otherwise cookie nt seem preserved tab browser closed route iek express route executed order defined request might matched one route executed order route power either pas request next matching route instead reply request ca nt however route executed every incoming request exist following order avant route happens beginning creates log object response including timestamp profiling debugging purpose public route requiring logged auth route gatekeeper route route check presence session cookie cookie present request replied code possibly notify u session invalid checking signature either determine expired session malicious attempt session valid associated user data retrieved database placed within request object case route invokes responsenext pas request following route csrf route check csrf token body every post request private route requiring logged admin gatekeeper route request looking match route one must admin otherwise returned admin route following typical auth route post authsignup post authlogin post authlogout post authdestroy eliminates account post authverifyemail route optional route requires sending email post authrecover route requires sending email post authreset post authchangepassword route possibly requires sending email notify user whitelisting user system predefined user easiest put list valid email address config signup logic triggered system allows whitelisted address create account pattern allows reusing code normal application open sign without backdoor without create user account seed script admin user implemented two way either setting flag database manually admin route effect reading list admin user config case admin user still use session normal user master node master node us cluster fork one child process recommended fork one process per cpu get number cpu command require o cpu length n node serving request one per cpu case worker node fails recommended use uncaughtexception handler notify error like clusterismaster return processon uncaughtexception function error notify error processexit important also exit worker process case suffered uncaught exception since exception potentially render process unstable master task task besides api done master process include uploading redis dump periodically notify cpu ram usage dangerously high perform db consistencycleanup check particularly dev usually write function end serverjs file route reply function keep code fluid short whenever replying request json data use reply function come already defined iek using express define top serverjs form reply response code body header validation autoactivation consider indispensable every route receives data perform deep validation payload break server whether inadvertently maliciously server fault embodied concept autoactivation performing validation use combination teishi custom code check usually done first synchronously sometimes asynchronously data stored database necessarily reached asynchronously global type validation happens actual route middleware check request applicationjson contenttype header contain body valid json payload style usually use get post method interchanging json often put patch dealt almost code post retrieving data get query request payload larger character case enough send query post since post request cached right response header entail performance issue apology church rest caching use etags compute cache header use actual response body json modification time file size case static file avoiding date caching made code debugging much much simpler testing since writing http apis make sense test http request use hitit tool trigger http request style testing end end api tested outside internals tested outcome provide route use internal convoluted enough moved module test far nt seen need code seems become either application separate tool whenever test fails entire suite fails error warning tolerated test suite either pass test use backdoor indistinguishable client request allowance make server code send email environment local simplify thing specifying test database running test locally since development environment similar environment code run seems work well test clean using request ie deleting test user created proper route another advantage testing http test payload work complementary documentation payload sent received server route recommend using random quantity sending payload make test replicable want break given route possible write function given schema generate invalid payload sort bruteforce way testing validation route example test run manually node test server already running development cycle first write server route along documentation write test route first try break ca nt break send different payload cover main case test passing backend ready time write client bug find likely client since server debugged way error nt chased side wire vagrant nt want install node redis host machine running ubuntu use vagrant set local ubuntu develop application sample vagrantfile set ubuntu vm redis node installed please note useful local environment provision remote instance vagrantconfigure config use ubuntu configvmbox use ram core configvmprovider virtualbox v vmemory vcpus end map server port replace port node server listen configvmnetwork forwardedport guest host configvmprovision shell inline shell always login root grep qxf sudo su homevagrantbashrc echo sudo su homevagrantbashrc update upgrade package sudo aptget update sudo debianfrontendnoninteractive aptget upgrade withnewpkgs install redis sudo aptget install redisserver install nodejs curl sl http sudo bash sudo aptget install nodejs provisioning command may want run shell end essential vagrant command vagrant create environment nt exist start already exists vagrant ssh enter environment created vagrant halt shut environment vagrant destroy destroy environment warning erase file within environment future direction recently decided take common part application building provide service future following advantage reduce code size line test line allow unlimited scaling provide unified admin see different aspect application service plan extract identity store identity cooky server code consist calling function within route plus sending email couple route rate limiting also piggybacked file service backed served fast nodepowered server also allow tailing filtering file fast querying redis redis accessible service http backed consistent configurable incremental partitioning model logging notify send data unified service allow sending email certain situation beat track cpu ram usage dashboard allow sending email certain situation stats unified statistic measuring different thing matured rolled publicly road ahead long indeed license document written federico pereiro fpereiro gmailcom released public domain
81,Lobsters,scaling,Scaling and architecture,Pressure Based Anti-Spam for Discord Bots,https://erikmcclure.com/blog/pressure-based-anti-spam-for-discord-bots/,pressure based antispam discord bot,architecture remove permission operation raid mode pressure system hacker news max pressure base pressure embed pressure length pressure line pressure ping pressure repeat pressure filter pressure worst case scenario conclusion available weird intermittent cloudflare error badly designed lock system,back discord wee little chat platform rate limiting whatsoever api already reverseengineered bunch bot developer went around spamming random server many message would crash client friend determined keep discord server public went creating first antispam bot discordi longer maintaining bot beyond simple bugfixes better thing time tired people trying use best antispam bot even deprecated post effort educate antispam bot ascend beyond simple mute someone send n message second filter hopefully make better antispam bot nt toarchitecturethere disagreement exactly antispam bot work widerange preferred behavior differ server depending community size server rate growth server administrator personal preference many antispam solution lock server forcing new member go airlock channel consider overkill result poor antispam bot bad actually stopping troll responding raidsmy moderation architecture go like channel moderation channel raid containment silence containment log channel role member silence newwhen firsttime setup run server bot query store permission given everyone role add permission member role go existing user add member role remove permission everyone add override raid containment anyone without member role speak raid containmentthen creates silence role adding permission override every single channel server prevent sending message channel except silence containment channel ensures matter new role might added future silence always prevent sending message channel silence containment admins server configure visibility containment channel server make raid containment visible anyone without member role ensure silence containment visible anyone silence role hide log channel everyone except adminsthis architecture chosen allow bot survive megaraid even bot account storm server bot go ratelimited ca nt anything nt given member role server protected default essentially automated airlock engages bot detects raid instead forcing new member go airlock admins like approach prefer personally audit every new member viable larger serversthe member role added also shortcircuit discord verification rule unfortunate consequence practice usually nt problem however help compensate bot also configured add temporary new role newer user expires configurable amount time role actually administrator usually simply disables sharing image embedsoperationthe bot two distinct mode operation normal operation new member join server automatically given member new enabled immediately allowed speak trigger antispam moderator us silence command silence role added message sent last second default deleted restricted speaking silence containment default silence happen automatically never rescinded moderator manually unsilences someone however server admins lazy automatic expiration added manual silence also configured expire set period time user trigger antispam filter containment channel bot automatically ban themit important note silenced user member silence role reason many server optin channel accessible add role simply removing member would suffice keep talking channel silence override permission ensure ca nt speak channel without mess anyone assigned rolesthe bot detects raid n join happen within second usually configured something like join second interval active server happens server enters raid mode automatically remove member user triggered raid alert sends message pinging moderator moderator channel stop adding member anyone join still raid mode also change server verification level actually effect newly joined member nt role assigned yet moderator either cancel raid alert automatically add member everyone detected part raid alert individually add member people vetted raid mode automatically end second would example moderator use command ban everyone joined raid alert selectively add member user let inthis method dealing raid ensures bot overwhelmed sheer number join crash taken server stay raid mode bot recover potential raider allowed inpressure systemall antispam logic bot done triggering automatic silence give someone silence role bot determines silence someone analyzing message sent using pressure system essence pressure system analogous gravity function used calculating hotness given post site like hacker newseach message user sends assigned pressure score calculated message length content whether match filter many newlines etc pressure designed simulate disruptive message current chat wall text extremely disruptive whereas saying hi probably nt attaching image message disruptive embedding single link nt bad sending blank message newlines incredibly disruptive whereas sending message two paragraph probably fine addition message assigned base pressure minimum amount pressure message even single lettermy bot look following value max pressure default maximum pressure pressure message generate pressure default message sent onceembed pressure image link attachment message generates additional pressure limit user image link single messagelength pressure individual character message generates additional pressure limit sending maximum length message onceline pressure newline message generates additional pressure limit newlines single messageping pressure every single unique ping message generates additional pressure limit user ping single messagerepeat pressure message sent exact previous message sent user copypasted generates additional pressure effectively doubling base pressure cost message mean user copy pasting message twice rapid succession silencedfilter pressure filter set admins add arbitrary amount additional pressure message trigger filter regex amount userconfigurableand simplified version implementation note adding pressure piecewise alert moderator tell exactly pressure trigger broke limit help moderator tell someone posted many picture spamming message waddpressure info track infoconfigspambasepressure return true waddpressure info track infoconfigspamembedpressure len mattachments return true waddpressure info track infoconfigspamembedpressure len membeds return true waddpressure info track infoconfigspamlengthpressure len mcontent return true waddpressure info track infoconfigspamlinepressure stringscount mcontent n return true waddpressure info track infoconfigspampingpressure len mmentions return true len mcontent mcontent tracklastcache waddpressure info track infoconfigspamrepeatpressure return true calculated disruptive given message add user total pressure score measure disruptive user currently however need recognize sending wall text every minute probably nt issue sending short message saying roflcopter second definitely spamming add message pressure user pressure check long since user last sent message decrease pressure accordingly second pressure nt decrease much minute pressure used probably gone heat algorithm nonlinearly experiment showed linear dropoff tends produce better result simpler implement bot implement simply decreasing amount pressure user set amount every n second second lose pressure reach zero implementation bot done pressure added timestamp botgettimestamp track wtrackuser author timestamp last tracklastmessage tracklastmessage timestampunix timestampnanosecond tracklastmessage last happen discord bad habit resending timestamps anything much touch message tracklastmessage last return false invalid timestamp never spam interval tracklastmessage last trackpressure infoconfigspambasepressure interval infoconfigspampressuredecay trackpressure trackpressure essence entire system superset simplistic n message second use base pressure maximum pressure determine absolute upper limit many message send given time period regardless content tweak rest pressure value quickly catch obvious instance spamming maximum pressure altered perchannel basis allows meme channel spam message without triggering antispam user pressure value look like time whole pressure system integrated regex filtering module server essentially create badword filter assigning huge pressure value certain match instantly creates silence regexes used look thing bot nt currently track like allcaps message link specific website pressure system allows integrating check unified way represents total disruptiveness individual user behaviorworst case scenarioa special mention go uncommon possible worstcase scenario spamming happens dedicated attacker really really want fuck server reason creating fake account randomized name wait discord long enough new discord message nt show bot join server extremely slowly pace maybe per hour wait another week unleash spam attack account send exactly message followed different account sending another messageif fast enough detect simply sheer volume message sent channel automatically put channel slow mode however principle completely impossible automatically deal attack banning everyone happens sending message spam event ban innocent bystander individual account nt sending message fast maybe one per second indistinguishable normal rapidfire conversationmy bot emergency method dealing track given user sent first message server use command ban everyone sent first message past n second effective work trivial spammer get past realize going need fake account say hi trickling course bunch account saying hi nothing else may raise enough suspicion catch attack happensconclusionhopefully article demonstrates make reliable extensible antispam bot deal pretty much imaginable spam attack code deprecated spam bot available reference code terrible bad idea add bot server anymore support channel longer accessiblemy hope someone use guideline build much effective antispam bot release torment hate web development rather spend time building native webassembly compiler instead worrying weird intermittent cloudflare error temporarily break everything badly designed lock system deadlock rare edge case godspeed bot developer
82,Lobsters,scaling,Scaling and architecture,What I'm Not Doing This Week in F#,https://youtu.be/_CuIV8Y0dBU,week f,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature week f youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature week f youtube
83,Lobsters,scaling,Scaling and architecture,"Headcount goals, feature factories, and when to hire those mythical 10x people",https://erikbern.com/2019/02/21/headcount-targets-feature-factories-and-when-to-hire-those-mythical-10x-people.html,headcount goal feature factory hire mythical people,headcount goal feature factory hire mythical people better output written headcount goal solving misalignment cost function productivity feature factory task overhead costbenefit analysis high output engineer plugged wolfram alpha getting value tech team xavier amatriain wrote blog post tagged startup hiring management math,headcount goal feature factory hire mythical people started building tech team better made conscious decision pay high end get people thought made sense cost bit money hire output usually compensates many fellow ctos went side spectrum mystery made sense output get started let clarify mean output productivity nt mean engineer hammering keyboard shipping code light speed talk refer whole range thing like helping coworkers introducing new framework improving process much written past ca nt really measure course manager try set salary alice bob level manager certainly believe precise idea relative value engineer headcount goal let dissect classic management objective headcount goal typical engineering hiring process cto high person figure roughly much need get done compared many engineer go cfo haggle bit get assigned headcount number salary range people get distributed across org recursively every hiring manager get target many people hire let say cto absolutely adamant need grow engineering team year bubble junior engineering manager running decentralized interview process know create great agency problem junior manager told success company partly measured well reach hiring goal course going lower bar hire nt think happens seen seen recruiting bar start slipping wellmeaning people pushing resource time average level engineering talent slowly decline solving misalignment right solution partly make interview process decision centralized team impose hiring standard aggressive headcount goal everyone team incentivized lower bar let also fundamental level headcount goal make underlying assumption every engineer roughly productivity reality engineer productivity dispersed target certain output level course engineer nt come label say one engineer cost one cost nt know let first talk relationship though think important understand cost function productivity maybe surprising cost function productivity seems sublinear function engineer might cost say clearly law imposed physic fit straight line think people done serious recruiting would concede follows something slightly le linear instance let say cost k x engineer k engineer pay engineer pay choice exponent bit arbitrary point reflect cost scale le lineary exponent le work purpose argument note exponent larger would exist efficient market one would hire engineer cost would simply hire engineer seems like nobrainer would nt everyone pay ton money hire senior engineer let throw headcount target window replace total output target maybe even go far total salary dollar target rather headcount besides challenge convincing cfo probably misaligns incentive even headcount target usually come salary band agree beforehand another weird constraint think expensive engineer higher roi cap cost thus productivity thing struggling understand turn formalize simple model rational hire two engineer instead engineer even total cost higher feature factory task overhead one common argument hiring cheaper engineering talent ton task straightforward unsexy boring maybe entrylevel engineer nt mind tweaking wordpress theme day senior engineer need challenge extreme end spectrum type company often derided feature factory suspect people imagine sweat shop super inexperienced engineer basically updating form html adding tracking pixel pretty unconvinced argument senior person find opportunity automate reduce repetitive part paying however slight variant idea think actually justify hiring le experienced engineer task overhead let consider toy model let say two engineer one called norm normal engineer one twanda engineer let say work company norm spends time actually working rest time lost task overhead maybe bunch bookkeeping going jira creating github pull request waiting ci etc overhead happen every task much productive twanda compared norm twanda generates much value general engineer spends c time task overhead item k x engineer output factor ck note spending time meeting nt impact hypothetical company time spent meeting faster engineer would still get work done time nt spent meeting model talking taskrelated overhead see toy model lot productivity gain higheroutput engineer diminished environment high task overhead really benefit lot productive people minimize amount task overhead costbenefit analysis high output engineer bunch assumption let u calculate output per cost k x engineer know output factor ck cost k output per cost frac ck k given value c solve optimal value k take derivative respect k set zero lazy person plugged wolfram alpha optimal value k function c turn k frac frac c let plot optimal value k respect c plot logscale shape come nicely beautiful let unpack picking point chart extreme case overhead best value money hire engineer overhead best value money hire engineer overhead best value money hire engineer overhead best value money hire engineer overhead best value money hire engineer extreme case overhead best value money hire engineer getting overhead work getting value tech team talked lot difference engineer term productivity v cost get value good news really two thing boil centralized recruiting process consistent high bar reduce task overhead minimum nt thing point trying hire super senior people particular probably better hiring average engineer xavier amatriain wrote blog post sort similar conclusion nt expect cherrypick element netflix culture drop startup might start development process hiring process asked wrote blog post company pay top dollar engineer nt probably would said company super tech focused truly get value really expensive engineer whereas company collection script using offtheshelf framework expensive engineer would nt make huge difference still think right think exact causality model posited post example google known paying much type challenge engineer work independently long time lower amortized task overhead mean get value expensive productive engineer company large quantity small project thus large task overhead meaning rationally nt pay top market definitely strike kind obvious hindsight maybe feel least know math back tagged startup hiring management math
84,Lobsters,scaling,Scaling and architecture,Work Is Work,https://codahale.com/work-is-work/,work work,work work anxious uncertain innovation token dogmatic slumber emic supervenience corporate america next top model organization work incredibly complex dynamic distributed parallel process psychohistory simple dynamic system ceiling low work capacity organization scale linearly new member added amdahl law ceaseless pursuit force multiplier possible route superlinear productivity improvement organization grows floor lava contention cost grow superlinearly new member added kingman formula staffing highly sequential effort entirely parallel lead catastrophe hell people coherence cost grow quadratically new member added total time spent communicating grow quadratically work capacity organization grows linearly massive business cost responsibility assignment matrix work organization done principle beyond space time keep work parallel group small resource local must combined arm doctrine prioritize development force multiplier possible factor work product independent module grow slowly optimize scale organizational effort across portfolio synergistic product software development schedule shorted flying dutchman blistering number failed service keep responsibility assignment matrix small sparse local prioritize asynchronous information distribution synchronous failure demand happens inside boundary important,work work return diminish jan every time written spoken organizational design regretted something staking position manages prove wrong year later long think got strap fuck point time every organization realizes slowing feature take longer ship folk spend time meeting everyone get panicky estimation planning internalized essentialist bullshit b player hiring c player maybe get nervous bar hiring anxious uncertain might get religion agile scrum whatever prone modernist flight fancy might decide spend innovation token trying disrupt year old industry getting human work together type eight hit gate chalk price success approach rarely yield result dogmatic slumber explanation organizational success failure crap emic within limited concept narrative exist within organization may structurally serve selfnarratives foster sense ingroup identity purpose come epistemological equivalent gravity well explanatory power outside organization usually terrible take etic outside see emic explanation organization success failure readily available counterfactuals organization agile flat organization code review monorepos open office fancy type system etc actually causal factor purported many organization adopt practice without success successful organization lack practice tell difference cum hoc ergo prompter hoc justso story actual causal factor importantly determine supervenience set factor organizational performance particular context across possible organization necessary priori truth organizational performance happens corporate america next top model squint hard enough organization work incredibly complex dynamic distributed parallel process good tool understanding rough outline work dating back least manabrea comment babbage regarding analytical machine pretend developed psychohistory given difficult predict behavior even simple dynamic system fully predictive model organizational success certainly seems impossible like kant attempting derive necessary precondition subjective experience critique pure reason sketch boundary organization capable dynamic grows way priori knowledge ten pound weighs five pound informs much shit expect fit bag modeling organization parallel process inform way design happens inside boundary matter execution effort happens outside boundary impossible ceiling low work capacity organization scale linearly new member added new member organization add constant number possible work hour total possible work hour company existing employee amdahl law state given fixed task parallel solution utilizing nnn processor run faster sequential solution factor nnn parallel resource added total time spent parallelizable portion task amortizes zero contrast total time spent sequential portion task never drop floor value true group people trying write software group cpu trying model behavior star galaxy intuition tell u larger organization exhibit superlinear behavior literally case hiring variable equation therefore hope superlinear productivity lie changing task executed thankfully work capacity productivity organization hire employee work productivity improvement must constant priority internal tooling training service must developed fielded ensure member able work problem continuously increasing impact ceaseless pursuit force multiplier possible route superlinear productivity improvement organization grows finally must emphasized linear bound work capacity ceiling floor one better linear one certainly worse many factor act drag work capacity organizationwide improvement productivity critical mitigating floor lava contention cost grow superlinearly new member added parallel solution task rarely perfectly concurrent indeed task rightfully called embarrassingly parallel often require sequential critical section line cpu people waiting enter critical section modeled queue allows u use queueing theory understand queue cycle time change queue size grows model line sequential section queue say without making assertion arrival process service time distribution assuming single queue server ie one cpu person hold lock arrive kingman formula mean wait time e wq mathbb e wq approx left frac rho right left frac right taue  notably wait time queue increase nonlinearly respect rho utilization quadratically respect coefficient variation arrival coefficient variation service time quantified form intuition queue either empty overflowing nonlinearity give u pause increasing number people contending shared resource thing increasing rho contention resource unmanaged organizational growth result catastrophic increase wait time point adding new member cause organization overall productivity decrease instead increase increase wait time due contention greater increase work capacity organizational version latency spike see server become overloaded shared resource necessarily physical thing like bathroom printer digital like file source code repository ticket bug tracker organizational like code review work assignment writing highlyconcurrent application building highperforming organization requires careful continuous search shared resource developing explicit strategy mitigating impact performance commonly applied rarely successful strategy using external consultant agency staff endrun around contention internal resource consultant indeed move quickly lowcontention environment integrating work product back contended resource often effect ballooning variation service time long critical section held produce quadratic spike wait time increase utilization turn produce superlinear spike wait time queueing theory harsh mistress successful strategy reducing contention include increasing number instance shared resource eg adding bathroom add employee developing stateless heuristic coordinating access shared resource eg grouping employee team heavily layered application distance designing organization work done greater risk unmanaged point contention topdown organizational method lead subdivision seem like parallel effort listed slide actuality highly interdependent interlocking staffing highly sequential effort entirely parallel lead catastrophe hell people coherence cost grow quadratically new member added working complex task using parallel resource group people requires communication group dyad group group group nnn people possible dyad pointtopoint communication ie talking modeled activation subset dyad organization chattier others communication essential sharing information coordination action free communication take time relative percentage people need talk get something done stay constant organization grows ie x x x dyad total time spent communicating grow quadratically work capacity organization grows linearly consider group meeting batching strategy reduce number entity involved pointtopoint communication effectiveness strategy depends heavily relative overlap group group structure degree group overlap essentially factor percentage dyad required communication group size bounded growth coherence cost reduced constant factor still grow quadratically may tempting try punt coherence ride dirty even subtle form incoherence massive business cost scalable strategy containing coherence cost limit number people individual need talk order job constant factor term organizational design mean limiting type number consulted constituency organization process additional person group responsibility assignment matrix geometrically increase area matrix additional responsibility assignment matrix geometrically increase cost organizational coherence also worth noting pairwise communication need formal planned even wellknown order cost neither employee handbook calendar accurate depiction work organization done unless organization staffed zombie member organization constantly subverting standard operating procedure order get actual work done even ant improvise accurate accounting hidden cost developed via honest blameless continuous endtoend analysis work happening principle beyond space time keep work parallel group small resource local presented set problem grow superlinearly intractable nnn increase best bet keep nnn small organization intent increase value delivery hiring people work effort must independent possible leader develop practice process ensure work effort strategy consider parallel actually parallel shared resource continuously managed contention possible resource group need colocated group eg work involves lot design staff designer group combined arm doctrine soldier prioritize development force multiplier organization largely working type problem previous year cause concern team dedicated internal tooling staffed given explicit direction building tool optimizing process help increase coworkers productivity percentage organization dedicated improving organization work begin fall ask hit global local maximum go long highleverage tool stay grounded whether actually help possible factor work product independent module grow slowly optimize work codebase document factored independent module key word independent slicing shit hundred microservices help everyone need change ten get anything done problem particularly amenable partition also problem benefit much additional worker problem fixed point look way optimizing sequential portion work know throwing body problem produce clusterfuck scale organizational effort across portfolio synergistic product smart business start single product go long product hypothesis put egg single basket swing fence lucky enough get traction double point got several battalion worth people milling around trying figure owns turboencabulator ui whether new marzelvanes fully antigravic big fall marketing push avoid organization leader keep development product portfolio explicit goal feature product idea complimentary organization overall business strategy naturally coexist main product developed separate product independent team evidence software development schedule shorted easy pick single product week two product week successful new product incrementally integrated existing product make sense tooling library framework developed force multiplier team reduce timetomarket new product carrying cost existing product unsuccessful new product gracefully removed market dramatically lower cost feature similar complexity removing feature product involves contention coherence cost adding feature rare feature present carrying cost greater cost removal thus prevalence flying dutchman feature concrete example virtue product portfolio imagine amazon web service single product staffed hundred thousand doomed soul fronted ui series button allowing provision operate virtual machine database data lake robotics application augmented reality apps iot dinguses creature would implode weight instead amazon web service portfolio synergistic product independent set feature developed operated independent set employee need provided another aws product eg storing virtual machine image sending metric cloudwatch crossproduct integration introduced product structure enables highly concurrent organizational structure enables amazon field blistering number new product every year continuing support develop existing product failed service sunsetted drawn without disrupting rest organization keep responsibility assignment matrix small sparse local organization matures adhoc role often developed full team specialization often critical building internal economy scale formalization new constituency kept check column responsibility assignment matrix expands possible set required interaction geometrically assignment matrix coordination point requiring waiting matrix indicates hightouch relationship two group eg group engineer working feature lawyer trying ensure legal compliance feature effort made reduce cost interaction colocating member eg embed lawyer engineer prioritize asynchronous information distribution synchronous significant source failure demand meeting status update desire organizational leader keep abreast situational awareness indeed important trying maintain calling meeting messaging people slack catching people hallway significant systemic drag organizational productivity better model staying informed development organization scale group publish status update part regular cadence work leader asynchronously read update need arise initiate additional synchronous conversation ask question provide feedback etc synchronous meeting reserved lowlatency collaboration complex issue likewise collaboration reserved synchronous meeting happens inside boundary important know boundary organizational performance dynamic excuse u using empathy build humane organization company group people compensated spend finite lifetime partner child pet super weird hobby deserve member organization honor time ensuring work value meaning mathematical model guide u goal thanks various fiascans reviewing post mistake article mine
85,Lobsters,scaling,Scaling and architecture,System design coherence,https://thomasvilhena.com/2019/11/system-design-coherence,system design coherence,create follow codebase convention implement clear software architecture monolithic architecture microservices architecture logical architecture manage dependency graph database longterm cost shortterm problem involve team system design decision shared knowledge environment allin rule shared knowledge,six year far developing managing application taught lesson one value pursuing system design coherence collective rather individual responsibility requiring entire development team commitment coherence defined quality logical consistent forming unified whole point view directly relates system design handled logical system design decision justified following clear line thought consistent system design decision compatible agreement current state unified whole system component fit together seamlessly working alongside listed five major practical guideline manage system design coherence software project create follow codebase convention one basic yet beneficial measure adopt improve code quality deal source code file written organized within codebase developer spend time reading writing code hence extremely important define enforce coding convention front product development life cycle targeting improved code readability personally adopt combination organizational clean coding convention segment filesdirectoriesnamespaces domain avoid multiple language one source file classinterface name noun noun phrase function name say avoid many argument function avoid function many line replace magic number named constant comment intuitive code discard dead code declare instance variable put brace tab v space list go result following convention source code uniform throughout codebase reducing cognitive effort searching reading code file implement clear software architecture definition software architecture still topic debate general understanding deal software developed term physical logical structure codebase software project follow clear architectural style whatever may deteriorates gradually new unstructured code added becoming harder modify hence importance putting hour design conservation adequate software architecture unfortunately magic architecture fit use case need take account several factor project choosing right path follow provide couple example monolithic architecture standard decade ago microservices architecture gained traction like several benefit drawback name pro shared component monolith share single code base infrastructure business component reused across application reducing development time performance code execution flow usually constrained single process making faster simpler compared distributed code execution con tight coupling code change shared component potentially affect whole system coordinated meticulously scalability scale component separately due interdependency whole application figure monolith software team working product deal complex data model need processing operation fast performant integrated may prefer go monolithic application hand microservices architecture address many situation monolith fail great fit distributed large scale web application pro decoupled application remain mostly unaffected failure single module also code change one microservice wont impact others providing flexibility scalability different microservices scale different rate independently con devops deploying maintaining microservices complex requiring coordination among multiple service testing effectively test single microservice testing distributed operation involving multiple microservices challenging figure microservices architecture architectural pattern concerned physical disposition application deployed logical structure also important define clear logical architecture guide developer structure code everyone team understands component talk responsibility segregated module manage dependency code execution flow look like additional language framework tool introduce system come additional development operational cost cost come different form illustrated following example member development team highly experienced nginx python postgresql web application team fluent stack development pipeline tidy new feature delivered frequently one day developer decides implement new strategic feature using different stack say apache java mysql also highly experienced colleague whenever developer busy colleague implement feature using different stack carefully since quite familiar yet programming language feature web server database mode operation etc original stack thus development time increase b assigned managing production environment application facing considerable growth rate goal deliver sla avaiability break downtime per year gather team evaluate technology plan infrastructure required support growth rate health check operational metric failure recovery autoscaling continuous integration security update plan implemented start fine tuning production environment dealing unforeseen event issue much effort production environment stable way deliver sla discover different tech stack introduced need deployed need reevaluate infrastructure also stack compatible current hosting environment potentially incur additional operational expense two illustrative situation showing impact adopting additional technology development productivity infrastructure complexity operational expense course different technology bring different possibility use limited set technology life developer would much harder instance scenario graph database outperforms relational database immensely scenario choice easy since benefit outweighs cost point always evaluate longterm cost technological decision making solve shortterm problem right relates system design coherence well believe system designed avoid redundant technology take current stack stable production environment whose team carefully evaluates structural change able sustain development productivity long run system clearly following definition coherence involve team system design decision stated beginning article system design collective shared responsibility individual local action potential affect system globally essential development team page regarding codebase convention employed architecture technology stack effective way build shared knowledge environment involve team system design decision benefit plenty individual feel valued part team important decision challenged entire team made system design strength weakness clearly understood everyone creates sense collective accountability trust mean every developer team equal decision power senior role certainly influence decision making junior role vital everyone opportunity give opinion participate le experienced developer definitely grow proceeding time team must aware productivity cost arise time needlessly spent arguing minor trivial decision optimize speed focus moving forward delivering result rather overcaring detail allin rule effort refactor system design conducted completion allin rather partially concluded great risk eroding system design developer feel free apply different coding style architectural pattern locally whenever see fit long end disconnected sometimes conflicting system design preserving system design also preserving validity team shared knowledge system extremely valuable development make several assumption behavior system based shared knowledge start lose validity unexpected issue start occurring developer become justifiably le confident system design implement feature carefully losing productivity challenge open improve system design knowing exceptionally expensive conduct large system refactor completion approach used similar situation isolate refactored service behind integration interface result two independent system design seamlessly working alongside rather mixed together figure integrated design five guideline served well past year helping keep productivity high optimize resource deliver standard mindset actual process like mindset constantly challenged subject improvement
86,Lobsters,scaling,Scaling and architecture,Mathematical diseases in climate models and how to cure them,https://www.youtube.com/watch?v=WGe-gYRSM7c,mathematical disease climate model cure,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature mathematical disease climate model cure youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature mathematical disease climate model cure youtube
88,Lobsters,scaling,Scaling and architecture,What's wrong with Scikit-Learn,https://www.neuraxio.com/en/blog/scikit-learn/2020/01/03/what-is-wrong-with-scikit-learn.html,wrong scikitlearn,pre deep learning era neuraxle problem top core developer scikitlearn inability reasonably automatic machine learning automl inability reasonably deep learning pipeline scikitlearn pipeline lifecycle method ready production complex pipeline metaestimators decorator design pattern solution found scikitlearn problem conclusion neat pipeline abstraction subscribe neuraxio update dotlayer layer,scikitlearn pipe filter design pattern simply beautiful use deep learning automl complex productionlevel pipeline scikitlearn first release pre deep learning era one known adopted machine learning library still growing top us pipe filter design pattern software architectural style make scikitlearn fabulous added fact provides algorithm ready use however massive issue come following able already automatic machine learning automl deep learning pipeline complex machine learning pipeline let first clarify missing exactly let see solved problem building new design pattern based one scikitlearn already us tl dr could thing work allow u list pipe filter design pattern architectural style particular scikitlearn api must redesigned include broader functionality allowing definition hyperparameter space allowing comprehensive object lifecycle data flow functionality step pipeline coded solution neuraxle get wrong used love scikitlearn still love use nice status quo offer useful feature ability define pipeline panoply premade machine learning algorithm however serious problem see deep learning thing problem problem highlighted top core developer scikitlearn scipy conference call new library solve problem instead within scikitlearn video source top core developer scikitlearn andreas c mller scipy conference inability reasonably automatic machine learning automl scikitlearn hyperparameters search space model awkwardly defined think builtin hyperparameter space automl algorithm scikitlearn despite pipeline step hyperparameters hyperparameter distribution really good gethyperparamsspace well getparams scikitlearn instance lack ability define distribution hyperparameters root much limitation scikitlearn regard automl technical limitation regarding constructor argument pipeline step nested pipeline inability reasonably deep learning pipeline think following feature trainonly behavior minibatching partial fit repeating epoch train shuffling data oversampling undersampling data augmentation adding noise data curriculum learning online learning testonly behavior disabling regularization technique freezing parameter mutating behavior multiple changing input placeholder multiple changing output head multitask learning unsupervised pretraining supervised learning finetuning evaluation strategy work minibatching aforementioned thing scikitlearn almost none hardly allows api strict built consideration mind instance mostly lacking original scikitlearn pipeline yet thing required deep learning algorithm trained thereafter deployed plus scikitlearn lack thing proper serialization also lack compatibility deep learning framework ie tensorflow kera pytorch poutyne also lack provide lifecycle method manage resource gpu memory allocation think lifecycle method method object init fit transform instance picture adding also setup teardown mutate introspect save load manage event life algorithm object pipeline also want pipeline step able manipulate label instance case autoregressive autoencoder x data extracted data fitting phase case applying onehot encoder label feed integer ready production complex pipeline parallelism serialization convoluted scikitlearn hard say broken step pipeline import library coded c object always serializable work usual way saving scikitlearn using joblib serialization library also build pipeline meant run production thing want add top previous one think nested pipeline funky multimodal data parallelism scaling multiple core parallelism scaling multiple machine cloud computing shortly put hard code metaestimators using scikitlearn base class metaestimators algorithm wrap algorithm pipeline change behavior wrapped algorithm ex decorator design pattern example metaestimators randomsearch hold another step optimize randomsearch also step pipeline hold several step pipeline also step used inside pipeline nested pipeline foreachdatainputs hold another step foreachdatainputs also step replacement one change dimensionality data adapting step data wrapping expanddim hold another step expanddim also step inversely foreachdatainputs augments dimensionality instead lowering metaestimators crucial advanced feature instance paralleltransform step could wrap step dispatch computation across different thread clusteringwrapper could dispatch computation step wrap different worker computer within pipeline upon receiving batch data clusteringwrapper would work first sending step worker already sent subset data worker pipeline metaestimator contains many different step many metaestimators also name meta step synonym solution found scikitlearn problem sure scikitlearn convenient wellbuilt however need refresh solution neuraxle make scikitlearn fresh useable within modern computing project conclusion unfortunately machine learning pipeline framework scikitlearn fail combining deep learning algorithm within neat pipeline abstraction allowing clean code automatic machine learning parallelism cluster computing deployment production scikitlearn nice pipeline abstraction already lack feature automl deep learning pipeline complex pipeline deploying production fortunately found design pattern solution allows technique named work together within pipeline making easy coder bringing concept recent frontend framework eg component lifecycle machine learning pipeline right abstraction allowing possibility better memory management serialization mutating dynamic pipeline also break past scikitlearn python parallelism limitation neat trick allowing straightforward parallelization serialization pipeline deployment production glad found clean way solve widespread problem related machine learning pipeline hope solution problem prolific many machine learning project well project actually deployed production liked reading subscribe neuraxio update kept loop also thanks dotlayer layer organization blog committee administrator generous peerreview present article
89,Lobsters,scaling,Scaling and architecture,Microservices and Biological Systems,https://battlepenguin.com/tech/microservices-and-biological-systems,microservices biological system,ground caveat context conclusion,several researcher yale attempted look biological system versus computer software design would expected biological system evolved million year much complex considerable amount redundancy lack direct topdown control architecture found software like linux kernel comparison entirely fair considering complexity biology fun thought experiment microservices newemergent phenomenon software engineering world many way microservice architecture evolve environment much closer biological model carefully architected topdown approach monolithic software introduced concept service orientated architecture general concept around soa large company lot team data department source record certain type data would also provide service access modify data initially took form shared library eventually evolved network service often web service using soap transport least idea concept reality often many different team would write similar service part database direct access lot data store team would open service others service would require multiple version maintained concurrently order transition one version another lead system complex coupled odd way probably could lead organization current era microservices graph service growth note end thousand even though said thousand reliable way sorta tracking time like bizarre somehow really hard get exact number service running production one time changing rapidly different team building different thing cranking new service every week service going away honestly tracking total count like sort weird anomaly sort thing people care matt ranney goto speed software engineer write deploy service grown significantly last couple year still possible quickly write lot bad software service without unit test committed without code review riddled security issue contribute technical debt however also become easier develop well written software good test coverage continuous integration pipeline mid large sized company cause rapid growth many interdependent service tooling pipeline party integration environment microservices start thrive much akin biological process traditional waterfall approach requires component laid linear fashion often rigorous design documentation component completed tested dependent task begin type process essential ensuring quality minimizing delay back era mainframe computer small mistake easy undo could lead delay ten thousand man hour million dollar even today hardware still need meet strict requirement pentium fdiv bug affected floating point math lead recall estimated cost million usd recent year intel hit numerous security concern sidechannel attack including spectre meltdown mitigating attack software lead considerable performance issue workload world physical engineering adhering strict approach heavily testing new approach essential oversight potentially disastrous bridge collapse florida international universitysweetwater ongoing grounding boeing lot software today written business case noncritical system software outage failure could lead loss money convenience people die reach instagram youtube day although people shut become incredibly annoying around type environment microservices tend incubate grow thrive ground microservices tend built around thing product built different speed throughout company uncommon service handle multiple version given message schema order ready time team transition political motivation pet project thing microservices usually work around also used introduce new technology without interfering current workflow sometimes service provide level redundancy least immutability team choose never change service deployed instead simply deploying new version telling everyone get old one bad service load two million record incorrectly team often fix service requeue message order backfill data tradeoff might choose build new service instead fixing something broken maybe seem like cost first maybe seems like feature wade old code risk breaking point cost always building around problem never cleaning old problem start factor another way say might trade complexity politics instead maybe awkward conversation human being really easy avoid write weird property matt ranney goto microservices built resilient environment filled service everything remote procedure call complex system built well resiliency handling bad data monitoring must built service order entirety business process reliable situation microservices evolved around system developed quickly tend grow breaking larger monolithic application core component never start microservice company newly hired principal engineer come microservice shop may tend immediately build small module individual repository empty stub everywhere terrible idea always start monolith ensure well tested well designed developed several iteration core developer trying start microservices lead change needing merged dependent project order increment version number feature available downstream project turn fragile mess empty stub missing documentation inconsistent project instead strong independent yet potentially redundant series system come natural evolution software development caveat want make disclaimer equivocating complexity microservices actual biological system simply using biology measure used aforementioned pnas paper made comparison cell regulation monolithic linux kernel article artificial intelligence make argument neuron modeled simple circuit closedform equation following image show gene regulatory network diagram e coli left literal poop microbe compare partial human gene regulatory network right important understanding variability cancer regulatory network e coli left compared subset regulatory network human cell right actual biological system insanely complex decade barely scratched surface understanding gene regulatory system context comparison showing simply make fun hopefully useful analogy conclusion microservices done right rather system evolve organization team really good well thought service large number unit integration test yet still ultimate product often weird evolutionary process tends muddled technical debt company policy legal requirement politics people try start microservice model asking world pain hurt good microservices come using foundation well written monolith template splitting creating smaller component build city molecule several layer abstraction place build brick structure building critical software like building bridge engineer attempt think component integration entirety mistake software pacemaker safety system vehicle bug literally never recovered contrast biological organism tend static unchanging component preform discrete set task although susceptible random mutation individual part organism vying best fitness given environment microservices akin biological evolution often resilient change inconsistency built handle interference outside world like biological organism also complex susceptible environmental change disease outside factor cause fail like degenerative disease cancer may failure propagate slowly silently way incredibly difficult track diagnose fix
90,Lobsters,scaling,Scaling and architecture,Strategies for Working with Message Queues,https://www.doxsey.net/blog/strategies-for-working-with-message-queues,strategy working message queue,strategy working message queue apache kafka model overview code pipelining ford installs first moving assembly line redis errgroup package batching vpaddq duff device btrees postgres mset nagle algorithm process stage parallelism sliding window protocol partitioning erie hashing modular arithmetic consistent hashing conclusion joel spolsky google slicer,strategy working message queue dec caleb doxsey message queue like apache kafka common component distributed system blog post look several different strategy improving performance working message queue model overview kafka consists topic one partition partition ordered immutable sequence record continually appended structured commit log record partition assigned sequential id number called offset uniquely identifies record within partition structured commit log consumer follows basic step consumer assigned particular topicpartition either manually automatically via consumer group previous offset read consumer begin last left message consumed kafka message processed way processed message offset committed back kafka type message queue like amqp similar flow message consumed processed acknowledged generally rely idempotent message processing ability process message twice ill effect err side committing certain done need give u durability guarantee every message processed even consumer process crash code code example post written go strategy work programming language go use officially supported confluentkafkago library package main import log func consumer server string error c err kafkanewconsumer kafkaconfigmap bootstrapservers server groupid example autooffsetreset earliest enableautocommit false err nil return fmterrorf error creating kafka consumer w err defer cclose csubscribetopics string exampletopic nil get msg err creadmessage err nil logprintln consumer error err continue process logprintln process msg commit ccommitmessage msg example consumes message process commits offset throughput limited quickly single consumer fetch process message commit offset nmessages tget tprocess tcommit improve pipelining henry ford announced goal ford motor company create motor car great multitude time automobile expensive custommade machine improve flow work needed arranged one task finished another began minimum time spent setup ford inspired meatpacking house chicago grain mill conveyor belt seen brought work worker spent le time moving divided labor breaking assembly model distinct step worker trained one step ford called frederick taylor creator scientific management time motion study determine exact speed work proceed exact motion worker use accomplish task ford installs first moving assembly line first performance improvement make utilize pipelining pipeline consists step stage output one step connected input next step single pipeline process multiple message often run pipeline step parallel step could handling message step handling message pipeline found many place computer instruction pipeline cpu example fetching next instruction done parallel execution current instruction unix pipe process output one command sent input another echo hello world cowsay hello world oo w protocol pipelining multiple request sent simultaneously example redis typically issue command read response also issue multiple command without waiting result first printf pingrnpingrnpingrn sleep nc localhost pong pong pong pipeline easy way u add parallelism code might also added benefit making code easier understand breaking complex workflow easier understand stage go create pipeline using channel func consumerpipeline consumer kafkaconsumer error gettoprocess make chan kafkamessage processtocommit make chan kafkamessage var eg errgroupgroup eggo func error get msg err consumerreadmessage err nil logprintln consumer error err continue gettoprocess msg return nil eggo func error process msg range gettoprocess logprintln process msg processtocommit msg return nil eggo func error commit msg range processtocommit err consumercommitmessage msg err nil return err return nil return egwait break flow stage put channel one run stage goroutine using errgroup package easier management tip need cancellation use context withcontext method package using approach message queue begin get next message even still processing current message general time nmessages max tget tprocess tcommit increasing parallelism also increase cpu usage though probably increase since time spent processing likely step time spent reading message committing offset involves asynchronous network io save u cpu cycle regardless often cpu core sitting idle server overall good thing batching ti season cooky one interesting facet baking cooky baking single cookie take roughly amount time baking dozen cooky even though double amount flour sugar nt take much longer mix crack egg cooking time nt change much either goal create cooky batch cooky always beat batch cooky let use intuition batching cooking apply programming batching refers combining multiple message together single operation like pipelining batching found place computer cpu support simd singleinstruction multipledata operation example vpaddq add integer single operation duff device unrolls loop typical loop consists conditional check followed sort operation duff device reduces number conditional check n time time btrees similar data structure store multiple record single node database like postgres support inserting multiple value single statement redis mset nagle algorithm combine small tcp packet together us batching effective multiple place batch number message retrieve kafka librdkafka underlying library using go already support capability enabled default although le dramatic sending multiple message channel go efficient sending single message send channel involves mutex lockunlock cost overhead roughly regardless sent channel benchmark demonstrating available batch offset commits kafka since store single offset per topicpartition consumer committing later offset implicitly commits offset using code one way implement batching commits would like eggo func error commit ticker timenewticker timesecond defer tickerstop type key struct topic string partition offset map key kafkaoffset select commit every second case tickerc len offset tps make kafkatopicpartition len offset k range offset tps append tps kafkatopicpartition topic ktopic partition kpartition offset delete offset k err consumercommitoffsets tps err nil return err case msg processtocommit k key msgtopicpartitiontopic msgtopicpartitionpartition msgtopicpartitionoffset store offset map offset k offset k return nil general batching improve performance consolidating get commits batch size might see tget tprocess tcommit also usually see improvement actual time process batch message well ie number batching eliminates overhead consolidating operation also end improving cache locality cpu far better processing contiguous record record randomly distributed across space time batching smushes message together particularly avoid passing pointer channel stick structsslices also added benefit improving data compression rate unfortunately batching panacea come downside universal optimum batch size modern system complex nearly impossible reason performance characteristic make guess based cache size network protocol overhead end day best approach benchmark discover best batch size change time something need reevaluate periodically batching introduce latency system since stage processing pipeline may wait time seeing enough data batch properly introducing timeout help also reduces benefit batching overbatching cause downstream issue example kafka maximum message size exceeding size result message rejected message broker large message also require large amount memory handle properly batching hurt randomaccess pattern btree example could store record single toplevel node would improve performance intend always read record would change performance characteristic log n n find single record case message queue may end consuming message nt intend use inefficient though always depends intend use data batching limit horizontal scalability hamper recovery time larger message size larger box going need process longer going take process imagine bundle million message together take minute process batch message server crash middle restart beginning smaller batch size would much easier process truth larger batch sizing see diminishing return difference may negligible process stage parallelism introducing parallelism processing stage obvious approach would simply start multiple processing stage look improvement eggo func error process msg range gettoprocess logprintln process msg processtocommit msg return nil unfortunately problem approach guarantee message processed order anymore since acknowledgement kafka done single commit offset really important nt commit offset sure message handled one way solve aggregation fanin step keep track message seen far forward upstream right order go far path reminds solution lower level protocol sliding window protocol tcp protocol surprisingly similar issue deal message tcp given sequence number acknowledged received rather wait acknowledgement tcp send several message receive window window full wait acknowledgement sending data keep message order allows sending parallel prevents u many flight let try implement similar approach problem first replace processing code call function eggo func error process return slidingwindow runtimenumcpu gettoprocess processtocommit func msg kafkamessage error logprintln process msg return nil goal slidingwindow function start multiple processor receiving incoming message gettoprocess processing sending processtocommit want make sure message sent processtocommit order received also process parallel sliding window function look like func slidingwindow size int chan kafkamessage chan kafkamessage process func kafkamessage error error var eg errgroupgroup start processor two channel one incoming one outgoing message toprocess make chan kafkamessage size fromprocess make chan kafkamessage size size request make chan kafkamessage result make chan kafkamessage eggo func error msg range request err process msg err nil return err result msg return nil toprocess request fromprocess result start feeding code eggo func error pointer offset within channel slice var pending completed int going leverage fact nil chans never proceed var nextin nextout chan kafkamessage switch pending completed case empty window wait incoming message nextin case size full window wait completed message nextout fromprocess completed size default otherwise room buffer outstanding request either proceed nextin nextout fromprocess completed size select case msg nextin incoming message process toprocess pending size msg pending case msg nextout completed message send msg completed return egwait code might seem clever half nt doubt bug two since wrote afternoon built top triedandtrue idea problem domain partitioning many community colorado rapidly growing example erie population well dealing sort rapid growth challenging across many dimension infrastructure school housing etc one rather pronounced illustration problem found election time though fair wo nt see colorado since switched entirely mailin ballot notice long line folk hoping vote work standing outside middle school cold dark winter staffed volunteer sort polling location nt always able keep change demographic one strategy utilize add voting machine form multiple line instead single long line often break constituent district last name al mz solution example partitioning breaking large data set smaller piece processed independently applied message queue mean basically make multiple message queue rather single one queue consumer immediately tripled throughput increase n mechanism practically unlimited scalability kafka designed partitioned start topic kafka certain number partition separate offset consumed independently use partitioning kafka simply create topic one partition run multiple consumer process use consumer group name kafka distribute partition amongst consumer automatically u want send message partition many strategy straightforward would use roundrobin random partitioning partition receive equal subset data example partition message partition would receive message crucially there nothing message tell partition end application unpredictability ideal example data might tied particular customer important message customer processed right order case like derive integer message customer id already integer done use hashing use modular arithmetic pick partition send message var msg struct customerid int data byte partition msgcustomerid numberofpartitions using approach customer message end partition thus guaranteeing processed order partitioning based part data also allows u partition downstream data storage layer example may separate sql database partition know sql database query based partition customer id correspond allows u scale downstream system store subset data particularly effective dealing caching since lru cache dynamically adjust based traffic receives separate cache partition cache store le data therefore hit rate improved andor necessary system resource reduced however partitioning significant problem chronic problem partitioned system hotspot example may one customer produce traffic average customer bad apple end partition end uneven distribution reduce effectiveness partitioning changing partitioning fact difficult kafka add partition described made assumption based data stored ie customer stored db changing number partition invalidates assumption customer expected stored db data need moved need come way system running downtime though check consistent hashing actually need implement solution like worse data problem like hard predict front know much data customer going generate ahead time significant change product work assumption make sense anymore basically guaranteed fail come picking partition data partitioning massively increase complexity system reading writing wrong partition catastrophic bad producer writes message wrong partition downstream system assumed get good data blindly write record database cache etc query data nt get anything back speaking experience unwinding gordian knot fact extremely difficult although partitioning help scale system often significant cost instead running single sql database run separate database require hardware backup replica etc often due hotspot size system according worst case lot unused resource find position recommend binpacking resource scheduler ie kubernetesmesos also dramatically increased number thing go wrong aws node might randomly crash percent time say per year tripled chance happening sure better worse sort outage rather ugly consequence outage subset customer since really rhyme reason customer grouped together make strange collection problem failure bad enough able understand precisely affected system broken able communicate effectively almost completely ignored today saas product conclusion partitioning area software development ripe disruption often reach far early design process optimizing code using bigger machine would adequate time exhausted low hanging optimization fruit starting use stupidly big instance problem adding partitioning late stage extremely difficult difficult scrapping starting may easier couple decade ago joel spolsky told u rewriting code scratch single worst strategic mistake software company make suppose good advice might leave scratching head rapidly growing startup often build something whole scale replace year two later big question lie one answer design appropriate one level scale often nt work applied different level scale admitting reality mean change design system get work condition change large scale design change often impossible pull existing code base make sense much churn nevertheless statusquo appears unstable rebuilding everything every year massively inefficient one promising solution problem google slicer slicer highly available lowlatency scalable adaptive sharding service remains decoupled customer binary offer optional assignment consistency feature consequent architecture driven need real application google slicer make easy exploit sharding affinity proven offer diversity benefit object caching write aggregation socket aggregation dozen deployed application similarly designed opensource solution automatic repartitioning would go long way making easier build lowscale system could adapt meet changing load requirement maybe day even see managed service google cloud aws
91,Lobsters,scaling,Scaling and architecture,Modular Monolith: Architectural Drivers,https://www.kamilgrzybek.com/design/modular-monolith-architectural-drivers/,modular monolith architectural driver,modular monolith primer modular monolith architecture enforcement modular monolith integration style introduction first post modularity modular architectural driver architectural driver clean architecture layered architecture event sourcing anemic domain model context king decision made given context decision made one context bring great result another cause devastating failure without critical thinking blog article software architecture developer set common thing really drive influence shape resulting software architecture software architecture continuous choice silver bullet level complexity wiki component interact multiple way part interact multiple way cap theorem service registry service discovery pattern pattern monolith first productivity deployability performance scalability wikipedia scale module need scale failure impact heterogeneous technology heterogeneous technology must running runtime environment summary shape architecture system influenced many factor everything depends context additional resource architectural driver chapter designing software architecture practical approach book humberto cervantes rick kazman software architecture developer book simon brown design book michael keeling collection article monolith microservices architecture named microservices modular monolith ddd github repository related post modular monolith primer,post part article series modular monolith architecture modular monolith primer modular monolith architectural driver modular monolith architecture enforcement modular monolith integration style introduction first post architecture modular monolith focused definition architecture description modularity reminder modular monolith system exactly one deployment unit explicit name monolith system designed modular way modularization mean module must independent autonomous everything necessary provide desired functionality separation business area encapsulated welldefined interfacecontract post would like discus popular opinion architectural driver lead either modular monolith microservices architecture exactly architectural driver architectural driver general say x architecture better say monolith better microservices clean architecture better layered architecture layer betterworse layer rule applies consideration orm v raw sql current state persistence v event sourcing anemic domain model v rich domain model objectoriented design v functional lot choose architectureapproachparadigmtoollibrary unfortunately best context king decision made given context project different result project definition context different implies decision made one context bring great result another cause devastating failure reason using people scompanies approach without critical thinking cause lot pain wasted money finally end project every project different different context however context general concept need something specified put practice architectural driver concept defined michael keeling writes blog article following way architectural driver formally defined set requirement significant influence architecture simon brown book software architecture developer describes architectural driver similarly regardless process follow traditional plandriven v lightweight adaptive set common thing really drive influence shape resulting software architecture architectural driver categorization main category fuctional requirement problem system solve quality attribute set attribute determine quality architecture like maintainability scalability technical constraint technology standard tool limitation team experience business constraint budget hard deadline architectural driver importantly architectural driver connected often focus one cause loss another tradeoff everywhere unfortunately let consider example service calculates important thing functional requirement second quality attribute performance new requirement appears calculation complex take second performance decreased go back second another technology could used time business constraint hard deadline nobody used company yet technical constraint team experience option increase performance move calculation stored procedure decrease maintainability readability quality attribute architectural driver example see software architecture continuous choice one driver another one right solution silver bullet mind let see popular architectural driver attribute discussed consideration modular monolith microservices architecture level complexity beginning let consider one greatest advantage modular monolith compared distributed architecture complexity definition complexity wiki follows complexity characterizes behavior system model whose component interact multiple way follow local rule meaning reasonable higher instruction define various possible interaction term generally used characterize something many part part interact multiple way culminating higher order emergence greater sum part see complexity component interaction modular monolith architecture interaction module simple module located process mean module want interact another module know exact address direct request sure address change request method call network needed target module always available security issue concern complexity modular monolith hand consider distributed system architecture architecture module service located server communicate via network mean service want communicate another must deal following concern need get somehow address target module may changed communication take place via network necessitates use special protocol like http serialization network may unavailable cap theorem secure communication module must ensured course find solution issue example solve addressing issue add service registry implement service discovery pattern however mean adding component algorithm system complexity rapidly increase aware scale problem generated microservices architecture recommend familiarize pattern used solve list large needed monolith architecture complexity distributed system summary architecture modular monolith definitely le complex distributed system high complexity reduces maintainability readability observability need experienced team advanced infrastructure specific organizational culture simplicity key architectural driver consider monolith first productivity team productivity delivering change measured two dimension context entire system single module context whole system matter clear architecture modular monolith le complex le complex easier understand productivity higher point view ease running entire system modular monolith ensures productivity maximum level download code run local machine distributed architecture matter simple despite technology tool like docker kubernetes facilitate process running entire system monolith v distributed hand productivity related development single module case microservice architecture better run entire system test one specific module architecture support team productivity opinion system modular monolith really large project ten hundred module microservices architectural driver development speed system huge better choice modular monolith case system expansion possible transition microservices could right move deployability deployability software system ease taken development production however must consider situation deployment entire system single module context entire system easier deploy one application several application course one application easier deploy seems modular monolith better option deployment modular monolith hand modular monolith always deploy whole system deploy one particular module one important disadvantage architecture deployment autonomy deployment process must coordinated may difficult deployability distribiuted system summary mind deployment whole system care autonomy deployment point behind modular monolith otherwise consider distributed architecture performance performance fast something usually term response time duration processing latency assuming scenario request processed sequential manner monolith architecture always efficient distributed system module operate process overhead communication distributed system overhead caused communication network serialization deserialization cryptography speed sending packet even real scenario monolith efficient time increase user request data complexity calculation may turn performance decrease come one main driver microservices architecture scalability scalability scalability wikipedia say scalability property system handle growing amount work adding resource system word scalability ability software deal request data best show example let assume one module must handle request initially assumed must increase resource responsible operation module always two way increase node computing power called vertical scaling add new node called horizontal scaling let see look point view monolith microservices architecture scaling seen architecture scaled monolith scaled vertical scaling difference horizontal scaling using approach scale modular monolith whole lead inefficient resource utilization microservices architecture scale module need scale lead better utilization resource main difference instance module must work significant difference hand scale lot maybe better accept le efficient resource utilization stay monolith take advantage good question ask situation failure impact sometimes architectural driver may limiting impact failure let say unstable module crash entire process case modular monolith whole system work one process whole system suddenly stop working availability decrease case microservices architecture risky module moved separate process stopped rest system work properly failure impact increase availability modular monolith increase number node scalability resource utilization highest level compared microservices architecture heterogeneous technology one attribute modular monolith bypassed way inability use heterogeneous technology whole system process mean must running runtime environment mean must written language platform support multiple language example net clr java jvm however use completely separate technology possible heterogeneous technology feature heterogeneous technology decisive switch microservices architecture often company use one technology stack one even think implementation component different technology team competence software license allow hand larger company project often use different technology maximize productivity using tailormade tool solve specific problem common case associated heterogeneous technology maintenance development legacy system legacy system often written old technology often bad way use new technology new servicesystem often created implement new functionality old system delegate request new one thanks development legacy system faster easier find people willing work disadvantage two system instead one whole system becomes distributed con architecture summary post intended describe architectural driver favor modular monolith microservices separate book created topic post wanted describe common discussed architectural driver opinion make clear shape architecture system influenced many factor everything depends context summarizing better worse architecture depends context architectural driver architectural driver categorization functional requirement quality attribute technical constraint business constraint monolith architecture le complex distributed system microservices architecture requires much tool library component team experience infrastructure management beginning monolith implementation productive monolith first approach later migration microservices architecture considered architectural driver migration exists deployment monolith easier support autonomous deployment architecture support scalability microservices way efficient resource utilization monolith better performance microservices need scaling appears depends scaling possibility failure impact greater monolith everything work process risk mitigated duplication cost microservices architecture monolith definition support heterogeneous technology additional resource architectural driver chapter designing software architecture practical approach book humberto cervantes rick kazman software architecture developer book simon brown design book michael keeling collection article monolith microservices architecture named microservices modular monolith ddd github repository related post modular monolith primer
93,Lobsters,scaling,Scaling and architecture,How the U.S. Air Force Deployed Kubernetes and Istio on an F-16 in 45 days,https://thenewstack.io/how-the-u-s-air-force-deployed-kubernetes-and-istio-on-an-f-16-in-45-days/,u air force deployed kubernetes istio day,portworx kubecon san diego department defense enterprise devsecops dod enterprise devsecops initiative department defense jedi cloud contract awarded microsoft last month either amazon web service govcloud microsoft azure open source stack dod scale via,portworx sponsored new stack coverage kubecon cloudnativecon north america hybrid cloud strategy go u military certainly taking unique approach like almost everything else military organization increasingly depend software turning array open source cloud tool like kubernetes istio get job done according presentation delivered nicholas chaillan chief software officer u air force kubecon san diego tool deployed interesting place weapon system even fighter plane yes running kubernetes legacy hardware built jet one point team demonstrate could done chaillan said challenged air force partner get kubernetes running jet day difficult sound team met goal running three concurrent kubernetes cluster said natural followup question course packed presentation following openingday keynote kubecon chaillan explained air force direction rest department defense making big bet container kubernetes istio flexible universal development platform software team across military prevents vendorlock video air force embarked project month ago military software team building software using oldfashioned waterfall process could take year new code make way production chaillan said even update testing even security review relied heavily human labor complete task commercial sector long ago decided automated military really afford fall far behind mainstream come technology need mission rival invest software capability could get edge battlefield security implication glacial process required update military critical application obvious important u reduce attack surface able mitigate threat chaillan said department defense enterprise devsecops chaillan team decided embrace open source software foundation new development platform called dod enterprise devsecops initiative initiative specified combination kubernetes istio knative internally developed specification hardening container strict set security requirement default software development platform across military software team different branch region discretion use tool disposal everything must constructed layer provided air force platform one team several thing team allowed change chaillan said interestingly given hubbub surrounded department defense jedi cloud contract awarded microsoft last month entire stack designed run either amazon web service govcloud microsoft azure chaillan expanded press conference hosted cloud native computing foundation following talk explaining want get locked one thing team chose use kubernetes specifically reason project like istio providing security networking layer dod stack want make sure istio running constantly across entire stack chaillan said course lot challenge along way kubernetes designed disconnected environment military must use many situation involving data ever reach internet chaillan said hinted dod lot suggestion kubernetes maintainer address portion project could help pave way kubernetes used sensitive operating environment used updating using internet connectivity internet getting update directly internet u bring entire stack u jet weapon system disconnected internet design said open source stack dod scale scale dod operates unlike almost commercial operation chaillan train people principle devsecops mention new tool culture shift interesting said press conference seemingly restraint scale reflection dod using new development platform lot mundane unclassified application million people work military flying running kubernetes jet interesting tiny piece rest work lot business system moving cloud native environment moving microservices built right getgo chaillan said entire dod enterprise devsecops initiative stack open source available anyone check chaillan said military agency getting better releasing work open source community said also noting going fork open source software cloud native computing foundation sponsor new stack feature image via pixabay
94,Lobsters,scaling,Scaling and architecture,I moved my sites from Google Kubernetes Engine to Netlify and saved $1000 / year (plus numerous hours of maintenance),https://labs.iamhamy.xyz/posts/i-moved-to-netlify-from-gke-and-saved-60-per-month/,moved site google kubernetes engine netlify saved year plus numerous hour maintenance,google kubernetes engine netlify one big project last year gke migrated site google kubernetes engine digital ocean moving away gke gke overkill site one post hit front page hacker news wolfram alpha calculation gke expensive needed digital ocean pricing calculator previous post comparing ad revenue browserbased crypto mining revenue gke maintenance free side project netlify mine good pricing page moving forward,month moved google cloud hosted kubernetes google kubernetes engine gke netlify migration onto gke one big project last year move netlify big move year wanted take time sit reflect journey decision made resultant outcome gke last year migrated site google kubernetes engine running adhoc machine digital ocean reason biggest one remember wanted structure le dealing individual machine wanted learn hot shit kubernetes wanted experience mainstream cloud provider given goal experience valuable served purpose learn ton prefer codefirst configuration provides configuring individual machine saved lot time lot headache get basic level understand approach scalable performance maintenance standpoint got experience google cloud see one could scale creation thousand user need arise moving away gke post title suggests though experience extremely valuable enough keep gke overkill site think site cool valuable part reason keep building really popular last month one post hit front page hacker news bringing traffic ever gotten visitor one day brought total monthly visitor screenshot spike looked like stats obviously lot traffic sit math real quick visitor month mean per second per minute per hour see wolfram alpha calculation much modern laptop could easily handle year old laptop probably trouble think even new raspberry pi could problem assuming keep overheating point setup good also way overkill actually using see cpu utilization last two week november time period hacker news traffic spike occurred utilization spike despite extra load kubernetes cluster cpu utilization last half november great cluster could support high load without even trying thing like happen wonder much gke expensive needed answer probably play cloud provider pricing using digital ocean pricing calculator basically range move starter tier running standard machine listed recommended minimum running doc know changed positive could run lower tier machine supporting performance constraint sure wanted get running google cloud expense site used approximately host site coming outofpocket due credit discount accrued site visitor mean visitor cost serve bit ridiculous consider average amount money one make per visitor ad calculated previous post comparing ad revenue browserbased crypto mining revenue much yes gke maintenance free final thing really straw broke bantha back kubernetes quite setitandforgetit system learned lot process would still say amateur thing go wrong rarely ability diagnose fix fast independently guesstimate spent somewhere hour troubleshooting problem cluster pod updating pod would die every quarter automatic ssl renewal go recent trial much grand scheme side project love creating project try work every day opportunity cost project work think people would agree building new project exciting troubleshooting certmanager spontaneously combusted past month recommended path purging reinstalling fails cert renewal silently particularly cool new project really excited work instead needed extra oomph could provide extra maintenance big deal cool project work worth realized really need extra work became meaningless inefficient read ripe chopping block netlify moved netlify main reason knew simple set knew worked online chatter personal experience building mine good figured use case would fit squarely within free tier far right able move gke hour including build deploys subdomain routing ssl cert renewal though routing may vary ofc problem fact think site might actually little faster maybe due extra cacheability different site sitting behind reverse proxy single ip idk pricing page little concerned bandwidth contraints another deal time come still le paying time gke list sla available u commoner sure really matter always revisit becomes problem big implicit win moving netlify infra almost totally managed worry infra go engineer handle slight paradigm shift typically like really really like infrastructure big architecture built maintained personal project focus infra project actually huge win focus logically projected output moving forward main takeaway experience okay optimize need work also need implement recurring habit continual optimization stick old optimization past usefulness chose good reason last year began realize decision longer served well considered changing said decision probably saved day since moved gke netlify like said smooth sailing thus far saving month something come netlify particularly bandwidth uptime limit report back likely undertake another move thanks reading always building hamyout
95,Lobsters,scaling,Scaling and architecture,Should you even bother loadtesting,https://abe-winter.github.io/2019/12/15/ltarch.html,even bother loadtesting,care workload passing criterion unitofscale testing conclusion think twice take care continuous load good metric horizontal scaling big chunk bill something fall scale idle hand workload fixture cache realtime order infra hop passing criterion unitofscale testing conclusion think twice take,last four job loadtesting big part job last two job started loadtesting soon landed problem hard loadtest well people agree acceptance criterion fall infinite rabbit hole building tooling little tooling also waste time loadtesting frequently low hanging fruit scalability improvement test often bad comparison prod traffic tested wrong scale test hard run iterate effectively sure scenario testing based mature prod codebases worked believe prod system never optimized run traffic hardware week analysis week targeted refactoring may matter annual cloud bill cost le person worth hiring person lower ton good tech company run box really good margin productivity cost focusing cloud perf nonzero getting box might worth still larger co economics different read care workload passing criterion unitofscale testing conclusion think twice take care loadtesting matter thing true continuous load ie traffic bursty turning new service continuous load learn lot load characteristic running better metric prod good metric already made investment understsanding stack setting metric logging timing span maybe longperiod reporting good metric able prove lt workload similar prod interpreting lt result uphill battle horizontal scaling big chunk bill shall profit man shall gain whole world lose soul something fall scale nonreproable error happens load performance get nonlinearly wonky certain scale point consider lt explain way idle hand loadtesting basically devil work idle hand prerequisite every scaleminded person place busy something else lt need really good bottomline justification ultimately goal either improve product reduce cloud bill make bottomline case one thing realistic project scope impact team running lt fine workload various way verify lt workload equivalent prod various bad thing happen workload rough ratio api call system complex thing matter fixture creating user data fly db dump init db dump date dataset synthetic captured edge case cache cache many layer unrealistic cache hit ratio make lt falsely appear succeed fail realtime order order call per user may matter route x critical setup route user typically hit x first lt capture realistic test even got x ratio right infra global bottleneck lt env prod database machine type network caliber binpacking service machine hop lt use load balancer way prod sourcing traffic internally load balancer infinite resource last time hosted big software aws take time scale factor play may need pull inhouse expert designed system explain happening replaying prod traffic theory way lt super maintainable repeatable pretty careful getting db snapshot frequently gotten workload wrong beginning gradually adding metric measure thing care cache hit ratio example discarding lt result metric band trying lt whole separate env direct test traffic prod cluster slow time zero risk use dark testing ie mirror production traffic experimental backend service technique make hard iterate experiment passing criterion mean lt pas depends frequently api route timing error rate matter may trying get pas first instead repro production failure debug might trying fail ie instead saying yesno handle traffic trying solve system fall given want run bunch traffic pattern bunch code version repeatable automatic loadtest better issue loadtest repeatability past hard split metric test especially metric highly granular traditional metric system frequently time series conducive producing report along arbitrary boundary pull metric per test run rewrite bunch prod alert stats context get binary passfail mark knowing lt system bad state art science may get flummoxed often repair step automatic aspect traffic sourcing backend deployment metric collection automatic mean tooling getting stuff set hard running lt without stuff often waste unitofscale testing another way approach lt test smallest unit system isolate typically single server process single machine repro loadbased error still lot experiment around max density maintain simplicity iteration speed fact testing central bottleneck con also pro obviously bad run global limit like connection count certain networking limit db cpu failure nonhorizontal service pro repeatable way easier design run test suite need integrationtest everything certain point trust glue ever set ci regression test scalability likely single unit scale testing range simultaneous connection still reveal lot weirdness show singleconnectionatatime test unitgration test suite test hit api db one example personal history lockup happened python process exceeded db pool conclusion think twice take scoping lt project really hard state tool pretty bad
96,Lobsters,scaling,Scaling and architecture,Parsing 18 billion lines JSON with Go,https://itnext.io/parsing-18-billion-lines-json-with-go-738be6ee5ed2,parsing billion line json go,parsing billion json line go tjek google bigquery flashback state flux erlang kafka cluster citus postgress cluster enemy within bigquery allow punctuation field name json loading pre sorted meant every single line json kafka needed transformed loaded tomorrow yesterday learning curve go multiwriter someone watch jsoniterator day honour aws sdk go cloud kubernetes docker note jsonl,parsing billion json line goat employer tjek recently decided rebuild event pipeline move google bigquery reduce complexity stack remove service longer maintainablebigquery offer bunch nice tool querying visualizing data would enable number internal team work directly data share customer without make request engineering departmentthe new service would replace old http service easy write came question moving historic data bigqueryto migrate old data backfill entire dataset accumulated last year bigquery story doneflashbackthe platform done described earlier postalso build pipeline platform described herestate fluxthe old event pipeline beast consisted multiple service written erlang backed kafka cluster citus postgress clustera http server would receive event client publish dirty event topica deduplication service consuming dirty event topic publishing new event clean event topica backup service consuming clean event topic storing database service consuming clean event topic inserting postgres clusterthis meant event needed load bigquery either kafka waiting written backupsthe enemy withinone first obstacle ran bigquery allow punctuation field name json loadingthe second problem required data going loaded pre sorted based destination tablesthe backup kafka topic contains different event type mixed otherthis meant every single line json kafka needed transformed loaded yesterdayremember backup service read x number event kafka topic save offset consumed gzip ship current setting event per topic partition partition topic meaning would likely around event clean kafka topic waiting backed upto get event kafka figured would stop backup service stable offset start consuming pas data back json parser used event backup fileslearning curvedownloading gzipped file something content sound bad wentwe time file totalling gzipped json period roughly year back time needed transformedsome year ago something similar done previous colleague moved citus time software used required copy running almost whole week completei wanted doable within reasonable time wait day result bash aws cli jq question needed fast scaleable process multiple file timethe backup file kafka centred designed around event formatthe kafka topic partition would file could contain different event type average event timestamps would range day started writing tool go would run worker download backup file processed parallel sorted output different folder file depending event type backup file originhistorically backup file written interval meaning could week data per file early year opening parallel trying write output different file proved muchio access would spike roof everything would grind stop writing several thousand file time running many workersi figured somehow needed process file concurrently serialising disk writes fewer larger file go easier underlying storage let control resource betteri decided go following file format hive inspired naming datatype deventyyyymmddjsonlgzthis would produce file per day event typei created go package call multiwriter help thisyou feed multiwriter filename ioreader want persisted diskthe multiwriter us lru cache hold file handle recent file written also us semaphore channel prevent x writes timei also changed input order file instead reading numerically sorted would sort file slice based partition belong consume file horizontally across start partition increase worker would theory replaying kafka stream written would also le open file time file would parsing contains largely date compared traversing whole file list vertically internally flow data even processor looked like thissomeone watch meduring first trial run parser took almost hour parse backup remember json expensive decode encode standard json package heavily relies reflectioni seen package promising much faster stock one decided give jsoniterator shot claimed drop replacementit also fastest setting sort field name output save even cpu cyclesday honourrunning backup parser done core machine awsfly baby flyonce file parsed next problem arose uploading would take age cli client upload file uploads file timei wrote function parser would spawn upload worker use aws sdk go upload file time concurrently cutting transfer time mere half hourfrom matter creating transfer job bigquery watch run waitin event parsed rewritten uploaded hoursremoving half hour uploading mean tool transformed million event per minutei hope enjoyed reading little json adventure question feel free comment tjek lookout another dedicated devops engineer love cloud kubernetes linux docker want work flexible working environment checkout position herenotesby json line referring jsonlalso called newlinedelimited json ndjson example name gilbert win straight one pair name alexa win two pair two pair name may win name deloise win three kind
97,Lobsters,scaling,Scaling and architecture,Scaling and the Friction of Dimension,https://michaelfeathers.silvrback.com/scaling-and-the-friction-of-dimension,scaling friction dimension,galileo galilei two new science squarecube law biological structure galileo scaling law tension n n network scaling tension n n brook law mythical man month fred brook universal scalability law twopizza team dunbar number ronald coase transaction cost theory firm plus minus two,software development know small web application illsuited massive load writing day black friday u year another large retailer site outage losing ten million dollar expected sale retailer definitely small system easy imagine system possible load much smaller system could simpler everything would ok past decade industry gone number cycle innovation aimed directly problem scale system many approach worth asking problem thing small work scaled immediate answer physic code run physical world relationship physical world easy miss fact small much like ignore relativistic effect apple fall tree interaction world inevitable scale memory size latency computational speed synchronization across distributed node many problem solved screenful code n grows large alter algorithm packaging deal scaling issue imposed atom photon electron nutshell scaling problem software go deeper one important insight scaling came galileo galilei century house arrest end life wrote book called two new science contained following observation sometimes called galileo scaling law squarecube law surface small solid comparatively greater large one surface go like square linear dimension volume go like cube simple statement profound world physical object build skyscraper using material support ratio would use building cottage would collapse volume scale faster surface area structure must different true way biological structure cheap scifi movie show u ant size elephant terrorizing city ant actually size elephant would collapse become puddle goo material ant exoskeleton simply strong enough support weight enclosed volume scale software well think deal problem software development one dimension physical system volume grows faster surface area networked system tendency number edge grow faster number node graph n node number edge tends toward becomes connected galileo scaling law tension network scaling tension n see tension software development think two obvious place one tension team size implied brook law tension dependency architecture let look brook law first mythical man month fred brook pointed adding people late project make later useful observation reasoning behind important part brook realized number communication path team grows square number team member time additional person added potentially new relationship form costly worse cost adding new team member grows time add one cost accelerate outside software development see communication cost grow excessively n grows large often quicker reach consensus smaller group people larger group people three people deciding go dinner usually take far le time thirty people deciding universal scalability law social context also smaller team tend better amazon twopizza team model good example advice aligns insight architecture another place brutal effect code know circular dependency bad aside directionality dependency better balance component fewer dependency component system go bad every piece start depend every piece n component start develop connection call bad think important realize sinister sort bad natural way system grow developer saying oh want mung system today tendency toward connection system working component find need something library b get benefit capability b dependency b connection attractive see tendency social system adding person team give team benefit new skill new perspective another set hand add person one person coordinate n grows slow start think splitting team even consciously split team tend divide internally adhoc basis sociology subgroup called clique dunbar number another example tension connection organizational level tension grows number piece system grows piece connect put absolute bound number piece increase cost system grow certain point cost outweigh benefit economics ronald coase transaction cost theory firm software modularize plus minus two working memory area reduced cost n thing fit mind connected way low cost n grows wish function class service bound scope need aware localized work generic term modularity tension modeled function cost edge graph cost zero benefit connection positive n becomes quickly cost grow system break apart federate form local hub creating structure see let go back galileo moment galileo showed relationship n grows structure need change cost relationship seems hold n maybe generalize say friction adjacent dimension generates structure
98,Lobsters,scaling,Scaling and architecture,64 Bits ought to be enough for anybody,https://blog.trailofbits.com/2019/11/27/64-bits-ought-to-be-enough-for-anybody/,bit ought enough anybody,sixtyfour magic number big number crunching number welcome github intel core xeon core naive loop loop vectorized loop good set introductory material topic clang auto vectorization sse slow processor much parallelized vectorized hyperthreading hyperthread physical core xeon platinum enter graphic taken nvidia volta architecture whitepaper cuda code geforce gt nvidia tesla show money conclusion contact u like,quickly use brute force guess number short answer depends resource available going examine problem starting naive approach expand technique involving parallelization discus parallelization cpu level simd instruction via multiple core gpus cloud computing along way touch variety topic microprocessor interesting discovery eg adding core always improvement cloud vcpus equivalent sixtyfour magic number try guess number modern processor operate quantity bit natural size magic number header marker fuzzing common run comparison magic value guessing value seen canonical impossible problem fortunately one use brute force situation better approach like removing comparison using premade input seed dictionary symbolic execution compiletime transformation problem brute force guess easy understand parallelize demonstrating effective parallelization herculean task looking problem also show hardware get faster new set computational problem become possible imagine achieve using full arsenal modern computing power still must consider intractable guess number simply trying possibility long would take much would cost big number number hold distinct grain sand earth cell human body figure smallest largest cell human body amount sand grain earth avogadro number star universe figure getting intuitive feel size comparing large number modern cpu execute billion instruction per second exhausting search space would take little two year thankfully brute force comparison embarrassingly parallel problem work evenly distributed among many processor coax processor one comparison time maybe kind service get lot processor short notice sudden starting look tractable crunching number disclaimer get number want emphatically state fun experiment benchmark attempt made ensure fair applestoapples comparison different processor machine plus code used generate performance number written c lack test may fastest possible version also certain multiple bug fact serious time measurement bug found review delayed post week fix suggestion welcome github measurement reflect average trial two machine tested cloud instance digital ocean google cloud respectively digital ocean high cpu instance report intel r xeon r platinum cpu running ghz google cloud gcp high cpu instance report intel r xeon r cpu running ghz neither selfreported identifier trusted virtualization platform usually lie underlying cpu cloud machine also shared tenant machine may affect cpu throughput physical machine tested macbook pro intel core processor xeon ghz shared server machine software running time measurement core running ghz old machine literally serving footrest cat scratching post revived project thanks gpu pull request wanted notice cpu arm system measurement much wanted include since project already taking long would start scratch learn arm simd instruction naive loop say premature optimization root evil let measure long generic loop take compare value figure list operation per millisecond average run performed subset full range estimate long would take try value device operation per millisecond year completion yr macbook pro core high cpu gcp high cpu xeon figure generic loop comparison value naive approach would take year clearly much long wait optimization order vectorized loop modern processor operate multiple quantity time via simd vector instruction currently intel lead pack like name implies operates vector let u compare eight quantity per iteration want know vector instruction cornell good set introductory material topic vectorization process transforming code run one quantity time scalar code operates multiple quantity simultaneously vector vectorization first optimization thought clang would automatically vectorize code unfortunately auto vectorization meant vectorize code without dependence loop variable like matrix multiplication instead relying compiler used artisanally handcrafted vectorized comparison sse handunrolled loop make maximum use multiple vector execution unit vector instruction continually improving every cpu support support vector others vector even support figure table figure graph compare different vectorization approach available hardware collection device method operation per millisecond year completion yr macbook pro naive macbook pro sse macbook pro core naive core sse high cpu naive high cpu sse high cpu high cpu gcp high cpu naive gcp high cpu sse gcp high cpu gcp high cpu xeon naive xeon sse xeon figure performance vectorized naive version comparison operation hardware platform support vectorization method table included completeness graph data provides easier visual comparison figure graphical representation figure performance different method compare number single core several thing stand data vectorization always help comparing value per iteration always faster even accounting setup time move value vector register four comparison time big improvement two comparison time expected twice fast conversely eight comparison time small improvement four comparison time suspect due littleknown side effect using instruction slow processor much processor power budget permit run full speed make heavy use even improvement would still take year check bit single core much faster go using multiple core parallelized vectorized problem finding needle haystack happens ridiculously parallelizable multiple core deliver linear increase throughput data figure show sometimes performance plateus even core added effect due hyperthreading hyperthreaded processor present physical core two virtual core operating system hyperthread appears independent underlying execution unit shared computationally intensive application hyperthreading significant surprising effect performance hyperthreaded environment cpuintensive workload core used matter almost much amount core especially important cloudbased workload vcpu hyperthread physical core figure operation per hour versus number core tested machine separated different method allocating core macbook pro core result omitted figure show multicore performance using two method allocate worker hyperthreads split core allocates separate physical core share core allocates physical core xeon machine represents difference using split core allocation performance peak amount physical core level using shared core allocation throughput follows step function increasing new physical core may also make inference cloud hardware using data vcpus highcpu gcp instance probably represent physical hyperthreaded core dedicated workload highcpu machine present puzzle effect observed assuming vcpus come real single xeon platinum difference core utilized happen possible explanation first vcpus physical processor xeon operate multiprocessing configuration second processor xeon another chip altogether finally may something wrong thread affinity timing measurement code regardless scaling result show valuable lesson using core always better matter core vcpus use little gain workload allocating worker physical core present machine always measure performance count completeness figure list operation per millisecond estimated year completion utilizing multiple core core using fastest supported singlecore method cloud machine core vcpu roughly equivalent one hyperthreaded core physical machine core one hyperthreaded core device core hyperthreaded operation per millisecond year completion yr xeon gcp high cpu high cpu core macbook pro figure best multiprocessing time observed many core outlier figure xeon best hyperthreaded core core best hyperthreaded core core machine shared handle workload core workload put one core core utilized workload spread core take scheduling time away cpuintensive integer comparison expected using multiple cpu drastically reduced compute time however still slow expect wait two year guess one number enter graphic gpus take different approach computation cpu cpu good multiple concurrent task gpus great simple operation huge volume data manifest main architectural difference highend cpu may come complex core highend gpu come simple core figure figure tesla gpu nvidia come cuda core powerful gpu tested graphic taken nvidia volta architecture whitepaper inadvertently picked problem tailormade gpu optimization brute force comparison easy parallelize involves complex decision entirely compute bound thanks role gpus machine learning cheap offpeak gpu capacity available every large cloud provider figuring use gpu computation took work absolutely worth look throughput figure order magnitude performance gain even though knowledge gpu programming cuda zero started cuda code approximately myfirstcudatutoriallevel optimization device gpus operation per millisecond year completion yr geforce gt nvidia tesla nvidia tesla nvidia tesla figure comparison throughput gpus estimated year search figure show using gpus instead cpu shortens required time year day difference right platform make old video card geforce gt performs par xeon machine using nvidia tesla gpus brute force comparison year day difference cpu gpu computation problem dramatic lone fast highcpu core make sense use cpu adding another gpu always better use resource relying cpu computation show bad cpu problem made handy graph figure throughput comparing number selected cpu gpus specific problem done much faster gpus make sense use cpu gone year naive loop day multiple gpus get hour minute much hardware would much would cost show money established gpus much faster cpu gpu time also much expensive gpu best different gpu family command rather different price using current pricing november see priceperformance gpus good cpu expensive delivers better performance per dollar gpu predecessor figure computeyears needed perhour cost preemptible total cost preemptible cpu gcp vcpu preemptible gpu preemptible gpu preemptible figure price cpu gpu time needed compare value number measured computeyears cost preemptible time google cloud something seemed insurmountable start final cost well within reach comparing number value would take preemptible gpu compute time problem parallelized efficiently total compute time almost directly interchangeable hardware cost availability example gpus look number day gpus would look number hour overall cost conclusion silly experiment show importance understanding problem hardware kind computing resource available average developer seemed like completely crazy idea turned mildly insane trying comparison would cost compute time accomplished hour day also learned thing along way parallelization always easy remarkably effective learning cuda made originally insurmountable problem go impractical tolerable expensive using core always better matter core use underlying hardware leak abstraction cloud cloud computing provides access nearly limitless ondemand hardware put use certainly many problem seem crazy completely reach solved via combination parallelizable algorithm cloud computing corporate credit card always developing way work faster smarter need help next project contact u like like loading
99,Lobsters,scaling,Scaling and architecture,From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers,https://github.com/StanfordSNR/gg,laptop lambda outsourcing everyday job thousand transient functional container,gg research paper laptop lambda outsourcing everyday job thousand transient functional container demo tech video build direction notice docker adept follow instruction usage environment variable redis installing function example,gg research paper laptop lambda outsourcing everyday job thousand transient functional container usenix annual technical conference usenix atc demo building ffmpeg gg aws lambda tech video gg presented usenix annual technical conference usenix atc talk given july build direction notice docker adept follow instruction build gg need following package gcc protobufcompiler libprotobufdev libcryptodev libcapdev libboostdev libssldev autopoint texinfo automake libtool pkgconfig libhiredisdev install dependency ubuntu newer running sudo aptget install protobufcompiler libprotobufdev libcryptodev libcapdev libboostdev libssldev autopoint libhiredisdev texinfo automake libtool pkgconfig build gg run following command fetchsubmodulessh autogensh configure make j nproc sudo make install usage environment variable use gg following environment variable must set ggmodelpath absolute path ggsourcedir srcmodelswrappers ggstorageuri bucketname region bucketregion redis redis username password host port gglambdarole role assigned executed lambda function must awslambdabasicexecutionrole permission awsaccesskeyid awssecretaccesskey aws access key awsregion aws region function installed installing function setting environment variable need install gg function aws lambda cd srcremote make ggfunctions example build mosh using gg first checkout mosh source code git clone http githubcommobileshellmosh make sure dependency building mosh ubuntu run sudo aptget install automake libtool g protobufcompiler libprotobufdev libboostdev libutempterdev libioptyperl libssldev pkgconfig inside mosh directory first need prepare mosh build autogensh configure gg init creates gg directory gg infer make j nproc creates thunk actually compile moshserver aws lambda parallelism execute important job come engine gg force job engine lambda srcfrontendmoshserver
100,Lobsters,scaling,Scaling and architecture,Reclaim unreasonable software,https://lethain.com//reclaim-unreasonable-software/,reclaim unreasonable software,big ball mud productivity reason big ball mud let reason behavior golden age apocalypse rewrite reclaim migration plan serviceorientedarchitecture belief property belief behavior property observed behavior tracked behavior verified behavior dedicated organizational program intended property tracked property asserted property software reclamation,big ball mud published twenty year ago ring true today prominent architecture successful growthstage company nonarchitecture crisp pattern slowly overgrown chaotic tendril quick fix productivity creep towards zero couple year wallowing zero many company turn towards grand migration pattern rewrite software rewrite quite challenging often fall alternative path wrote reason big ball mud stop reasoning software instead move towards empirical experimentation guide software change experimentation lends consistent result rewrite suffers quite slow attempt sketch third path reclaim unreasonable software verified behavior asserted property approach alternative experimentation rewriting well general framework maintaining reasonable software excellent jessica kerr wrote let reason behavior idea well golden age apocalypse software never successfully address domain aspires solve good news least never need maintain software maintenance reserved valuable thing software succeed addressing problem domain becomes golden age software golden age software best software best circumstance team maintaining team designed intimately familiar quirk problem solves mostly problem designed solve within constraint designed satisfy point find extraordinarily elegant extension software allowing solve important new problem part original design solution heralded validation flexible software retrospect realize beginning golden age end elegant solve foretold growing inability reason software time foster complexity decay software team decay folk moving project company point look realize maintaining postapocalyptic software whose evolution become lost technology harrowing operate extend book incantation tend allow keep turning one comfortable making meaningful change rewrite reclaim abstract deity known business value care whether software easy reason care quite bit ability release new feature operate software well enough retain user point slowing velocity become discussion planning strategy meeting one resounding refrain status quo working point many perhaps company choose rewrite software mark lost cause design replacement craft thoughtful migration plan minimize risk sometimes pulling new risk portfolio risk move monolith serviceorientedarchitecture however increasingly confident rewrite inevitable intentionally reclaim software entered postapocalyptic phase evolving belief behavior property belief property postapocalyptic software hard modify nt behave predictable way instead belief work belief might software state change caused http request written atomically process always attempt read state redis reading mysql read writes safe performed primary server unsafe secondary current time greater time write plus current replication lag process state request handled correctly regardless process handle hard write software based belief often wrong end empirically verify solution step step fail bunch say slow undirected form empirical validation make unreasonable software reasonable turning belief either behavior property behavior aspect software validated empircally environment property aspect software validated statically local exercise distinction bit nuanced may find behavior another company property viceversa exploring distinction example every file least one test property enforce build server request complete within two second behavior monitor production environment every execution branch exercised test property verified running coverage test host build host nt require environment exercise read write performed primary database sort thing company would verify either logging verifying behavior log b realtime assertion error bad readafterwrite behavior would make behavior state mutation request handled within single transaction something verify statically forcing mutation constrained interface appropriate sematics make property company might unwilling constrain behavior extent might instead monitor number transaction per request would make behavior let go bit developing taxonomy three interesting gradient behavior observed behavior notice log metric dashboard tracked behavior get informed change operating updated software environment example alert informing increase production latency verified behavior behavior rely consistently true timely mechanism ensures deployment complete three common mechanism ensuring behavior endtoend integration test b fault injection routinely trigger relevant failure mode c load performance testing verifies software completing deployment kind annoying confirm verified behavior require attempted deploy easy design around confident remain true tracked behavior interesting ought easy design around tracked tend degrade time without dedicated organizational program ensuring team fix regression company nt program relevant behavior practice rather hard design around architectural change preceeded period fixing regression tracked behavior intended change made similar gradient applies property intended property ask folk maintain software often propagated design review training best practice document tracked property reported without enforcement mechanism code coverage fall category many company report change nt block deployment asserted property enforced deployment begin generally static analysis code linters unit test find particularly interesting property worst misleading harmful nt conversation someone bemoans team properly observing undocumented intended property best easiest design around provide far best development experience productivity advantage asserted property behavior even verified best obvious comparing large codebase productivity dynamically statically typed programming language statically typed language provide immediate feedback loop several category error would otherwise require full deployment cycle vast majority company zero verified behavior closetozero asserted property particularly challenging never considered powerful software reason conversely longlived company able maintain execution velocity significant number either verified behavior asserted property often company allowed rewrite exactly afterwards got learn reclaim software instead software reclamation coming back postapocalyptic software regain confidence make change list belief need order confident modifying software belief approach evolving belief property ladder get best result evolving belief asserted property followed verified behavior surprising amount value moving tracked behavior property well help understand level effort required begin reasoning software scorecard low enough know going easier reclaim software rewrite high enough maybe rewrite really easier approach nt useful reclaiming software useful way ratchet software reasoning regression first place prevention remains best fix
101,Lobsters,scaling,Scaling and architecture,The Service Mesh: What Every Software Engineer Needs to Know about the World's Most Over-Hyped Technology,https://servicemesh.io,service mesh every software engineer need know world overhyped technology,introduction william morgan linkerd buoyant dive service mesh linkerdproxy service mesh make sense reliability feature observability feature security feature service mesh good idea service mesh give feature critical running modern serverside software way uniform across stack decoupled application code feature critical running modern serverside software uniform across stack decoupled application code service mesh help service mesh solve problem independent business logic difficult implement correctly effective implemented uniformly example feature service mesh observability reliability security service mesh platform nonservice mesh application service mesh make sense ratio microservices developer people talk much service mesh content warning developed pretty public backlash tulip mania humble software engineer care service mesh pure businesslogicimplementin developer role platform role org using kubernetes platform role org using kubernetes microservices platform role org monolith conclusion take second install faq ignore whole service mesh thing go away want use service mesh esb middleware downfall enterprise service bus different api gateway envoy service mesh network service mesh service mesh service mesh help reactive asynchronous message queuebased system service mesh use linkerd think article suck think suck thanks credit log every software engineer know realtime data unifying abstraction many many many many oliver gould,introductionif software engineer working anywhere near backend system term service mesh probably infiltrated consciousness time past year thanks strange confluence event phrase rolling around industry like giant katamari ball glomming successively bigger piece marketing hype showing sign stopping time soonthe service mesh born murky trendinfested water cloud native ecosystem unfortunately mean huge amount service mesh content range lowcalorie fluff use technical basically bullshit real concrete important value service mesh cut noisein guide going attempt provide honest deep engineerfocused guide service mesh going cover also finally going attempt describe think particular technology attracted crazy level hype interesting story itselfwho hi william morgan one creator linkerd first service mesh project project gave birth term service mesh sorry also ceo buoyant startup build cool service mesh stuff like linkerd diveas might imagine biased strong opinion topic said going best leave editorializing minimum except one section people talk much unveil opinion best write guide way objective possible need concrete example primarily rely linkerd difference know mesh implementation call outok good stuff service mesh hype service mesh architecturally pretty straightforward nothing bunch userspace proxy stuck next service talk next mean bit plus set management process proxy referred service mesh data plane management process control plane data plane intercept call service stuff call control plane coordinate behavior proxy provides api operator manipulate measure mesh wholewhat proxy layer tcp proxy like haproxy nginx choice proxy varies linkerd us rust proxy simply called linkerdproxy built specifically service mesh mesh use different proxy envoy common choice choice proxy implementation detailwhat proxy proxy call service course strictly speaking act proxy reverse proxy handling incoming outgoing call implement featureset focus call service focus traffic service differentiates service mesh proxy say api gateway ingres proxy focus call outside world cluster wholeso data plane control plane simpler set component provide whatever machinery data plane need act coordinated fashion including service discovery tl certificate issuing metric aggregation data plane call control plane inform behavior control plane turn provides api allow user modify inspect behavior data plane wholehere diagram linkerd control plane data plane see control plane several different component including small prometheus instance aggregate metric data proxy well component destination service discovery identity certificate authority publicapi web cli endpoint data plane contrast single linkerdproxy next application instance logical diagram deployed may end three replica control plane component hundred thousand data plane proxy blue box diagram represent kubernetes pod boundary see linkerdproxy container actually run pod application container pattern known sidecar container architecture service mesh couple big implication one since proxy featureset designed servicetoservice call service mesh really make sense application built service could use monolith would whole lot machinery run single proxy featureset great fitanother consequence service mesh going require lot lot proxy fact linkerd add one linkerdproxy per instance every service mesh implementation add one proxy per node host vm lot either way heavy use proxy couple implication whatever data plane proxy better fast adding two proxy hop every call one client side one server sidealso proxy need small light one consume memory cpu consumption scale linearly applicationyou going need system deploying updating lot proxy want handbut least level really service mesh deploy ton userspace proxy stuff internal servicetoservice traffic use control plane change behavior query data generatenow let move whywhy service mesh make sense encountering idea service mesh first time forgiven first reaction mild horror design service mesh mean add latency application also consumes resource also introduces whole bunch machinery one minute installing service mesh next suddenly hook operating hundred thousand proxy would anyone want two part answer first operational cost deploying proxy greatly reduced thanks change happening ecosystem lot laterthe important answer design actually great way introduce additional logic system ton feature add right also add without changing ecosystem fact entire service mesh model predicated insight multiservice system regardless individual service actually traffic ideal insertion point functionalityfor example linkerd like mesh layer feature set focused primarily http call including grpc feature set broad divided three class reliability feature request retries timeouts canary traffic splittingshifting etcobservability feature aggregation success rate latency request volume service individual route drawing service topology map etcsecurity feature mutual tl access control etcmany feature operate request level hence proxy example service foo make http call service bar linkerdproxy foo side load balance call intelligently across instance bar based observed latency one retry request fails idempotent record response code latency similarly linkerdproxy bar side reject call allowed rate limit record latency perspective onthe proxy stuff connection level example foo linkerdproxy initiate tl connection bar linkerdproxy terminate side validate others tl certificate provides encryption service cryptographically secure form service bar prove say arewhether request connection level one important thing note feature service mesh operational nature anything linkerd transforming semantics request payload eg adding field json blob transforming protobuf important distinction touch talk esbs middlewareso set feature service mesh provide implement directly application bother proxy service mesh good idea featureset interesting core value service mesh actually feature could implement feature directly application fact see later genesis service mesh put single sentence value service mesh come service mesh give feature critical running modern serverside software way uniform across stack decoupled application codelet take one bit timefeatures critical running modern serverside software building transactional server side application connected public internet take request outside world responds within short web apps api server bulk modern serverside building system collection service talk synchronous fashion continually modifying software add functionality tasked keeping system running even modifying congratulation building modern serverside software glorious feature listed actually turn critical application must reliable must secure must able observe exactly service mesh help ok snuck opinion one approach modern way build serverside software people world today building monolith reactive microservices thing fit definition might different opinion turn opinion opinion wrong either way service mesh useful uniform across stack feature provided service mesh critical apply every service application regardless language service written framework us wrote deployed detail development deploymentdecoupled application code finally service mesh provide feature uniformly across stack way requires application change fundamental ownership service mesh operational ownership configuration update operation maintenance purely platform level independent application application change without service mesh involved service mesh change without application involvedin short service mesh provide vital feature way global uniform independent application yes feature service mesh could implemented service code even library linked every service approach would provide decoupling uniformity heart service mesh value propand add lot proxy promise going talk operational cost adding proxy soon first need pit stop examine idea decoupling perspective peoplewho service mesh help inconvenient may turn order technology actually impact must adopted human being adopts service mesh benefit building described modern server software roughly think team divided service owner business building business logic platform owner building internal platform service run small organization may people organization get larger role typically get defined even subdivided lot said changing nature devops organizational impact microservices etc let take description given seen lens immediate beneficiary service mesh platform owner goal platform team build internal platform service owner run business logic way keep service owner independent possible gory detail operationalization service mesh provides feature critical accomplishing way turn incur dependency service ownersthe service owner also benefit albeit indirect way goal service owner productive possible building logic business fewer operational mechanic worry easier rather hook implementing eg retry policy tl focus purely business logic concern trust platform take care rest big plus wellthe organizational value decoupling platform service owner overstated fact think might key reason service mesh valuablewe learned lesson one earliest linkerd adopter told u adopting service mesh allowed talk people platform team large company migrating kubernetes app handled sensitive information wanted encrypt communication cluster hundred service hundred developer team looking forward convincing dev team add tl roadmap installing linkerd shifted ownership feature hand developer imposition hand platform team toplevel priority linkerd solve technical problem much solved organizational problemin short service mesh le solution technical problem solution sociotechnical problemdoes service mesh solve problem yes er look three class feature outlined security clear service mesh complete solution domain linkerd retry request know idempotent make decision return user service entirely application must make decision linkerd report success rate etc look inside service report internal application must instrumentation linkerd thing like mutual tl free lot security solution thatthe subset feature domain service mesh provides one platform feature mean feature independent business logic way traffic latency histogram computed call foo bar totally independent foo calling bar first placedifficult implement correctly linkerd retries parameterized sophisticated thing like retry budget naive approach retries sure path retry storm distributed system failure modesmost effective implemented uniformly mechanic mutual tl really make sense everyone thembecause feature implemented proxy layer rather application layer service mesh provides platform application level matter language service written framework use wrote got proxy function independent ownership operational ownership configuration update operation maintenance purely platform levelexample feature service meshobservabilityreliabilitysecurityservice meshservice success ratesrequest retriesmutual tl servicesplatform nonservice mesh log aggregationmultiple replica datasetencryption data restapplicationinstrumentation internal feature usagehandling failure entire component downensuring user access datato summarize service mesh complete solution reliability observability security broader ownership domain necessarily involves service owner ops sre team part organization service mesh provide platformlayer slice domainwhy service mesh make sense point may saying ok service mesh thing great rolling million proxy stack ten year ago shallow answer ten year ago everyone building monolith one needed service mesh true think miss point even ten year ago concept microservices feasible way building highscale system widely discussed publicly put practice company like twitter facebook google netflix general sentiment least part industry exposed microservices right way build highscale system even gosh really painful doof course company operating microservices ten year ago large installing proxy everywhere form service mesh looked closely though something related many organization mandated use specific internal library network communication sometimes called fat client library netflix hysterix google stubby library twitter finagle finagle example mandatory every new service twitter handled client server side connection implemented retries request routing load balancing instrumentation provided consistent layer reliability observability across entire twitter stack independent service actually sure worked jvm language programming model build whole app around operational feature provided almost exactly service meshso ten year ago microservices protoservicemesh library solved many problem service mesh solves today service mesh something else needed change firstand deeper answer lie buried another difference happened past ten year dramatic reduction cost deploying microservices company listed publicly using microservices decade netflix facebook company immense scale immense resource need talent build deploy operation significant microservice application sheer amount engineering time energy went twitter migration monolith microservices boggles imagination sort infrastructural maneuver essentially impossible smaller companiescontrast today might encounter startup even ratio microservices equipped handle running microservices plausible approach startup clearly something reduced cost adopting microservicesthe dramatic reduction cost operating microservices result one thing rise adoption container container orchestrator deeper answer question change enabled service mesh lie made service mesh operationally viable thing making microservices operationally viable kubernetes dockerwhy well docker solves one big thing packaging problem allowing package app nonnetwork runtime dependency container app fungible unit thrown around run anywhere token docker make exponentially easier run polyglot stack container atomic unit execution deploy operational purpose really matter inside container whether jvm app node app go python ruby run itkubernetes solves next step bunch executable thing also bunch thing execute executable thing aka machine need mapping broad sense give kubernetes bunch container bunch machine figure mapping course dynamic evershifting thing new container roll system machine come operation kubernetes figure kubernetes going deploytime cost running one service much different running ten service fact different service combine container packaging mechanism encourages polyglot implementation result ton new application implemented microservices written variety environment service mesh suited forand finally come service mesh feasible uniformity kubernetes provides service directly applicable operational challenge service mesh package proxy container tell kubernetes stick em everywhere voila got service mesh deploytime mechanic handled kubernetesto summarize reason service mesh make sense opposed year ago rise kubernetes docker dramatically increased need run service mesh making easy build application polyglot microservices architecture dramatically reduced cost running service mesh providing mechanism deploying maintaining fleet sidecar proxieswhy people talk much service mesh content warning section resort speculation conjecture inside baseball opinionone need search service mesh encounter kafkaesque fever dream landscape full confusing project lowcalorie recycled content general echo chamber distortion shiny new tech certain level service mesh seems particularly bad case well partly fault done best talk linkerd service mesh every opportunity countless blog post podcasts article like one powerful really answer question talk service mesh landscape impossible talk landscape without talking one project particular istio open source service mesh billed collaboration google ibm lyftwhat remarkable istio two thing first sheer amount marketing effort google particular placing behind estimation majority people know service mesh today introduced istio second remarkable thing poorly istio received obviously horse race trying objective seems istio developed pretty public backlash way uncommon though unheard open source projectleaving aside personal theory happening believe google involvement really reason service mesh space hypey specifically combination istio promoted heavily google b corresponding lackluster reception c recent meteoric rise kubernetes still fresh everyone mind combined form kind heady oxygenfree environment capacity rational thought extinguished weird kind cloudnative tulip mania remainsfrom linkerd perspective course guess would describe mixed blessing mean great service mesh thing case linkerd first got ground really hard get anyone pay attention problem suck service mesh landscape confusing hard understand even project service mesh never mind one fit use case best everyone disservice certainly situation istio another project would right choice far onesizefitsall solution linkerd side strategy ignore noise continue focusing solving real problem community basically wait whole thing blow level hype eventually subside get livesin meantime though going suffer humble software engineer care service mesh software engineer basic rubric whether care service meshif pure businesslogicimplementin developer role really need care service mesh mean certainly welcome care ideally service mesh directly affect anything life keep building sweet sweet business logic get everyone around paidif platform role org using kubernetes yes care unless adopting purely run monolith batch processing case would seriously ask question going end situation lot microservices written people talking tied together one unholy bundle runtime dependency going need way deal since kubernetes several service mesh option informed opinion one even whether want start linkerd platform role org using kubernetes microservices yes care going complicated sure could get value service mesh deploying lot proxy everywhere nice part kubernetes deployment model roi equation going look different manage proxy yourselfif platform role org monolith probably need care operating monolith even collection monolith welldefined infrequentlychanging communication pattern service mesh add much probably ignore hope go awayconclusionthe service mesh probably actually hold title world overhyped technology dubious distinction probably go bitcoin ai maybe merely top cut layer noise real value anyone building application kubernetesfinally love try take second install kubernetes cluster even minikube see exactly talking aboutfaqsif ignore whole service mesh thing go away sadly service mesh staybut want use service meshthen see guide whether need understand itisn esb middleware service mesh focus operational logic business logic downfall enterprise service bus keeping separation critical service mesh avoiding fatehow different api gateway million article google itis envoy service mesh service mesh envoy proxy used make service mesh many thing generalpurpose proxy service mesh itselfis network service mesh service mesh despite name service mesh marketing fun right service mesh help reactive asynchronous message queuebased system service mesh help youwhich service mesh use linkerd duhi think article suck think suckplease share link friend see much suck suckthanks creditsas might guessed title article inspired jay krep fantastic treatise log log every software engineer know realtime data unifying abstraction met jay interviewed linkedin almost decade ago inspiration ever sincewhile like call linkerd maintainer reality mostly maintainer linkerd readmemd linkerd today work many many many many people would possible without amazing community contributor adoptersfinally special shoutout creator linkerd oliver gould primus inter pares took plunge whole service mesh thing many year ago
102,Lobsters,scaling,Scaling and architecture,"We built network isolation for 1,500 services to make Monzo more secure",https://monzo.com/blog/we-built-network-isolation-for-1-500-services/,built network isolation service make monzo secure,investigating network isolation number service connection platform make difficult building modern backend use microservices first isolated one service trial engineering principle came tool analyse code trial highlighted issue approach single responsibility principle investigated way test policy safely calico used tool get visibility network kubeiptablestailer calicoaccountant redesigned represent connected service wrote policy applied many service came way engineer manage rule rolled service service call six others average,security team monzo one goal move towards completely zero trust platform mean theory able run malicious code inside platform risk code would nt able interact anything dangerous without security team granting special accessthe idea nt want trust anything simply inside platform instead want individual service trusted based short deliberate list service allowed interact make attack substantially difficultwe investigating network isolationas work towards zero trust platform spent part last year investigating network isolation practice mean nt want service control image pot able talk service move money service specifically defined manually approved list communicate anything else blockedwith handful service could probably maintain list manually already service list allowed path huge constantly changing fact unique connection example serviceemoji serviceledger number service connection platform make difficultthe sheer number service connection made project really challenging figure way generate allowed path code store rule way easy engineer manage machine interpret enforce without breaking anythinghere network specified part project service represented dot every line enforced network rule allows communication two serviceshere graph popular node removed coloured team interested decided build kind system check blog post wrote back building modern backend talk one engineer use microservicesfirst isolated one service trialbefore ever decided apply isolation service decided apply one highsecurity service serviceledger source truth customer balance money movement one engineering principle ship project iterate best start small especially complex system reason picked important service first finance team really wanted lock service particular keen help u trialwe came tool analyse codeto start wrote tool called rpcmap would read go code platform attempt find code looked like making request another service tool map connection service service call nt perfect good enough build list service need call ledger manually checked list make sure accuratewe ended simple list service next wanted enforce service list could make request ledger knew could use straightforward feature kubernetes software orchestrates service called networkpolicy resource policy part ledger configuration simply listed set allowed calling service traffic source right label allowed example allow service labelled servicepotwhen engineer need add new caller ledger add list redeploy ledger store file place ledger code change finance team review let keep track calling critical service important make sure control automated check running new code reminds engineer whitelist calling service call ledgerbut trial highlighted issue approachthis approach worked well ledger knew would nt possible scale service reason nt safe way test policy rolling manual check policy generated need able roll policy know definitively would drop traffic without actually dropping traffic would let u generate policy automatically instead checking manually simply wait day see correctwhile worked well ledger team nt always want review every new caller service ideal list allowed service property receiving serviceengineers would edit kubernetes configuration file whitelisting nt something generally used prone errorsrolling back service becomes really risky go back earlier version service correct code change might also roll back calling service whitelisted blocking traffic suddenly highlight fundamental problem approach serviceemoji call serviceledger property emoji deployed make property ledger whitelist service becomes shared state get sync hard maintain wanted follow single responsibility principleso decided look way testing policy rethink defined ruleswe investigated way test policy safelywe actually implement kubernetes network policy using calico networking software let service talk spoke calico community testing network policy found could use feature calico nt accessible kubernetes make policy testablewhenever applied network policy service decided label used calico network policy extra feature beyond kubernetes give u make following statement traffic come service network policy itand traffic would allowed policythen log traffic allow ithere policy look like essentially policy run kubernetes network policy high order field traffic reach policy must nt allowed kubernetes policiesthe ability select service already policy crucial otherwise dry run policy like one would capture traffic service without policy would nt know much traffic would dropped dry run policy nt existwe used tool get visibility networkwith dry run policy traffic would ever dropped policy also get log entry whenever would dropped watched log handy tool called kubeiptablestailer could create graph showing service dropping traffic wherewe little concerned much logging incorrect policy would drop load traffic would create problem platform decided turn logging proven rate would low wrote new tool calicoaccountant able count many packet would dropped given service without actually logging count useful see service bad policy still needed log case log also show source service dropped trafficeven started enforcing network policy drop disallowed traffic kept logging enabled could quickly alert engineer dropped traffic bug attackerwe redesigned represent connected serviceswe thought lot hypothesis traffic serviceemoji serviceledger represented property emoji came quite idea essentially wanted service somehow specify service needed talk sum figure inbound service ledgerat first thought writing one network policy per link emoji ledger number policy would led big performance issue realised could simply label service need needed talk serviceledger could label like monzocomegresssledger ledger network policy could look like key breakthrough straightforward identical policy write service simple way denote destination service calling service wrote policy applied many serviceswe knew also cover service need talk large group service example monitoring service talk pretty much everything nt want label monitoring service every single service platform huge constantly changing listwe could added something every service policy allow monitoring add new service call everything deploy service new policy decided instead add another label servicetype loosely group service need call themfor example backend service type label called one group service api label slightly different group enforce catchall policy like policy select thing already network policy mean allow new traffic risk blocking trafficwe came way engineer manage rulesthe really nice way instruct computer path allowed would nightmare expected engineer keep label updated ca nt ask people think whether need whitelist service convert service name label find right configuration file add label instead wanted completely automate processfirst updated rpcmap ran service would scan whatever call generate rule file simply file per called service represents fact call b serviceemoji would file serviceemojiegressserviceledgerrulewe set rpcmap run everyone code whenever push code github reminds engineer keep rule updated also check rule file accepts code course rule file also need reviewed humanif team want keep track caller critical service instruct github ask review serviceledgerrule asked review rule file even though resides inside another team servicenext updated deployment pipeline convert rule file label service able maintain ability make call service reviewable owner destination service make link service really easy understand also make really easy figure service call ledger simply look file named serviceledgerrule nice thing enforcing rule platform give u provable information know list caller ledger exhaustivewe rolled serviceswith control place able roll fairly rapidly despite huge number service first generated rule file service started enforcing maintained code accepted moment reasonably accurate network connected servicesnext rolled network policy test environment identical platform actually run monzo discovered fixed bunch case rpcmap nt able infer service called another service showed dropped traffic generally fixed case adding special comment code told rpcmap link also manually cover nongo service nt many within couple day fixed drop test environment changed dry run policy drop disallowed trafficnow could confident engineer would nt accidentally ship code nt appropriate rule file code would fail test environmentnext rolled network policy production platform packet dropping logging disabled alert would fire anything violates rule already big win security also kept logging disabled potential huge log volume rolled label allow traffic proven log volume would small using calicoaccountant enabled logging dropped traffic find remaining missed case rpcmap nt clever enough let u get dropped count zerowe decided leave packet dropping turned month sure nt path rarely used might longer allowed still get lot security value alerting violation also turning packet dropping completely sure nothing ever dropped part normal business processesnow service call six others averagewe moved every service able call others every service able call six others average review every new pairing solving human issue around managing rule particularly interesting challenge incredible win security huge step towards zero trust platform find project interesting hiring head monzocomcareers see open role defensive offensive security post published november
103,Lobsters,scaling,Scaling and architecture,"Why Sourcegraph switched from cloud SaaS to on-premises, self-hosted software",https://about.sourcegraph.com/blog/from-saas-to-on-premises,sourcegraph switched cloud saas onpremises selfhosted software,trust great control better selfhosted cloudhosted open core summit going saas trend transition cloud saas onprem docker kubernetes aws google cloud digitalocean better developerfriendly doc code base security ensuring scalability performance infrastructure visible logging metric conclusion,trust great control better german proverb december sourcegraph went traditional cloudhosted software service saas product onpremises selfhosted product blog post cover drove decision challenge encountered explains still believe onpremises selfhosted best way ensure success product dealing sensitive customer data focus technical benefit challenge decision overlap positive business aspect mentioned selfhosted cloudhosted open core summit talk sourcegraph ceo quinn slack going saas trend challenging year sourcegraph despite launched saas product performed code search code intelligence blazing speed across ten thousand repository managed close handful paying customer company constantly reaching requesting information product turn endorsed high demand sourcegraph yet conversion interest adoption low asked company decision maker opted using sourcegraph despite obvious demand engineer answer always want push private code base external software provider matter many benefit would provide frequent feedback made u question saas model despite popularity high success rate many business right solution product dealing vulnerable data decided go saas trend turn product onprem selfhosted solution transition cloud saas onprem decade ago onpremises solution difficult setup required onsite installation continuous integration ci made possible verify build different environment increase successful deployment offsite docker shipping kubernetes container orchestration simplify packaging software running aws google cloud digitalocean infrastructure eased way sourcegraph transition multitenant cloud product selfhosted one make simple customer stay latest sourcegraph version optimized updating process following change moved continuous delivery monthly iteration cycle emphasized feature development encourage keeping newest sourcegraph version also support policy ask customer stay newest version established standard release testing process better detect bug release wrote better developerfriendly doc tackle challenge running sourcegraph smoothly different environment code base security many customer high security requirement feel comfortable uploading private code external cloud software provider prefer solution run infrastructure security term authorization mechanism full control access since release sourcegraph onpremises selfhosted product experienced much higher sourcegraph adoption rate enterprise usage company deploy inhouse without exposing code sourcegraph employee provider allows anyone company start running sourcegraph locally hasslefree instead needing ask permission grant access codebase external cloud infrastructure provider ensuring scalability performance performance locally run software depends mostly much hardware customer willing allocate benefit onpremises software contrary saas company restrained onesizefitsall performance framework scale software based best hardware best suit need resource sourcegraph able work existing infrastructure customer scale perform accordingly flip side sourcegraph tackle diverse challenge come running software different environment infrastructure visible running onpremises devops work exposed customer making aware quality code point view blessing disguise help u set expectation higher quality documentation occasionally thing go wrong one challenge dealing customer hosted software debugging process requires access error log alert local configuration deployment environment overcome hurdle improved sourcegraph observability building logging metric application give information root cause bug fortunately customer quick alert u whenever issue conclusion switching saas onpremises essential sourcegraph become de facto developer platform engineer use among top company like lyft uber plaid convoy many main challenge maintaining high standard code quality reduce tedious debugging process onpremises solution ensuring sourcegraph run smoothly different environment majority customer run fairly uptodate sourcegraph version able achieve making upgrade process simple possible communicating enticing new feature customer saas may right solution variety product providing onpremises deployment turned deciding factor securityconscious company adopt sourcegraph
104,Lobsters,scaling,Scaling and architecture,Postgres Is UnderratedIt Handles More than You Think,https://dev.to/heroku/postgres-is-underrated-it-handles-more-than-you-think-4ff3,postgres handle think,adding another data store always good idea advantage choosing boring technology lesserknown powerful feature postgres postgres cache sharedbuffer plan pgbuffercache pgprewarm refer article text searching tsvector documentation function postgres postgres offer powerful extension postgres website geospatial data postgis keyvalue data type hstore tutorial semistructured data type json xml tip scaling postgres overindex partial index save space understanding postgres index type btree index amcheck brin index bloom filter index gin gist index need another data store special data type hyperloglog data type heavy realtime processing instant fulltext searching conclusion heroku postgres cloud database workload jon daniel,thinking scaling beyond postgres cluster adding another data store like redis elasticsearch adopting complex infrastructure take minute think quite possible get existing postgres database scale heavy load offer powerful feature obvious first sight example possible enable inmemory caching text search specialized indexing keyvalue storage reading article may want list feature want data store check postgres good fit powerful enough application adding another data store always good idea fred brook put mythical manmonth programmer like poet work slightly removed pure thoughtstuff build castle air air creating exertion imagination adding piece castle getting lost design endlessly fascinating however real world building castle air get way hold true latest hype data store several advantage choosing boring technology someone new join team easily make sense different data store another team member come back year later could quickly pick system work need change system add feature many piece move around factored maintenance cost security upgrade accounted unknown failure mode running new data store production scale although managed thoughtful design adding multiple datastores increase complexity exploring adding additional datastores worth investigating additional feature existing datastores offer lesserknown powerful feature postgres many people unaware postgres offer way sql database already postgres stack add piece postgres job postgres cache misconception postgres read writes disk every query especially user compare purely inmemory data store like redis actually postgres beautifully designed caching system page usage count transaction log query need access disk especially refer data many query tend sharedbuffer configuration parameter postgres configuration file determines much memory use caching data typically set total memory postgres also us operating system cache operation memory recurring query referring data set need access disk set parameter postgres cli alter system set sharedbuffer value managed database service like heroku offer several plan ram hence cache major differentiator free hobby version offer dedicated resource like ram upgrade ready production load make better use caching also use advanced caching tool example check pgbuffercache view see occupying shared buffer cache instance another tool use pgprewarm function come part base installation function enables dba load table data either operating system cache postgres buffer cache process manual automated know nature database query greatly improve application performance really brave heart refer article indepth description postgres caching text searching elasticsearch excellent many use case get along fine postgres text searching postgres special data type tsvector set function like totsvector totsquery search quickly text tsvector represents document optimized text search sorting term normalizing variant example totsquery function select totsquery english boy girl totsquery boy girl sort result relevance depending often field query appeared result example make title relevant body check postgres documentation detail function postgres postgres provides powerful serverside function environment multiple programming language try preprocess much data postgres server serverside function way cut latency come passing much data back forth application server database approach particularly useful large aggregation join even better development team use existing skill set writing postgres code default plpgsql postgres native procedural language postgres function trigger written plpython plperl javascript extension postgres plr example creating plpython function checking string length create function longerstringlength string string return integer alen blen b return return b language plpythonu postgres offer powerful extension extension postgres plugins mean many application suitable use postgres extension also mean work data store extra functionality many extension available listed main postgres website geospatial data postgis specialized extension postgres used geospatial data manipulation running location query sql widely popular among gi application developer use postgres great beginner guide using postgis found code snippet show adding postgis extension current database o run command install package assuming using ubuntu sudo addaptrepository ppa ubuntugisppa sudo aptget update sudo aptget install postgis log postgres instance install extension create extension postgis create extension postgistopology want check extension current database run command select pgavailableextensions keyvalue data type postgres hstore extension allows storing searching simple keyvalue pair tutorial provides excellent overview work hstore data type semistructured data type two native data type storing semistructured data postgres json xml json data type host native json binary form jsonb latter significantly improve query performance searched see convert json string native json object select blue green tag price discounted false json json blue green tag price discounted false tip scaling postgres considering switching postgres due performance reason first see far get optimization offer assume done basic like creating appropriate index postgres offer many advanced feature change small make big difference especially keep complicating infrastructure overindex avoid unnecessary index use multicolumn index sparingly many index take extra memory crowd better us postgres cache crucial performance using tool like explain analyze might surprise often query planer actually chooses sequential table scan since much table row data already cached oftentimes elaborate index even used said find slow query first obvious solution see table missing index index vital use correctly partial index save space partial index save space specifying value get indexed example want order user signup date care user signed create index usersignupdate user signupdate issignedup understanding postgres index type choosing right index data improve performance common index type use one btree index btree index balanced tree used sort data efficiently default use index command time btree index suffices scale inconsistency larger problem use amcheck extension periodically brin index block range index brin used table naturally already sorted column need sort column example log table written sequentially setting brin index timestamp column let server know data already sorted bloom filter index bloom index perfect multicolumn query big table need test equality us special mathematical structure called bloom filter based probability us significantly le space create index using bloom select x gin gist index use gin gist index efficient index based composite value like text array json need another data store legitimate case adding another datastore beyond postgres special data type data store give data type get postgres example linked list bitmap hyperloglog function redis available postgres previous startup implement frequency cap counter unique user website based session data like cooky might million ten million user visiting website frequency capping mean show user ad per day redis hyperloglog data type perfect frequency cap approximates set membership small error rate exchange time small memory footprint pfadd add element hyperloglog set return element set already set pfadd userids integer pfadd userids integer pfadd userids integer heavy realtime processing situation many pubsub event job dozen worker coordinate may need specialized solution like apache kafka linkedin engineer originally developed kafka handle new user event like click invitation message allow different worker handle message passing job process data instant fulltext searching realtime application heavy load ten search going time need feature like autocomplete may benefit specialized text solution like elasticsearch conclusion redis elasticsearch kafka powerful sometimes adding harm good may able get capability need postgres taking advantage lesserknown feature covered ensuring getting postgres save time help avoid added complexity risk save even time headache consider using managed service like heroku postgres scaling simple matter adding additional follower replica high availability turned single click heroku operates really need expand beyond postgres data store mentioned redis apache kafka elasticsearch easily provisioned heroku go ahead build castle anchor reliable foundation dream better product customer experience information postgres listen cloud database workload jon daniel software engineering daily
105,Lobsters,scaling,Scaling and architecture,Gray Failure: The Achilles Heel of Cloud-Scale Systems,https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-1.pdf,gray failure achilles heel cloudscale system,,obj length filter flatedecode stream p jbd c e endstream endobj obj length filter flatedecode stream bi u x je h l h p ep ou ebd ex sz cg
106,Lobsters,scaling,Scaling and architecture,Three Rules For Structuring (Redux) Applications,https://jaysoo.ca/2016/02/28/organizing-redux-application/,three rule structuring redux application,series content three rule project structure rule organize feature redux react angularjs ruby rail object group together form feature module better way structure redux react project note application within main application rule create strict module boundary simplicity matter complecting close proximity barrier couple bad good avoid coupling state another module bad good instead haphazardly reaching inside module think forming maintaining contract rule avoid circular dependency circular dependency example ajs bjs depend action type within depend action type within make hairball complecting worst possible way litmus test project structure try extract external module summary next post,series looking code organization context react redux application takeaway presented applicable application reactredux series content application grow often find file structure organization crucial maintainability application code want post present three organizational rule personally follow project following rule application code easier reason find wasting le time file navigation tedious refactoring bug fix hope tip prove useful developer want improve application structure know start three rule project structure following basic rule structuring project noted rule framework language agnostic able follow situation however example react redux familiarity framework useful rule organize feature first start going common way project organized object role redux react action todosjs component todos todoitemjs constant actiontypesjs reducer todosjs indexjs rootreducerjs angularjs controller directive service template indexjs ruby rail app controller model view may seem reasonable group similar object together like controller controller component component however application grows structure scale add change feature start notice group object tend change together object group together form feature module example todo app change reducerstodosjs file likely also change actionstodosjs componentstodos j instead wasting time scrolling directory looking todos related file much better sitting location better way structure redux react project todos component actionsjs actiontypesjs constantsjs indexjs reducerjs indexjs rootreducerjs note go detail inside file next post large project organizing feature affords ability focus feature hand instead worry navigating entire project mean need change something related todos work soley within module even think rest application sense creates application within main application surface organizing feature may seem like aesthetic concern see next two rule way structuring project help simplify application code rule create strict module boundary ruby conf talk simplicity matter rich hickey defines complexity complecting interleaving thing couple module together picture actual knot braid forming code relevence complexity project structure place object close proximity one another barrier couple lower dramatically example say want add new feature todo app want ability manage todo list project mean create new module called project project component actionsjs actiontypesjs reducersjs indexjs todos indexjs obvious project module dependency todos situation important exercise discipline couple api exposed todosindexjs bad import action todosactions import todoitem todoscomponentstodoitem good import todos todos const action todoitem todos another thing avoid coupling state another module example say within project module need grab information todos state order render component better todos module expose interface project query information rather complecting component todos state bad const projecttodos todos div todosmap todoitem todo div connect todos state const projecttodoscontainer connect state redux state prop react component prop state prop const project stateprojects propsprojectid couple todos state bad const todos statetodosfilter projecttodoidsincludes tid return todos projecttodos good import createselector reselect import todos todos const projecttodos todos div todosmap todoitem todo div const projecttodoscontainer connect createselector state prop stateprojects propsprojectid let todos module provide implementation selector good todosselectorsgetall combine previous selector provides final prop project todos return todos todosfilter projecttodoidsincludes tid projecttodos example project module concerned internal state todos module powerful freely change structure todos state without worrying breaking dependent module course still need maintain selector contract alternative search whole bunch disparate component refactor one one artificially creating strict module boundary simplify application code turn increase maintainability application instead haphazardly reaching inside module think forming maintaining contract project organized feature explicit boundary feature one last thing want cover circular dependency rule avoid circular dependency take much convincing believe say circular dependency bad yet without proper project structure easy fall trap time dependency start innoculously may think project module need reduce state based todos action grouping feature see large manifest action type within global actiontypesjs file easy u reach grab need time without second thought say within todos want reduce state based action type project easy enough global actiontypesjs file however soon learn easy feat explicit module boundary illustrate consider following example circular dependency example given ajs import b b export const name alice export default consolelog b bjs import name export default hello name happens following code import might expect alice printed actuality would print undefined name export available imported b due circular dependency implication project depend action type within todos todos depend action type within project get around restriction clever way go road guarantee come bite later make hairball put another way creating circular dependency complecting worst possible way imagine module strand hair module interdependent form big messy hairball whenever want use small module within hairball choice pull giant mess even worse change something inside hairball would hard break something else following rule make hard create circular dependency fight instead use energy properly factor module three rule one last topic want discus detect project smell litmus test project structure important u tool tell u something smell code experience project start clean mean stay way thus want present easy method detect project structure smell every pick module application try extract external module eg nodejs module ruby gem etc actually least think perform extraction without much effort know well factored term remains undefined need come measure whether subjective objective run experiment module application jot problem find experiment circular dependency module breaching boundary etc whether choose take action based finding afterall software industry tradeoff least give much better insight project structure summary project structure particularly exciting topic discus however important one three rule presented post organize feature create strict module boundary avoid circular dependency next post dive deeper code example learn rule applied react redux project
107,Lobsters,scaling,Scaling and architecture,Don't Look Into the Light,https://christine.website/blog/dont-look-into-the-light-2019-10-06,nt look light,look light happened yet john murphy contact,look light previous job working maintained system system powered significant part core product actually used far usage metric reported time bolted something onto side product take action based number product tracking year cycling various people system hard understand data would flow one end go aggregation layer get sent storage another aggregation layer eventually metric calculated system fairly expensive operate stressing datastores relied beyond company called theoretical limit oh make thing even fun part make action based data barely keeping needed supposed run check minute running second planning meeting started complain state world godawful everything become undocumented probably undocumentable organic nature system gotten hand thought could kill two bird one stone wanted subsume another product took action based data well create generic platform reimplement older actiontaking layer top rule set groundwork laid decided would big rewrite based lesson learned past operating behemoth project would futureproof project would test coverage reported ci project would built microservices architecture road probably massive alarm bell going head one thing look like good idea paper probably passed good idea management actually implemented happened set quest write software repo created ci configured script optimized dump code coverage output strived document everything day took advantage datastore using everything looking great product team came noticed fresh meat soon realized could big thing customer wanted get soon possible suddenly deadline pushed forward needed get whole thing testing yesterday set set trigger task worked testing consistently continuous functional testing tooling told product okay limited set customer mistake fell apart second customer touched struggled understand dug core beast created managed discover made critical fundamental error heart task matching code monstrosity cross join took people team sheet graph paper break understand task execution layer worked perfectly testing almost never production week solid debugging including making deal team satan jesus pope try understand made progress almost kind gremlin code randomly making thing fire one internal user triggering apologize product team apparently lot product team go damage control result imagine trickleddown impact project internal company lesson threefold first big rewrite almost surefire way ensure project fails avoid temptation look light look nice may even feel nice statistically speaking nice get side second lesson making something microservices gate terrible idea microservices architecture planned evolutionary result fully anticipated feature finally design future future happened yet nobody know going turn future going happen either adapt happens fail make thing overly modular lead insane thing like dynamically linking part application http future proof system build today chance future arrives system unmaintainable incomprehensible john murphy kind advice probably gon na feel like slap face lot people people really put heart work feed ego massively painful say something someone really passionate even lead people changing career plan depending person truth matter far tell generally happens big rewrite centred around best practice cloud native software successful design decision wholly utterly subjective every kind project come across work system probably work perfectly system b everything unique snowflake embrace share mastodon article posted fact circumstance may changed since publication please contact jumping conclusion something seems wrong unclear tag practice bigrewrite
108,Lobsters,scaling,Scaling and architecture,Google: polling like it's the 90s,https://www.ably.io/blog/google-polling-like-its-the-90s/,google polling like,ably world cup amp problem overhead bad protocol choice websockets sse serversent event xhr streaming http long polling spreadsheet used generate graph raw gist protocol overhead detail see example request see example response see example websocket request problem high variable latency diminished user experience problem inefficient scoring synchronization increased bandwidth initial state object operational transformation wave byte changed every second ably platform xdelta vcdiff see spreadsheet wrapping realtime hard engineering problem scale frontend engineering google considered important google opting optimize later http developersgooglecomspeed author ably caveat reading article,technical review live score feature google search result delivered surprisingly oldschool tech ably recently pleasure delivering realtime scoring commentary update fan laver cup tennis championship behalf tennis australia third year row event saw google embeds live score update within search result pretty nifty seems first appeared result sometime received update world cup animated gif screen capture live scoreboard laver cupbeing curious engineer realtime geek jumped browser dev console started reverse engineering google magic given sheer scale everything google anticipating offthewall microoptimization work squeeze every last byte minimize bandwidth energy consumption google pioneering light web year initiative like amp expected nothing le find literally technology blog post dive google design choice surprisingly bad term bandwidth demand energy consumption battery life unnecessary contribution global warming ultimately sluggish user experience google scale expected see use common shared primitive efficient streaming pubsub api dogfooding product problem overhead bad protocol choice websockets sse serversent event xhr streaming good protocol choice streaming update browser provide relatively low overhead transport push update astheyhappen server client addition supported pretty much every browser used today google strangely chose http polling confuse http long polling http request held open stalled update server google literally dumb polling server every second offchance update blunt tool imagine graph minute window see bandwidth overhead google polling approach versus suitable protocol google http polling le efficient raw websocket solution minute window total overhead uncompressed v long polling measly byte websockets see spreadsheet used generate graph raw gist protocol overhead detail let look responsible increase overhead google http polling request made every persistent connection push update browser http stateless context little room optimization request carry unnecessary request response overhead request overhead client circa byte almost half cookie overhead entirely unnecessary given update personalised way scoring update go user see example request response overhead circa byte see example response minute therefore request made carrying overhead resulting total overhead compare long polling approach total overhead reduces smaller factor score update come every second average user experience better significantly lower latency update pushed immediately websockets protocol designed streaming shift dial much overhead needed per frame handful byte bandwidth overhead needed setting connection upgrading http websockets minute window delivering average score update overhead go byte using le bandwidth polling see example websocket request problem high variable latency diminished user experience look video capture live scoring notice progress indicator flash every second response new polling request sent google given score update average every second latency score update range best around second worst latency compared google polling v streaming protocolwebsockets sse xhr streaming even long polling large degree streaming transport thus offer relatively fixed latency overhead hundred millisecond globally average latency google long polling solution roughly slower streaming transport could used v problem inefficient scoring synchronization increased bandwidth order set scoreboard google request scoreboard state google server http whilst lot unnecessary noise initial state object roughly arguably acceptable oneoff cost required set state scoreboard given polling used request made every expected see google use form data synchronization protocol deliver change request google past invested huge amount synchronization protocol operational transformation wave stranger solving incredibly hard problem synchronizing state instead saw shocked right back every request every second sends entire state object averaged game byte changed every second put context unnecessary data sent every browser point scored instead byte changed representing synchronization lack overhead delta invested lot time year introducing native delta support ably platform intrigued see delta would perform realworld use case originally looked using json patch obvious choice get something door quickly decided due inefficiency course work json instead wanted find algorithm portable efficient compression performance perspective cpu cost generating delta needed directly line payload size found xdelta best fit along vcdiff delta encoding algorithm see graph applying delta polling long polling sse websocket transport get practically ongoing overhead deliver scoring update minute window initial state set google polling solution consumes data google server whereas using xdelta encoded websocket transport byte needed represents le bandwidth needed minute window le bandwidth including initial state set see working example using xdelta ably platform would like see calculation graph see spreadsheet wrapping hard know google engineering team invested optimizing aspect search result including overhead data synchronization inefficiency approach requires bandwidth websocket delta solution delivers latency higher average company operating scale would forced design implement efficient method simply due bandwidth cost theory realtime hard engineering problem scale without pubsub primitive google engineering team rely internet scale perhaps easier keep simple poll like forget customer suffering bandwidth inefficienciesplease note realise google product like firebase use case however firebase explicitly limit service concurrent connection per database perhaps make unsuitable realtime score frontend engineering google considered importantgoogle without doubt amazing company incredible engineering talent however quite plausible best engineer deployed core system infrastructure frontend engineering treated secondclass citizen google opting optimize laterpremature optimization rarely good idea perhaps optimizing part system gained enough traction support product team google preferred option bandwidth cost google google customer material seems unusual choice given google constantly telling everyone else speed bandwidth optimisation matter prioritize search result based http developersgooglecomspeed either way encouraging see internet becoming realtime applaud google nonetheless supporting realtime score search result look forward reviewing optimization work future iteration feature author matt ceo cofounder ably interested realtime problem realtime engineering industry headed reason cofounded ably provides cloud infrastructure apis help developer simplify complex realtime engineering organization build ably make easy power scale realtime feature apps distribute data stream thirdparty developer realtime apis interested chatting realtime problem distributed system article please reach mattheworiordan ablyrealtime twitter caveat reading article tcp overhead factored given transport tcp quic underlying networking protocol overhead remain relatively constant compression http gzip deflate brotley websocket frame compression level factored transport support compression compression ignored overhead dependency use various transport provide delta encoding capability included numerous way approach problem permutation dependency endless given cdns support dependency cached idempotent ie cached near indefinitely feel relevant experiment either websocket calculation based raw websocket streaming connection something ably officially support production normal websocket connection additionally carry envelope provides richer functionality knowing message id able resume point time automatic generic encoding decoding based encoding field
109,Lobsters,scaling,Scaling and architecture,"Software Architecture is Overrated, Clear and Simple Design is Underrated",https://blog.pragmaticengineer.com/software-architecture-is-overrated/,software architecture overrated clear simple design underrated,distributed payment system rib first none design used standard software architecture planning tool uml model adr dependency diagram one describing information flow one outlining class structure relationship component second architect team owned design architect enterprise architect third practically reference common architecture pattern martin fowler architecture guide software design tech company startup start business problem brainstorm approach whiteboard approach write via simple documentation simple diagram clear easy follow language talk tradeoff alternative circulate design document within teamorganization get feedback used send software design document engineer approach different commonly referred software architecture literature simple jargonless software design architecture pattern goal designing system simplicity gang four design pattern role architecture pattern singleton pattern facade actually pretty common useful engineer goal solving solution learning rather picking shiny architecture pattern hope solve problem getting better designing system pull teammate whiteboard design approach write design simple document share team asking feedback seen done uber design two different way contrast two design explicit tradeoff review design better designing system start simple stay simple try avoid complexity complex architecture formal tool inherently introduce followup twitter thread post hacker news lobster reddit habr infoq china,fair share designing building large system taken part rewriting uber distributed payment system designing shipping skype xbox one opensourcing rib uber mobile architecture framework system thorough design going multiple iteration lot whiteboarding discussion design boiled design document circulated feedback started buildingall system large scale hundred developer build top power system used million people per day also greenfield project payment system rewrite replace two existing payment system used ten system dozen team without business impact rewriting uber app project hundred engineer worked simultaneously porting existing functionality new architecturelet start thing might sound surprising first none design used standard software architecture planning tool use uml model adr dependency diagram created plenty diagram none followed strict rule plain old box arrow similar one describing information flow one outlining class structure relationship component two diagram within design document often different layout often added modified different engineerssecond architect team owned design architect enterprise architect true neither uber skypemicrosoft handsoff software architect position engineer higher level like staff engineer expected still regularly code project experienced engineer involved however one person owned architecture design experienced developer drove design process even junior team member involved often challenging decision offering alternative discussthird practically reference common architecture pattern jargon referenced common software architecture literature martin fowler architecture guide mention microservices serverless architecture application boundary eventdriven architecture lot come brainstorming however need reference design document themselvessoftware design tech company startupsso get thing done follow approach suggested wellknown software architecture approach discussion peer engineer working tech company fang facebook amazon netflix google well smaller startup team project however large small shared similar approach design implementation start business problem trying solve product trying build measure success brainstorm approach get together team multiple session figure solution work keep brainstorming small start high level going lower levelswhiteboard approach get team together person draw approach team converging able explain architecture systemapp whiteboard clearly starting highlevel diving deeper needed trouble explanation clear enough work required detailswrite via simple documentation simple diagram based explained whiteboard keep jargon minimum want even junior engineer understand write using clear easy follow language uber use rfclike document basic templatetalk tradeoff alternative good software design good architecture making right tradeoff design choice good bad depends context goal architecture split different service mention decided going one large service might benefit like straightforward quicker deployment choose extend service module new functionality weigh option building separate service module instead pro con approach would becirculate design document within teamorganization get feedback uber used send software design document engineer around u larger still distribute widely started balancing signalnoise ratio encourage people asking question offering alternative pragmatic setting sensible time limit discus feedback incorporate needed straightforward feedback quickly addressed spot detailed feedback might quicker settle inpersonwhy approach different commonly referred software architecture literature actually approach different principle architecture guide almost guide suggest starting business problem outlining solution tradeoff also nt use many complex tool many architect architecture book advocate document design simple using straightforward tool tool like google doc assume main difference approach boil engineering culture company high autonomy little hierarchy trait tech company startup share something sometimes le true traditional company also reason place lot common sensebased design processdriven design stricter rulesi know bank automotive company developer actively discouraged making architecture decision without going chain getting signoff architect level overseeing several team becomes slower process architect get overwhelmed many request architect create formal document hope making system clear using much tool common literature describes document also reinforce topdown approach intimidating engineer architect question challenge decision already documented using formal method wellversed usually nt fair company often want optimize developer exchangeable resource allowing reallocate people work different project short notice surprise different tool work better different environmentssimple jargonless software design architecture patternsthe goal designing system simplicity simpler system simpler understand simpler find issue simpler implement clear language described accessible design avoid using jargon understood every member team least experienced person able understand thing equally clearlyclean design similar clean code easy read easy comprehend many great way write clean code however rarely hear anyone suggesting start applying gang four design pattern code clean code start thing like single responsibility clear naming easy understand convention principle equally apply clear architectureso role architecture pattern see similarly usefulness coding design pattern give idea improve code architecture coding pattern notice singleton pattern see one raise eyebrow dig deeper see class act facade callthroughs yet think call abstract factory pattern fact took lot time understand pattern aha moment working lot dependency injection one area pattern actually pretty common useful also admit although spent lot time reading comprehending gang four design pattern far le impact becoming better coder feedback gotten engineer codesimilarly knowing common architecture pattern good thing help shorten discussion people understand way architecture pattern goal wo nt substitute simpler system design designing system might find accidentally applied wellknown pattern good thing later reference approach easier last thing want taking one architecture pattern using hammer looking nail use onarchitecture pattern born engineer observed similar design choice made case design choice implemented similarly choice named written extensively talked architecture pattern tool came solution solved hope making life others easier engineer goal solving solution learning rather picking shiny architecture pattern hope solve problemgetting better designing systemsi heard many people ask tip becoming better architecting designing system several experienced people recommend reading architecture pattern reading book software architecture definitely recommend reading especially book provide lot depth short post suggestion handson readingpull teammate whiteboard design approach draw working thing make sure understand ask feedbackwrite design simple document share team asking feedback matter simple complex thing working may smaller refactor large project summarize way make sense way others understand inspiration seen done uber share team format allows commenting like google doc others ask people add thought questionsdesign two different way contrast two design people design architecture go one approach one popping head however architecture blackandwhite come second design could also work contrast two explaining one better list second design briefly alternative considered arguing decided againstbe explicit tradeoff make made thing optimized clear constraint exist take accountreview design better assuming culture people share design via whiteboarding session document get review review people try take thing becoming oneway observer instead ask clarifying question part clear ask alternative considered ask tradeoff taken constraint assumed play devil advocate suggest another possibly simpler alternative even better one asking thought suggestion even though thought much design person presenting still add lot value learn morethe best software design simple easy understand next time starting new project instead thinking architect system battletested pattern use formal methodology document think come simplest possible design way easy anyone understand software architecture best practice enterprise architecture pattern formalized way describe system tool useful know might come handy one day designing system start simple stay simple try avoid complexity complex architecture formal tool inherently introduceread followup twitter thread post post received much discussion industry professional shared view experience architecture simplicity read interesting discussion hacker news lobster reddit also read russian translation article habr chinese translation infoq china
110,Lobsters,scaling,Scaling and architecture,SQS FIFO Queues: Message Ordering and Exactly-Once Processing Guaranteed?,https://www.ably.io/blog/sqs-fifo-queues-message-ordering-and-exactly-once-processing-guaranteed/,sqs fifo queue message ordering exactlyonce processing guaranteed,line sqs fifo queue marketing fifo message ordering nuance know exactlyonce processing guaranteed theoretical background total order broadcast flp result conclusion reading problem typically encountered ensuring duplication across distributed system solved available read deep dive implementing idempotency across distributed system ably realtime provides cloud infrastructure apis help developer simplify complex realtime engineering make easy power scale realtime feature apps distribute data stream thirdparty developer realtime apis,converting distributed system fifo exactlyonce message processing requires considerable user effort article look case serving guide bear mind implementing fifo exactlyonce message processing scenario using sqs fifo queue line sqs fifo queue reading amazon marketing around sqs fifo queue led believe provides exactly fifo order processing outofthebox solution correspondingly one could easily believe replacing sqs queue existing distributed system sqs fifo queue would change needed convert system fifo exactlyonce processing would indeed really nice believe amazon engineer done heavy lifting difficult functionality wholly available flip switch reality somewhat complicated amazon delivers side deal seems pretty impossible amazon complete exactly say tin without user input user input entail fifo message ordering first useful break function fifo order processing fifo sqs queue guarantee message served order received alternatively could group message according group id field guarantee ordered delivery group different group group message could treated fifo queue purpose analysis follows however guarantee fifo delivery distributed system using fifo sqs queue define system encompass producersender well consumerreceiver message example simple case single producer single consumer system would look like nuance know indeed guaranteed message get sqs fifo queue get served older message still queue however mean consumerreceiver get message order producersender submitted sqs fifo queue guaranteed two additional assumption need satisfied queue must get message order produced producersender consumerreceiver must get message order come sqs fifo queue order assumption satisfied need single synchronous producersender single synchronous consumerreceiver transmitting data synchronous transport layer adding asynchronicity part system remove ordering guarantee part result ordering guarantee entire system removed well looking detail fairly obvious asynchronous transport layer could break message ordering also large degree obvious asynchronous sender guarantee order message sent asynchronous receiverconsumer could break fifo order message assumption need complete processing fifo order last least multiple sender receiver virtually amount singleinstance asynchronous one ie would also break systemwide fifo order message exactlyonce processing guaranteed exactlyonce processing would mean duplicated message would affect state system ie recognised duplicate ignored sqs fifo queue would require unique message deduplication id would create one hashing message content together message use id discount duplicate message first glance seems using sqs fifo queue alone opposed nonfifo one would enough entire system attain exactlyonce processing property taking deeper look however subtle caveat emerge meaning exactlyonce processing guaranteed time condition two limit introduced sqs fifo queue implementation make impossible guarantee exactlyonce processing possible case first one max minute timeout storing given message deduplication id duplicated message arrives minute original sqs fifo queue would accept continue process exactlyonce missioncritical requirement part distributed system would take care guaranteeing message uniqueness second limit sqs fifo queue implementation come kind exactlyonce guarantee give consumerreceiver guarantee exactlyonce delivery guarantee exactlyonce processing mean sqs fifo queue receive acknowledgement consumer message processed stop returning future message request word message might actually get delivered way work sqs fifo queue implementation keeping message inaccessible consumer certain period time delivered consumer give consumer received chance process delete queue deleting message queue serf acknowledgement message processed period time known visibility timeout last hour assumption period long enough allow receiving processing message sending delete request back queue however assumption might hold case exactlyonce processing missioncritical across system fact using sqs fifo queue guarantee exactlyonce processing come guaranteed consumer message run independent bookkeeping combining exactlyonce processing fifo potential negative impact performance sqs fifo queue guarantee fifo ordering subsequent message served previous one acknowledged processed visibility timeout expires sqs fifo queue deliver group message instead single one minimal impact next group delivered queue know last one delivered processed consumer easy see system throughput would rapidly deteriorate scenario subpar connection quality large visibility timeouts theoretical background evident sqs fifo queue guarantee inorder exactlyonce processing within queue entire distributed system would use queue sqs fifo queue simply convenient building block use distributed system aiming systemwide fifo message delivery exactlyonce processing nothing taking step back distributed computing field exists total order broadcast problem total order broadcast specifies message delivered participant order long everybody fifo delivery message distributed system actually custom case problem additional restriction imposed order research total order broadcast problem aid understanding limitation possible solution fifo delivery problem simply put total order broadcast problem equivalent problem achieving distributed consensus ie participant distributed system agree message delivery order distributed consensus problem proven impossible solve general case described groundbreaking paper known flp result consequence existing distributed consensus total order broadcast algorithm guarantee successful completion possible case impose restriction distributed system run turn limit number possible case applied lifting restriction mean algorithm fail case necessarily render unusable practice surprisingly theory demonstrated throughout discussion far guarantee fifoexactlyonce delivery continued introducing various restriction distributed system eg synchronous transport layer transaction consumer side etc conclusion conclude problem fifoexactlyonce delivery distributed system hoped might solved using sqs fifo queue alone turn much harder simple marketingoriented definition would suggest sqs fifo queue certainly useful tool building solution problem constitute solution guaranteeing exactlyonce publishing outside queue problem frequently fall distributed system developer solve reading problem typically encountered ensuring duplication across distributed system solved available read deep dive implementing idempotency across distributed system ably realtime provides cloud infrastructure apis help developer simplify complex realtime engineering make easy power scale realtime feature apps distribute data stream thirdparty developer realtime apis
111,Lobsters,scaling,Scaling and architecture,Got microservices? Service mesh management might not be enough,https://cloud.google.com/blog/products/api-management/got-microservices-service-mesh-management-might-not-be-enough,got microservices service mesh management might enough,,
112,Lobsters,scaling,Scaling and architecture,Old Is the New New (2018),https://www.youtube.com/watch?v=AbgsfeGvg3E,old new new,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature goto old new new kevlin henney youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature goto old new new kevlin henney youtube
113,Lobsters,scaling,Scaling and architecture,Absolute scale corrupts absolutely,https://apenwarr.ca/log/20190819,absolute scale corrupts absolutely,wiredcom excellent article kessler syndrome common strain banana dying deming would say reduce variation adversarial attack computer vision doctor need wash hand lot tailscaleio equifax breach capital one aws hack read equifax pii database argentina matter stupid password fire upon deep rifters trilogy technological singularity horror,absolute scale corrupts absolutely internet gotten big growing like many computery people generation idealist believed better faster communication would unmitigated improvement society world peace better communication said older coworker millenium coming end people could understand others point view would reason fight government propaganda never work citizen two warring country talk realize side human like teach really true wiredcom excellent article sort belief system lot learn world said maybe said naive thing ever heard entire life ca nt remember exactly either would appropriate turn actually happened pattern nt see talked much seems apply sort system big small theoretical practical mathematical physical pattern cheaper interaction become intensely system corrupted faster interaction become faster corruption spread corruption nt know right technical definition know see living system virus cancer predator computer operating system malware password authentication phishing attack politics lobbyist grift finance highfrequency trading credit default swap twitter propaganda bot earth orbit space debris kessler syndrome internet botnets ddos attack thing common nt happen system small expensive system vulnerable back usually even nt worth corruption nt corrupt attack nt evolve mean evolve nt know technical definition use evolutionary process bacteria nt evolutionary process spam phishing attack malware foreignsponsored twitter propaganda space junk lowearth orbit biological infection evolve think random mutation imperfect natural selection malware evolves people redesign space junk turn violent accidental collision natural yet exact opposite careful human design whatever cause corruption happens case intelligent human attacker one way create new corruption fast persistent one human connect kind corruption generate human nt trying create corruption nt matter rare corruption spread quickly larger faster standardized network let attack offer bigger benefit attacker without increasing cost diversity one natural defense corruption diversity problem right supposedly common strain banana dying genetically identical wrong fungus right time kill one way limit damage would grow say kind banana banana plague kill say crop replace next year might work okay banana human disease would nt want one unlucky computer virus maybe operating system still nt want unlucky one also nt want stuck best operating system best browser diversity nature defends corruption human engineer fact major goal modern engineering destroy diversity deming would say reduce variation find best solution deploy consistently everywhere keep improving read adversarial attack computer vision worse magic eye drawing human optical illusion perfectly targeted optical illusion street sign would fool subset human time neural net configured differently everyone else every neural net every car particular brand model susceptible exactly illusion take perfect copy ai bring lab design perfect illusion fool subtracting natural diversity turned boring visual attack disproportionately effective one attack work search engine email spam filter get copy algorithm even query quickly enough transparently enough black box test design message defeat seo industry email newsletter industry nutshell company nt want tell clause unevenlyenforced term service violated knew fine tune behaviour evil quite evil enough trip line human moderator still work better computer moderator human make unpredictable mistake harder optimize attack rule wo nt stay constant back internet hope nt think going tell fix twitter facebook u politics truth idea see pattern corruption nobody bothered corrupt twitter facebook got influential enough matter everybody bothered corrupt defense start filtering bot course must people build better bot like email spam autogenerated web content captcha solver fighting ai fighting highly incentivized lot lot money power ok wait nt know fix giant social network know general workaround whole class problem slow thing choose carefully interact interact fewer people make sure certain people sound like religion advice sex probably coincidence also nt allow foreigner buy political ad country local newspaper better national one free trade go bad often even though also often good doctor need wash hand lot hospital staff like internet bacteria unfortunately general workaround translates smash facebook stop letting stranger interact twitter effective gon na happen b would destroy lot useful interaction like said got nothing sorry big network hard avery internet said oh right cofounder tailscaleio thinking particular formulation problem let forget internet scale problem like giant social network moment thing problem internet scale make newsworthy hate bearer bad news chance problem internet scale hard launch simple web service say customer employee equifax breach happen obviously outsider supposed access equifax data capital one aws hack happen capital one clearly decade experience leaking data place claim internet big equifax data reachable internet even though accessible support employee capital one data surely used behind layer layer misconfigured firewall unhelpful proxy server maybe even pretcpip legacy mainframe protocol moved aws eliminating diversity adhoc layer protection nobody say modernizing system wrong choice yet result result always get lose diversity aws banana aws permission bug exploit banana fungus attacker perfect attack try everywhere scale like crazy back worked super dumb database apps running lan stored password plaintext file readable everyone never digital attack scale capital one internet well nt employee bit tech skill could easily attack database surely got white collar crime occasionally heard story kid hacking school grading system giving even made fake account bb two random people foreign country nt hack database kid nt give million kid school nt thing corruption contained lost sight world everything internet scale interaction internet scale instance program restricted small set obviously trusted people people foreign country invited read equifax pii database argentina matter stupid password nt even able connect database nt able see exists nt short internet hand properly authorized user internet would like able reach anywhere requiring employee come office location job physical security seems kinda obsolete leaf u conundrum nt would nt nice though could server like simple architecture used sloppy security policy developer freedom somehow reach anywhere like network internet one nt reachable internet even addressable internet one us internet substrate banana working literary afterthought certainly first bring various scifi address problem system corruption due excess connectivity liked fire upon deep vernor vinge part universe much better connectivity others nt go well also rifters trilogy peter watt internet time nearly unusable cluttered machinegenerated garbage still interested hearing real science general topic system corruption large scale higher connectivity math predict point fall apart define upper limit potential hyperintelligence prevent technological singularity logistical note normally east coast person visiting san francisco bay area august catch friend talk people tailscale horror etc feel free contact around would like meet
114,Lobsters,scaling,Scaling and architecture,"Hugo and IPFS: how this blog works (and scales to serve 5,000% spikes instantly!)",https://withblue.ink/2019/03/20/hugo-and-ipfs-how-this-blog-works-and-scales.html,hugo ipfs blog work scale serve spike instantly,update june fun fact reading article using distributed web run static website ipfs hacker news reddit effect importance normalizing unicode string released hereditas traffic higher nothing meet hugo generate html page using hugo italypalealewithblueink replace jekyll hugo previous ipfs guide meet ipfs ipfs popular document replicated akin cdn na ipfs cluster vms previous article let see architecture build azure pipeline release job release pipeline dnslink ipfs address gatewayipfsio ipfscompanion distributed web gateway learning realworldexperience overall positive experience going well learnt could go better ipfs previous article issue purge cache ipfs gb,update june blog served ipfs anymorefun fact reading article using distributed web since midfebruary blog blue ink served ipfs cloudflare distributed web gatewaylast november blogged run static website ipfs already running couple apps way used family felt time migrate blog took bit longer expected dealt issue explained around month ago flipped dns switch definitively turned singleinstance vm hosting blogthat decision made anxious moment month thing look almost entirely goodthe hacker news reddit effect tsince migrating ipfs something happenedjust week ago published blog post around importance normalizing unicode string went almost viral peaking front page hacker news gaining top spot rprogramming included popular newsletter thanks love great discussion monday released hereditas new open source project got pretty good exposure reddit toofor blog used average le page view per month happened wednesday march traffic higher day week earlier single day blue ink got almost double page view typical month thatdespite significant increase traffic happened cpu usage one main ipfs node serving website nothingthanks distributing content via ipfs serving via cloudflare cdn blue ink saw virtually impact performance availability following traffic spikenot test show website incredibly fast user around world consistentlymeet hugowith blue ink static website write content bunch markdown file generate html page using hugo entire website content theme script open source published github italypalealewithblueinkwhen started blog three year ago originally chosen jekyll another popular static site generator however working migration ipfs replace jekyll hugo jekyll support relative url jekyll generated url start fixed base url work browsing content via ipfs base url dynamic see previous ipfs guide detail matter migrating hugo brought great benefit hugo small app written go much much faster jekyll ruby gem hugo speedier building website really feel almost instant thanks single selfcontained binary also installs much faster ci environment ci build went five minute le one also hugo lot powerful interesting feature actively maintainedmeet ipfsthe interplanetary file system ipfs protocol network distributes immutable content peertopeer fashionif familiar ipfs think distributed cdn start ipfs node use publish document ipfs network people around world request directly best thing soon someone request file immediately start seeding others mean using ipfs popular document replicated faster others download itdistributing file ipfs fast resilient thanks distributed peertopeer ipfs network resistant censorship ddos attacksadditionally document ipfs addressed hash content also tamperproof someone change single bit file whole hash would change address would differentthe problem ipfs content distribution protocol storage service akin cdn na still need server currently three configured cluster ipfs cluster small inexpensive vms vcpu gb ram azure running three different region around world read set previous articlethanks using ipfs simple relatively inexpensive solution able deliver uptime ddosresistant website automatically replicated across node cluster start seeding right away vms geographically distributed user get great speed around worldlet see architecturethe architecture blog relatively simple pushing new code master branch github trigger automatic build azure pipeline clone source code run hugo build website free see configuration azurepipelinesyaml file inside repoafter build done azure pipeline trigger automatic release job release pipeline two stage read configured ipfs article copy file one ipfs vms via ssh invoking ipfsclusterctl pin add command add document cluster replicate across nodesmake rest call cloudflare apis update dnslink txt dns record dnslinkwithblueink containing ipfs hash websitewhile first stage happens automatically gate requiring manual approval administrator stage two run let test make sure website loading successfully via ipfs using full hash making available anyone visiting withblueinkafter release pipeline done anyone running ipfs daemon visit website ipfs address simple easy remember work ipfs daemon running know use gateway eg try gatewayipfsio curious try ipfs ipfscompanion extension firefox chrome let browse ipfs network easily external gateway builtin onemost user still using http normal web browser cloudflare come assistance free distributed web gateway edge node cloudflare network act ipfs gateway serve document published via ipfs network set simple cloudflare manages dns thanks cname flattening use root domain withblueink without www learning realworldexperiencei serving web apps ipfs six month blog month overall positive experience learnt thing worth sharing looking using ipfs yourselfwhat going wellin general relying ipfs delivered interesting benefit uptime document ipfs network long least one peer serving content recently viewed website kind client pinned three server blog reachable ipfsspeed user visit website ipfs faster becomes everyone elsethe website also ddosresistant natural wayin reality however user access blog ipfs instead visit http cloudflare gateway still worked fairly well since document ipfs immutable cloudflare caching website extensively edge node around world need cdn connect upstream server check new content long dnslink latency test multiple location worldwide show consistent speedy page load time blog front page fully load including image around second fresh cache le consistently every corner planet quite impressivesetting thing really simple besides pointing cname cloudflare gateway asking enable tl certificate domain thing work need configure highavailbility loadbalancing replicating content across multiple server etcthe cloudflare cdn also amazing thing including supporting http spdy gzipping response etcwhat learnt could go betterhttp turned thirty year old month ipfs still new technology ipfs thing work differently used others work allipfs serverless also definitely free need least one server seeding data good news need large server burstable vm offer enough cpu running ipfs cluster however need gb memory adding three node like probably overkill great learning really fun url website must relative explained detail previous article wrote ipfs short user visit website multiple base url case http withblueink http gateway ipnswithblueink http gateway ipfs ipfshash use absolute url html page also main reason switch jekyll hugoas wrote user browse website ipfs directly rather cloudflare mean actual uptime depends cloudflare working fine far offer sla free service even le clear sla ipfs gateway sadly data many visitor use ipfs moment expect tiny minoritywhen using cloudflare ipfs gateway certain thing available including ability set custom http header problem two case want enable hsts simply way want manually set contenttype ipfs gateway determine content type file extension using heuristic see issue custom pagesno serverside analytics even cloudflare option use hosted solution like google analyticsanother issue noticed cloudflare ipfs gateway always reliably purge cache change value dnslink take hour everyone see uptodate content biggest issue farafter updating dnslink value bit cold start time issue first page taking extra second load experience bad happens ipfs client cloudflare gateway need traverse dht find node serving content soon content replicated becomes faster faster point issue anymorelastly one issue experienced running ipfs node use quite bit bandwidth making network work even serving content greatly mitigated ipfs azure vms still measuring around outbound traffic gb ipfs many issue including caching coldstart time serverside analytics custom http header page could mitigated implementing custom ipfs gateway rather relying cloudflare official ipfsio website something considering issue caching cloudflare improve
115,Lobsters,scaling,Scaling and architecture,Stacking Theory for Systems Design,https://medium.com/@jlouis666/stacking-theory-for-systems-design-2450e6300689,stacking theory system design,stacking theory system design mode operation stopping development production cloud stacked design one level,stacking theory system designin recent year adopted method system design think yield good result lack better word overloaded stack yet use metaphor designas everything else silver bullet design equally good tradeoff come erlang background course design going influenced design think method widely applicable however could easily use preferred programming language order make thing clear first little bit erlang terminology topdown perspective erlang release comprises erlang runtime system containing interpreter garbage collector builtin part standard library need code module release completely selfcontained requires outside library order able run essentially almost dockercontainer inside erlang release application either standard system kernel stdlib crypto application building release release manager figure dependency among application wish run make sure include themeach application consists process process sits somewhere supervision tree process erlang terminology point concurrent execution operate independently communicates via message passing one physical core execute parallel systemthe code process run resides module single process run code many module module run many process module roughly static component insofar exist time programmer writes included application process dynamic sense exists runtime executing code modulesin erlang system process also defines boundary isolation state process scrutinized outside process way interact process send message wait replymodes operationwhen boot erlang system go several mode operation first runtime executed start booting initsequence sequence initialization begin loading module system avoids accidentally missing module later ononce every module loaded go configure application start configuration level usually stop system anything wrong configuration error human error aborting often better trying cope mistake configurationat point system operationalhowever tend call baseline system nothing wrong system rather claim operational like cut operational behavior stack baseline level stack try move system upwards stack adding another level important stress transitioning best effort method make attempt increasing operational level system stay put current levelwe could instance introduce database connection pool system level start pool spawn proxyprocesses connection worker try connect level guarantee baseline system running able carry service client connection database transition level periodically try connect connection proceed higher operational level get database connection log fact might raise alarm system order tell devops something amissthe key made level assume connectivity database system would far le flexible suddenly experience intermittent transient network error level baseline must terminate system whole stacking service go back level start besteffort transition level structure ratchetmechanism erlang system higher level continue operating error lead faulttolerance handling acting upon system move operating level resetting restarting affected process continues operation lower levelnote safe state database connectivity stable state try establishing one error occurs internal state single database connection complex model trying recover state insanely complicated terminate connection reset known good state connected system level transition level level may connect message queue broker thing applies database connection make besteffort getting error reset u level database connection need broker connectionand level try go level enable cowboy web server listen port point assume level system underlying part must operational hence give service outside world introduce loadbalancer callback service tell loadbalancer able give service point entered service pool loadbalancer system operates nominallyan error level single process give rise total system failure reset many error transient intermittent erlang fault tolerance principle defines policy threshold deem operation level failure due many error short timespan solution course gradually try resetting web server internals fault removed worst case reset lower operating level stackstoppingonce leveled stack operation stopping service becomes easy stop going level first remove listen socket request enter system drain request currently operating level move level point terminate systema common mistake botch closedown procedure system test usually load system try terminate loaded often see crashing burning closedown phase request currently running fails nonstandard way problem modern world use elastic computing machine added removed time automatically load requires happening situation load balancer coerced participate closedown procedure help since drain connection always casedevelopment productionanother test say start system development environment connectivity system need database central logging metric broker system fails boot due network connectivity problem chance fail boot production system assuming presence system unwritten dependency chain boot production system certain order thing work often bound troublefurthermore suppose decide move database another address one advantage stacked design often add another database pool system without yet allows deploy system first await presence new database cluster system pick automatically start using building system least one system able handle transition infrastructure build system far easier manage often use trick add new restful endpoint deploy production start using avoids u coordinate client launch approval committee apple might prove problem extending trick infrastructure nice support version protocol time decide move one put onus later decisionmakerin development stacked design also nice may need start centralized logger og metric gatherer developing system need test something metric logging simple invocation netcat nc tool suffices help production lost metric server coordinate main service take wellthe cloudcloud environment notoriously flaky weekly disconnect among service small disruption common need build system tolerate small amount noise stacked design excellent tolerating noise instance lost single database connection pool pick another try replenishing lost connection lost back level advantage tolerating small amount noise often tolerate larger amount connection error rate suddenly rise system cope picked absolute path everything entirely correct time made reboot failure thousand time likely hurt serviceuse alarm handler erlang one built set clear alarm connection outside world say second raise alarm turn system going tell world problem rather poor devops person figure wrong system saying broken connection database thus give service far better one rolling around rebootrestart loop timestacked design one level upfinally stacked design stop servicesystem use one level architecture well far better deploy machine starting empty machine configuring installing software starting contrast mutation existing machine rebuilding environment essentially build whole data center scratch every time make sure safe even system get hosedwhen error occur find core dump erlang crashdump file log file ship elsewhere wipe machine build new one take allows postmortem analysis error keeping system operational postmortem analysis paramount want stable system need distinguish error benign due rare event chain error need programmer fix done analyzing error occur note benign error system must never fixed fixing certain error requires much change code base change pose greater risk benign error unless find elegant approach problem fix themalso make ordering assumption architecture u operate environment error occur break ordering assumption running build system cope stacked design long always one stack increase level system eventually untilt especially important microservice architecture dependency tend complex nobody actually tested possible interactionsfinally stacked design cover bad system design long system cope failure often saving grace architecture deploying system able cope trouble avoid total failure nygard circuit breaker pattern come handy software
116,Lobsters,scaling,Scaling and architecture,The Cult of Kubernetes,https://christine.website/blog/the-cult-of-kubernetes-2019-09-07,cult kubernetes,cult kubernetes old state world dokku digitalocean managed kubernetes cluster terraform dyson configuration script example app templated github action github action ci website commit log deployment manifest tell hcl yaml quite excellent documentation digitalocean marketplace readmes monstrosity tutorial repo premade action contact howto,cult kubernetes got blog onto autodeployment via github action world simple place thing used make sense least nt many layer became difficult tell hell going complexity happened tale literally recreated meme deployed blog one reading right kubernetes old state world deployed blog kubernetes used dokku year dokku great emulates heroku git push nt care workflow server selfmanage blessing curse real advantage managed service like heroku literally hand operation heroku team case dokku unless pay someone lot money going manage server dokku server unmanaged run many apps listing taken started move apps apps bsnk cinemaquestria fordaplotbackup graphvizchristinewebsite identicond ilokesi johaus maison olin printerfacts since tulpaforcetk tulpanomicon enough apps plus already migrated really nt make sense paying something like heroku really make sense use free tier either decided time properly learn kubernetes set create cluster via digitalocean managed kubernetes cluster decided would good idea create cluster using terraform mostly wanted learn use better use terraform work figured would also way level skill mostly sane environment creating playing small terraform wrapper tool called dyson tool probably overly simplistic written nim config configdysondysonini simplify terraform usage moving secret terraform code directly also avoid api token exposed shell avoid accidental exposure secret dyson simple use dyson usage dyson subcmd subcommand option parameter subcmd one help print comprehensive percmd help apply apply terraform code production destroy destroy resource managed terraform env dump envvars init init terraform manifest generate somewhat sane manifest kubernetes app based argument plan plan future terraform run convert herokudokku slug docker image dyson h help args print message dyson helpsyntax give general cligen syntax help run dyson help subcmdsubcmd help see help subcmd run dyson help get comprehensive help wrote config maintf provider digitalocean resource digitaloceankubernetescluster main name kubermemes region varregion version varkubernetesversion nodepool name workerpool size varnodesize nodecount variablestf variable region type string default variable kubernetesversion type string default variable nodesize type string default ran dyson plan many line plan output dyson apply many line apply output working mostly unconfigured kubernetes cluster configuration thing started go downhill wanted thing cluster could consider ready use deploying application wanted following lot trial error pain suffering like created script pasting look want get streamlined overview set thing set deploy example app manifest look something like apiversion kind service metadata name hellokubernetesfirst annotation externaldnsalphakubernetesiohostname exanplewithinwebsite externaldnsalphakubernetesiottl optional externaldnsalphakubernetesiocloudflareproxied false spec type clusterip port port targetport selector app hellokubernetesfirst apiversion kind deployment metadata name hellokubernetesfirst spec replica selector matchlabels app hellokubernetesfirst template metadata label app hellokubernetesfirst spec container name hellokubernetes image port containerport env name message value henlo exanple deployment apiversion kind ingres metadata name hellokubernetesingress annotation kubernetesioingressclass nginx letsencryptprod spec tl host exanplewithinwebsite secretname prodcerts rule host exanplewithinwebsite http path backend servicename hellokubernetesfirst serviceport time wondered making mistake moving dokku dokku really lot abstract almost everything involved nginx away really show however side effect everything declarative kubernetes really assuming anything lot freedom basically anything want nt specially magic name task like web worker like herokudokku deployment belongs app happens expose tcp port happens correlating ingres associated lucky apps write fit general format one nt mostly use format without ingres templated sucker subcommand dyson let command like dyson manifest namehlang domainhchristinewebsite dockerimagedockerpkggithubcomxexh useprodletrue kubectl apply f service get shunted cloud without extra effort part also automatically set let encrypt dns thing manual dokku setup save time want go add service future create docker image somehow identify port exposed give domain name number replica send merry way github action however mean deployment longer simple git push nt care github action come play claimed ability run full endtoend cicd application using ci website pleased decided give try set continuous deployment commit log deployment manifest tell took lot trial error one main source problem github action recently lot change made configuration usage compared private beta included changing configuration schema hcl yaml course documentation outside github quite excellent documentation date wrong tried following tutorial digitalocean exact thing wanted referenced old hcl syntax github action work make thing worse example marketplace readmes simply work written old github action syntax frustrating say least trying make work anyways combination use latest version button marketplace prayer gratuitous use withargs field step gave decided manually download tool needed upstream provider execute hand ended monstrosity name configuredeployverify kubernetes run curl l http tar xz doctl auth init digitaloceanaccesstoken doctl kubernetes cluster kubeconfig show kubermemes kubeconfig curl lo http storagegoogleapiscomkubernetesreleaserelease curl http storagegoogleapiscomkubernetesreleasereleasestabletxt chmod x kubectl kubectl kubeconfig kubeconfig apply n apps f deployyml sleep kubectl kubeconfig kubeconfig rollout n apps status deploymentchristinewebsite env digitaloceanaccesstoken secretsdigitaloceantoken almost certain wrong nt know robust sure done another way thing could get working definition working edit got fixed see git push thing master branch blog repo automatically get deployed kubernetes cluster work digitalocean reading post please get someone update tutorial readme repo example listed work private beta github action would also nice better documentation use premade action usecases like mine wanted download kubernetes configuration file run apply yaml file edit complaint fixed see simpler way thing thanks reading hope entertaining well share mastodon article posted fact circumstance may changed since publication please contact jumping conclusion something seems wrong unclear series howto tag kubernetes digitalocean githubactions
117,Lobsters,scaling,Scaling and architecture,Dealing With Software Collapse,https://hal.archives-ouvertes.fr/hal-02117588/document,dealing software collapse,,obj type catalog version page r openaction r fit name r outline r endobj obj creator title subject author producer creationdate keywords moddate trapped false ptexfullbanner pdftex version tex live kpathsea version endobj obj type page kid r r r r r count endobj obj resource r type page parent r content r annots r r cropbox mediabox rotate endobj obj dests r endobj obj type outline first r last r count endobj obj type page content r resource r mediabox parent r group r annots r r r r r cropbox rotate endobj obj type page content r resource r mediabox parent r annots r r cropbox rotate endobj obj type page content r resource r mediabox parent r cropbox rotate endobj obj type page content r resource r mediabox parent r annots r cropbox rotate endobj obj font r xobject r r procset pdf text imagec imageb imagei endobj obj length filter flatedecode stream z w l j y  endstream endobj obj type annot subtype link border c r rect endobj obj type annot subtype link border c r rect endobj obj kid r r r r limit docstart section endobj obj title reference r parent r next r endobj obj title biography r parent r prev r first r last r count endobj obj length filter flatedecode stream vc n h q u c z nl j rm e x px e ad bh r h ro u endstream endobj obj font r xobject r procset pdf text endobj obj type group transparency true c devicergb endobj obj type annot subtype link border h c rect r endobj obj type annot subtype link border h c rect r endobj obj type annot subtype link border h c rect r endobj obj type annot border h c rect subtype link r endobj obj type annot subtype link border h c rect r endobj obj length filter flatedecode stream x i jeo  h aix z  lv f r x tq n c f c z ul r  endstream endobj obj font r procset pdf text endobj obj type annot subtype link border h c rect r endobj obj type annot subtype link border h c rect r endobj obj length filter flatedecode stream x f g p v pi g mg wg b  w b g   b r bq tq x endstream endobj obj font r procset pdf text endobj obj length filter flatedecode stream v k r endstream endobj obj font r procset pdf text endobj obj type annot border h c rect subtype link r endobj obj r r r r endobj obj length filter dctdecode metadata r intent perceptual colorspace iccbased r type xobject subtype image width height bitspercomponent stream mm b j adobe photoshop macintosh  h h adobecm adobe z p c z
118,Lobsters,scaling,Scaling and architecture,Modern applications at AWS,https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html,modern application aws,invent launch reinvent relaunch start rinse repeat amazoncom modern application development architectural pattern microservices data management purposebuilt database onetoone mapping database microservice amazon neptune amazon elasticache software delivery automated release pipeline operational model serverless possible aws lambda security everyone responsibility journey yelp edmundscom bynder,innovation always part amazon dna year ago went radical transformation goal making iterative invent launch reinvent relaunch start rinse repeat faster change made affected built application organized company back small fraction number customer amazon serf today still knew wanted expand product service offered change way approached application architecture giant monolithic bookstore application giant database used power amazoncom limited speed agility whenever wanted add new feature product customer like video streaming edit rewrite vast amount code application designed specifically first bookstore long unwieldy process requiring complicated coordination limited ability innovate fast scale created blueprint change distributed computing manifesto internal document describing new architecture manifesto began restructuring application smaller piece called service enabled u scale amazon dramatically changing application architecture half story back every amazon development team worked application every release application coordinated across every team support new approach architecture broke functional hierarchy restructured organization small autonomous team small enough could feed team two pizza focused twopizza team specific product service feature set giving authority specific portion app turned developer product owner could quickly make decision affected individual product breaking organization application structure bold idea worked able innovate customer much faster rate gone deploying dozen feature deployment year million amazon grown perhaps dramatically success building highly scalable infrastructure ultimately led development new core competency resulted founding aws continue work twopizza team today alone quest innovate quickly remain competitive company must increase agility continually uncover new opportunity create better product customer embarking journey amazon moving modern application development new approach requires shifting monolithic architecture component microservices best practice modern application involve much changing way design build technology also need rethink manage succeed using application development increase agility innovation speed organization must adopt five element order microservices purposebuilt database automated software release pipeline serverless operational model automated continuous security architectural pattern microservices company like amazon start business monolithic application fastest easiest system develop however problem tightly combining process running single service one process application experience spike demand entire architecture scaled handle load one process moreover adding improving feature becomes complex code base grows making difficult experiment implement new idea monolithic architecture add risk application availability many dependent tightly coupled process increase impact single process failure microservices emerge company grow microservices architecture application made independent component run application process service service built business capability online shopping cart service performs single function run independently managed single development team service updated deployed scaled meet demand specific function application shopping cart example support much larger volume user sale organization move monolith microservices many developer find want manage dependency within service create new way packaging application running code innovation instance longer compute option also use container aws lambda function container popular option packaging code great tool modernizing legacy application offer excellent portability flexibility application setting lambda get simplicity code write business logic another consideration microservices need way communicate many application continue use api connection several option sending data service include streaming realtime data processing event triggering response data change service mesh applicationlevel communication observability choose integration method best meet application need data management purposebuilt database modern application built decoupled data store onetoone mapping database microservice rather single database important shift traditional application architecture monolithic application pose scaling fault tolerance challenge grows database plus single database single point failure hard single database meet specific need set varied microservices decoupling data along microservices free choose database best fit need many application best choice would still relational database many apps different data need example run application work highly connected datasets recommendation engine could choose graph database like amazon neptune store navigates relationship application need realtime access data could choose inmemory database like amazon elasticache commonly used gaming iot apps general best database database exactly microservice need software delivery automated release pipeline moved away monolithic architecture amazoncom reorganized twopizza team stopped using single release pipeline began enabling team release independently removed coordination challenge making delivering update decentralizing development release process presented new set challenge maintaining release process quality consistency across team became difficult especially step release process manual created possibility human error solution twopronged standardization automation first defined software delivery process bestpractice template provide standard modeling provisioning infrastructure resource cloud environment infrastructure code template help team get started right foot template provision entire technology stack application code rather using manual process amazon ensures team configuring process deployment according requirement second started using automation remove manual process software delivery workflow automated release pipeline including continuous integration continuous deployment cicd rapidly test release lot code minimizing error ci team regularly merge code change central repository run automated build test detect problem early cd team commit change multiple time day flow production without human touch first found deploying without human intervention scary invested time writing right test failsafes found dramatically increase speed agility also improved quality code operational model serverless possible modern application lot moving part rather single application database modern application may composed thousand service purposebuilt database team releasing new feature continuously moving part divided two category activity part company secret sauce make successful market like creating unique user experience developing innovative product activity often refer undifferentiated heavy lifting task must get done nt provide competitive advantage business task include thing like server management load balancing applying security patch introduced concept serverless launch aws lambda compute service let run code without provisioning managing server support overall goal helping customer optimize resource around secret sauce offloading undifferentiated task aws become critical element modern application development going serverless free focus activity set company apart like product innovation say serverless referring service run without need infrastructure provisioning scaling builtin availability security use payforvaluebilling model serverless nt entire application stack application stack typically consist three component compute service like aws fargate run application logic data store like mysql postgresql relational database amazon aurora persist data integration layer like event bus amazon eventbridge move data serverless building block enable company construct application maximize benefit serverless model amazon completely serverless moving direction many customer fact anticipate soon whole generation developer never touched server write business logic reason simple whether building net new application migrating legacy using serverless primitive compute data integration enables benefit agility cloud offer security everyone responsibility past many company treated security magic sprinkle application ready release nt work well continuous release cycle organization take new approach security building firewall around entire application also introduced challenge security setting applied every piece application problematic application built independent microservices may need different setting reason modern application security feature built every component application automatically tested deployed release mean security longer sole responsibility security team rather deeply integrated every stage development lifecycle engineering operation compliance team role play security integrated within tooling like code repository build management program deployment tool applied release pipeline software released pipeline serverless service security posture even easier maintain underlying infrastructure operating system version update software patching built service journey customer actually making change modernize application although single path common pattern including approach took amazon made decision focus innovation dramatically scale amazoncom refactored monolithic application restructured organization optimized cloud automation abstraction customer like yelp taken similar path customer starting application hosted premise common approach rehost lifting shifting application cloud many customer start leveraging managed service cloud offloading thing like database api management aws focus business logic today customer taking path reinvention building new application serverless microservices enable organization take full advantage cloud correct way modernize aws platform application coexist state interact successfully path common thing seen though customer build modern application see benefit across entire business especially allocate time resource spend time logic defines business scale system meet peak customer demand easily increase agility deliver new feature market faster often example edmundscom offer uptodate vehicle information car shopper reduced time needed roll new website feature six month one week startup bynder also decreased time take bring product market one year one month changing way approach technology company improve way business power modern application
119,Lobsters,scaling,Scaling and architecture,"Stepping on the ""Scale"" with Kubernetes",https://tech.okcupid.com/discovering-the-requirements-for-high-availability/,stepping scale kubernetes,example service architecture naive approach daemontools next level goal kubernetes achieves goal next,okcupid app phone update new version close relaunch app complete update happens every week time update get choose restart app however backend software okcupid update multiple time per day update nt wait ready imagine everyone world stop using okcupid app minute backend software updated new version would unacceptable instead need backend software work time called high availability perfect uptime realistic achieve get pretty close addressing several common cause downtime two cause planned downtime maintenance unplanned downtime due hardware failure cause solution end mostly software highly available need multiple redundant instance software running need communicate one need way picking one instance talk two piece got tool need solve high availability example service architecture okcupid backend comprised multiple service serve incoming request make request service asciiart diagram might look like api messageservice notificationservice db doubletakeservice voteservice profileeditservice request come via publicfacing api routed either messageservice doubletakeservice profileeditservice service might communicate another service database course actual backend much complex order different service rather pictured service longrunning server process deployed machine need instance service worth pointing backend nt one big piece software called backendexe broken multiple process communicate tcp socket want deploy new version backend need maintenance service actually changing example notificationservice naive approach let state goal choose solution need able maintenance software disruption service example deploying new version need able endure amount hardware failure without significant disruption let use example one machine losing power suddenly simple effective approach run multiple instance service spread across multiple machine might look something like messageservice notificationservice doubletakeservice voteservice profileeditservice messageservice notificationservice doubletakeservice voteservice profileeditservice messageservice notificationservice doubletakeservice voteservice profileeditservice case hardware failure shuts suddenly still got plenty instance service running machine maintenance use case need restart one instance service time always instance time serve traffic seems simple enough one big detail addressing yet want talk service pick instance talk better pick instance reason service appear unavailable need way know time instance okcupid system called membership decent job solving problem registry instance service running includes information instance running instance start register instance shuts maintenance deregisters event unexpected shutdown instance registration time minute want contact instance service fetch complete registry pick one instance known system worked pretty well several year one implementation detail include service actually running machine use daemontools start service restart crash worked pretty well several year next level goal serving traffic high availably several year get requirement need add machine need scale number service able handle throughput need able maintenance machine upgrading operating system mean hour nothing running machine one inconvenient implementation detail membership number instance service preconfigured changing configuration tricky challenging scale add machine scale service run uneven balance instance running different machine perhaps need add new instance doubletakeservice add two new machine obvious thing would add instance doubletakeservice onto two new machine add new machine scale messageservice time eventually end uneven distribution service want maintenance machine going need stop instance running instance might necessary performance might unused compute resource machine could move service instance machine onto available machine requires reconfiguring daemontools run file carefully bringing old instance bringing new one remember number instance preconfigured membership ca nt new one old one running time juggling act doable error prone daemontools designed rebalancing instance across different machine okcupid continues grow needing rebalances often time look better handling use case kubernetes achieves goal rather focusing solving specific problem like automatically move daemontools run file machine let step back enumerate actual goal include mention daemontools implementation detail objective need following without disrupting service deploy new version software service rescale number instance software service add remove machine tolerate singlemachine failure provide way software connect working instance given service really nt care machine service running need somewhere run wherever end software need know contact kubernetes designed goal mind kubernetes requires software deployed container usually docker container make system dependency problem go away could write whole article updating glibc version container much nicer deal let get back talking kubernetes service like doubletakeservice deployed kubernetes config file look like kind service metadata name doubletakeservice spec port port type loadbalancer kind deployment metadata name doubletakeservice spec replica template spec container image doubletakeservice name doubletakeservice port containerport greatly simplified example nt try config home kid image doubletakeservice specify executable code service example doubletakeservice would identify docker image version also declared service listens port want instance replica service running kubernetes take specification make reality host cluster utilized reasonably balanced way run instance host added kubernetes cluster become available handle instance without author doubletakeservice configuration even needing aware host removed cluster maintenance instance running host automatically transferred host machine dy kubernetes constantly monitoring host detect failure automatically transfer instance host want scale service change number replica replica anything else kubernetes start instance want deploy new version service change image doubletakeservice say instead kubernetes start instance new version shut instance old version operation complex scaling instance many configurable option controlling final requirement ability software connect working instance service first block configuration come service type loadbalancer however many replica service kubernetes know kubernetes put kubernetes maintains dns entry accessible name doubletakeservice resolve one working service instance client nt need fetch manage registry instance location instead connect doubletakeservice next although far abandoning old system look promising kubernetes though perfect take much toil risk away scaling upgrading backend software
120,Lobsters,scaling,Scaling and architecture,Proposal: Code Cognitive Load,http://tiny-giant-books.com/Entry1.html?EntryId=rece2L2D3B2ZeyHTe,proposal code cognitive load,,
121,Lobsters,scaling,Scaling and architecture,Software Architecture Guide by Martin Fowler,https://martinfowler.com/architecture/,software architecture guide martin fowler,,people software industry talk architecture refer hazily defined notion important aspect internal design software system good architecture important otherwise becomes slower expensive add new capability future like many software world long wary term architecture often suggests separation programming unhealthy dose pomposity resolve concern emphasizing good architecture something support evolution deeply intertwined programming career revolved question good architecture look like team create best cultivate architectural thinking development organization page outline view software architecture point material architecture site
122,Lobsters,scaling,Scaling and architecture,The Role of Qualification Gates in Getting to Beta and Beyond,https://storj.io/blog/2019/08/the-role-of-qualification-gates-in-getting-to-beta-and-beyond/,role qualification gate getting beta beyond,background qualification gate last town hall file durability service availability segment retrievability upload performance cosbench download performance cosbench proven capacity number vetted node vetted node churn gate chaos monkey caveat,today mark really exciting day storj lab launched first beta storj software simultaneously launched beta tardigrade cloud storage service expect first many decentralized storage service built many organization using storj softwarethis post discus important qualification gateswe established major milestone pioneer beta pioneer beta voyager production also discus stand term meeting gatesbackgroundour goal tardigrade ambitious aim deliver first truly enterprisegrade decentralized cloud storage service mean deliver security scale privacy promise decentralization must also deliver enterpriseclass durability availability performance support also measure comparable wellestablished cloud service also aim fraction price also need provide proper economic support incentive large growing network storage node operation open source partner demand side partnersthis easy task introducing new storage technology difficult cloud storage even bigger uphill battle especially given well established generally wellregarded cloud service offered largest company world hitting lofty goal decentralized network enterprisegrade characteristic much difficult building centralized cloud service fact really ever done storj network launched achieved scale rapidly reached pb capacity globally distributed node steady usage faster expected however deliver durability performance comparable centralized player expansion factor eight ie consumed gb total storage every gb stored made difficult make network work economically ensure network sustainably enterprisegrade took different approach compliant expansion factor promise far better durability performance availability also proceeded much deliberately emphasis code network growth also network quality stability performance one important element achieving adoption qualification gatesqualification gatesif listened last town hall learned established formal series qualification gate two tardigrade beta informed u going production general get one chance win customer cloud storage space help ensure truly enterprisegrade holding gate gate specific objective criterion must meet order officially move particular phase post like go bit detail gate various phase stand nowwe established set gate going major phase subset gate find outlined table first column list gate second column list third fifth column see gate entering pioneer beta pioneer beta finally voyager production item green met see met gate heading beta many case also met exceeded gate beta productiona note explanation gate file durabilitythis perhaps important gate goal file ever lost last major alpha release month ago lost single file segment ie durability production goal nine far likely win lottery struck lightning lose file tardigrade measure durability randomly downloading large number file also looking segment durability random test sample segment durability look entire population segment reminder file broken one segment mb segment using reed solomon divided piece used reconstitute segment part piece located different storage node network segment would lost number available piece dropped however long point would initiate repair process reconstituting piece theory never lose file since node low correlation risk operating different hardware different power supply different network different operator etc see graph constantly measure number piece available segment network report median number piece segment distribution piece segment lowest number piece mean least healthy segment never dipped piece median holding steady around case many metric durability improve add node network broader distribution nodesfigure segment health chart segment must minimum piece reconstituted service availabilityservice availability measure availability satellite currently availability production satellite meaning service whole unavailable four minute given month production goal ie service unavailable two minute given month four second per day segment retrievabilitythis measure many attempt download segment per day successful test randomly selected segment every minute far seeing success rate ie le one attempt one thousand successful first attempt mean file gone durability mean second attempt necessary generally satellite available fewer node available serve piece either temporarily offline max capacity production goal trying take number even higher ensure retrievability large file composed multiple segment retrievability also improve add node network upload performancewe measure time take upload mb file repeat test file compare result aws generally considered gold standard centralized cloud service look median time upload also look long tail performance across really broad range file size location comparable time faster awsfor beta pretty encouraging moreover really tight distribution percentile time slower median time ie slowest file uploaded almost fast median point power decentralization get better add node continue add node distributed closer end user ultimately speed light becomes factor result uploading location virginia ie eastern u conjunction satellite iowa ie central u look uploading belgium satellite iowa time still look really goodnb intend publish result different file size different geography beta also exploring cosbench benchmarkingdownload performancewe measure time take download reconstitute mb file repeat test file compare result aws across really broad range file size location comparable time faster aws median download time second especially excited tight distribution percentile time slower median time ie slowest file uploaded almost fast median point power decentralization get better add node continue add node distributed closer end user ultimately speed light becomes factor result downloading location virginia ie eastern u conjunction satellite iowa ie central u look downloading location belgium conjunction satellite iowa time still look really goodnb intend publish result different file size different geography beta also exploring cosbench benchmarkingproven capacityour sno operator offered pb capacity network however keeping trust verify approach tend look proven capacity storj lab partner uploaded lot test production data network alpha pb data network production goal pb proven capacity expect exceed goal wide marginnote believe capacity network significantly higher held conservative proven number number order magnitude lower network capacity pb peak several partner beta customer several petabyte capacity aiming grow network gradually generally three month excess capacity time help u ensure node receiving economically interesting payouts number vetted nodesthis number currently available vetted node node first join network probationary period node prove eg maintaining certain uptime performance level passing content audit vetting period node receives small amount noncritical data vetted node start receiving data test data must continue maintain uptime audit requirement avoid disqualification number vetted node table node excludes probationary node node temporarily offline node voluntarily quit network node disqualified eg due missing uptime audit requirement production goal fraction numbervetted node churnour current node churn excluding probationary node beta gate beta gate almost yet production goal getting close system resilient individual node even significant percentage node churn however performance economics statistic better bring average node churn downother gateswe wide variety gate include gate around code quality test coverage user storage node operator set success rate various capability payment success rate peak network also gate around enablement nonstorj lab tardigrade satellite aiming chaos monkey even chaos gorilla resilient productionwe hit seven gate beta hit additional two gate beta caveatswe achieved result date fairly small scale network user testing network addition including large number ospp partner developer waitlist including large customer large data requirement scale certainly expect hit hiccup encounter unexpected issue added extra beta cycle cautiously increasing rate add new user new snos network however sign looking really positive hope take brake completely next couple monthsstay tuned additional suggestion gate measurement please send way
123,Lobsters,scaling,Scaling and architecture,Serverless Functions With WebAssembly Modules,http://jamesthom.as/blog/2019/08/06/serverless-and-webassembly-modules/,serverless function webassembly module,http,http
124,Lobsters,scaling,Scaling and architecture,Building petabyte-scale analytics with BigQuery and HLL,https://medium.com/permutive/petabyte-analytics-with-bigquery-hll-af0f7a70b66d,building petabytescale analytics bigquery hll,new hope hyperloglog blog post go default hll key strength composability preaggregation periodic sketch generation scheduled query feature request google querying data inclusionexclusion principle wikipedia limited single parameter,new hope hyperlogloghyperloglog hll algorithm count distinct item multiset interesting primer found blog post interested purpose article need know count distinct item efficiently due statistical approximation trade accuracy accuracy controlled precision parameter bigquery case go default tradeoff increasing accuracy need store bigger sketch increase cost amount data need queried increasesa simple hll query bigquery would look follows simple query using hll bigquerythere two important function preceding query init initialises hll using provided column output sketch represents element second parameter precision higher number better approximation function get actual count distinct element laterin example publisher query return indicated count second saving second still within exact counthll key strength composabilityso thats look ok interesting part well init function generates sketch compose love functional programming permutive love thing compose hll via merge mergepartial functionscomposability allows u continue counting left merge result two count form complex countimagine table store preaggregated sketch row containing hll represents user id visited given page publisher website pageusers tablethere user visited homepage given article still want get count distinct user id first need merge sketch merging hll sketchesthis result new sketch represents user id page without double counting user visited page use extract function get final countbigquery provides convenience function called merge combine two step mergepartial followed extract single step using single step extract functioncomposability vastly enhances hll could take step use maximise performance minimise cost could preaggregate sketch based predefined attribute compose build extremely complex queriespreaggregation periodic sketch generationbigquery support scheduled query name say everything query run periodically appends result another table case query run daily append result another tablewe use cost saving feature bq generate previously mentioned bigquery optimises query multiple subqueries referencing table billed byte even though data used multiple place querythe obvious solution would generate table enumerates possible combination enumerate tablesbut couple issue sketch become increasingly tiny set wrote small set issue previouslyanother issue cost order equivalent select distinct count userid filtering select sketch table merge themfinally merging potentially million sketch quickly becomes slowso need better solution utilises hll composability better composable sketcheswe call table master table contain field necessary separate sketch query table directly instead using view use scheduled query generate sketch given daythis costefficient selected data order insert preaggregated table also bigquery bill selected column mean want select sketch country could adding country null select country countryhll way column referenced billedfinally master table structure might allow u write really small efficient query bigquery merge function accepted multiple argument better solution sadly possibleunfortunately currently supported made feature request google far taken upquerying dataso far talked two main thing use hll benefit performance composability utilising composability preaggregate reusable sketch turn used create complex querieshowever far done union set enough insightful report want filtering example let say another table group user country grouped countryusing traditional nonhll approach could something like country using traditional approachthe trouble storing sketch separately anymore need intersection talk storing sketch later post assume sketch stored separately reason unfortunately hll natively support intersection luckily solutioninclusionexclusion principleto quote wikipedia inclusionexclusion principle counting technique generalises familiar method obtaining number element union two finite setswhile main use case principle union equation rearranged get intersection instead case expressed b b binn word size intersection set set b equal sum individual size minus size union fortunately hll operator builtin look like intersection using hllit look great merge function limited single parameter mean sketch column ideally could something like following currently possible bigquerya second parameter passed mergegoing back result intersection end final count sketch important mean lose hll comparability hence must always final step query
125,Lobsters,scaling,Scaling and architecture,Scaling Engineering Teams via Writing Things Down and Sharing - aka RFCs,https://blog.pragmaticengineer.com/scaling-engineering-teams-via-writing-things-down-rfcs/,scaling engineering team via writing thing sharing aka rfcs,planning building something new capture plan short written document select people approve plan starting work send planning document engineer company everyone follow step rfclike process unlikely might sound process work scale really well power writing thing everyone agrees project done writing approach piece cake reviewer spreading knowledge across organization type information pushed people organization shape culture considerably tailoring process via iteration process scale request comment publication process baseui published reviewed rfc approved one thing help engineering scale greatly type rfc process place early,recently talking small midsize company sharing engineering best practice see u use uber would recommend tech company adopt growing one topic get raised eyebrow well aha moment one planning process engineering worked since early year uber working large company like microsoft smaller one like skyscanner two thing related planning always bugged first lack visibility others building built thing team second tech architecture debt accumulated due different team building thing differently approachwise qualitywise said way tackle issue pretty well using simple step word warning one step sound little crazy planning building something new inperson whiteboarding talking team member long clear get thing done capture plan short written document clear team relatively quick write nt go overboard select people approve plan starting work similar good quality gate merge pull request someone review make huge difference starting work project relevant people validate planned work senior engineer people team use feature send planning document engineer company let anyone everyone comment yes step probably sound crazy everyone follow step every project beyond super trivial complexity iterate lightweight rfclike process work well org company unlikely might sound process work scale really well handful engineer team thousand address issue visibility reducing techarchitecture debt also spreading knowledge engineer engaged day day one simple process recommend small medium tech team especially growth phase also process successfully used iterated uber going ten engineer couple thousand one power writing thing writing sharing writing others creates accountability also almost always lead thorough decision simple way increase code quality code review writing merging simple way meeting le waste time written agenda meeting write send decision action afterwards simple way run project fewer surprise team write planning share others u engineer hate time waster documentation often seen one time waster mostly boring planning often seen kind documentation natural tendency skip step efficiency like flip argument around saving time though everyone agrees project done writing approach piece cake need capture shared understanding approach architecture change made tricky part someone team able hour team member giving thumbsup reading usually see le straightforward meant talked important edge case forgot change could break part system thing often come writing plan great discussion realization project halfway done people nt agree project done lot change already sound like project take lot longer people think writing thing least give clearer picture reviewer spreading knowledge across organization writing good way organize one thought different exercise write someone else understand good idea specify need read document give thumbsup plan writtenup surest way make sure people actually read doc require confirming writing via eg comment part sending plan project built engineering organization via email something sound crazy people often worry creating lot noise true noise level definitely increase thankfully email filter easy create make easy filter kind email time lot le project plan sent even engineering size hundred thousand people typically expect type information pushed people organization shape culture considerably organization culture push engineering plan everyone example via email invite anyone comment set tone trust responsibility first month uber used ob reading planning doc variety team chiming one relevant domain knowledge eg technology first month learned lot team worked year working large company information visible day still look plan team make getting sense going outside little bubble finally allowing anyone everyone chime key part keeping consistent engineering bar across organization uber seen multiple case people one side organization realize another team side organization planning something similar done different approach often different quality bar example u team building new feature might considered part world team india pointing gap localization approach team transparency selfbalancing selfcorrection happens quite naturally tailoring process via iteration uber process described used early year detail planning review process worked refined company grew matured started email every engineer changed mailing list per domain backend mobile web template built engineer help convey information consistent matter company growing thousand engineer tooling built make process searching approving even easier interesting part iteration seen evolution template people review many engineering proposal often type question question like motivation work tested archtiecture change made common question seeing repeat engineer came template keep updated make reading writing plan easy give example roughly backend frontend mobileweb template evolved year ago since switched generated template backendmobileweb list approver abstract project architecture change service slas service dependency load performance testing multi datacenter concern security consideration testing rollout metric monitoring customer support consideration list approver abstract project ui ux architecture change network interaction detailed library dependency security concern testing rollout analytics customer support consideration accessiblity iterating customizing need engineering team key case template started include important thing cared thing like reliability scale security uber built many smaller service thing consider like load performance testing slas part domain accessibility became big focus mobile section made template get idea process scale uber called process rfc request comment given many similarity request comment publication process tech community process stuck started early enough engineering team small helped scale knowledge eliminate silo company kept growing rapidly scaled remarkably well ten engineer low thousand starting thousand engineer new challenge arise kind process good problem keep overly theoretical see rfc process happen open checking open source project baseui baseui web design system comprised modern responsive living component rfcs published reviewed development start rfc approved process unique uber far larger tech company like google facebook microsoft amazon came something resembles level rfc process scale thorough documentation doc pushed strictly reviewed standardized across different orgs different company tech company planning still adhoc without structured way recordingdistributing one thing help engineering scale greatly type rfc process place early give try iterate always start
126,Lobsters,scaling,Scaling and architecture,An Elegant Puzzle Book Review: An Overdue Read for Software Engineering Leads,https://blog.pragmaticengineer.com/an-elegant-puzzle-book-review/,elegant puzzle book review overdue read software engineering lead,elegant puzzle system engineering management transitioned engineering management book note went along elegant puzzle date handson perspective engineering management within highgrowth techfirst organization read long overdue book engineering manager lead management engineering management book like book rarely present best approach instead share worked starting devops transformation elegant puzzle engineering manager product manager engineer working highgrowth company find good read said want book bookshelf manager path buy book also decided write book book note irrational exuberance organization tool approach culture career appendix organization sizing engineering team productivity age hypergrowth one fastestgrowing tech company however real productivity killer system rewrite migration introduction system thinking pretty much difficult problem worth trying represent system migration sole scalable fix tech debt productivity age hypergrowth running engineering reorg quickly growing company two managerial skill disproportionate impact organization success making technical migration cheap running clean reorganization presenting senior leadership approach saying one important activity undertake engineering leader philosophy management core believe management ethical profession great relationship possible come together solve almost anything instead avoiding hardest part double best management philosophy never stand still continues evolve come contact reality culture make peer first team peer provide excellent feedback aware work thinking work similarly speed learning two suggestion rapidly expanding company make peer first team career role rocket ship hypergrowth weak predictor personal growth nt treat growth forgone conclusion growth come change something influence career level designation momentum etc keep learning something new time go performance cycle suspect widely shared experience appendix tool operating growing organization section line management middle management managing organization roll new process solve personal pain point handing process manager book found useful book could even better diagram first chapter struck odd start chapter le cohesive others briefest medium training career narrative model document share book real end designing interview loop appendix want book bookshelf amazon,recently read elegant puzzle system engineering management written larson working yahoo digg engineering manager two year uber leaving stripe right around started transitioned engineering management though nt know personally fact part experience come uber made curious dive book delight reading taking book note went alongan elegant puzzle date handson perspective engineering management within highgrowth techfirst organization read long overdue book engineering manager lead read many management engineering management book set book apart start right others end elegant puzzle waste time especially beginning covering generic manager toolkit giving feedback team building many book devote good chunk content instead talk engineering pain point come highgrowth organization team management fundamental place system slowing u many migration good way pay tech debt say much work enough people grow seniority evenly across team reading book full aha moment joined uber amsterdam office engineer started manage people moving engineering management three year later amsterdam office people doubling every year team tripled many problem faced challenge book discus detail challenge like setting visionstrategy team building robust hiring onboarding pipeline dealing neverending stream migration tone book casual feel like sitting coffee talk problem faced different company systemic approach seen work best giving example thing worked past like book rarely present best approach instead share worked healthy dose system thinking approach recommends peer manager teamthe book read well reference personally prefer physical book onethis book best resource come across engineering leader experience belt people work similar environment like advice relevant place considered top tier company meaning engineering environment strong leader technical compensation close topofmarket engineering valueadd cost centerworking amsterdam place silicon valley tech culture like uber seem exception norm many engineering lead europe work different environment different constraint manager often le technical place still starting devops transformation place like tech debt something business often want hear tech still seen cost center revenue generator manager working company idea book still valuable go company operate along principle outlined book hire retain easier also move faster competition implementing idea might involve challenging org status quo transforming culture betteran elegant puzzle engineering manager product manager engineer working highgrowth company find good read discipline working engineering recruiter operation find empathetic read head product large startup recently told devouring book found model book simple model building highperforming team really useful falling behind add people treading water pay debt recruiting manager read book said would highly recommend book colleague recruitment world aspire people manager book written leader engineering team concept translate situation managing people want book bookshelfif engineering lead highgrowth organization recommend physical book within arm reach together manager path working highgrowth tech company reading build lot empathy type problem engineering manager engineer deal day day environment hoping book first many software engineeringheavy management book definitely first kind read buy book hereoh also decided write book pragmatic software engineering practice within highgrowth startup tech company subscribe email update bottom post like kept loop development bookbook notesmy recommendation reading book dive relevant section starting table content chapter well subsection within chapter read independently summarising relevant part book quote stuck methe interesting thing book published draft chapter blog irrational exuberance wrote content found raw form blog linking easy readingthe book series independent problem area engineering manager face highgrowth environment read order book structure section loosely mapped chapter organization topic organization team designtools system thinking tool manage change org team individual levelapproaches common difficult situation engineering manager often find dealing theseculture effort shift culture within organizationcareers hiring evaluating performance promoting retaining peopleappendix tool operate organization line manager middle manager manager manager level well reading list organizationsthe book start chapter organizational design manager manager find chapter especially useful along people want insight reorgssizing engineering team first section section wish shared insight one opinionated piece would good hear led come size team prefers report manager highgrowth team also targeting around report per manager seeing worked people bandwidth however also dependent maturity team seniority manager anywhere direct report past year lower number strong preference well though yet put exact reason whyproductivity age hypergrowth one favorite section deja vu aha moment working uber one fastestgrowing tech company every hypergrowth something gotten used year weird see many system decommissioning time stressful oncall many production incident kept kept asking normal something wrong aha moment came observation saying system survive one magnitude growth company designing system last one order magnitude doubling every six month reimplement every system twice every three year creates great deal every platform team working critical scaling also create great deal resource contention finish concurrent rewriteshowever real productivity killer system rewrite migration follow rewrite poorly designed migration expand consequence rewrite loop individual team supporting system entire surrounding organizationother section chapter chapter managing change different level part overlap previous chapter come managing organizational changeintroduction system thinking recommend starting book intuitively using toolset book great example successfully applying tool across board developer velocity excellent example takeaway start thinking system find hard stop pretty much difficult problem worth trying represent system even without number plugged find powerful thinking aidsmigrations sole scalable fix tech debt favorite section together productivity age hypergrowth past three year uber migration always top mind top mind engineer working around refreshing get take different source take informed one section gold summarizing migration matter usually available avenue make meaningful progress technical debt tip running good migration derisk enable finish given done many migration charter felt short missing important tip making migration successful thing like purposebuilt monitoring shadowing reverse shadowing traffic well dealing fact migration almost always take longer due long tail lot complicated suspect minority one though perhaps queue write overdue post onerunning engineering reorg dive good reorgs run team grows fast reorg matter time easy get wrong hidden section thought strongly agree based wellrunning organization seen within uber quickly growing company believe two managerial skill disproportionate impact organization success making technical migration cheap running clean reorganization well skip lovely running stand still sensation invest attention fruitfullypresenting senior leadership spoton advice wish gotten earlier uber multiple initiative wanted get buyin higherups first time tried convince management chain great idea response le enthusiastic time figured go better give detailed advice prepare better absolutely single right way present senior leader hopefully template useful starting tie topic business establish historical narrative one two sentence answer question anyone care explicit ask looking audience one two data driven diagnosis decision making whats next return explicit ask lot luck format general think find pretty useful starting pointother section chapter approachesthe chapter discus situation engineering manager often find say partnering manager setting org direction saying section resonates well highgrowth organization always impactful work need done team handle saying explaining team constraint folk outside team one important activity undertake engineering leader articulating constraint depends particular issue hand find two topic frequent venue disagreement first velocity taking long take couple hour prioritization ca nt work important project philosophy management section exemplifies time every manager develop rule thumb managing explains management philosophy good one start thinking defining revising regularlyat core believe management ethical profession see nt look mirror rather treat member team succeeding believe almost every internal problem traced back missing poor relationship great relationship possible come together solve almost anything often profession asked deal difficult situation set rule guide safely every scenario found postponement never best solutioninstead avoiding hardest part double themif poor relationship manager member team spend even time meet every day dinner final thought best management philosophy never stand still continues evolve come contact reality worst theory management one second worst one nt changeother section chapter culturea short chapter effort shift culture within organizationmake peer first team advice wish would read earlier moved management focused effort team making sure great team time neglected building relationship fellow engineering manager stakeholder never occurred think engineering manager peer first team recentlythe best learning nt always come directly manager one important thing first team provide community learning peer provide excellent feedback aware work thinking work similarly likewise thinking peer work able learn approach differently anticipate soon team rate learning sum challenge longer restricted ownlong term believe career largely defined getting lucky rate learn advice luck speed learning two suggestion rapidly expanding company make peer first teamother section chapter careersa chapter interviewing hiring process well coaching engineer grow careerroles rocket ship hypergrowth weak predictor personal growth share good advice tenure weak predictor personal growth working highgrowth company nt mean get grow quickly without mucheven hypergrowth company tend team largely sheltered change either management far away company primary constraint get attentionby tracking era transition avoid lingering era beyond point developing new mastery allow continue personal growth even working would describe boring mature company advice applies within quickly growing company startup nt treat growth forgone conclusion growth come change something influencecareer level designation momentum etc section talk lot performance management system career level change rapidly growing company calibration process change level split expansion drift thing seen happen uber doubled every year section specific highgrowth company manager good idea challenge expect old system break lot bigger organizationthere surely hundred interesting topic come performance system work practice opposed design although seem quite simple keep learning something new time go performance cycle suspect widely shared experienceother section chapter appendixthe appendix come nice surprise instead usual reference list contains tool operating growing organization section one brief refresher key process team lead frontline manager multiple team lead manager organization want placeline management around time team reach three engineer want running sprint process many successful way run sprint try see resonates middle management move middle management become responsible two five line manager need shift away daytoday execution give line manager room make impact free make larger impact well managing organization organization start get even larger mostly managing middle manager playbook shift along way remember old problem still exist folk dealing instead roll new process solve personal pain point handing process manager keeping intact runningthe book close book found useful section referenced book accompanied short summarywhere book could even bettergiven elegant puzzle brings much original content idea full applicable idea people like working highgrowth tech company much hold however thing would made book even pleasant readdiagrams book often page away mentioned find adding much context text especially true kindle version well large wholepage diagram many inline diagram work well found getting much wholepage yellow diagram fullpage diagram one thing found work better website printinline diagram like read well wish fewer yellow fullpage onesthe first chapter struck odd start given much book build system thinking would put chaptersection first coming back first chapter later made sense mesome chapter le cohesive others section seemed odd shift style content length example briefest medium training short section career narrative model document share section little anything connecting three also formatting quite different three nt get wrong three section good however lining based train thought might made better readthe book real end one mind treat book reference various problem area however think missed opportunity closing section recapping journey reader went together instead finishing reading designing interview loop section immediately get appendix book suddenly endsall however small nitpicks concluded book note want book bookshelf grab amazonfinally interested getting update book pragmatic software engineering starting write subscribe
127,Lobsters,scaling,Scaling and architecture,Load balancing: Beyond healthchecks,https://www.brainonfire.net/blog/2019/07/21/load-balancing-beyond-healthchecks/,load balancing beyond healthchecks,tl dr foundation client talking server client receives request pick server call sidebar clientside v dedicated value nave solution defining health healthchecks choosing vantage point intrinsic health observed health measure health latency failure rate concurrency queue size defining failure hey wait healthchecks hysteresis binary scalar binary health check anomaly detection much health healthcheck check health relative sidebar starvation trap health wrapup sidebar subtle peril correlation thundering herd problem xfetch selfreport utilization using health load balancing variety algorithm selection algorithm according ability mobbing twochoice power two random choice weighted random selection combining health metric tiered priority level merged loadbalanced capture lifecycle load shedding concurrencylimits conclusion acknowledgement company blog,became interested finding perfect load balancer series incident work involving service talking database behaving erratically first focus making database stable clear could vastly reduced impact service able loadbalance request effectively database several read endpoint looked state art surprised discover far solved problem plenty load balancer many use algorithm work one two failure incident seen variety failure mode post describes learned current state load balancing high availability understanding problematic dynamic common tool think go disclaimer based primarily thought experiment casual observation much luck finding relevant academic literature critique welcome tl dr point like take away server health understood context cluster health load balancer use active healthchecks kick server may unnecessarily lose traffic healthchecks fail representative real traffic health passive monitoring actual traffic allows latency failure rate metric participate equitable load distribution small difference server health produce large difference load balancing system may oscillate wildly unpredictably randomness inhibit mobbing unwanted correlated behavior foundation quick note terminology post refer client talking server reference connection node etc given piece software function client server even time request flow scenario described app server client database server focusing clientserver relationship general case n client talking server also going ignore specific request simplicity say client request optional fallback possible call fails client experience degradation service big question client receives request pick server call note looking request longlived connection might carry steady stream burst traffic request varying interval also nt particularly matter overall conclusion whether connection made per request whether reuse connection sidebar clientside v dedicated might wondering every client talking every server commonly called clientside load balancing although post terminology load balancer also called client make client work quite common put server behind dedicated load balancer catch one dedicated load balancer node single point failure traditional stand least three node notice client need choose load balancer talk load balancer node still need choose server send request nt even relocate problem double two problem saying dedicated load balancer bad problem load balancer talk conventionally solved dns load balancing usually fine lot said using centralized point routing logging metric etc nt really allow bypass problem since still fall prey certain failure mode generally le flexible clientside load balancing value value load balancer optimizing order depending need reduce impact server network failure overall service availability keep service latency low spread load evenly server nt overly stress server others spare capacity predictability easier see much headroom service spread load unevenly server varying capacity may vary time server equitable distribution rather equal distribution sudden spike large amount traffic right server startup might give server time warm gradual increase traffic level might fine nonservice cpu load installing update might reduce amount cpu available single server nave solution trying solve everything let look simplistic solution distribute request evenly well roundrobin client cycle server guaranteed even distribution random selection statistically approach even distribution without keeping track state coordinationcpu tradeoff static choice client chooses one server request dns load balancing effectively client resolve service domain name one address client network stack pick one cache incoming traffic balanced dedicated load balancer client nt need know multiple server sort like random work ok dns ttls respected significantly client server similar request rate happens one server go configuration server request fail success rate pretty bad even single nine best possible success rate scenario assuming perfect load balancer sufficient capacity two remaining server get defining health usual solution healthchecks healthchecks allow load balancer detect certain server network failure avoid sending request server fail check general wish know healthy server whatever mean may predictive value answering core question server likely give bad response send request higher level question server likely become unhealthy send traffic return health send le another way saying case unhealthiness may dependent load others loadindependent knowing difference essential predicting route traffic unhealthiness observed broadly speaking health really way modeling external state service prediction count unhealthy measure choosing vantage point going detail important note two different viewpoint use intrinsic health server whether server application running responding able talk dependency severe resource contention client observed health server health server also health server host health intervening network even whether client configured valid address server practical point view server intrinsic health nt matter client ca nt even reach therefore mostly looking server health observed client subtlety though request rate server increase server application likely bottleneck network host start seeing increased latency failure rate server might mean server suffering request load implying additional request burden could make health worse alternatively server might plenty capacity client observing transient loadindependent network issue perhaps due nonoptimal routing case additional traffic load unlikely change situation given general case difficult distinguish case generally use client observation standard health measure health client learn server health call making latency long take response come back broken connection establishment time time first byte response time complete response minimum average maximum various percentile note conflates network condition server loaddependent source respectively majority case failure rate fraction request end failure failure mean bit concurrency many request currently flight conflates effect server client may inflight request one server either server backed client decided give larger proportion request reason queue size client maintains queue per server rather unified queue longer queue may indicator either bad health unequal loading client queue size concurrent request count see measurement health per se also indicative loading directly comparable client presumably want give request healthier lessloaded server metric used alongside intrinsic one latency failure rate measurement made client perspective also possible server selfreport utilization although largely wo nt covered post also measured across different time interval recent value sliding window rolling bucket decaying average several combination defining failure health indicator failure rate perhaps highest significance use case caller would rather get slow success failure sort different kind failure imply different thing state server call time might networking routing issue causing high latency server might heavy load call fails fast different implication dns misconfiguration broken server bad route fast failure le likely loaddependent unless perhaps server using loadshedding intentionally fail fast heavy case possible stressed load look applicationlevel failure transportlevel failure critical careful choosing criterion marking call failed example http call fails return due timeout etc unambiguously failure wellformed response error status code may indicate server problem individual request may triggering datadependent server error representative overall server health common see burst response due caller badly formed request caller affected judging server unhealthy basis would unwise hand somewhat le likely read timeout specific bad request hey wait healthchecks far mostly talking way client passively glean information server health request already making another approach use active healthchecks aws elastic load balancer elb healthchecks example configure load balancer call http endpoint server every second elb get response timeout time row take server consideration normal request keep making healthcheck call though server responds normally time row put back rotation demonstrates use hysteresis ensure host nt flap service readily familiar example hysteresis way air conditioner thermostat maintains window tolerance around desired temperature common approach work reasonably well scenario server either way healthy unhealthy change state frequently le common situation persistent low failure rate affect healthcheck normal traffic elb default configuration would see consecutive failure frequently enough keep host service healthchecks need designed carefully lest wrong effect load balancer type answer healthcheck call might intended provide smoke test make one realistic call see expected response come back functional dependency check server make call dependency return failure fail availability check see server respond call eg get ping yield ok response body pong important healthcheck representative possible real traffic otherwise may yield unacceptable false positive false negative instance server number api route one route broken due failed dependency server healthy smoke test healthcheck hit route client see server entirely broken alternatively route one working client may see server perfectly healthy functional check comprehensive necessarily better since easily result server server marked even single optional dependency useful operational monitoring dangerous load balancing result many people configure simple availability check active healthchecks generally provide binary view health server even tracked time since server may degraded state consistently answer request others passively monitoring traffic health hand give scalar even nuanced view health since least client know proportion request receiving critically passive monitoring receives comprehensive view traffic health type check track latency information course distinction hold failure rate metric binary health check anomaly detection much health healthcheck check binary view lead serious trouble since nt allow health comparison across server simply grouped based single call type may representative even multiple health check call guarantee stay representative server health api expands client need change even worse correlated failure could lead unnecessary cascading failure look scenario host passing active health check ideal load balancer route host passing route nt matter failing since rest cluster undoubtedly handle load passing route bet healthcheck wrong irrelevant rather crushing passing check passing route fail request nt route say closer passing fraction host get zero likely failure something external host even something wrong healthcheck imagine healthcheck depends test account test account deleted perhaps one dependency go request still served nevertheless healthchecks fail elb take every single one host service even though incoming request serviced perfectly fine clear health relative server healthier neighbor even problem easier see using scalar instead booleans essentially like load balancer performing kind simple anomaly detection small fraction server behaving oddly exclude send headsup ops behaving oddly nt make thing worse putting load small handful even worse none key evaluate server health view entire cluster rather atomically closest seen far envoy load balancer panic threshold default keep host service failing healthchecks using healthchecks load balancer consider using approach may notice skipped question server failing check situation may indicate true failure may either loaddependent loadindependent sure possible load balancer know situation applies even willing clever ab traffic load experiment find even worse putting load relatively small number server may take server besides loadshedding much done situation sure could fault either design keep server service one take within middle one human loop production incident nt clear u moment either sidebar starvation trap another difference active passive approach active checking information server health updated steady rate regardless traffic rate upside traffic slow downside high second failure long time request per second passive checking contrast failure detection speed proportional request rate one major downside pure passive healthchecking server go load balancer quickly remove service mean traffic traffic mean client view server health never change stay zero forever way deal course also address nodata edge case client startup replacing single server client server list need specially addressed using passive checking health wrapup summing passive monitoring traffic necessarily give comprehensive nuanced view health active check multiple ax along evaluate health server health understood relative cluster information realvalued number combined meet goal lower latency minimal failure evenly spread load like first take digression family failure mode discus common healthaware load balancing approach finally list possible future direction sidebar subtle peril correlation uncoordinated action surprising consequence imagine large corporate office sends email employee offering massage employee auditorium today come whenever think people show guess would big crowd time day right away lunch late afternoon going home uneven distribution massage therapist sometimes one work time long enough line people give maybe even trying later neither desirable without coordination somehow still show group accidental correlated behavior scenario easy prevent using commonplace tool signup sheet software land closest analog would batch processing system accepts job schedule convenience return result asynchronously turn number similar phenomenon api traffic often grouped together moniker thundering herd problem classic example cache service consulted hundred application node cache entry expires application need recreate value fresh data requires extra work likely extra network call server hundred app node simultaneously observe popular cache entry expiring constantly receiving request data simultaneously attempt recreate simultaneously call backend service responsible producing fresh data wasteful best case single app node perform task per cache lifetime could even crush backend server normally sheltered behind cache classic solution thundering herd problem cache expiry probabilistically expire cache entry early percaller basis rather expire instant everywhere simplest approach add jitter small random number subtracted expiration date whenever client consults cache refinement technique xfetch bias jitter delay refresh last possible moment another familiar problem occurs large number user service set periodic task call api perhaps every user backup service installs cron job upload backup midnight either local time zone likely utc backup server get overloaded midnight utc largely unused day standard solution onboarding new user generate suggested crontab file install using randomly selected time user even work without central point coordination backup software writes crontab file selecting random time first installed might notice similar approach could work massage scenario central signup sheet could nt used reason employee randomly pick time day free go time even necessarily optimal time schedule two expiry randomized make use randomness counter uncoordinated yet correlated behavior important principle randomness inhibits correlation see come addressing challenge relevant load balancing also see massage scenario alternative approach relying central point coordination one advantage using small cluster powerful server dedicated server higherlevel view traffic flow larger number client would another way increase coordination server selfreport utilization parasitic metadata response always possible serverreported utilization give client aggregated information would otherwise access could give clientside load balancer moreglobal view sort dedicated load balancer might bonus may help time distinguish server network failure implication loaddependent v loadindependent interpretation aspect system dynamic mind let return looking load balancer use health information using health load balancing load balancer commonly separate usage health information two concern deciding server candidate request deciding candidate select request classic approach treat two totally separate tier aws elb alb nlb instance use variety algorithm spreading load random roundrobin deterministic random leastoutstanding separate mechanism largely based active healthchecks determining server participate selection process based doc sound like nlbs also use passive monitoring decide whether kick server detail scarce random roundrobin deterministic random flowhash completely ignore health server either leastoutstanding algorithm hand us passive health metric note even algorithm server selection kept totally separate active check used taking server cluster leastoutstanding pick server lowest request concurrency one several approach using passive health metric allocating request based optimizing one metric mentioned earlier latency failure rate concurrency queue size selection algorithm according ability load balancing selection algorithm choose server best value metric face make sense give current request best shot succeeding quickly however lead term mobbing latency health metric choice one server exhibit slightly lower latency others seen client client send traffic one least begin suffer load possibly even start fail server begin suffer effective latency increase possibly different server gain title globally healthiest may repeat cyclically instigated nothing slight difference initial health mobbing behavior involves confluence several defect system latency delayed health metric concurrency inflight request count used instead client would mob since concurrency metric instantly updated client side soon request allocated server delayed measurement even damping lead undesirable oscillation resonance client global view situation therefore acting uncoordinated fashion produce unwanted correlated behavior small difference server health produce large difference load balancing behavior since feedback latter former fit one description chaotic system highly sensitive initial condition remedy see use fast health metric possible indeed common load balancing selection algorithm send request server least inflight request sometimes called leastconnections leastoutstanding depending whether connection request connection longlived carry many request lifetime contrast nt believe seen pickleastlatency algorithm probably reason either attempt approximate global view situation using dedicated load balancer small number server incorporating serverreported utilization use randomness inhibit unwanted correlated behavior use algorithm approximately behavior approximately input nt continuouslyvariable behavior use randomness achieve something approximating popular alternative pickthebest called twochoice described paper power two random choice discus general approach resource allocation specific even centered load balancer certainly relevant approach two candidate selected one better health used approximates even distribution longterm health server approach identical value even small persistent difference health vastly unbalance load distribution simplistic simulation feedback illustrates select index one n server health ranging n defn selecttc n let spread n top bottom health range overlap half compute health server index health fn spread spread rand randomly choose two server without replacement take shuffle range n pick index healthier server health health run trial host report number time host index selected sortby key frequency repeatedly selecttc assuming increased load nt affect health metric would produce difference request load healthiest unhealthiest host even approximate ranking health note host health range host despite apart absolute term slight bias magnified large imbalance load twochoice reduces mobbing quite well bias present may well case feedback occur clear appropriate selection mechanism use delayed health metric additionally paper appears focused max load reduction given identical set option case healthaware load balancer hand twochoice work well leastoutstanding feedback instantaneous selfcorrecting leastoutstanding challenging potentially small quantized value server one open connection twice healthy server two zero one leastoutstanding easier work relatively client dedicated load balancer relation request load resulting easier comparison eg v small average value randomization tiebreaker becomes important lest first server list always receive request client one connection open client may collectively mob one server twochoice randomization present natural antidote mobbing resulting leastoutstanding small discrete value promising option though still academic weighted random selection server assigned weight derived health metric server picked according weight instance server weight would chance selected time respectively use algorithm requires care avoid starvation trap weight derivation need use wellchosen nonlinear function server health others receives greatly reduced weight perhaps relative work experimenting approach high hope local integration experiment nt yet seen tested realworld traffic pan likely go detail future post new loadbalancing algorithm combining health metric putting question use multiple health metric mind hardest part cut core whole matter define health application let say tracking latency failure rate concurrency matter combine failure rate bad increased latency point would rather take chance available server one showing massive latency spike two general strategy come mind might take tiered approach defining threshold acceptability metric picking server acceptable failure rate none pick acceptable latency etc maybe spillover threshold defined acceptable pool small server next tier considered well idea bear resemblance envoy priority level alternatively could use merged metric metric combined according continuous function perhaps put weight currently experimenting deriving weight factor health metric multiplying together raised higher power squared cubed give weight suspect large power could used implement something like tiered approach even using merged metric combiner also worth considering metric might covary suggesting possible benefit advanced modeling server connection health consider server entered bad state spewing failure response quickly health metric latency server look like healthiest cluster therefore receives traffic rachelbythebay call loadbalanced capture effect fast always healthy depending configuration merged approach may may sufficiently suppress traffic rogue server tiered approach prioritizes low failure rate would exclude entirely latency failure rate general tied nonobvious way besides spewing failure quickly scenario also matter timeout v nontimeout failure high latency condition client produce number timeout error failure per se excessively highlatency response affect latency metric failure rate metric compare failure due bad dns record fast connection failure recommendation record latency number success failure know indicate timeout sockettimeoutexception similar java coworker suggests alternative recording latency value failure make latency average worse lifecycle mostly assumes client talking static collection server server replaced either one time large group new server added cluster load balancer hit full share traffic right away instead ramp traffic slowly period warmup period allows server become fully optimized disk instruction cache warming hotspot optimization java etc haproxy implement slowstart end beyond warmup also time uncertainty client history server limiting dependence limit risk using metriccombination approach may convenient use server age pesudohealth metric starting near zero ramping full health course minute starting precisely zero may dangerous depending algorithm client may learn complete replacement set server reconfigured point different cluster briefly consider server zero health likely mechanism handling total replacement server list also suffice handle client startup well load shedding lightly touched load shedding service heavy request load attempt respond request failure quickly effort reduce cpu load resource contention sometimes best effort nt enough need keep service alive long enough scaled load shedding gamble predicated idea returning failure traffic might allow respond successfully traffic later trying handle traffic right might take service entirely know though much suspect largely separable concern load balancer good enough distributing load simply putting something like hystrix concurrencylimits front might sufficient one place could see benefit would managing additional load healthy server server unhealthy server healthy reasonable take normal share load load balancer might reasonably decide cap overage never ask one server take load share server marked unhealthy feasible desirable sure fully adaptive sense overage cap still configured configuration easily fall date irrelevant eg lowtraffic period conclusion based believe many existing option loadbalancing generic highavailability environment tend work well distributing load normal condition select set error condition variously fall short condition due mobbing insufficient responsiveness failure overreaction correlated degraded state ideal highavailability load balancer would eschew active healthchecks normal operation instead passively track variety health metric including current inflight request decaying rolling metric latency failure rate client tracking metric far better position perform anomaly detection one observing periodic active healthcheck result course truly ideal load balancer would embody perfect efficiency even increasing request load request handled successfully quickly possible right system reach theoretical limit point suddenly fails start shedding load rather gradually showing increasing stress would file problem love highlight need review monitoring tool load balancer particularly good hiding server failure outside world main open question mind combine health metric use server selection way minimizes chaotic behavior issue mentioned post still remaining generally applicable currently betting multifactor weighted random selection still remains seen performs real world acknowledgement thanks employer brightcove allowing time really dig problem version post appears company blog thanks also coworkers feedback early draft despite length especially putting going load balancer week end
128,Lobsters,scaling,Scaling and architecture,A walkthrough to learn the concepts of MVC by building a simple app in JavaScript,https://www.taniarascia.com/javascript-mvc-todo-app/,walkthrough learn concept mvc building simple app javascript,modelviewcontroller todo app prerequisite goal note model view controller model view controller model view controller initial setup getting started understanding class javascript model local storage input output view dom introduction dom controller setting event listener respond callback model add local storage use local storage javascript add live editing functionality many written conclusion,wanted write simple application plain javascript using modelviewcontroller architectural pattern hopefully help understand mvc difficult concept wrap head around first starting made todo app simple little browser app allows crud create read update delete todos consists indexhtml stylecss scriptjs nice simple dependencyframeworkfree learning purpose prerequisite goal create todo app browser plain javascript get familiar concept mvc oop objectoriented programming note since app us latest javascript feature wo nt work asis browser like safari without using babel compile backwardscompatible javascript syntax model view controller mvc one possible pattern organizing code popular one model manages data application view visual representation model controller link user system model data todo application actual todos method add edit delete view data displayed todo application rendered html dom cs controller connects model view take user input clicking typing handle callback user interaction model never touch view view never touch model controller connects like mention mvc simple todo app actually ton boilerplate would really overcomplicating thing app wanted create made whole system point try understand small level understand scaled system might use initial setup going fully javascript app mean everything handled javascript html consist single root element body indexhtml doctype html html lang en head meta charset meta name viewport content widthdevicewidth meta httpequiv xuacompatible content ieedge title todo app title link rel stylesheet href stylecss head body div id root div script src scriptjs script body html wrote small bit cs make look acceptable find save stylecss going write cs focus article okay html cs time actually start writing app getting started going make really nice simple understand class pertains part mvc make model class view class controller class take model view app instance controller familiar class work read understanding class javascript class model constructor class view constructor class controller constructor model view thismodel model thisview view const app new controller new model new view nice abstract model let focus model first simplest three part nt involve event dom manipulation storing modifying data model class model constructor thistodos id text run marathon complete false id text plant garden complete false addtodo todotext const todo id thistodoslength thistodos thistodoslength id text todotext complete false thistodospush todo edittodo id updatedtext thistodos thistodosmap todo todoid id id todoid text updatedtext complete todocomplete todo deletetodo id thistodos thistodosfilter todo todoid id toggletodo id thistodos thistodosmap todo todoid id id todoid text todotext complete todocomplete todo addtodo edittodo deletetodo toggletodo self explanatory add appends new todo array edit find id todo edit replaces delete filter todo array toggle switch complete boolean property since browser app accessible window global test easily typing something like appmodeladdtodo take nap add todo list log content appmodeltodos good enough model right end store todos local storage make semipermanent todos refresh time refresh page see model deal actual data modifying data nt understand knowledge input modifying output end displaying point everything need fully functioning crud app manually type action console view output console view going create view manipulating dom document object model since plain javascript without aid react jsx templating language kind verbose ugly nature manipulating dom directly neither controller model know anything dom html element cs anything relating view familiar dom dom different html source code read introduction dom first thing make helper method retrieve element create element view class view constructor createelement tag classname const element documentcreateelement tag classname elementclasslistadd classname return element getelement selector const element documentqueryselector selector return element far good constructor going set thing need view root element app root title heading form input submit button adding todo form input button todo list ul make variable constructor easily refer view class view constructor thisapp thisgetelement root thistitle thiscreateelement thistitletextcontent todos thisform thiscreateelement form thisinput thiscreateelement input thisinputtype text thisinputplaceholder add todo thisinputname todo thissubmitbutton thiscreateelement button thissubmitbuttontextcontent submit thistodolist thiscreateelement ul todolist thisformappend thisinput thissubmitbutton thisappappend thistitle thisform thistodolist part view wo nt change set two small thing getter resetter input new todo value using underscore method name signify private local method wo nt used outside class view get todotext return thisinputvalue resetinput thisinputvalue setup done complex part displaying todo list part change every time change made todos view displaytodos method create ul li todo list consists display every time todo changed added remove displaytodos method called todos model resetting list redisplaying keep view sync model state first thing remove todo node every time called check todos exist nt display empty list message view thistodolistfirstchild thistodolistremovechild thistodolistfirstchild todoslength const p thiscreateelement p ptextcontent nothing add task thistodolistappend p else loop todos display checkbox span delete button every existing todo view else todosforeach todo const li thiscreateelement li liid todoid const checkbox thiscreateelement input checkboxtype checkbox checkboxchecked todocomplete const span thiscreateelement span spancontenteditable true spanclasslistadd editable todocomplete const strike thiscreateelement striketextcontent todotext spanappend strike else spantextcontent todotext const deletebutton thiscreateelement button delete deletebuttontextcontent delete liappend checkbox span deletebutton thistodolistappend li view set model set nt way connect event watching user make input handler handle output event console still exists temporary controller add remove todos controller finally controller link model data view user see far controller controller class controller constructor model view thismodel model thisview view first link view model make method call displaytodos every time todo change also call constructor display initial todos controller class controller constructor model view thismodel model thisview view thisontodolistchanged thismodeltodos ontodolistchanged todos thisviewdisplaytodos todos controller handle event fired submit new todo click delete button click checkbox todo event fired view must listen event user input view dispatch responsibility happen response event controller create handler event controller controller handleaddtodo todotext thismodeladdtodo todotext handleedittodo id todotext thismodeledittodo id todotext handledeletetodo id thismodeldeletetodo id handletoggletodo id thismodeltoggletodo id setting event listener handler controller still nt know call put event listener dom element view respond submit event form click change event todo list skipping edit since slightly complicated view bindaddtodo handler thisformaddeventlistener submit event eventpreventdefault thistodotext handler thistodotext thisresetinput binddeletetodo handler thistodolistaddeventlistener click event eventtargetclassname delete const id parseint eventtargetparentelementid handler id bindtoggletodo handler thistodolistaddeventlistener change event eventtargettype checkbox const id parseint eventtargetparentelementid handler id need call handler view going bind method listening event view used arrow function handle event allows u call view using context controller use arrow function would manually bind like thisviewbindaddtodo thishandleaddtodobind yikes controller thisviewbindaddtodo thishandleaddtodo thisviewbinddeletetodo thishandledeletetodo thisviewbindtoggletodo thishandletoggletodo submit click change event happens specified element corresponding handler invoked respond callback model something left event listening handler invoked nothing happens model know view update know make view update displaytodos method view solve mentioned earlier model view know like listening event model fire back controller let know something happened already made ontodolistchanged method controller deal make model aware bind model way handler view model add bindtodolistchanged ontodolistchanged model bindtodolistchanged callback thisontodolistchanged callback bind controller like view controller thismodelbindtodolistchanged thisontodolistchanged every method model call ontodolistchanged callback model deletetodo id thistodos thistodosfilter todo todoid id thisontodolistchanged thistodos add local storage point app mostly complete concept demonstrated make little bit permanent persisting data local storage browser persist locally refresh nt aware local storage work read use local storage javascript set initial todo value local storage empty array model class model constructor thistodos jsonparse localstoragegetitem todos make commit private method update value localstorage well model state model commit todos thisontodolistchanged todos localstoragesetitem todos jsonstringify todos every change thistodos call model deletetodo id thistodos thistodosfilter todo todoid id thiscommit thistodos add live editing functionality last piece puzzle ability edit existing todo editing always little trickier adding deleting wanted make simple require edit button replacing span input anything also nt want call edittodo every single time letter typed rerender whole todo list ui decided make method view update temporary state variable new editing value another call handleedittodo method controller update model input event get fired type contenteditable element focusout fire leave contenteditable element view constructor thistemporarytodotext thisinitlocallisteners initlocallisteners thistodolistaddeventlistener input event eventtargetclassname editable thistemporarytodotext eventtargetinnertext bindedittodo handler thistodolistaddeventlistener focusout event thistemporarytodotext const id parseint eventtargetparentelementid handler id thistemporarytodotext thistemporarytodotext click todo item enter editing mode update temporary state variable tab click away todo save model reset temporary state make sure bind edittodo handler controller thisviewbindedittodo thishandleedittodo contenteditable solution quickly implemented sort issue need consider using contenteditable production app many written conclusion dependencyfree todo app plain javascript demonstrates concept modelviewcontroller architecture link completed demo source hope tutorial helped understand mvc using looselycoupled pattern add lot boilerplate abstraction application also predictable familiar pattern commonly used across many framework important concept know developer
129,Lobsters,scaling,Scaling and architecture,The Unsolved Load Balancing Problem of WebSockets,https://movingfulcrum.com/the-unsolved-load-balancing-problem-of-websockets/,unsolved load balancing problem websockets,solution conclusion,using websockets bound run issue unfortunately none available popular websocket library solves thislets say websocket based application running server b connected rabbitmq load balancer lb endpoint usersconnections come lb distributes evently across node notice connection load high server add another server chowever c nt get existing traffic websocket connection persistent stick whatever node connected restart b hoping client would reconnect thus load would evenly distributed assuming b restart exact time happens next c connection b none even worse started happened b restarting existing connection reconnected c restart c back original situation traffic b none cthis persistent connection behavior surprise many way simple operation like rolling upgrade node result unexpected behavior like traffic sent one node last node upgraded getting traffic result huge scalability issue applicationany solution surprisingly nt sam point tough problem requires inventing whole protocol websocket framework client server side correctly unfortunately none currently kinda sorta solution aws api gateway also pointed sam aws api gateway websockets connected gateway instead server gateway talk application via http since gateway using vanilla websockets even run issue described underneath connection end connected single server inside gateway seems folk twitter caution conclusionif looking use websockets might worth pondering issue least due additional complexity introduced websockets used plain old http really slow youi hope websockets framework like spring websockets put thought issue build protocol solve
130,Lobsters,scaling,Scaling and architecture,Field Guide: Mitigating Risk While Transitioning Databases,https://blog.sentry.io/2019/07/25/field-guide-mitigating-risk-transitioning-databases,field guide mitigating risk transitioning database,field guide mitigating risk transitioning database recently introduced snuba clickhouse greater visibility checking map data capacity gathering supply service delegator first installment series service delegator jaccard similarity published call table clickhouse cluster rechecking map visual aid quick detour finding race condition bimodal distribution tla back path increasing similarity beginning final ascent performance trekking throughput pas circumnavigating latency peak x mark spot launch day made end,july guide mitigating risk transitioning databaseswelcome series blog post thing sentry perhaps get u wrong regret decision sharing note case also choose path le traveled post giving context everything needed happen introduced snuba new storage query service event data perhaps know sentence directly recently introduced snuba new primary storage query service event data power sentry snuba backed open source columnoriented database clickhouse used search graph issue detail rule processing query every feature mentioned push greater visibility word snuba excellent using power pretty much everything transition existing storage system snuba mind might picturing something dramatic golden idol scene indiana jones something simple flip switch connected snuba sentry development environment unit integration test pas ran load test production hardware point cut traffic old system new system hand crisp highfives go home right would make good story short blog post reality process lot complex lot interesting migrating snuba undertaking could say charting unexplored territory creating something new technology new u dealing data want lose event needed find way smoothly transition old system new system without compromising data integrity performance crushed giant rolling boulder find solution detailed explore data validation abstraction risk management checking map data capacity sentry user rely sentry collect store communicate health application sentry critical many user monitoring workflow incident response process taking downtime losing data transition process option introducing snuba system architecture changed way write read event data needed able absolute confidence data stored queried correctly snuba introducing world several conventional approach ensuring two database state contain data one straightforward approach check row database also exist database b data vice versa unfortunately correctly generally requires database readonly mode consider inability process store event downtime approach option u another approach deploy change wait user tell thing seem weird many application user create type content content show user know immediately quickly notify support staff issue sentry user manually send error much query volume come automated system internal external query sentry additional information issue data event everything still look fine application error time notice already lost lot data lost trust good option u third approach rely unit andor integration test coverage approach work presence comprehensive specification test suite exhaustively validates specification without even test coverage mean test cover thing knew could go wrong test written case test often written several year snuba ever existed specification system often work way work faster scalable considering alternative realized best option test snuba returning result existing system query performed production gathering supply service delegator described first installment series migration snuba relied fact already defined abstract interface search tagstore tsdb time series database service backends since already existing abstraction layer place service knew type query performing various system well expected response data structured test snuba backend returning result existing backends type query ran implemented intermediary called service delegator delegator class conforms abstract interface exposed service presenting synchronous api expected calling code background delegator capable performing concurrent request different backends service delegator also provides ability define callback executes concurrent request complete successfully otherwise path example request sentry application make request tsdbgetsums via service delegator cause concurrent request redis snuba result redis returned caller case delegator allowed u execute query existing backends also executing request new snuba backends user saw response existing system change performance data integrity regardless state snuba time request system complete timed logged request detail service method name argument value well response timing system kafka implemented kafka consumer processed result creating similarity score scale backend represented similarity difference backend compared result returned user totally different example result logged request tsdbgetsums routed two different backends completed processed kafka consumer request timing response data redis existing implementation shown red request timing response data snuba new implementation shown blue example similarity response redis snuba since addition numeric comparison equality comparison string booleans implemented recursive comparison variety different composite data structure including jaccard similarity set similarity pairwise similarity sequence list keywise similarity mapping processed result published call table clickhouse cluster stored detail query later analysis rechecking map visual aid imagine collected lot data far many independent data point human observer make sense without little help towards end development process snuba recording request performed search tagstore tsdb service first challenge figuring summarize visualize data way enabled u understand system performing first attempt visualizing system behavior create similarity histogram service method grouping similarity value tenth value would aggregated alongside value greater equal le histogram gave u overview method commonly returning accurate result method needed improvement see data point introduced fall overall similarity distribution tsdbgetsums method big picture view similar query type baseline result allowed u focus effort area able make significant improvement early since recording data request performed able focus request exceptionally low similarity score method access parameter response data request allowed u hone specifically causing inconsistency fix applied could replay request original parameter verify inconsistency longer present could plot new histogram every day get pointintime snapshot query performance past hour provide visual indicator significantly change improving result similarity time common approach showing relationship two numeric variable case time similarity use scatter plot however case far many data point generate useful plot scatterplots often useful dealing relatively small amount data plotted many result visual noise plot covered meaningful signal reduce amount distinct data plot required plot borrowed technique histogram approach created heat map binned data point daily segment xaxis binned tenth similarity scale histogram plot yaxis resulting grid display cell shading intensity corresponded number data point similarity bin day data written snuba improvement made score trended upwards example binning aggregating data together allowed u get lower resolution better overview system performing three dimension time similarity frequencydensity quick detour finding race condition generated histogram heat map followed typical pattern result similarity generally clustered around single mean however method mostly tsdb call like example followed bimodal distribution significant subset result entirely dissimilar another subset result similar result fell somewhere since logged method argument part service delegator payload identified commonality query structure mapped back call site sentry codebase case accurate result coming web uiapi inaccurate result coming postprocessing worker check alert rule send email webhooks integrate plugins etc weird remember introduced apache kafka processing pipeline writes snuba performed kafka writes existing system initiated existing processing pipeline utilized rabbitmq celery result depicted implicit sequencing dependency write path postprocessing code postprocessing task relied data present tsdb backend make decision whether send email etc previous architecture everything happened sequentially data guaranteed written database postprocessing occurred introduction kafka longer case sometimes write happened data written sometimes read happened data written window inconsistency two different processing path small exist playing game chance drawing short straw lot time without issue identification alerting decision rely incorrect data lead false negative alerting system sure user happy receiving fewer alert realize reason could avoided defining modeling testing complex system interaction hard realistic expect engineer team engineer comprehensive understanding large continually evolving software system academic spent whole career creating framework formally specifying relationship system resulting tool tla similarity test suite would never caught without exercising full endtoend system test assumption explicitly stated casual testing loading ui would never uncovered problem week would likely gone would made aware issue validation process able identify understand implement solution problem fix upcoming post ever became actual problem back path increasing similarity time ready release snuba user many graph result solidly similar range result range result improved time fixed bug data recorded new system note dramatic jump similarity score well beginning final ascent performance also needed ensure snuba performance snuff turning user ultimately performance concern included ensuring ready handle typical throughput query per second minimizing maximum latency one want suffer embarrassing incident brand new database melting launch day trekking throughput pas even cloud code run computer finite amount cpu iop network bandwidth memory know enough resource serve sustain production traffic often underestimate capacity planning load estimation difficulty complexity behavior impact single user different thousand even million concurrent user different shape query different performance characteristic using different parameter type query lead different database behavior query return one row wildly different performance characteristic resource need query return thousand row getting representative result synthetic benchmarking requires deep understanding data user query pattern new product challenging luckily u already knew type query performed query already running fancy modeling needed service delegator enabled u choose backends used runtime could load test snuba production traffic approach varied simple random sampling mirror dual dark read percentage read request snuba complicated approach using method argument part routing logic could sample request based specific parameter value interested organization project increased snuba query volume tuned database setting modified query learn clickhouse performed production environment sometimes thing got weird able quickly shed load reducing number query sending snuba investigate turbulence settled continued increase percentage dark read completely dual reading gradual switch enabled operation team test failure scenario failovers administrative change production load none production risk circumnavigating latency peak point confident new system could sustain traffic without falling pretty low bar one talk good database crash time way people talk great car continuously breaking fast car cool fast database cool database need reliable needed responsive least responsive system replacing ideally faster faster mean measure database horsepower torque quartermile time discussed regarding throughput performance varies wildly based type query running many etc record latency time execute operation though collect data million data point make sense much like similarity metric remember recorded data collected service delegator clickhouse could run analytical query performance result fortunately u clickhouse sweet spot good first step making sense data segmenting latency metric query type case service name method name could collect many summary statistic time series data quantiles etc get bigpicture view type query performing example earliest recorded percentile latency millisecond snuba well existing implementation two different backend method tsdbgetsums method discussing well search query method select service method backend quantilestdigest latency select service method resultbackend backend resultfinished resultstarted latency call array join result result prewhere service tsdb method getsums service search method query timestamp todatetime timestamp todatetime resultstatus success group service method backend order service asc method asc backend asc latency search query django search query snuba tsdb getsums redis tsdb getsums snuba row set elapsed sec processed million row gb million row gb addition general backend performance number could also importantly directly compare performance snuba backend relative backends exact query parameter select service method baseline quantilestdigest delta select service method arrayjoin arrayfilter backend status backend snuba status success arrayenumerate resultsbackend resultsbackend resultsstatus resultsbackend baseline resultfinished resultstarted resultsfinished resultsstarted delta call array join result result prewhere service tsdb method getsums service search method query timestamp todatetime timestamp todatetime resultbackend snuba resultstatus success group service method baseline order service asc method asc baseline asc delta search query django tsdb getsums redis row set elapsed sec processed million row gb million row gb initial result shockingly bad knew going challenge since existing backend implementation carefully cleverly optimized several year working system new u aggregate point data allowed u identify isolate area improve performance well evaluate broader change clickhouse configuration cluster topology change made performance improvement routinely checked see query latency reduced similarly monitored change result similarity time launched snuba response time often equivalent faster existing system method replacing able improve performance many query several month iteration experimentation shown recorded latency distribution search backend example cyan bar earliest recorded latency orange bar latency near launch day note xaxis scaled logarithmically lot improvement x mark spot launch day month work iterative improvement thing looking good data accurate running throughput several week comfortable latency distribution recording service delegator allowed u swap primary secondary backends runtime also allowing u continue dualwriting old system period case revert quickly time nobody noticed good infrastructure probably place work like excitement work action movie sequence point another ordinary day office extra high five made end yes took big big risk also took many many precaution avoid disaster gained operational experience running month production ever visible endusers best way test system test help good interface abstraction service delegator tested launched without user impact collecting mountain data gave u visibility new system relative old one allowed u focus effort observe effect change making gain confidence new system launching making launch day practically nonevent
131,Lobsters,scaling,Scaling and architecture,Benchmarking: Do it with transparency or don't do it at all,https://www.ongres.com/blog/benchmarking-do-it-with-transparency/,benchmarking transparency nt,introduction benchmarking right performance benchmark postgresql mongodb check presentation summarizing benchmark result definitely benchmark comparing postgresql mongodb transparent approach whitepaper reproduced code open source open sourced bucket mongodb mistake mixing benchmark data one developed percona connection pooling also without bucket driver quality created mongodb mongodb mistake misunderstanding ycsb transparency mongodb mistake used official mongodb java driver enterprisedb press release official mongodb java driver mongodb mistake show tuning work production note postgresql configuration human parameter tuned parameter represent expertly tuned postgresql mongodb mistake dismiss transaction benchmark strong reason built mongodb director developer advocacy emea lookup question publicly available benchmark source code mongodb mistake tune benchmark query tune specific query db benchmarking tune system specifically expected query cstorefdw timescaledb torodb project already proved tpcc tpce tpch asya talk last word discussion twitter like,introduction post reply mongodb benchmarking right post wrote response whitepaper performance benchmark postgresql mongodb published sponsored enterprisedb performed ongres long read close page encourage least read executive summary page relevant section right context quick overview check presentation summarizing benchmark result nutshell mongodb claimed ongres postgresql expert made range basic error use mongodb mongodb produced new performance result refuting without clarifying achieved number publishing source code ironically advocated blog response made benchmark reproducible published result full went lecture ongres benchmark well anything definitely benchmark comparing postgresql mongodb continue transparency transparent approach ongres maintain transparency wrote comprehensive whitepaper explaining great detail everything especially adhered strict benchmark ethic policy detailed whitepaper page reproduced benchmark presentation created automated platform run benchmark record monitoring information process result code open source used reproduce result make change run benchmark benchmark written test acid transaction support also open sourced raw result public stored several gb bucket anyone inspect love criticism constructive correct lead conversation enriches database user unfortunately found tone mongodb post arrogant unprofessional worse mix benchmark quote phrase context ignores thing detailed whitepaper full mistake post analyzes responds significant mistake proposes collaborative iterative approach benchmarking transparency key driver mongodb mistake mixing benchmark data start transaction test production mongodb driver connection pooling make ongres selection mongodb driver particularly strange decided use experimental unsupported nonproduction lua driver sysbench implementation created perform one three test response quote observed mongodb actually began transaction test switched discus oltp test make mistake several time article worst mongodb kept mentioning experimental driver throughout article used three benchmark used one oltp benchmark also interesting note also inaccurately implied wrote sysbench implementation fact developed percona see page whitepaper actually connection pooling test postgresql also without connection pooling latter result unrecognized mongodb response result case detailed report page bucket store result reason choose use connection pooling deeply discussed driver quality sysbench written lua mongodb concludes reasonable tester would looked alternative benchmark disagree mean performance lua user get moreover tell world much mongodb actually care lua user specially driver created mongodb remains unmaintained official lua driver created interestingly mongodb bother patch sysbench benchmark update lua driver add connection pooling replace another driver run benchmark publish result published source code automation would allowed quite easily instead stated head engineering comms twitter thread several people asking way reproduce number mongodb neither interest intellectual obligation respond ongres material code certainly wasting time submitting pull request repo designed cheat u mongodb mistake misunderstanding ycsb sysbench according ongres option oltp performance testing expert found benchmark repository run ycsb test production driver side choose publish result claim completely false ycsb initially evaluated ultimately decided use briefly explained whitepaper main reason ycsb considered resemble real data model even close ycsb modeled support almost nosql database includes even keyvalue database data model extremely simple contains small record resemble reasonable data model mongodb postgresql user would use real life except corner case wellestablished benchmark deal real data model like sysbench strongly preferred mongodb post state expert found benchmark repository run ycsb test production driver side however need expert find anyone browse repository find called transparency fact source code run ycsb mean considered evaluated never completed full run benchmark scenario contemplated oltp benchmark automate discarded option code remains full transparency worth mentioning partial run ycsb benchmark postgresql faster mongodb scenario except inmemory benchmark data set result pretty similar one obtained sysbench benchmark mongodb mistake used official mongodb java driver ongres leaned heavily sysbench benchmark analysis headline post link assume mongodb refers enterprisedb press release graph significant headline refer transaction benchmark oltp benchmark transaction benchmark used official mongodb java driver us connection pooling postgresql used without pgbouncer count mongodb wrong mongodb mistake show tuning work test ongres took mongodb offtheshelf compared heavily tuned postgresql also ignored number mongodb best practice tuning like documented production note part mongodb documentation mongodb performing faster wow faster following production note include magical tuning default everyday user also benefit performance obviously read analyzed mongodb production note tried create level playing field basically performance configuration parameter recommended tuned within lengthy production note one mention storagewiredtigerengineconfigcachesizegb want decrease wired tiger cache many recommendation strictly followed like binary version journaling directory etc affect o basically sysctl applied applied postgresql either benchmark intended resemble reasonably optimized production platform tuned reasonable dba best knowledge recommended best practice production note followed mongodb disagree please explicitly enumerate reasonable best practice omitted critical performance impact let world know show one changed config file get exceptional number mongodb also claim compared mongodb heavily tuned heavily tuned postgresql anything ongres obsessed postgresql tuning recommend best practice expert postgresql tuning may read example postgresql configuration human talk tune postgresql system typically configure parameter benchmark tuned parameter page measurable impact performance without tuned postgresql would severely disadvantaged ship conservative default tuned parameter benefit performance sharedbuffers essentially postgresql memory cache default mongodb default us system ram must tuned postgres match randompagecost postgresql default assumes disk rotational however ssds used parameter frequently tuned easily found searching web eg stackoverflow defaultstatisticstarget may may improve quality generated execution plan configuration represent expertly tuned postgresql even close basic tuning compete reasonable playground mongodb tune far better including o chose intentionally mongodb mistake dismiss transaction benchmark strong reason oltp benchmark based teaching example python user written mongodb developer advocate ongres ported java built benchmarking top led unnecessary us lookup join aggregation relational trait mongodb known impact performance simply mongodb relational database response claim mongodb yet mix oltp benchmark acid transaction one respect transaction benchmark took transaction code built mongodb director developer advocacy emea ported java mongodb considers synthetic benchmark unrealistic workload actually intent resemble even realworld workload including real database flight schedule application model airline selling ticket maybe synthetic unrealistic workload mongodb good example benchmark lookup question interesting mongodb critique use lookup operation first exists may used mongodb claiming slow used please update documentation clearly advertised second avoiding come string attached either use embedding lead data duplication inconsistency due nontransactional update increased disk storage consequently decreased query performance use reference applicationlevel join increase application development effort errorproneness believed lookup fair choice actually original performance mongodb one order magnitude le one published added index mongodb optimize lookup performance resulted win equivalent postgresql index resulted win yet published result indexed version definitely cared improving mongodb performance yet mongodb still disagree send merge request publicly available benchmark source code rerun benchmark chosen data model explain potential downside approach mongodb mistake tune benchmark query addition simple hint direct query use index mongodb query magnitude faster postgresql mongodb also recommends use compound index something postgresql documentation argues mongodb addition compound index got one query run faster postgresql field queried query exist database record added compound index field mongodb postgresql could answer instantly nothing search mongodb describes called gaming benchmark tune specific query idea behind olap benchmark respond analytical query launched business intelligence tool predict query run run thus tune every query create dozen index hope picked index may harm oltp write performance anything db benchmarking tune system specifically expected query worse mongodb publish index created server run test configuration tuning used mongodb result unicorn someone reproduce indeed surprised postgresql performance expect postgresql fare well json native benchmark importantly postgresql expert could definitely tuned postgresql heavily get order magnitude better performance example could used jsonb path lookup instead chaining operation faster exploited postgresql parallel query used postgresql new jit facility compile onthefly sql query native code significantly improving execution time olapstyle query used partitioning could optimized significantly query execution specially coupled parallel query used immutable compressed columnar storage postgresql cstorefdw good fit event data used timescaledb extension last least could applied transformation json data relational schema would done may achieved result time faster mongodb torodb project already proved tpcc mongodb considers tpcc realistic transactional workload one would certainly love compare postgresql mongodb tpcc honestly implement benchmark standardized year first version mongodb released work adapting tpce even tpch much modern complete case also going vldb already planned attend asya talk would happy team discus mongodb postgresql benchmarking vldb last word found mongodb response discussion twitter incredibly shocking anyone publicly traded adhere higher professional standard demeaning others accusing creating benchmark designed cheat place professional tech note anywhere else matter claim benchmark perfect open stand corrected receiving merge request mongodb improve querying indexing configuration new benchmark take discussion software public open source follow spirit join u benchmarking journey least exercise transparency transparency paramount benchmark claim performance without transparency thirdparty reproduce mongodb result thus benchmark meaningless sincerely hope mongodb followsup objective clear response including full benchmark methodology source code mean reproduce like
132,Lobsters,scaling,Scaling and architecture,Open Sourcing Brooklin: Near Real-Time Data Streaming at Scale,https://engineering.linkedin.com/blog/2019/brooklin-open-source,open sourcing brooklin near realtime data streaming scale,change data capture cdc,hypothetical example kafka mirrormaker kmm used aggregate kafka data across two data center contrast brooklin mirroring topology kmm cluster needed one sourcedestination pair dynamic provisioning management brooklin creating new data pipeline also known datastreams modifying existing one easily accomplished http call rest endpoint kafka mirroring use case endpoint make easy create new mirroring pipeline modify existing pipeline mirroring whitelists without needing change deploy static configuration although mirroring pipeline coexist within cluster brooklin expose ability control configure individually instance possible edit pipeline mirroring whitelist add resource pipeline without impacting others additionally brooklin allows ondemand pausing resuming individual pipeline useful temporarily operating modifying pipeline kafka mirroring use case brooklin support pausing resuming entire pipeline single topic within whitelist even single topic partition diagnostics brooklin also expose diagnostics rest endpoint enables ondemand querying datastream status api make easy query internal state pipeline including individual topic partition lag error since diagnostics endpoint consolidates finding entire brooklin cluster extremely useful quickly diagnosing issue particular partition without needing scan log file special feature since intended replacement kafka mirrormaker brooklin kafka mirroring solution optimized stability operability introduced improvement unique kafka mirroring importantly strived better failure isolation error mirroring specific partition topic would affect entire pipeline cluster kmm brooklin ability detect error partition level automatically pause mirroring problematic partition autopaused partition autoresumed configurable amount time eliminates need manual intervention especially useful transient error meanwhile processing partition pipeline unaffected improved mirroring latency throughput brooklin kafka mirroring also run flushlessproduce mode kafka consumption progress tracked partition level checkpointing done partition instead pipeline level allows brooklin avoid making expensive kafka producer flush call synchronous blocking call often stall entire pipeline several minute migrating linkedin kafka mirrormaker deployment brooklin able reduce number mirroring cluster hundred dozen leveraging brooklin kafka mirroring purpose also allows u iterate much faster continuously adding feature improvement change data capture cdc second major category use case brooklin change data capture objective case stream database update form lowlatency change stream example linkedin sourceoftruth data job connection profile information resides various database several application interested knowing new job posted new professional connection made member profile updated instead interested application make expensive query online database detect change brooklin stream database update near realtime one biggest advantage using brooklin produce change data capture event better resource isolation application online store application scale independently database avoids risk bringing database using brooklin built change data capture solution oracle espresso mysql linkedin moreover brooklin extensible model facilitates writing new connector add cdc support database source
135,Lobsters,scaling,Scaling and architecture,"Building a ""Simple"" Distributed System - The What",https://jack-vanlightly.com/blog/2019/1/25/building-a-simple-distributed-system-the-what,building simple distributed system,system invariant invariant eventually rebalanser summary link image,diagram see application resource group extra application standby ready allocated resource case new resource added application shutting failing case app fails app take point resource accessing timethe library configure group appropriately permit application access given resource time simply creating virtual resource let say two resource want three application access one create resource rebalanser group make six resource point two real one far rebalanser concerned though resourcessystem invariantsbefore finish summarize desired behaviour library want introduce word invariant invariant rebalanser library must ensure mostmany already know mean case invariant rule assertion system object must remain true throughout lifetime rebalanser following invariant resource accessed two different node instance library resource accessed reasonable amount time rebalancinginvariant need hold circumstance node failing network partition could randomly addingremoving resource node randomly killing node every second week resource ever two node connected itbut one fundamental constraint rebalanser control even knowledge application access real resource rebalanser allocating resource admin registered group resource string rebalanser could work perfectly programmer written event handler properly application successfully start stop accessing resource might end two resource concurrently accessed accessed also invariant somewhat difficult prove really define reasonable amount time perhaps use word eventually basically mean rebalancing get stuck leave resource accessed also rebalancing interrupted rebalanser put time limit start stop event handling application want application load bunch state database either event handler mean rebalancing could theoretically take long time time resource may changed application could failed etc even short rebalancings suffer failure node midway cause new rebalancing get triggered new rebalancing cause current one abort obviously could disruptive want provide minimum time period rebalancingsso two invariant resource accessed time two different node given node correctly start stop access real resource instructed toa rebalanser group never become stuck hung given chance resource present beginning rebalancing eventually accessedi referring two invariant throughout whole serieswhat rebalanser summaryso let summarize list thing library saying itdetect resource added removed detect node added shutdown failed otherwise unreachablecome agreement balanced set resource allocation resource allocated evenly possiblefire onstart onstop event inform application resource start stop accessingensure invariant always hold even failure scenario node fail network partitioned library need ensure invariantsnext look protocol behaviour govern rebalanser library act order satisfy list requirement invariantsnote post work work pretty much done need write one expect next post course next couple weeksbanner image credit esoc malin link image
136,Lobsters,scaling,Scaling and architecture,"Nobody wants to be woken up at 4 AM, a monitoring journey",https://www.farfetchtechblog.com/en/blog/post/nobody-wants-to-be-woken-up-at-4-am/,nobody want woken monitoring journey,beginning blackbox whitebox turning point red use google four golden signal language infrastructure component grafana thanos blackbox exporter http wikiexamplecomtroubleshootingalertmanagernotificationsfailing summary thanos published,beginningfarfetch journey monitoring started since foundation year agoas one might expect several approach tested implemented discarded along way organic growth brought high proliferation tool solve specific usecases usually aiming consistent holistic solution company monitoring needsspeaking infrastructure context could distil monitoring tooling used two component paid opensource blackbox type software also integrated centralised logging system querying data via active check paid closedsource whitebox softwareasaservice saas two main solution required monitoring infrastructure saas normally useful debugging issue much alerting regarding blackbox tool ten thousand adhoc check created throughout yearswith amount alert traceability ownership became lost although manual process configure new check alerting route helping either management toil system tremendous hundred triggering alert became status quo alert fatigue settled became usual woken several time night trivial thing like single instance cpu pressurethe turning pointwe decided time stop think solve ongoing issue nobody want woken something impact customersmonitoring current context interpreted metric related visualisation alertingso started talking potential stakeholder collected requirement added came list whitebox blackbox monitoringhighly available easily scalableflexible pluggable alert routing slack ticketing system email pager etc selfservicecomplete automated workflowfully auditable process changesmetric collection infrastructure service database queuing service caching service web server homegrown tool etc instance container level metric collectionfuture proofing choice apmfuture proofing choice crossdatacenter aggregationfuture proofing choice longterm metric storagewe validated several solution freepaid closeopen source list weighed pro con fully transparent way entire company could pitch retrospective believe one cornerstone success current solution everything laid everyone see benefit importantly shortcoming expectation could mismatchedas tough problem technical also needed shift mindset regarding alerting needed step away alert normal dropped alerting level altogether replaced following approach urgency kind delivery method alert pager call later notification ticket email slackanother major shift regarding documentation new alert approved documentation must available meaningfulness alert troubleshooting step one could go fixing moreover approach made u start paying close attention service instead instance level signal stepping red use google four golden signal territoryto achieve goal collectively decided prometheus main tool back incubating project cloud native compute foundation cncf obviously moving away saas would incur time spent something related company core business specific case figured could gain much besides obvious cost reduction choosing free opensource pathcoincidently time embarked endeavour another companywide project born abstract interaction infrastructure business unit using concept blueprint exposed previous post language infrastructure definition requirement infrastructure yamlbased construct allows automation example application build definition instance provisioning deployment access control etc integrated alerting onto blueprint well making entirely selfservicewhat nowwe designed prometheusbased stack could easily provisioned datacenters making cloud vendor agnostic possible ensuring manual configuration required following diagram represents logical representation final result provide overview componentfrom getgo needed isolate state stack prometheus component could scaled horizontally without effortabout componentsgrafanagrafana represents visualisation component stack choice nobrainer due tight integration prometheus promql prometheus query languagethe dashboard built agnostic datacenter environment enforcing templating pretty much prometheus query added via merge request proper validation deployed every instancebecause fully reset grafana deployment hit issue folderdashboards id would change across instance causing folder structure break requesting data different node worked around using sticky session load balancer permanently fix problem built module talk grafana api ensure correct id foldersdashboards sticky session load balancer longer required dashboard readonly since change available solely via source controlanother issue faced due sharding prometheus server making extremely complex build dashboard using metric across shard thankfully brand new opensource project called thanos starting get traction kindly call thanos silver bullet prometheus shortcoming like longterm storage case solving crossshard dashboard queryingprometheusthe heart soul stack made sharding aspect cluster would easily manageable via source control could add remove shard quickly required includes shardspecific configuration one issue bumped service discovery cloud provider since service discovery run per scrape job thousand instance quickly realised scale furthermore suddenly started hitting rate limit provider api thus built service discovery enginealongside prometheus server also deployed handy blackbox exporter use run icmp check instance validate tl certificate probe application healthchecks etcalerting rule also deployed shard via blueprint example code snippet alerting contact email example examplecom slack examplechannel pager examplepagertoken alert alertname alertmanagernotificationsfailing expression rate alertmanagernotificationsfailedtotal notify environment dev route slack email environment prd route slack email pager dashboard alertmanagernotificationsstats description labelsjob instance labelsinstance failing send notification integration labelsintegration troubleshooting http wikiexamplecomtroubleshootingalertmanagernotificationsfailingsimilar dashboard alerting rule agnostic datacenter notification route tailored per environmentas several team using workflow wanted provide best experience possible decreasing complexity alert creation process mandatory achieve goal abstracted exotic promql query bitesized expression containermemoryworkingsetbytes id docker containerspecmemorylimitbytes id docker instance groupleft labelreplace nodememorymemtotalbytes instance instance containerspecmemorylimitbytes id docker using prometheus recording rule previous alert converted containermemusedpercent name containerexample greatly streamlines onboarding new team eas code review process improves overall prometheus server performancehere find highlevel deployment method deployment strictly idempotent allows roll many time requiredalertmanageralertmanager responsible routing alert destination extra requirement regarding generation report fired alert could better understand going globally distributed infrastructure fulfil need built service push every triggered alert elasticsearch making easier visualise history alertsalso guarantee aware alertmanager reason unable send alert also implemented deadman switch basically always firing alert ever stop firing get paged investigateexporterssomething missing diagram exporter completely honest lost count number different exporter currently actively contribute plan continue thriving community around prometheus incredible proud part itsummarythe initial working proofofconcept stack delivered within two month successfully replacing saas solution take risk relying new technology risk paid obviously backup plan always thought progress never blocked couple month later first official communication thanos published august google office munich attending promcon announced prometheus became second project graduate cncf following kubernetes announcement validated choice madethe mindset still changing across farfetch full ownership selfservice approach make enticing jump onboard currently ingested metric deployed stack sum data point per second increasing daily basisit quite ride seem going slow time soon excited hope get opportunity opensource code writtenthis birdseye overview journey far sleep pattern became indeed much better
137,Lobsters,scaling,Scaling and architecture,Distributed architecture concepts I learned while building a large payments system (2018),https://blog.pragmaticengineer.com/distributed-architecture-concepts-i-have-learned-while-building-payments-systems/,distributed architecture concept learned building large payment system,rewriting app moving engineering management working uber little distributed system experience operating large distributed system reliable way designing dataintensive application sla slas availability minute downtime accuracy capacity latency latency slas matter building large payment system horizontal v vertical scaling successfully scaled vertically scaling strategy matter building large payment system consistency several consistency model strong consistency weak consistency eventual consistency eventual v strong consistency consistency matter building large payment system data durability durability good article data durability matter building payment system message persistence durability message queue one message persistence durability matter building large payment system idempotency ben nadel writes different strategy used idempotency matter building large payment system sharding quorum sharding resharding nice postmortem shared quorum sharding matter building payment system uber use quorum actor model actor model csp communicating sequential process actor model minute brian storti actor library framework akka toolkit actor model matter building large payment system reactive architecture reactive manifesto video reactive architecture matter building large payment system akka closing operating large distributed system reliable way interested learning distributed system designing dataintensive application big idea behind reliable scalable maintainable system amazon builder library russian translation japanese translation,building large scale highly available distributed system architecture concept need use practice post summarizing one found essential learn apply building payment system power uber system load thousand request per second critical payment functionality need work correctly even part system downas background joined uber two year ago mobile software engineer backend experience ended building payment functionality app rewriting app way afterwards ended moving engineering management heading team meant getting exposed backend team responsible many backend system enable paymentsbefore working uber little distributed system experience background traditional computer science degree decade full stack software development however able draw box talk tradeoff much understanding appreciation distributed concept like consistency availability idempotencyis list go complete probably stuff would made life easier known earlier let dive thing like slas consistency data durability message persistence idempotency thing needed learn job update read also followup post article operating large distributed system reliable way also recommend book designing dataintensive application readingslawith large system process million event per day thing bound go wrong diving planning system found important thing decide system healthy mean healthy something actually measurable common way measure healthy slas service level agreement common slas seen used availability percentage time service operational tempting want system availability achieving really difficult well expensive even large critical system like visa card network gmail internet provider nt availability year second minute hour many system four nine availability minute downtime per year considered high availability getting level quite work usually get toaccuracy ok data system inaccurate lost percentage acceptable payment sytems worked accuracy needed meaning data allowed lostcapacity expected load system able support usually expressed request per secondlatency time system respond time request request served system usually lot noisy request hence latency practical usage real worldwhy slas matter building large payment system put together new system replacing existing one make sure build right thing system better predecessor used slas define expectation availability one top requirement defining target needed consider tradeoff architecture able meet thishorizontal v vertical scalingassuming business using newly built system grows load increase point existing setup able support load capacity need added two common scaling strategy vertical horizontal scalinghorizontal scaling adding machine node system increase capacity horizontal scaling popular way scale distributed system especially adding virtual machine cluster often easy click buttonvertical scaling basically buying biggerstronger machine either virtual machine core processing memory distributed system vertically scaling usually le popular costly scaling horizontally however major site like stack overflow successfully scaled vertically meet demandwhy scaling strategy matter building large payment system decided early would build system scale horizontally vertical scaling possible case payment system already projected load pessimistic single superexpensive mainframe could even handle today mention future also engineer team worked large payment provider tried failed scale vertically largest machine money could buy timeconsistencyavailability system important distributed system often built top machine lower availability let say goal build system availability minutesyear using machinesnodes average availability hoursyear straightforward way get availability number add bunch machinesnodes cluster even node others overall availability system higher availability individual componentsconsistency key concern highly available system system consistent node see return data time going back previous model added bunch node achieve higher availability ensuring system stay consistent trivial make sure node information need send message keep sync however message sent fail deliver get lost node might unavailableconsistency concept spent time understanding appreciating several consistency model common one used distributed system strong consistency weak consistency eventual consistency hackernoon article eventual v strong consistency give nice practical overview tradeoff model typically weaker consistency required faster system likely return latest set datawhy consistency matter building large payment system data system needed consistent consistent part system strongly consistent data would example knowing payment initiated something needed stored strongly consistent way part le missioncritical eventual consistency something considered reasonable tradeoff good example listing recent transaction could implemented eventual consistency meaning latest transaction might show part system return operation return lower latency le resource intensive data durabilitydurability mean data successfully added data store available going forward case even node system go offline crash data corrupteddifferent distributed database different level durability support machinenode level durability cluster level nt provide box form replication usually used increase durability data stored multiple node one node go data still available good article achieving durability distributed system challengingwhy data durability matter building payment system many part system data could lost given something critical like payment distributed data store built needed support cluster level data durability even instance would crash completed transaction would persist day distributed data storage service like cassandra mongodb hdfs dynamodb support durability various level configured provide cluster level durabilitymessage persistence durabilitynodes distributed system perform computation store data send message key characteristic sending message reliably message arrive missioncritical system often need zero message lostfor distributed system messaging usually done distributed messaging service rabbitmq kafka others messaging service support configured support different level reliability delivering messagesmessage persistence mean failure happens node processing message message still process failure resolved message durability usually used message queue level durable message queue queue node go offline message sent still get message come back online good article read topic onewhy message persistence durability matter building large payment system message could afforded lost message person initiated payment ride meant messaging system used needed lossless every message needed delivered however building system delivers message exactly one delivers least different complexity decided implement durable messaging system least delivery chose messaging bus top would build ended going kafka setting lossless cluster case idempotencywith distributed system thing go wrong connection could drop midway request time client often retry request idempotent system ensures matter many time specific request executed actual execution request happens good example making payment client make request pay request successful client time client could retry request idempotent system person paying would get charged twice nonidempotent system coulddesigning idempotent distributed system require sort distributed locking strategy earlier distributed system concept come play let say intend implement idempotency optimistic locking place avoid concurrent update order optimistic locking system need strongly consistent time operation check another operation initiated using sort versioningthere numerous way achieve idempotency depending constraint system type operation designing idempotent approach nice challenge ben nadel writes different strategy used distributed lock database constraint designing distributed system idempotency one easily overlooked part come across scenario team burned ensuring correct idempotency key operationswhy idempotency matter building large payment system importantly avoid double charge double refund given messaging system least lossless delivery need assume message might delivered multiple time system need ensure idempotency chose handle versioning optimistic locking sytems implement idempotent behaviour use strongly consistent store data sourcesharding quorumdistributed system often store lot data single node one go storing bunch data certain number machine common technique using sharding data horizontally partitioned using sort hash assign partition many distributed database implement sharding hood sharding interesting area learn especially around resharding foursquare hour downtime due hitting sharding edge case nice postmortem shared root causesmany distributed system data computation replicated across multiple node order make sure operation performed consistent way voting based approach defined certain number node need get result operation successful called quorumwhy quorum sharding matter building payment system uber basic concept pretty commonly used personally came across concept looking setup cassandra replication cassandra distributed system use quorum local quorum ensure consistency across cluster funny side effect meeting enough people room someone would ask start quorum actor modelthe usual vocabulary describing programming practice thing like variable interface calling method assume single machine system talking distributed system need use different set approach common way describing system following actor model think code term communication model popular match mental model would think example describing people communicate organization another also popular way describing distributed system csp communicating sequential processesthe actor model based actor sending message reacting actor limited set thing create actor send message others decide next message simple rule complex distributed system described well also repair actor crash short overview recommend article actor model minute brian storti many language implemented actor library framework example uber use akka toolkit systemswhy actor model matter building large payment system building system many engineer lot distributed experience decided follow standard distributed model coming distributed concept potentially reinventing wheelreactive architecturewhen building large distributed system goal usually make resilient elastic scalable may payment system another high load system pattern similar people industry discovering sharing best practice work well case reactive architecture popular widely used pattern spaceto get started reactive architecture suggest reading reactive manifesto watching video topicwhy reactive architecture matter building large payment system akka toolkit used build much new payment system heavily influenced reactive architecture many engineer building system also familiar reactive best practice following reactive principle building responsive resilient elastic messagedriven system thus came quite naturally model fall back check progress right track something found helpful using model building future system wellclosingi lucky enough participate rebuilding highscale distributed missioncritical system one powering payment uber working environment picked lot distributed concept use previously summarized hope others find helpful start continue learning distributed systemsthis writeup strictly focused planning architecting system whole lot thing said building deploying migrating highload system well operating reliably topic another post operating large distributed system reliable wayfor interested learning distributed system recommend book designing dataintensive application big idea behind reliable scalable maintainable system practical book found far cover different distributed concept gone together realworld example amazon builder library also specific indepth topic distributed systemsnote read russian translation post well japanese translation
138,Lobsters,scaling,Scaling and architecture,Code Versus Data,https://theartofmachinery.com/2016/06/21/code_vs_data.html,code versus data,problem two solution comparison dataoriented solution win know polymorphism implemented,year back wrote program c using obvious objectoriented architecture later rewrite c learned pretty good lesson software design problem original program little hard explain invent new problem use example making classic hackandslash roguelike game player control hero dungeon fighting monster like orcs dragon model monster code two solution learned oop obvious application polymorphism create monster base class use inheritance create subclass like orc dragon type monster unlike toy example polymorphism one actually valid work correctly call codeoriented solution upon time would accepted obviously proper model thought another solution create single monster class single monsterspec class monster regardless type straightforward instance monster make orc orc dragon dragon pointer immutable monsterspec instance orcs share common monsterspec instance specifies orc attribute like strength speed aggressiveness well orc animation sound effect similarly another monsterspec every type monster call dataoriented solution comparison want implement monster moving around dungeon attacking player two solution minor pro con mostly equivalent difference start appear add feature codeoriented solution solves problem using code related problem end solved code well likewise dataoriented solution make data solution problem example say want game load predesigned dungeon map monster codeoriented oop solution writing monsterfactory orcfactory dragonfactory implementation create appropriate monster specified map data dataoriented solution requires table look monsterspec instance name something want implement saving loading game hence monster codeoriented solution end writing serialisation deserialisation code subclass monster dataoriented solution need way de serialise monster instance big one suppose want dungeon map able create new dungeonspecific monster codeoriented solution requires plugin system way map data generation integrated code build system map data stay binary compatible code dataoriented solution need spec new monster dataoriented solution win deliberately picked example highlight mistake originally made jumping model embodied solution code ie class hierarchy problem data counterpoint game needed monster complex specialised ai would problem code though popular real game development embed scripting language behaviour data hybrid approach possible know polymorphism implemented might notice monsterspec like vtable contains data instead function pointer using language like c drawback dataoriented architecture code statically checked data data language need simple language hard dichotomy codeisrigiddataisflexible extreme lisp data code code data downside even code statically checked language runtime reflection eg java compiletime reflection eg conscious code versus data important simple maintainable software
139,Lobsters,scaling,Scaling and architecture,The convoy phenomenon,https://blog.acolyer.org/2019/07/01/the-convoy-phenomenon/,convoy phenomenon,convoy phenomenon convoy form great dorset steam fair contention coherence scalability killer sleep holding lock breaking convoy slow,convoy phenomenon blasgen et al ibm research report revised today jumping hotos topic hot topic thanks pat helland recommendation jim gray one author combination hard ignore setup relayed pat permission work part good sized team working large system implementation one senior engineer year experience mentioned problem system seems test load would behave beautifully performance fell floor system crawled forever never seemed get state work getting done pathetic rate said convoy response huh forwarded paper convoy phenomenon confess heard convoy phenomenon either go take moment think possible cause system behaviour described lot thing cause performance cliff interesting thing system never recovers suggestive queue queue depth increased service arrival time finely balanced always seeing long wait time situation expect see unusually high latency normal throughput partially right steadystate queue involved plus although described performance degradation observed case would almost certainly poor latency poor throughput convoy form driving twolane road passing one often encounter cluster car fastmoving car soon bump slow one equilibrium state system everyone convoy behind slowest car english broad one common cause tractor moving field public highway dorset maybe english county definitely dorset seems penchant steam engine rally summer eg great dorset steam fair bit like distributed denial service attack road network lot slow steam engine converge given point enough tractor steam engine process case system r transaction also bump contending shared resource contention appears conflicting request lock know contention coherence scalability killer still explain performance never recovers duration lock average number instruction executed lock held execution interval lock average number instruction executed successive request lock process collision cross section lock fraction time granted ie lock grant probability single processor duration duration execution interval first approximation ignoring wait time switching time hightraffic lockprotected resource system r buffer pool recovery log system entryexit respective stats process go wait state hold high traffic lock l eg buffer pool process scheduled le immediately request l high traffic lock remember transaction find lock l busy wait hold lock sleeping process waiting lock hold already look bad turn trap system find hard escape system sleep wake run release l say instruction granted l executes instruction buffer pool execution interval request l l busy many process ahead queue process dispatched rerequests l enqueues lock go sleep n process processor n much bigger lock queue contain nm process process execution interval instruction system situation lock thrashing cpu dedicated task switching stable phenomenon new process sucked convoy process leaf convoy io wait lock wait return convoy probably still exist sleep holding lock convoy observed system r mv ims system r code followed following rule never lock wait high traffic lock held never io high traffic lock held log lock occasional exception never page fault high traffic lock held control process scheduling greatly reduce probability caught holding high traffic lock still reduce probability zero given high consequence occur still problem u stuck preemptive scheduler ie general purpose operating system virtual memory hence convoy occur problem make evaporate quickly occur rather persist forever breaking convoy trivial solution would run one process time way lock contention however obvious scalability issue next simplest solution avoid lock one go long way shrewd use atomic machine instruction compare swap programming done lot unable completely eliminate high traffic lock program lock try minimise contention reducing increasing lock granularity eg row lock table lock lock mode eg nonexclusive lock help make convoy le likely le stable probably eliminate system r team also experimented spin lock cpu wastage turned worse caused convoy using technique system r reduced lock wait high traffic lock factor ten lock wait high traffic lock system r solve end key issue convoy associated granting lock firstcome firstserved order elect grant lock request random order hope eventually everyone get service theory process might starve never granted request fact underlying scheduler stochastic nature real system cause process eventually get service note talking adding randomness waiter given lock get access next changing order given process acquires lock way lead deadlock given single processor convoy exists high contention lock released releasor dequeues member convoy lock mark lock free signal member convoy releaser continues run high probability let call p terminate lock free whichever signalled process run next able acquire lock queue n process convoy disappear probability pn multiprocessor case another processor may look acquire lock critical section solution lock requestor spin instruction hope lock becomes free enqueue sleep slow let go back opening story see turned next day told system indeed convoy solution simple
140,Lobsters,scaling,Scaling and architecture,Aggressively tuning Cosmos DB (the long way round),http://blog.tdwright.co.uk/2019/06/29/aggressively-tuning-cosmos-db-the-long-way-round/,aggressively tuning cosmos db long way round,headup lab spreading load timezone cohort random implementing index document shielding cosmos redis documentation azurefunctionsautofac still tweaking retries helpful doc logging cosmos db transaction outsmart sdk beware update calculating ru cost azure cosmos capacity calculator one slightly smaller small one naive split total preemptive scaling tuning index policy documentation seems support oneline fix performance tip reflection code consolidation growing problem doc lack documentation partial logging conclusion,many way tune cosmos db eventually pretty desperate attempt coax scaling nicely tried approach became arcane saw diminishing return flatout counterproductive eventual solution case singleline fix embarrassing journey really interesting journey want share today regular reader know work headup lab make health app applies data science user data order help understand body use range azure technology ingest data wearable device apply data science scale since launching app back october rapidly scaled reach many ten thousand user one hand scaling rate validated lot strategic decision took early technology use whole pleased azure function cosmos db hand rapid intake user unforgiving come detail implementation one main battle overloading cosmos db since early day found provisioning enough request unit ru peak time expensive flip side failing provision enough ru mean request get throttled start see dreaded status code log scaled found period managing completely swamp database period number request eliciting status requestratetoolarge could dwarf number successful request eventually realised problem primarily caused really simple misconfiguration skip end want skinny issue fix solution really interesting bit though reason wanted write blog post talk thing worked big issue journey took technique tried interesting topic hope sitting spreading load first way tried tame cosmos simple highly effective architectural approach many job driven timertriggered azure function instance might run certain data model user day staggering execution task able perform amount work much lower ru ceiling instead triggering job midnight utc would instead place many item azure storage queue different visibilitytimeout property prevents item dequeued specified amount time elapsed depending type task use variety strategy determine amount delay item common pattern use user timezone offset task whole number hour task always happen roughly time day local userapply offset within hour based user cohort arbitrary designation might result group spaced minute apartwithin cohort band apply random number second user distributed approximately evenly throughout minute period overall effect smooth massive daily spike smaller constant stream work since need provision cosmos able handle biggest spike strategy result big cost saving implementing index document another problem initial naive implementation around complicated query logic user may large collection rating relating different aspect health maintain several category rating may different status common query might make would find recent rating document combination category status sort query quickly became expensive number user number rating per user grew able buy time switching query using textbased representation datetime inbuilt t property numeric representation time document last changed eventually though needed rethink approach altogether route ended taking introduce new type document ratingsindex document basically grouped collection id rating document elsewhere cosmos update document whenever change rating query work rating use might seem counterintuitive replaced one query two trick lie fact actually replacing one expensive query combination cheap query read read operation like call filesystem database query insanely efficient effectively shifted logic selecting rating readtime writetime since read lot write found highly beneficial cost combination extra write update index extra read queried index offset gain made removing expensive frequentlycalled query shielding cosmos redis even spreading load growth meant cost kept rising application read lot write decided expand cosmos implementation say expand already using redis cache database call made api project commonplace using layered design aspnetbased api database logic centralised repository layer made straightforward weave caching defensive layer problem api actually pretty thin heavy lifting performed numerous azure function idiomatic way access database function accepting client via binding parameter example lifted straight documentation functionname docsbyusingdocumentclient public static async task lt iactionresult run httptrigger authorizationlevelanonymous get post route null httprequest req cosmosdb databasename todoitems collectionname item connectionstringsetting cosmosdbconnection documentclient client ilogger log stuff client dozen function written form despite caching logic implemented repository sitting api project real way using reimplementing caching function would meant massive amount duplicate code well errorprone hard debug luck changed discovered unofficial implementation autofac azure function azurefunctionsautofac suddenly could inject dependency function indeed api project already using autofac di extremely simple start sharing service repository remove duplicated functionality much code deleted day removing direct binding cosmos azure function replacing cacheaware repository call able slash number request making way database still thing looking lot better able grow userbase without unduly cranking provision ru succumbing unacceptable number throttled request started see unpleasant symptom peak hour number request would rise beyond capacity stay hour two time would see higher volume requestratetoolarge exception status code purple hill definitely downer event became frequent significant time worse beginning see application misbehave result job finish data would end strange state get well might imagine drive find solution became urgent tweaking retries one idea latched onto early somehow combination using storage queue azure function net cosmos db sdk queue database attempt retry failed operation see database unable handle query instance sdk back try set number time throwing exception exception cause function fail point claim queue item lapse meaning another instance function free swoop try implication single queue item may result many time request database might expected according helpful doc default value retries cosmos sdk storage queue word single item queue could cause database hit could seeing severe failure rate true implied two thing firstly suggests raising provisioned throughput little bit might big effect failure rate additional success might prevent request secondly prompted u think might want retries behave operation really need chance succeed encouraged bump throughput little bit also reduced number retries backend application left api project alone figuring impact failure likely greater reasoning something along line eventual consistency might guessed great strategy rate barely affected found system managed get pretty strange state increasing number operation failed hard reverted change logging cosmos db transaction one key lesson adventure failed experimentation retries exhausted normal idea flying blind accordingly started look available data newfound vigour metric built azure well application insight able tell u broad stroke issue lay could tell instance spending higher proportion ru upserts previously effect caching suppose digging deeper really possible azure portal however found unable derive anything actionable confidently point particular hotspot code frustrated borrowed trick past world sql decided try profiling cosmos db usage thankfully earlier move centralise database logic set shared repository paved way repository used underlying cosmos db client class relatively painless add code log request decided write request type query calling method cost ru new azure storage table leaving profiling code enabled day able hook power bi table transaction data quite match overall pattern seeing final section make reason painfully obvious strong lead pursue outsmart sdk thing tried looking profile data yielded fruit put together class bittersweet realisation bitter see improvement hoping sweet realised net sdk cosmos already smarter gave credit dwell give example left inappropriate setting place deep bowel implementation following enablecrosspartitionquery true thought perhaps maybe query incurring ru cost partition given database spanned many partition seemed though might explain explosion ru consumption simple thing try query access partition key thankfully disabled enablecrosspartitionquery explicitly set partitionkey within feedoptions object result change whatsoever sdk clearly working magic away prying eye optimising query without u even knowing whilst one hand great know sdk smart hand search solution would continue beware update one thing learnt profiling effort seemingly disproportionate cost update operation maxime rouiller point blog post subject calculating ru cost update effectively delete followed create result ru cost time greater would incurred reading document looking output profiling clearly showed hot spot caused update significant case frequently updating single property relatively large document since cost ru proportional size document decided split document two one small document containing property often need update one large document rest read two document come cost two document least standard set core property eg t etag mean necessarily reading data splitting document cost difference reading writing writing smaller document result big enough saving overcome penalty incurred reading two document use azure cosmos capacity calculator estimate economics split leave general setting default value change section relating document imagine single big document like one need read update document like per second would require u provision ru read writes imagine took couple property big doc leave slightly smaller moved small one read time per second wrote smaller one frequency total throughput drop ru read writes toy example already see improvement naivesplit reallife example reducing size document update frequently able shave close ru cost one frequent operation preemptive scaling whilst effort point resulted noticeable gain still facing big peak trough consumption peak invariably seeing huge number throttled request demand exceeded throughput provisioned keeping database scale would satisfy demand looked prohibitively expensive order handle peak without paying odds quieter period wrote function automatically scale cosmos anticipate demand fortunate period high demand fairly predictable could scale schedule turn altering number provisioned ru per second possible documentclient code scale database straightforward var coll cosmosdbclient createdocumentcollectionquery urifactorycreatedatabaseuri databasename c cid collectionname asenumerable single var offer cosmosdbclient createofferquery oresourcelink collselflink asenumerable single var newoffer new offer newrus await cosmosdbclientreplaceofferasync newoffer see hard part creating new offer object new provision run timertriggered function look storage table schedule check schedule quarterto quarterpast hour scale database depending known pattern demand granted solve problem higherthananticipated ru demand make significantly cheaper whilst find proper solution tuning index policy increasing desperation reduce ru spend started look cosmos indexing policy default recommended approach allow cosmos index every property every document initially accepted sceptical everything could index metadata vast harming performance tried switching away default policy every property indexed instead used data profiling exercise identify property actually using query instructed cosmos index motivation behind reduce cost write operation documentation seems support strategy result impressive hoped oneline fix started run idea still closer performant system began despair despair started aimlessly combing code even started looking code knew worked knew worked written month ago used ever since modified since long start current woe possibly fault still leafed file scrolling hopelessly ancient method one class looked cosmos client singleton first started writing code access cosmos followed performance tip implemented client shared singleton looked fine wanted double check autofac properly resolving singleton decided step yep definitely sharing single instance method take run ran behaviour strange lady gentleman turn left particular method call query method instead constructor method call either call method proactively fetch metadata collection order speed query given size database metadata request taking hundred millisecond costing u large number provisioned ru fix comically simple end moved openasync method performing actionsqueries method instantiated singleton soon deployed change plateau huge number response went away throughput dropped much able reduce number ru provisioning word fix instantly slashed database bill everyone delighted cost way exception nonexistent overnight app started behaving designed reflection week since wrapped thinking writing blog post thinking whole load related question chief among take u long get right think answer nuanced interesting fundamentally break following factor code consolidation problematic class used api first started using function project started injecting dependency repository word spread problem part fixa growing problem openasync bit black box doc say little hood think pull dictionary partition key partition would make sense would allow sdk make query lightning fast would also explain notice issue database much smaller nonprod db upshot problem literally growing parallel databaselack documentation similar vein readfeed operation could see azure metric clearly something problem googling answer question know openasync performs readfeed operation manage make connection afterwardspartial logging remember said data profiling work quite match seeing azure well considered openasync might costing ru tracking cost query upserts etc imperfect data able target sensible incremental optimisation missed fundamental reason system misbehaving underpinning suppose good dollop new tech inexperience hindsight already provisioned throughput needed familiar enough technology alarm bell ring hand u ever learning thing learnt cosmos db route sometimes necessity best instructor well knowledge gained kept lot change made still use variety technique spread load smooth spike fact central part architectural thinking regardless issue code availability model cosmos suited baseload spikesindex document something kept extended document frequently read infrequently written lot efficient rolled technique document typesinjecting repository caching logic still smart move even initially spread defect db client still use repository throughout application encapsulate logic around caching database accesswe left code log call cosmos profiling new subsystem future need flip flag config database call get logged along detail responsible repository methodwe retained ability scale database schedule even everything else fixedimplemented still peak offpeak time mainly due u bias geographic spread user desire certain action run certain time day user local timezones kept code scale provisioned throughput throughout day lot way problem encountered made u wiser driven u create tool ongoing utility conclusion last couple week prior u finding fix pretty unpleasant team feeling pressure doubt creeping around whether cosmos even right tool job emerged side change dramatic whole system behaves designed paying much le safe say doubt cosmos evaporated tried lot tactical solution problem end far fundamental along way learnt load built cool tool twas nought decided publish list thing done still deep wood guess answer would simple moving one method call imagined writing definitive playbook tuning cosmos stumbled solution first thought dismiss thing tried ultimately though journey interesting hope read full post agree
141,Lobsters,scaling,Scaling and architecture,"An open ledger for writing scalable, mission-critical, decentralized WebAssembly applications",https://wavelet.perlin.net,open ledger writing scalable missioncritical decentralized webassembly application,webassembly rust click click,miss using ide debugging tool ability write unit test benchmark endtoend pipeline come writing smart contract wavelet allows developer write missioncritical robust scalable decentralized application form webassembly smart contract battery initially included development rust use ole tool know love efficiently costeffectively create next decentralized application click info wavelet rust smart contract sdk click info wavelet assemblyscript smart contract sdk
142,Lobsters,scaling,Scaling and architecture,Forget monoliths vs. microservices. Cognitive load is what matters,https://techbeacon.com/app-dev-testing/forget-monoliths-vs-microservices-cognitive-load-what-matters,forget monolith v microservices cognitive load matter,excessive cognitive load work effective team ownership overview monolith microservices traditional monolithic software architecture design based microservices start monolith extract microservices tammer saleh start microservices beginning stefan tilkov simon brown distributed microservices big ball mud team topology organizing business technology team fast flow devops enterprise summit london define cognitive load psychologist john sweller total amount mental effort used working memory intrinsic cognitive load extraneous cognitive load germane cognitive load managing cognitive load team learning jo pearce cognitive load applied team use team cognitive load rightsize microservices build run julia evans reducing cognitive load team mean setting interface boundary three way reduce team cognitive load improve flow create welldefined team interaction pattern russell ackoff almost always product interaction part never action single part collaboration xasaservice facilitating use independent streamaligned team build thinnest viable platform justin kitagawa reduce engineer cognitive load net promoter score apifirst selfservice gatekeeper declarative imperative build empathy lighten load devops enterprise summit london keep learning,monolith versus microservices debate often focus technological aspect ignoring strategy team dynamic instead starting technology smartthinking organization beginning team cognitive load guiding principle effective delivery operation modern software system excessive cognitive load work effective team ownership supportability software approach problemoverview monolith microservicesmany organization moving traditional monolithic software architecture design based microservices serverless allowing take advantage newer runtimes help team take ownership software serviceshowever difficult software architect team lead technical leader ass right size service microservice limited line code start monolith extract microservices tammer saleh recommends start microservices beginning advised stefan tilkov avoid simon brown call distributed microservices big ball mud research forthcoming book team topology organizing business technology team fast flow working client different part world realized many organization fail consider important dimension decision around size software service team cognitive loadmost confusion around sizing service go away reframe problem term cognitive load single serviceowning team handle see special coverage devops enterprise summit london define cognitive loadbut first mean cognitive load applies team psychologist john sweller defined cognitive load total amount mental effort used working memory went describe three different kind cognitive load intrinsic cognitive load relates aspect task fundamental problem space example class defined java extraneous cognitive load relates environment task done example deploy component germane cognitive load relates aspect task need special attention learning high performance example service interact abc service broadly speaking attempt minimize intrinsic cognitive load training good choice technology hiring pair programming etc eliminate extraneous cognitive load boring superfluous task command add little value retain working memory leave space germane cognitive load valueadded thinking lie great overview cognitive load applies software development see article managing cognitive load team learning jo pearcecognitive load applied teamswhen apply concept cognitive load whole team need limit size software system team expected work nt allow software subsystem grow beyond cognitive load team responsible strong quite radical implication shape architecture software system software architecture becomes much teamshaped explicitly consider cognitive load indicator supportability operabilitythe drive minimize extraneous cognitive load also lead need focus developer experience operator experience using explicitly defined platform component team able reduce extraneous cognitive loadsome organization even begun use cognitive load explicit input software architecture system boundary decisionswhy use team cognitive load rightsize microservicesin world build run whole team responsible successful operation software service imperative remove unnecessary barrier team ownership software obscure command arcane configuration option increase extraneous cognitive load team member effectively reducing capacity acquiring improving businessoriented aspect germane cognitive load another typical example waiting another team provision ticket infrastructure update configuration interrupt flow dependent team resulting reduction effective use cognitive capacityreduced team cognitive capacity put strain team ability fully software service team spending much time dealing complicated configuration errorprone procedure andor waiting new environment infrastructure change pay enough attention important aspect testability runtime edge casesas software developer julia evans say reducing cognitive load team mean setting interface boundary every techie organization nt need kubernetes expertput another way ensuring cognitive load team high better chance enhance supportability operability software team working better service team understands betterthree way reduce team cognitive load improve flowthere magic formula reducing cognitive load team worked many large organization around world including china europe u recommend three helpful approach welldefined team interaction pattern independent streamaligned team thinnest viable create welldefined team interaction patternstoo often organization relationship team well defined understood russell ackoff said problem arise organization almost always product interaction part never action single part likely heard complaint collaborate team team provide u need sign team interaction within organization ambiguous team topology book identify three core team interaction mode help clarify define team interact collaboration working together another team defined period time discover new way working new tool new solutionsxasaservice consuming providing something service clear api clear expectation around service levelsfacilitating helping helped team gain new skill new domain awareness adopt new technologywith welldefined team interaction pattern place begin listen signal organization level team interaction working well including problem cognitive loadfor example collaboration interaction go long perhaps signal aspect technology would better provided service platformsimilarly one team expects consume monitoring tool service constantly need work providing team diagnose problem could signal much cognitive load consuming team need simplify use independent streamaligned teamsit increasingly common large small organization see small crossfunctional team mix skill owning entire slice problem domain idea live service team often called product feature teamsbut comingofage iot ubiquitous connected service call streamaligned product loses meaning youre talking manytomany interaction among physical device online service others product often physical thing case streamaligned team aligned stream change required segment organization whether line business market segment specific geography government serviceit hugely important ensure streamaligned team analyze test build release monitor change independently team vast majority work dependency introduce substantial amount cognitive load eg waiting microservices environment able test microservicesfocused monitoring ensuring streamaligned team substantially independent daytoday flow work remove unhelpful extraneous cognitive load allowing team focus intrinsic germane domainrelevant aspect work part independence come able use effective platformin larger organization useful align two three team close partnership delivering large complicated system close relationship help avoid one team waiting anotherobviously team depend service associated team providing infrastructure runtime apis tooling dependency dont block flow work streamaligned team able selfservice new test environment deployment pipeline service monitoring example nonblocking dependency streamaligned team consume independently build thinnest viable platformstreamaligned team expect consume service welldefined platform avoid massive unfriendly platform yesteryear instead build thinnest viable platform tvp smallest set apis documentation tool needed accelerate team developing modern software service systemssuch tvp could small single wiki page defines public cloud provider service team use larger organization might decide build additional service atop underlying cloud iot platform extra service always thick enough accelerate flow change streamaligned team thickeravoid frequent mistake past internal platform bloated slow buggy terrible user experience make matter mandatory usea good platform act force multiplier streamaligned team helping focus core domain functionality attention developer experience ease use simplicity tooling richness documentation short build run platform product service streamaligned team internal customer using standard agile devops practice within platform itselfthe engineer cloud communication company twilio taken approach internally delivery squad presentation qcon senior director engineering justin kitagawa described twilio internal platform evolved reduce engineer cognitive load providing unified selfservice declarative platform build deliver run thousand global microservicesfurthermore platform developer experience regularly assessed via feedback internal customer using net promoter scorethe internal platform twilio explicitly follows key principle apifirst empower dev team innovate platform feature via automationselfservice gatekeeper help dev team determine workflowdeclarative imperative prefer build empathy understand need frustration people using platformthis approach enabled twilio scale customer base organization worldwideby reducing cognitive load good platform help dev team focus differentiating aspect problem increasing personal teamlevel flow allowing whole team effectivelighten loadteam cognitive load important dimension considering size shape software system boundary ensuring team cognitive load isnt high increase chance team member able build operate service effectively properly understand system buildingwe recommend use three core team interaction mode clarify interaction team ultimately help reduce cognitive load used independent streamaligned team thinnest viable platform team interaction mode help organization detect cognitive load high different part systemswant know cognitive load attend talk monolith v microservices missing point start team cognitive load devops enterprise summit london run june keep learning
143,Lobsters,scaling,Scaling and architecture,"Hydra Chronicles, Part I: Pixie Dust",https://www.brodieg.com/2019/05/17/pixie-dust/,hydra chronicle part pixie dust,dawn know nothing lernean hydra computing group statistic next week brodiegaslam including time required sort data disclaimer latency enemy microarchitecture rescue machine instruction modelling cache effect pictured die current working set observed available appendix degree pathology subset c code previous loop c function branch prediction optimization turned faustian bargain md meltdown spectre downright funny loose end hash table value duplicated us one assign unique index twenty million randomly conclusion appendix reference every programmer know memory assembly language programming ubuntu efficient algorithm exploiting multiple arithmetic unit improving directmapped cache performance addition small fullyassociative cache prefetch buffer microarchitecture intel amd via cpu meltdown reading kernel memory user space image credit hydra fedora larry wentzel conveyor belt westmere core bad concurrency post intel skylake fritzchens frits asus motherboard meltdown spectre foreshadow natascha eidl md marina minkin function definition systime cache model index chasing branch prediction body post benchmarking code,dawn know nothing supposed way looking drawing lernean hydra laughing as post going short sweet useful instead turned manyheaded monster smugly sitting chest smothering follows retelling flailing attempt chopping one head started lark see could make base r competitive undisputed champ datatable computing group statistic thought alone scoffworthy enough deserve little sympathy offtherails whole thing gone tl dr ordering data huge impact quickly processed microarchitectural factor cache outoforder execution branch prediction yes even r let skip back carefree hydraless day twitter known better post great example fo work rdatatable team contributes radix sort rstats faster ordering eg type improvement fundamental operation invaluable next week thank mattdowle arunsriniv brodiegaslam next week aaaah fool almost three month ago key observation back datatable contributed radix sort r result order became faster work load also alluding something fairly remarkable happens order data process let illustrate item group rngversion setseed n ngrp grp sample ngrp n replacetrue x runif n first step computing group statistic split data group g split x grp str head g display structure first group list num num num split break vector x list vector defined grp see structure first three vector slow step group statistic computation let time systime wrapper around systemtime systime g split x grp user system elapsed compare happens order input first systime order grp xo x go grp gso split xo go user system elapsed sorting value group prior running split make twice fast including time required sort data doubled speed important workhorse r function without touching code get exact result identical g gso true pixie dust datatable pixie dust disclaimer special knowledge microarchitecture learned researching post guarantee accuracy detail herein broad outline mostly correct backed experimentation hope relative inexperience topic make writing accessible latency enemy package rack please something going underneath hood dramatically affect performance random scenario fundamental problem main memory ram access laggy past decade cpu processing speed grown faster memory latency dropped nowadays may take cpu cycle cpu data request fulfilled main memory main memory latency terrible thankfully bandwidth kept pace processor speed useful analogy long conveyor belt connecting storage warehouse main memory factory cpu first item requested warehouse slow arrive latency conveyor belt loaded efficiently keep factory fully supplied moment first item arrives bandwidth analogy give u idea type problem caused random input suppose factory place request item warehouse taking care order item request form request look like midprocess processed item yellow warehouse request fp tz gz ft yj fp ft gz tz yj fp ft gz tz yj fp ft gz tz yj fp ft gz tz yj take time worker change station long package requested repeatedly station worker fully utilize conveyor belt soon need change station gap conveyor belt corresponding time take unsurprisingly item requested randomly ratio package gap worse warehouse request fp tz gz ft yj ft yj tz yj fp tz ft yj gz gz fp tz ft tz gz gz fp yj fp ft analogy bear passing resemblance part happens memory system concept applies generally anytime high latency high bandwidth predictable request utilize bandwidth much better unpredictable one microarchitecture rescue chip designer engaged microarchitectural heroic conceal main memory latency microarchitecture implementation detail higher level program like c compiled machine instruction simpler time machine would follow instruction nowadays cpu pretend behind scene sight program reinterprets whatever way see fit key constraint cpu state visible program must consistent instruction leaf room hair raising optimization everything happens behind scene microarchitecture two main class microarchitectural trick used mask main memory latency first one involves interpreting instruction way conveyor belt kept fully utilized ie gap first item requested includes preloading memory area cpu likely need based prior access pattern executing operation outoforder memory request later operation initiated earlier one wait fulfilled speculating conditional branch executed condition known outoforder execution continue past branch optimization second involves amount low latency memory known cache conveyor belt analogy might shelf next assembly station room item item retrieved cache insulated main memory latency cache limited requires expensive design must close cpu typically exclusive real estate cpu die half westmere core xeon die see three level cache labeled first two inside core last shared across core core cache though label within one core cache split data cache instruction cache get sense different cache main memory looking bigger picture main memory different continent closeup motherboard cpu many case combination trick sufficient conceal least amortize latency many access pathological memory access pattern defeat splitting randomly ordered vector example treeprof give u detailed timing random sorted split call time millisecond random sorted split splitdefault asfactor sort uniquedefault sortdefault sortint order many difference two run let focus highlighted number start correspond internal c code scan input vector normalized group vector copy input value result vector indicated group code take exact number step irrespective input order yet speedup sorted randomized version modelling cache effect ideally would keep whole working set cache worry main memory latency unfortunately working set often larger cache interested exploring cache working set size affect speed splitdefault calculation system three level cache hierarchy pictured die level larger slower previous one cpu clocked like mine cycle access take cycle main memory access take one way explore cache effect build model think memory access time based working set size data cache effective would expect processing time follow model show current working set size element vertical dashed line modeled access time colored line observed timing dot outside anomaly model work remarkably well predicts slowdown random v sorted case element working set close observed reasonably accurate working set size splitdefault six memory access per element processed input ordered sequential two longer sequential sequential access simple predictable result memory system builtin optimization prefetching stream buffer form speculative execution assume sequential access met cache speed without consuming cache conversely random access benefit optimization assume subject full memory system latency magnitude vary depending much working set fit level cache transition slower level memory manifest increase perelement time asymptotically approach slower memory latency implementation cache model available appendix since modeling cache effect timing line closely implies optimization random access effect cache mitigate cost random access effect quickly dissipates working set size increase degree pathology reasonable understanding splitdefault work much faster ordered unordered input explained combination ordering input splitting faster splitting directly ordering process involves random memory access order grp go grp random access xo x random access splitdefault xo go sequential even ignoring order call indexing step required order x grp xo go random access ran timing x well version x presorted superimposed cacheonly model access many interesting thing going notable one random case substantially outperforms corresponding cacheonly model let examine subset c code shown pseudo r guise re numeric length grp seqi seqalong grp seqi val x sequential fetch x value copy idx sequential fetch write offset re idx val random write x value offset slow step re idx val idx point location result vector nice thing loop iteration independent others allows processor begin execution multiple loop iteration without waiting first one complete ie outoforder outoforder instruction kept inflight reorder buffer ondie memory structure store number pending complete instruction purpose buffer allow instruction executed outoforder result committed logical program order result committed becomes part cpu state visible program let pretend system reorder buffer support eight instruction inflight recast first two iteration previous loop following instruction pseudo r code instruction loop iteration seqi pending x pending pending re pending loop iteration seqi pending x pending pending re pending renamed val idx variable appending loop number allows cpu reorder without instruction later iteration overwriting earlier one executed following order seqi pending seqi pending x pending x pending pending pending re pending re pending processor free chose instruction execute depend others case instruction eligible processor could execute one even simultaneously shortly first main memory access complete state seqi retired seqi complete seqi pending x pending x pending pending pending re pending re pending computed additionally instruction retired meaning result visible program longer occupies one eight inflight slot instruction retired despite complete logical flow program come re command still pending instruction take slot freed retirement instruction dynamic play val idx step lead seqi retired x retired retired seqi complete seqi complete x complete x complete complete complete re pending re pending outoforder execution really shine prior memory access sequential executing outoforder marginal value subsequent read would fast anyway writes re random subject full memory latency outoforder execution allows u run concurrently value main memory substantial parallelism builtin ie warehouse multiple worker available load unload conveyor belt processing speed type access grow degree main memory parallelism size inflight instruction window one way confirm outoforder execution play prevent otherwise similar workload adding sequential loop dependency use chasing index like one vector element contains index next element visit arrow illustrate path result following index unfortunately possible efficiently pure r code instead wrote c function superimposed timing previous one blue data set sequential dependency timing back line theoretical cacheonly model demonstrating random subset likely benefited outoforder execution sure enough turn splitdefault code sequential dependency us offset vector track group vector write next value vector updated loop random access pathological modern memory system sequential dependency downright killer order input vector first isolate random access simple operation without sequential dependency give memory system best chance mitigate latency branch prediction one optimization glossed branch prediction outoforder cpu put instruction inflight past branch eg statement depends incomplete previous instruction know side branch put inflight modern cpu make educated guess speculate instruction put inflight maintain benefit outoforder execution guess right ironically key feature branch predicting cpu ability guess rather ability recover incorrect guess reorder buffer assist marking inflight operation predicated branch guess speculative may discarded guess discovered incorrect branch condition computation completes mispredictions cost cycle system cpu designer invest substantial amount effort improve branch prediction branch prediction likely play minor role example however completeness sake contrived new one demonstrates effect compiler notoriously aggressive removing branch controlled testing wrote toy example c compiled optimization turned sexp sexp x rxlent size xlength x int xptrinteger x int accum rxlent size xptr accum else accum branch xptr return scalarinteger accum function computes sum logical vector interpreting true value false value test logical vector varying proportion true value random sorted colored branch misprediction penalty model line use cycle penalty per misprediction fit observed timing well randomly ordered case branch predictor better predicting higher frequency branch average sorted case essentially misprediction penalty predicting recently seen branch direction good strategy cycle misprediction penalty likely best case scenario mispredictions interfered main memory access instead registeronly operation cost would much higher faustian bargain meltdown specter md foreshadow clockwise much ingenuity gone designing implementing modern memory system easy forget whole thing massive hack case point raft severe processor security vulnerability published past two year exploit microarchitectural optimization limitation microarchitecture expose supposedly hidden implementation detail probing via observation timing expense silicon implement led shared concurrently across process combination make horror show severe difficult fix vulnerability md bottomright inset announced process writing post sixteen month meltdown spectre vulnerability disclosed top row staggering change intel introduced newest processor mitigate meltdown made md vulnerability worse little segu little relevance post microarchitecture involved timing disclosure recent vulnerability reading headline past year wondered exactly encourage read paper remarkably accessible occasionally downright funny loose end internal code splitdefault largest proportional difference sorted unsorted case asfactor uniquedefault together account larger part overall difference time millisecond random sorted split splitdefault asfactor sort uniquedefault sortdefault sortint order step turn group factor level used offset result vector splitdefault rely hash table unique us one detect whether value duplicated asfactor us one via match assign unique index distinct value input sorting effect le pronounced likely hashing nature undoes benefit sort r hash table stored vector twice size vector hashed uniquedefault grp generating hash table twenty million element eventually use one million worse memory system perspective hashing function try distribute value evenly randomly throughout hash vector nothing like large memory allocation accessed randomly make memory system happy sorting data still help group hash key accessed repeatedly first access group subjected full main memory latency optimization notwithstanding one last thing point ordered vector reaped microarchitectural benefit three different place part explains get much performance improvement even account time take order data conclusion irony microarchitectural mess slogged sideshow principal reason care ordering data open possibility different efficient processing algorithm independent microarchitectural factor ran inyourface manifestation could resist exploring stay tuned part ii hydra chronicle explore ordered data amazing thing originally planning ending post hydra drawing minus couple head grown feel bad henceforth mascot name series blog post make sense except like read first one okay inside joke appendix reference subset reference consulted found particularly helpful many others linked main body document footnote ulrich drepper every programmer know memory comprehensive review modern memory system emphasis main memory cache ed jorgensen assembly language programming ubuntu interested exploring microarchitecture want least basic understanding assembly language great introduction skim reasonably quickly get sense r tomasulo efficient algorithm exploiting multiple arithmetic unit implicitly introduces concept register renaming allow outoforder execution back memory latency le issue main objective better utilize multiple execution unit available processor paper substantially predates reorder buffer speculative execution outoforder retirement possible independent calculation outoforder speculation past branch norman jouppi improving directmapped cache performance addition small fullyassociative cache prefetch buffer introduces idea stream buffer among thing allows fast read sequential data without polluting cache agner fog microarchitecture intel amd via cpu microarchitectural detail processor starting pentium includes characteristic number reorder buffer entry cache latency time organization introduction microarchitectural concept moritz lipp etal meltdown reading kernel memory user space meltdown vulnerability paper includes useful background section give good idea thing go wrong image credit hydra fedora larry wentzel cc bync haberdashery haberdasher removed conveyor belt unknown public domain westmere core posted redstern license unknown inpost version annotated cropped three core annotation derived bad concurrency post correct correct since westmere architecture shrunken version nehalem intel skylake fritzchens frits public domain asus motherboard asus pr meltdown spectre foreshadow logo natascha eidl public domain md logo marina minkin public domain function definition systime please note error function rep odd return timing run faster median run since timing taken error effect still comparable systime function exp re matrix rep timecall quote systemtime null timecall substitute exp gc seqlen rep re eval timecall parentframe structure re function x print structure x order x floor nrow x namesc userself sysself elapsed userchild syschild classproctime cache model implementation cache model random access splitdefault testsizes c accesstimes c memsizes c cachetimes function setsizes memsizes accesstimes hitrates outer setsizes memsizes function x pmax x x time matrix numeric nrownrow hitrates ncolncol hitrates mult rep nrow hitrates seqlen ncol hitrates time hitrates accesstimes mult mult mult hitrates time two randomly accessed data set result vector numeric offset vector integer roughly tenth size result vector since one offset per group setnames testsizes setnames testsizes cachetimes accesstimes cachetimes accesstimes time rowsums rowsums index chasing simulate pointer chasing chain generating index reference next index go make index numeric rather integer match working set size used example randpath function size isnumeric size length size isna size size stop bad input size idx sample asinteger size re numeric size re c head idx idx re tail idx re walk index c code writes index value new vector order walk exactly equivalent indexing vector random order number random memory access limiting element careful x produced randpath walkrand inline cfunction sigc xnumeric body rxlent len xlength x double xptr real x sexp re protect allocvector realsxp len double resvec real re rxlent idx rxlent len idx xptr idx resvec idx unprotect return re branch prediction function body post due need compile optimization turned take special step use first temporarily updated rmakevars cflags masmintel disassembly cflags wall assembly optimization r session tool rcmd shlib dynload call sample c true false reptrue benchmarking code ended bit messy code run benchmark store result rdses code inline post load rdses display data splitdefault time c rep c n rev n time rev time rep rep rep re vector list seqalong n rngversion setseed n n x runif n g sample n reptrue go sort g gfo asfactor go gf asfactor g writelines sprintf f n timenormal systime j seqlen time duplicateddefault g systime j seqlen time splitdefault x gf repsreps timesorted systime j seqlen time splitdefault x gfo repsreps systime j seqlen time duplicateddefault go re append re list normaltimenormal sortedtimesorted nn timestimes print timenormal print timesorted saverds re index chasing size time pmax size sizessub seqalong size tail seqalong size samps lapply size sizessub randpath re vector list seqalong samps idx sizessub writelines sprintf f size idx timenormal systime j seq time idx walkrand samps subsequent processing code relied sorted v unsorted data write value twice re append re list normaltimenormal sortedtimenormal nsizes idx timestimes idx print timenormal saverds re branch predict fraction vecs lapply fraction function x true x falses x sample c rep true true rep false falses reptrue vecss lapply vecs sort re list seqalong vecs writelines sprintf f fraction timenormal systime j call vecs timesorted systime j call vecss re append re list normaltimenormal sortedtimesorted fractionfractions print timenormal print timesorted saverds re
144,Lobsters,scaling,Scaling and architecture,Inductive Consensus Tree Protocol: A Scalable Blockchain,https://ictp.io/ictp-whitepaper.pdf,inductive consensus tree protocol scalable blockchain,,obj stream j h r n xi endstream endobj obj stream g a endstream endobj obj stream 
145,Lobsters,scaling,Scaling and architecture,Scaling to 1M active GraphQL subscriptions on Postgres,https://github.com/hasura/graphql-engine/blob/master/architecture/live-queries.md,scaling active graphql subscription postgres,scaling million active graphql subscription live query hasuraio githubcomhasuragraphqlengine tl dr setup test graphql subscription implementing graphql livequeries refetching result graphql query hasura approach idea compile graphql query single sql query idea make authorization declarative http idea batch multiple livequeries one sql query refetch talk testing github benefit approach future work,scaling million active graphql subscription live query hasura graphql engine postgres provides instant graphql apis authorization read hasuraio githubcomhasuragraphqlengine hasura allows live query client graphql subscription example food ordering app use live query show livestatus order particular user document describes hasura architecture let scale handle million active live query table content tl dr setup client webmobile app subscribes data liveresult auth token data postgres million row updated postgres every second ensuring new result pushed per client hasura graphql api provider authorization test many concurrent live subscription client hasura handle hasura scale vertically andor horizontally single instance configuration active live query cpu load average ram ram ram million live query postgres load peak number connection around note configuration aws rds postgres fargate elb used default configuration without tuning rds postgres ram postgres hasura running fargate ram per instance default configuration graphql subscription graphql make easy app developer query precisely data want api example let say building food delivery app schema might look like postgres app screen showing order status current user order graphql query would fetch latest order status location agent delivering order underneath query sent string webserver par applies authorization rule make appropriate call thing like database fetch data app sends data exact shape requested json enter livequeries live query idea able subscribe latest result particular query underlying data change server push latest result client surface perfect fit graphql graphql client support subscription take care dealing messy websocket connection converting query live query might look simple replacing word query subscription client graphql server implement implementing graphql livequeries implementing livequeries painful database query authorization rule might possible incrementally compute result query event happen practically challenging webservice layer database like postgres equivalent hard problem keeping materialized view date underlying table change alternative approach refetch data particular query appropriate authorization rule specific client approach currently take secondly building webserver handle websockets scalable way also sometimes little hairy certain framework language make concurrent programming required little tractable refetching result graphql query understand refetching graphql query hard let look graphql query typically processed authorization data fetching logic run node graphql query scary even slightly large query fetching collection could bring database quite easily query problem also common badly implemented orms bad database make hard optimally query postgres data loader type pattern alleviate problem still query underlying postgres database multiple time reduces many node graphql query many item response live query problem becomes worse client query translate independent refetch even though query since authorization rule create different session variable independent fetch required client hasura approach possible better declarative mapping data model graphql schema could used create single sql query database would avoid multiple hit database whether large number item response number node graphql query large idea compile graphql query single sql query part hasura transpiler us metadata mapping information data model graphql schema compile graphql query sql query fetch data database graphql query graphql ast sql ast sql get rid query problem allows database optimise datafetching see entire query nt enough resolvers also enforce authorization rule fetching data allowed therefore need embed authorization rule generated sql idea make authorization declarative authorization come accessing data essentially constraint depends value data row fetched combined applicationuser specific session variable provided dynamically example trivial case row might container userid denotes data ownership document viewable user might represented related table documentviewers scenario session variable might contain data ownership information pertinent row eg account manager access account information present current database present session variable probably provided data system model implemented authorization layer similar postgres rls api layer provide declarative framework configure access control familiar rls analogy current session variable sql query http sessionvariables coming cooky jwts http header incidentally started engineering work behind hasura many year ago ended implementing postgres rls feature application layer landed postgres even bug equivalent insert returning clause postgres rls fixed http implementing authorization application layer hasura instead using rls passing userdetails via current session setting postgres connection significant benefit soon see summarise authorization declarative available table view even function function return setof possible create single sql query authorization rule graphql query graphql ast internal ast authorization rule sql ast sql idea batch multiple livequeries one sql query idea implemented would still result situation connected client could result proportional load postgres query fetch latest data let say update happen update relevant client however considering applicationuser level session variable available api layer actually create single sql query refetch data number client let say client running subscription get latest order status delivery agent location create relation inquery contains queryvariables order id session variable user id different row join query fetch actual data relation ensure get latest data multiple client single response row response contains final result user allow fetching latest result multiple user time even though parameter session variable provide completely dynamic available querytime refetch experimented several method capturing event underlying postgres database decide refetch query listennotify requires instrumenting table trigger event consumed consumer webserver might dropped case consumer restarting network disruption wal reliable stream lr slot expensive make horizontal scaling hard often available managed database vendor heavy write load pollute wal need throttling application layer experiment currently fallen back interval based polling refetch query instead refetching appropriate event refetch query based time interval two major reason mapping database event live query particular client possible extent declarative permission condition used live query trivial like orderid userid cookiesessionid becomes intractable anything complicated say query us status ilike failed declarative permission also sometimes span across table made significant investment investigating approach coupled basic incremental updating small project production take approach similar talk application unless write throughput small end throttlingdebouncing event interval anyway tradeoff approach latency writeloads small refetching done immediately instead x m alleviated quite easily tuning refetch interval batch size appropriately far focussed removing expensive bottleneck first query refetching said continue look improvement month come especially use event dependency case applicable potentially reduce number live query refetched every interval please note driver internally event based method would love work usecase current approach suffice hit u discord help run benchmark figure best way proceed testing testing scalability reliability livequeries websockets challenge took u week build testing suite infra automation tooling setup look like nodejs script run large number graphql livequery client log event memory later ingested database github script creates write load database cause change across client running live query million row updated every second test suite finish running verification script run database logsevents ingested verify error event received test considered valid error payload received avg latency time creation event receipt client le benefit approach hasura make livequeries easy accessible notion query easily extended livequeries without extra effort part developer using graphql query important thing u expressivefeatureful live query full support postgres operatorsaggregationsviewsfunctions etc predictable performance vertical horizontal scaling work clouddatabase vendor future work reduce load postgres mapping event active live query incremental computation result set
146,Lobsters,scaling,Scaling and architecture,How we optimized Magic Pocket for cold storage,http://blogs.dropbox.com/tech/2019/05/how-we-optimized-magic-pocket-for-cold-storage/,optimized magic pocket cold storage,inhouse multiexabyte storage system first major tech company adopt smr storage lifetime file magic pocket magic pocket work requirement initial design facebook warm blob storage system facebook warm blob storage system reedsolomon erasure code highly tuned network stack called courier jeff dean excellent talk velocity hiring acknowledgement,ever since launched magic pocket inhouse multiexabyte storage system continuously looking opportunity improve efficiency maintaining high standard reliability last year pushed limit storage density first major tech company adopt smr storage post discus another advance storage technology dropbox new cold storage tier optimized le frequently accessed data storage run smr disk active data internal network lifetime file access characteristic file dropbox varies heavily time file accessed frequently within first hour uploaded significantly le frequently afterwards cumulative distribution function file access file uploaded last year file retrieval dropbox data uploaded last day data uploaded last month data uploaded last year pattern unsurprising new upload trigger number internal system fetch file order augment user experience perform ocr parse content extract search token generate web preview office document user also tend share new document file also likely synced device soon upload general people much likely access file recently uploaded rather file uploaded year ago refer data accessed frequently warm infrequently accessed data cold difference access characteristic warm cold data open opportunity cost optimization tailoring system class data magic pocket magic pocket system storing file dropbox split file chunk called block size block metadata operation related complexity around mutation revision history handled metadata layer top magic pocket job durably store serve large block system already designed fairly cold workload us spinning disk advantage cheap durable relatively highbandwidth save solidstate drive ssds database cache magic pocket also us different data encoding file age first upload file magic pocket use nway replication across relatively large number storage node later encode older data efficient erasure coded format background within given geographic region magic pocket already highly efficient storage system replicating data across geographical region make magic pocket resilient large scale natural disaster also significantly le efficient aggregate crossregion replication understand cold storage development first need high level understanding magic pocket work magic pocket store block highly reliable manner within storage region also store data independently least two separate region within region thing get quite complicated interface region quite simple basically glorified version put get delete many company replicate data across multiple data center located within ten hundred mile dropbox set bar durability availability region outage much higher replicate data across data center located thousand mile apart effectively store double amount data would otherwise better survive largescale disaster overhead long inefficiency wanted rectify ever since built magic pocket kept asking maintain high bar safety efficient way requirement order better tailor system different workload decided operate two storage tier built new cold storage tier renamed original magic pocket system warm tier asynchronously migrate data background becomes cold everything cold considered warm high level requirement cold storage system simple sacrifice durability period matter rarely file used dropbox user trust u important stuff need still able tolerate full region outage multiple rack failure surviving region simultaneously availabilitywise sacrifice read availability interestingly actually care write availability since userfacing writes get written warm storage tier pause writes cold storage tier time without affecting user tolerate slight increase latency cold data since magic pocket already fast compared time take send file internet however still need reliably fast access probably need tax record often want immediately minute initial design needed somehow remove full crossregion replication still able tolerate geographic outage replicate data across region instead keeping full internallyreplicated copy region could reduce storage cost would come increased widearea network cost need reconstruct file region outage potential higher network cost fact good tradeoff lower storage overhead cold data however since data fetched frequently unfortunately get right first attempt spent significant amount time developing solution ended unhappy better understand ultimate good solution let first look couple seeminglygood idea pan single erasure code spanning multiple region obvious approach remove strict boundary region one replicates independently instead single software instance erasure coding across region erasure code enough internal redundancy tolerate large scale regional disaster actually went quite far approach got prototype new system running stage environment relatively quickly started finding issue hidden cost additional complexity design also made storage initiative involved result slowed development biggest problem approach truly mitigate durability risk promised system would sacrifice durability purelytheoretical term still provided many nine durability original system practice could deliver promise given single largescale region running single version software single software bug could wipe everything matter many copy store magic pocket independent region model extremely resilient human human writing software bug operator error executing wrong command eliminating strong logical separation two storage region take away last line defense significantly elevates possibility u losing data long run killed project nine month active work engineer easy give something trying make work long early conversation controversial ultimately everyone agreed killing project right decision dropbox user facebook warm blob storage system rediscovered value independent failure domain trying get rid went back drawing board one interesting idea previously ruled early exploration facebook warm blob storage system idea pair volumestripeblock buddy volumestripeblock different geographic region store xor buddy third region example block fetched region region available block reconstructed fetching block b xor b region region respectively performing xor neat idea need change anything magic pocket region internals replicate differently top decided explore detail gave soon maintaining globally available data structure pair block came set challenge dropbox unpredictable delete pattern needed process reclaim space one block get deleted added lot complexity made le suitable use case previous work wasted spending much time trying make previous design work helped u build better intuition tradeoff network stack ended challenging one early implicit assumption really need request single block able fully served within single region bestcase scenario region given cold data infrequently accessed fine always incur higher network cost accessing removing constraint serving single region led u following design illustrate idea going use three region example xor generating parity block method generalized applied larger set region using something like reedsolomon erasure code generate parity similar previous design want change internal architecture within region however instead finding pairing similar sized block split single block piece called fragment stripe fragment across multiple region put block three region example split two piece called fragment put first fragment region second fragment region compute xor fragment form third parity fragment put region migrating data cold tier asynchronously ignore complicated situation one region available region pause migration three region back get block issue get request three region wait fastest two response cancel remaining request absolutely preference two fragment use perform reconstruction performing xor erasure decoding negligible operation compared reading data disk transferring network delete block need delete fragment block longer referenced another nice property independently region allows u use different version deleter software run deletion different time prevent software bug wiping fragment mistake latency obvious downside model even bestcase satisfy read without fetching fragment multiple region big problem thought also turned blessing disguise many dimension latency measured various percentile rolling new storage tier begin percentile significantly higher cold tier need least one crossregion network roundtrip retrieve data cold tier looking percentile difference warm cold tier stay constant roughly one network roundtrip dropbox network stack already heavily optimized transferring large block data long distance highly tuned network stack grpcbased rpc framework called courier multiplexing request transport result warm tcp connection large window size allows u transfer multimegabyte block data single roundtrip initially puzzled result percentile however cold storage tier lower tail latency warm tier confusing initially investigation started make sense cold tier getting fastest request retry logic warm tier first fetching closest region issuing request region first attempt fails timesout always arbitrary slowdown large scale distributed system warm tier getting majority traffic needed slightly smarter simply issue two request parallel instead ended modifying warm tier optimistically issue retry second region first request attempt arrive given time budget allowed u also bring high tail latency warm tier small network overhead learn technique jeff dean excellent talk velocity one beautiful property cold storage tier always exercising worstcase scenario plan plan b regardless whether region retrieving data always requires reconstruction multiple fragment unlike previous design even warm tier region outage result major shift traffic increase disk io surviving region made u le worried hitting unexpected capacity limit emergency failover peak hour mentioned requirement fixed target latency directional guidance overall result got significantly beat expectation small difference would affect end user experience dominated transferring data internet allowed u aggressive data consider cold eligible migration durability biggest win term durability keeping architecture layered simple operate region independently run different version software allowing u detect issue might slip testing affect copy data even rare bug take long time occur could slip release process extremely unlikely affect data two region simultaneously invaluable last line defense unknown unknown although hard quantify math equation practice allows u move faster assured extremely high level durability still vulnerable bug replication logic might occur data persisted across independent region however surface area amount data risk number code path involved limited significantly mitigate risk bug immediately purging data warm tier cold tier migration perform multiple round extensive validation purge data original location cost saving warm tier replication model replication scheme one data fragment one parity fragment plus replication within region course three region cold tier example replication scheme two data fragment one parity fragment plus internal region replication total replication factor warm tier regioninternalreplicationfactor cold tier regioninternalreplicationfactor thus reduce disk usage using three region setup mentioned previously approach generalized region use region willing tolerate losing model would give u saving also extended let say depending tradeoff one willing make cost saving number simultaneous region loss tolerated exploring various solution building cold storage tier learned set constraint define huge impact way approach problem sometimes adding new constraint keeping independent failure domain cost narrow search space help u focus ruling design early however sometimes removing constraint serving read within region steady state open whole new realm possibility also learned although aim make datadriven decision intuition sometimes wrong rarely data make decision sometimes even know data point look way make something work try potentially fail bad decision might damaging long willing step back keep honest admit defeat course correct needed course learned something problemsolving also ended cold storage system great fit need also got significant reduction storage cost without compromising durability availability without introducing significant complexity architecture interested solving challenging openended engineering problem large scale distributed system hiring acknowledgement thanks current past member dropbox storage team contributed building cold storage tier preslav le alex taskov rebecca wang rajat goel cristian ferretti james cowling facundo agriel alexander sosa lisa kosiachenko omar jaber sandeep ummadi
147,Lobsters,scaling,Scaling and architecture,Damage Control in Distributed Systems,https://rjzaworski.com/2019/04/damage-control-in-distributed-systems,damage control distributed system,took mighty nasdaq exchange system thinker simple tool surprising complexity canceled queue rejected request jitter state machine hystrix chasing mystery,began squirrel judging tail illconceived violation transformer inner sanctum greeted righteous thunder otherwise minor blip across united illuminating company grid took mighty nasdaq exchange incident came blaze glory grieving mother family nest pequonnock river surge power flowing back line grid operator system thinker greybeard sysadmins tell system hard defining assigning synchronizing work tricky enough one computer even simple tool yield surprising complexity assembled network failure state arise deeply interconnected system difficult anticipate resolve sometimes best simply stop spreading keep rest system running good day amount thoughtful application design resolve problem begun somewhere else embracing failure owning impact user service least strive make thing worse follows several useful pattern containing responding common fault presented strippeddown form neither configurability runtime visibility would need production still useful place start say study use caveat emptor sake example let pretend every action inside big imaginary network represented immediate request eventual response type action r request promise response nothing stopping u representing future action continuation stream favorite asynchronous programming model stick one simple enough good let go make thing fail one ponder long longrunning action go customer even patient digital customer loses interest outcome pull chair upper bound could way guarantee service within reasonable window set bound service might mean error end timely error almost always improvement waiting heat death universe implementation work like expect start race timer pending promise timer come back first declare promise timed unblock const delay number promise void new promise resolve settimeout resolve class timeouterror extends error readonly code etimeout type timelimiteropts timeout number function timelimiter opts timelimiteropts return function pending promise return promiserace pending delay optstimeout promisereject new timeouterror timed one big caveat though intent purpose carrying canceled promise javascript least canceled request see timeouterror resource computation attached pending promise may still running completion associated reference may yet released cutting longrunning request sensible step heading sort congestion might slow first place benchmarked system worrying much loaded behavior behavior worth benchmarking system already rough sense likely fail number try keep traffic knowngood threshold preempt trouble start pareddown implementation type supplier promise type ratelimiteropts limit number interval number type ratelimiter f supplier supplier class ratelimiterror extends error readonly code eratelimit function ratelimiter opts ratelimiteropts ratelimiter let count let periodstart return supplier supplier return const datenow periodstart optsinterval periodstart count count optslimit const err new ratelimiterror exceeded rate limit return promisereject err count return supplier order control invocation pattern wrap supplier lieu single promise blocking invocation beyond userspecified limit crude lever control supplier throughput useful enough though plenty done improve ease use generous implementation might warn customer approach limit allowing throttle request accordingly even queue rejected request reprocessed later raise interesting question failure happens come next well something broke maybe selfimposed limit timer ratelimiter maybe genuine honesttogoodness exception action either case need decide whether dutifully relay failure customer first attempt recovery error look network flaked service temporarily usually start latter easiest step may simply retry type retryeropts limit number class retryerror extends error readonly code eretry retryer opts retryeropts return supplier supplier promise let attempt const onerror async attempt optslimit throw new retryerror exceeded retry limit await delay attempt return supplier catch onerror return supplier catch onerror spared nicety name straightforward example heart watch error provided already exceeded reasonable retry limit go ahead try robust implementation would likely want way filter retryable error recoverable sort well way review error related initial request subsequent retry attempt may also want ability cancel retries inflight even check progress project another day one worth taking however schedule retries sent retry policy sensible first step terrific way load artificial traffic onto alreadybeleaguered service failure may due request throttling overwhelmed service good idea well good form take breath subsequent retries original retryer time room different backoff strategy export type backoffstrategy attempt number promise void type retryeropts limit number backoff backoffstrategy function retryer opts retryeropts return supplier supplier promise let attempt const onerror async attempt optslimit throw new retryerror exceeded retry limit await optsbackoff attempt attempt return supplier catch onerror return supplier catch onerror const backoff fixed number return attempt number delay linear number return attempt number delay attempt backoff quite enough though consider case surge request cause many client fail nearsimultaneously client share common retry policy amount backingoff save coordinated surge follows successive retry save jitter backoff calculation const jitter number mathrandom export const backoff linear number return attempt number delay jitter attempt may still face original collision bit entropy sprinkled beautiful deterministic backoff algorithm least prevent unintentionally coordinated sort far put bound around sort fault relatively easy specify request need settle within certain window cut service fail certain load limit volume request production fault accommodating may know something want detect avoid making worse enter circuit breaker type circuitbreakeropts threshold number waitduration number type circuitbreaker f supplier promise class circuitopenerror extends error readonly code ecircuitopen type state closed open halfopen const circuitbreaker opts circuitbreakeropts f supplier promise let count let rejected let state state closed let lastchangeat const setstate next state count rejected state next lastchangeat datenow setstate closed return async state open datenow lastchangeat optswaitduration setstate halfopen else throw new circuitopenerror circuit breaker open count try const result await f state halfopen setstate closed return result catch err state halfopen setstate open rejected state open const failurerate rejected count failurerate optsthreshold setstate open throw err real world circuit breaker save sort incendiary unpleasantness cutting power circuit exceeded design load version open circuit certain signal case error rate exceeds userdefined threshold waiting open state breaker relax halfopen position point success failure next request either restore normal operation trip back open production version circuit breaker would also ability filter safe error well exposing insight present state state machine embedded inside breaker likely surface eventemitter push state change failure rate metric ship attempt throughput latency rest faulttolerance toolkit couple point worth considering putting pattern practice first complexity added pattern solve pressing problem within system answer anything le certain leave second consider whether proven library like hystrix port favorite language provide feature need probably leaning save trouble verifying benchmarking ironing kink homegrown safety equipment tired chasing mystery big teetering network though pattern like help make failure obvious maybe combine wrapping circuit breaker around ratelimited component layer sanitychecks fallback behavior maybe live sidecar container api middleware proxy fronting serverless function whatever however use ability recognize handle error critical part coming term complexity distributed system assume failure bad news make worse
148,Lobsters,scaling,Scaling and architecture,Long Polling: Concepts and Considerations,https://www.ably.io/concepts/long-polling,long polling concept consideration,brief history long polling come netscape communication browser war xmlhttprequest long polling work long polling websockets consideration using long polling message ordering delivery guarantee reliable message ordering performance scaling device support fallback websockets ably realtime open source long polling solution github repo implementation github repo github repo twisted framework github repo ably long polling ably realtime websockets ably realtime platform reference reading,article provide insight technique known long polling came implemented might use application use ably brief history long polling come early day web http made sense simple requestresponse protocol designed way distribute structured document interested reader document generally link others could browsed reader leisure requestresponse model right strategy browser needed make request resource user wished consume job done technology time vastly limited today holding connection open would significant issue web server load management would littletono value anyway concerning goal use case early web designed solve early browser mosaic first version netscape navigator implemented mind support javascript feature might position web anything distributed resourcesharing network netscape communication hired brendan eich implement scripting capability netscape navigator tenday period javascript language born capability language initially limited compared modernday javascript ability interact browser document object model dom even limited javascript mostly useful providing limited enhancement enrich document consumption capability example inbrowser form validation lightweight insertion dynamic html existing document browser war heated microsoft internet explorer reaching version beyond battle robust feature set led microsoft introduction ultimately became xmlhttprequest object browser universally supported well decade xmlhttprequest could thought black swan event web opened potential web developer start building trulydynamic web application could communicate server silently background without interrupting user browsing experience browser dom capability also expanding significantly particularly internet explorer approached version dynamic web application started become commonplace offering everything rich webbased email interface inbrowser website content management system progress begets progress say naturally developer began explore way implement application realtime aspect function webbased chat room simple game early example time http protocol made sort use case challenging implement though common see application polling server repeatedly check new data seeing server way proactively notify user whenever new data became available given extreme inefficiency approach creative way manipulate http requestresponse model realtime medium began emerge among technique probably popular emerge long polling long polling work long polling essentially efficient form original polling technique making repeated request server waste resource new incoming connection must established http header must parsed query new data must performed response usually new data offer must generated delivered connection must closed resource cleaned rather repeat process multiple time every client new data given client becomes available long polling technique server elect hold client connection open long possible delivering response data becomes available timeout threshold reached implementation mostly serverside concern client side single request server need managed response received client initiate new request repeating process many time necessary difference basic polling far client concerned client performing basic polling may deliberately leave small time window request reduce load server may respond timeouts different assumption would server support long polling long polling client may configured allow longer timeout period via keepalive header listening response something would usually avoided seeing timeout period generally used indicate problem communicating server apart concern little else client need would different engaging basic polling contrast server need manage unresolved state multiple connection may need implement strategy preserving session state multiple server load balancer use commonly referred session stickiness also need gracefully handle connection timeout issue much likely occur designedforpurpose protocol websockets standard arrive year long polling established conventional technique pseudorealtime communication consideration using long polling long polling really improvisation applied underlying requestresponse mechanism come additional degree complexity implementation various concern need account considering design system architecture message ordering delivery guarantee reliable message ordering issue long polling possible multiple http request client flight simultaneously example client two browser tab open consuming server resource clientside application persisting data local store localstorage indexeddb inbuilt guarantee duplicate data written could also happen client implementation us one connection time whether deliberately result bug code another issue server may send response network browser issue may prevent message successfully received unless sort message receipt confirmation process implemented subsequent call server may result missed message depending server implementation confirmation message receipt one client instance may also cause another client instance never receive expected message server could mistakenly believe client already received data expecting concern need considered implementing robust support long polling realtime messaging system performance scaling unfortunately complexity difficult scale effectively maintain session state given client state must either shareable among server behind load balancer task significant architectural complexity subsequent client request within session must routed server original request processed form deterministic sticky routing problematic design especially routing performed basis ip address place undue load single server cluster leaving server mostly idle instead spreading load around efficiently also become potential denialofservice attack vector problem requires layer infrastructure mitigate might otherwise unnecessary device support fallback modern time time article long polling le relevant web application development given widespread availability realtime communication standard websockets webrtc said case proxy router certain network block websocket webrtc connection network connectivity make longlived connection protocol le practical besides certain client demographic may still numerous device client use lack support newer standard long polling serve good failsafe fallback ensure support everyone irrespective situation given long polling implemented back xmlhttprequest near universal support device still nonnegligible degree remaining relevant modern web usually much need support fallback layer case exception must handled though server queried new data support long polling let alone modern technology standard basic polling sometimes still limited use implemented using xmlhttprequest via jsonp simple html script tag long polling implemented via jsonp absolutely necessary said given time effort mention inefficiency resource consumption involved implementing approach care taken ass whether support worth added cost developing new application system architecture many case may able get away exclusive support something modern websockets alternatively may wish offload management sort concern specialist cloud provider ably realtime open source long polling solution library implement long polling isolation transport general long polling usually accompanied transport strategy either fallback transport fallback long polling work beyond standalone long polling library particularly uncommon given technique quickly losing relevance face widespread support modern alternative nevertheless handful option different language decent library implementing long polling go language import githubcomjcugagolongpoll launch goroutine creates channel plumbing manager err golongpollstartlongpoll golongpolloptions default option pas manager around create closure publish managerpublish subscriptioncategory data string obj convertible json managerpublish differentcategory data expose event browser see subsection interact subscription handler httphandlefunc event managersubscriptionhandler httplistenandserve nil find library original github repo per readme phplongpolling simple demonstration longpolling ajax jquery php improved cleaned documented fork phpajaxlongpolling http githubcomlincolnbritophpajaxlongpolling code one extremely simple take look implementation directly fairly selfexplanatory find library original github repo readme pollymer generalpurpose ajax library provides convenience longpolling application request retries exponential backoff request randomized request delaying workarounds browser busy indication also implement multiple transport ensure crossdomain access work major browser optional extra include support jsonp logging var req new pollymerrequest reqon finished function code result header reqon error function reason var header var body data reqmaxtries try twice reqstart post http examplecompath header body find library original github repo minimalist http longpolling server python language support multiple channel crossdomain request requires python twisted framework unlike library mentioned server run kind broker sits client serverside application server wish publish message client make request http endpoint easily hidden behind firewall ensuring accessible serverside process readme detail provides example registering channel pushing data channel registering client monitoring server reading data channel find library original github repo ably long polling ably realtime implemented robust suite transport client library make available communication server websockets primary protocol realtime communication however long polling fullysupported automatic fallback transport newer standard websockets available clientside find ably realtime platform reference reading
149,Lobsters,scaling,Scaling and architecture,Why Turning on HTTP/2 Was a Mistake,https://www.lucidchart.com/techblog/2019/04/10/why-turning-on-http2-was-a-mistake/,turning mistake,turning mistake haproxy nginx support buggy implementation buffering limit effectiveness,turning mistake great leap forward http increase bandwidth efficiency using binary compressed format header decrease latency multiplexing request tcp connection allows client specify priority request many case moving right thing application cause significant problem last year lucidchart enabled load balancer service shortly thereafter noticed server behind load balancer higher cpu load slower response time server first cause unclear server traffic appeared normal problem seemed unrelated code change closer inspection realized average number request usual actual flow request become spikier instead steady flow request getting short burst lot request although overprovisioned capacity based previous traffic pattern enough deal new request spike response request delayed timed happening well browser limit number concurrent connection given origin combination scheme host port connection must processed series single connection mean browser effectively limit number concurrent request origin meaning user browser throttle request server keep traffic smooth remember said add multiplexing well browser send http request concurrently single connection web client perspective great theory client get resource need quicker since longer wait response server making additional request however practice multiplexing substantially increased strain server first received request large batch instead smaller spreadout batch secondly request sent staggered like start time closer together meant likely time fix fortunately possible solve problem without increase computing capacity fact potential solution although take little effort enabling load balancer throttle load balancer perhaps obvious solution load balancer throttle request application server traffic pattern application server point view similar using required level difficulty depends infrastructure example aws alb mechanism throttle request load balancer least moment even load balancer like haproxy nginx getting throttling right tricky load balancer support throttling still put reverse proxy load balancer application server throttling rearchitect application better handle spiky request another perhaps better option change application handle traffic load balancer accepts traffic depending application probably involves introducing tweaking queueing mechanism application accept connection process limited number time actually already queuing mechanism place switched previous code decision properly limiting concurrent request processing queue request careful process request client timed waiting need waste unnecessary resource turn make sure application handle difference different traffic pattern application may designed specifically pattern whether intentionally lot benefit also difference bite careful postscript another gotcha look software supporting fully mature yet although getting better time software enough time become mature solid existing software particular server support http prioritization spotty best many cdns load balancer either support buggy implementation even buffering limit effectiveness also several application support http security point view great however complicates development debugging inside secure network need want tl different component mean need manage ca certificate localhost local development log session secret order inspect request wireshark similar tool may require additional compute resource encryption
150,Lobsters,scaling,Scaling and architecture,Improving Key Expiration in Redis,https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/improving-key-expiration-in-redis.html,improving key expiration redis,infrastructure improving key expiration redis redis documentation,infrastructure improving key expiration redis recently interesting issue ran performance issue redis cluster lot time spent debugging testing able reduce memory use cluster change key expiration internally twitter run multiple cache service one backed redis redis cluster store data important use case impression engagement data ad spend counting direct message background info problem back early cache team large update architecture redis cluster thing changed among update redis version version update couple issue came user started see memory use align expected provisioned use latency increase key eviction key eviction big problem data removed expected persistent traffic going origin data store originally initial investigation team affected along cache team started investigate found latency increase related key eviction happening redis receives write request memory save write stop evict key save new key still needed find increase memory usage happening causing new eviction suspected memory full key expired deleted yet one idea someone suggested use scan would read key causing expired one deleted redis two way key expired actively passively scan would trigger passive key expiration key read ttl checked expired throw away return nothing active key expiration version described redis documentation start function called activeexpirecycle run internal timer redis refers cron run several time second function cycle keyspace check random key ttl set percentage threshold  met expired key repeat process time limit met idea scanning key worked memory use dropped scan completed seemed redis efficiently expiring key unfortunately resolution time increase size cluster hardware key would spread around would memory available disappointing project upgrade redis mentioned earlier reduced size cost running cluster making efficient redis version changed version implementation activeexpirecycle changed every database checked time ran maximum many database checked version also introduced fast option function run timer run checking event event loop fast expiration cycle return early certain condition also lower threshold function timeout exit time limit also checked frequently overall line code added function returning investigation recently time go back revisit memory use issue wanted explore regression see could make key expiration better first theory many key instance redis sampling enough thing wanted investigate impact database limit introduced scale way sharding handled make running redis twitter unique large keyspaces million key typical user redis shard represented keyspace instance redis multiple shard instance redis lot keyspaces sharding combined scale twitter create dense backends lot key database testing improvement expiration number sampled loop configured variable activeexpirecyclelookupsperloop decided test three value ran one problematic cluster ran scan measured difference memory use large change indicates sizeable amount expired data waiting collected test positive result initially memory use test control three test instance sampled key number arbitrary value based output  statistical samplesize calculator total key population size chart even looking starting number test instance clear performed better percentage difference running scan show around overhead expired key although sampling key helped find expired key negative latency effect could tolerate graph show percentile latency millisecond show latency correlated increase key sampled orange use value green used blue used control yellow line match color table seeing latency affected sample size idea see sample size could adjusted automatically based many key expire would key expire latency would take hit work would scan le key perform faster idea mostly worked able see memory use lower latency affected metric tracking sample size showed increasing decreasing time however go solution introduced latency spike occur control instance code also kind convoluted hard explain intuitive also would adjust cluster ideal would like avoid adding operational complexity investigating regression version also wanted investigate changed redis version new version introduced variable called crondbspercall set max number database would checked run cron test impact simply commented line  dbspercall serverdbnum timelimitexit dbspercall serverdbnum would compare effect limit checking database run result benchmark excitingly positive test instance one database though logically line code made difference modified unmodified version variable always set started look one line commented made drastic difference since statement first thing suspected branch prediction took advantage builtinexpect change code compiled make difference performance next looked assembly generated see exactly going statement compiled important instruction mov cmp jg mov load memory register cmp compare two register set another based result jg conditional jump based value another register code jumped code block else  took statement put compiled assembly redis tested effect instruction commenting different line tested mov instruction see kind performance issue loading memory cpu cache issue difference tested cmp instruction difference ran test jg instruction included latency went back unmodified level finding tested whether jump specific jg instruction added non conditional jump instruction jmp jump jump back code running performance hit spent time looking different perf metric tried custom metric listed manual nothing conclusive one instruction caused performance issue theory related instruction cache buffer cpu behavior jump executed ran time decided come back future possibly resolution needed pick solution problem better understanding cause problem decision go simple modification able configure steady sample size startup option able find value good trade latency memory use even though removing statement caused drastic improvement uncomfortable making change without able explain better graph memory use first cluster deployed top line pink hidden behind orange median memory use cluster top line orange control instance middle part chart canary new change third part show control instance restarted compare canary memory use quickly increased control restarting patch contains new option seen pretty big investigation end included engineer multiple team reduction cluster size pretty nice result learned lot would like take another look code see optimization make help team focused performance tuning seems like potentially still lot gain engineer made big contribution investigation mike barry rashmi ramesh bart robinson
151,Lobsters,scaling,Scaling and architecture,"Monolith vs Microservices - a tale from Python at ""scale""",https://medium.com/@jimjh/monolith-vs-microservices-a0322100160f,monolith v microservices tale python scale,monolith v microservices tale python scale hello nik unsplash thought experiment martijn baudoin unsplash promise stable programming interface scalability performance cooper review micro good idea use monolith anemic crud service vadim sherbakov unsplash ci developer experience accelerate pace change ben white unsplash sharing library,monolith v microservicesa tale python scale basically clone war photo hello nik unsplasha thought experimentsuppose web app named myapp could django spring ruby rail started single small application code repository everything deployed single artifact table database app grows attracts user get data also get developer table database get hosted machine codebase start snowball get successful try scaleit depends issue start becoming painful example suppose hundred table database table joined table performance problem mount becomes confusing troubleshoot kind query optimization natural solution would involve namespacing isolation separate table different database slap programming interface around let call group myaggregate might host database myaggregate separate machine isolate resource allows u provision dedicated cpu memory storage myaggregatedependency management point career wait photo martijn baudoin unsplashsuppose try stretch dollar notice work done around myaggregate slow memoryintensive done asynchronously ingenious engineer suggests main application publish task queue offload work would spawn process artifact consume task might also move consumer process separate specialized hostsa day later engineer realizes modifying code affect myaggregate rest app could deploy consumer shortly engineer working myaggregate create cicd process promise maintain stable programming interface exchange test written interface guaranteeat point single artifact multiple database multiple disparate host multiple deployment process interface wall myaggregate rest myapp could even consider creating separate artifact perhaps someone team really care size artifact sometimes really need start consumer process quick test downloading entire artifact take longis still monolith quite microservice modular monolith scalability performancethere occasional claim microservices help improve scalability performance claim benefit important evaluate ass retrospective usually main impetus breaking monolith nice bonus refactoring breaking monolith often unlock performance gain deleting technical debt rewriting old software rewiring network layout queue splitting monolithic data store reallocating resource improved isolationin case breaking monolith using plain rpc worsens performance add node critical path flake block io even grpc serialization validation add enough overhead warrant custom apis handle batched requeststhe real scalability wrt number employee specialized team engineering product organization work area expertise reduces cognitive load communication overhead specialized team depend stable upstream apis focus providing stable api downstream consumer word think kind scalability motivates monolith decomposition thinking kind accelerate pace change rapidly hiring growing organizationwill scale trick appear smart meeting cooper reviewso engineer microservices wronghow micro real test micro many specialized team independently build deploy software within scope responsibility condition reached one continue decomposing monolith stop right organization grows continues specializing even process start overan important observation organization specialized team good idea use monolith contributor workflow cut across multiple function single product forcing microservices early slow development process migrating one involves analysis amount context one would need work component required context need fit inside one headusing test organization avoid ending situation developer work across multiple microservices concerning difference version thirdparty library dealing separate deployment microservice every change likewise also avoids common antipattern one go single monolith large number anemic crud service overnightfor example consider organization engineer middle manager go significant reorganization creates specialized team engineer responsible maintaining shared messaging component used across product team probably get dedicated manager companywide kpis team primarily evaluated incentivized based reliability performance messaging componentthese metric include latency send request loss rate delivery rate andwhether send million email notification text message marketing campaign hoursit ripe team design messaging api break component monolith develop deploy monitor support independently freshly minted team could even negotiate different slas online apis offline batch apis important note part daily workflow figure run service upstream downstream laptop cicd process might even start service though careful product owner would test significant change stage deployment productionif team also responsible maintaining communication preference user also choose combine function messaging micro service extend api instead creating new onemicroservices inside camera small photo vadim sherbakov unsplashis micro know sure productive drink happy hour friday instead begging unfortunate oncall permission deploy monolith normal hoursci developer experienceit turn important aspect successful transition microservices delivery infrastructure devx since goal accelerate pace change much important choosing microservice framework use moreover organization different habit expectation reason go monolith hundred microservices overnight take dedicated effort iron kink first decomposed service figure acceptable testing strategy involves development tooling tovalidate stability compatibility apis help developer start stop service locally help developer find metric log environment send notification alert right people context health service threatened andenable operator developer troubleshoot fix problem servicethis real work improves scalabilitythe face get say devops devx make sound smarter apparently photo ben white unsplashsharing librariesone important decision around management dependency internal library many difficult question answer including artifact share version library would internal library work range library tested process upgrading minor version library major responsible artifact use head internal library internal library use semantic versioning instead shall responsible maintaining internal library shall upgrade version used different artifact many major version would supported version conflict internal library library resolved process ensuring judiciously upgrade latest version internal library library one specify bill material building artifact control cyclic dependency internal library many differing opinion many way set organization make work however important recognize problem managed poorly become source friction contention allowing version library differ artifact make easier different team upgrade important library separate schedule requires affected internal library support range major versionsit could enticing option pin everything without clear responsibility upgrade process project languish pool outdated potentially vulnerable dependency long time
152,Lobsters,scaling,Scaling and architecture,Deep dive into serverless AWS automation using VS Code,http://tiny-giant-books.com/Entry1.html?EntryId=recak5uVmYH6H7cLV&ref=lob,deep dive serverless aws automation using v code,,
153,Lobsters,scaling,Scaling and architecture,Give me back my monolith,http://www.craigkerstiens.com/2019/03/13/give-me-back-my-monolith/,give back monolith,microservices back monolith setup went intro chem quantum mechanic long understanding system debug maybe test tradeoff good reason right day day scaling database,feel like starting pas peak hype cycle microservices longer multiple time week see blog post migrated monolith service often hear bit counter hate monolith care thing stay performant actually seen migration microservices back monolith go one large application multiple smaller service number new thing tackle rundown thing simple get revisit setup went intro chem quantum mechanic setting basic database application background process pretty defined process readme github often hour maybe running started new project onboarding new engineering least initial environment would done first day ventured microservices onboarding time skyrocketed yes docker orchestration day help time start running cluster onboard new engineer order magnitude larger saw year ago many junior engineer burden really unnecessary complexity long understanding system let stay junior engineer perspective moment back monolithic apps error clear stacktrace see originated could jump right debug service talk another service queue something message bus another service process error piece together piece eventually learn service version service q expecting vesion already contrast standard consolidated log let forget interactive terminaldebugger wanted go step step process debugging understanding inherintly complicated debug maybe test continuous integration continuous development starting become common place new apps see day automatically build run test new pr require test pas review checkin great process place big shift lot company really test service bring complete working version application remember back onboarding new engineer service cluster well get teach ci system bring system actually test thing working probably bit much effort going test piece isolation sure spec good enough apis clean service failure isolated impact others tradeoff good reason right lot reason migrate microservices heard case agility scaling team performance give resilient service reality invested decade development practice tooling around monolith still maturing day day work lot folk different stack usually talking scaling running limit single node postgres database conversation focus scaling database conversation fascinated learn architecture journey microservices interesting trend see reaction happy monolithic app road microservices may work fine lot benefit may outweigh bumpy road get personally give monolithic app beach somewhere happy
154,Lobsters,scaling,Scaling and architecture,Lamby - Simple Rails & AWS Lambda Integration using Rack,https://technology.customink.com/blog/2019/03/13/rails-on-aws-lambda-with-sam/,lamby simple rail aws lambda integration using rack,rail aws lambda sam aws lambda rubyrails sam aws lambda introducing lamby simple rail aws lambda integration using rack get started rail lambda live demo http lambycustominktechcom custominklambydemo lot todos issue,rail aws lambda sam move basic hello world function get ready super charge next lambda project productive web framework convenient cloud infrastructure following arlington ruby presentation aws lambda rubyrails sam last week excited share today get started rail aws lambda introducing lamby simple rail aws lambda integration using rack goal project provide minimal code along comprehensive documentation get rail application running aws lambda gem code focus mainly converting api gateway event context object rack env send rail application everything else documentation get started rail lambda live demo want dig demo rail application running aws lambda visit http lambycustominktechcom checkout github repository custominklambydemo deployed custom domain name behind cloudfront distribution def handler event context lambyhandler app event context end lot todos start project list several todos happen coming week make sure watch repository update learn accomplish anything rail asset compilation using bucket tagged logging cloudwatch please submit issue question want see documentation around certain topic using aws rail lambda thanks
155,Lobsters,scaling,Scaling and architecture,Efficient Stream Processing with Pulsar Functions,https://www.jowanza.com/blog/2019/3/9/efficient-stream-processing-with-pulsar-functions,efficient stream processing pulsar function,search experimental design tiered storage api,search experimental designwhile gathering requirement new system became evident stream processing created equal streaming job simple transformation put message back onto stream others complex memory cpu intensive process designed handle complex event processing use case case data may come order time window certain event may trigger new stream processing pipeline spawn latter case tool like spark heron flink seemed like nobrainer simple case question adopting complex topology distributed state small computation stream data care order data decided narrow list research tool would enable simple stream processing topology casesoutside managed offering apache pulsar distributed pubsub queuing system pulsar function simplest topology additional benefit using pulsar around ease operability kubernetes pulsar flexibility store data long term tiered storage api pulsar function lightweight function run pulsar node consume pulsar topic execute predefined logic message batch pubsub message ideologically similar using aws lambda kinesis however shared resource pool function pulsar node additional benefit set would reduced network latency since data streamed processed hardware hesitancy time surrounding scalability pulsar function test proved pulsar could handle message volume required pulsar function beta time unclear processing data pulsar node would affect entire system would trouble backpressure cpu constraint
156,Lobsters,scaling,Scaling and architecture,Polylith - Functional thinking at the system scale,https://polylith.gitbook.io/polylith/,polylith functional thinking system scale,,prefer start beginning take thing pace already exactly right place keep reading
157,Lobsters,scaling,Scaling and architecture,"Microservices, Containers and Kubernetes in 10 minutes",https://gravitational.com/blog/microservices-containers-kubernetes/,microservices container kubernetes minute,microservices container kubernetes minute microservice example amazon product listing product page amazon difference microservices container use microservices separation computing storage asynchronous processing embrace message bus api versioning rethink security kubernetes microservices kubernetes gravity like cattle pet managing cloudnative application multicloud world conclusion gravity related post kubernetes programming security want stay informed,microservices container kubernetes minute mar ev kontsevoy microservice microservice using microservices microservices related container kubernetes thing keep coming daytoday need overview minute blog post fundamentally microservice computer program run server virtual computing instance responds network request different typical railsdjangonodejs application different fact may discover already dozen microservices deployed organization new magical technology qualify application called microservice microservice defined built fit broader system solution make service microservice generally microservices narrow scope focus smaller task well let explore looking example example amazon product listing let examine system serf product page amazon contains several block information probably retrieved different database product description includes price title photo etc recommended item ie similar book people bought sponsored listing related item information author book customer review browsing history item amazon store quickly write code serf listing simple approach would look something like user request come browser served web application linux window process usually application code fragment get invoked called request handler logic inside handler sequentially make several call database fetch required information needed render page stitch together render web page returned user simple right fact many ruby rail book feature tutorial example look like complicate thing may ask imagine happens application grows engineer become involved recommendation engine alone example maintained small army programmer data scientist dozen different team responsible component rendering page team usually want freedom change database schema release code production quickly often use development tool like programming language data store choice make tradeoff computing resource developer productivity preference maintenancemonitoring functionality imagine team agree everything ship newer version web store application become difficult time solution split component smaller separate service aka microservices application process becomes smaller dumber basically proxy simply break incoming page request several specialized request forward corresponding microservices process running elsewhere application microservice basically aggregator data returned specialized service may even get rid entirely offload job user device code run browser singlepage javascript app microservices separated development team working microservice deploy service frequently wish without disrupting team scale service way see fit example use aws instance type choice perhaps run specialized hardware monitoring backup disaster recovery specific service difference microservices container container method packaging deploying running linux programprocess could one giant monolithic application container could swarm microservices user container container useful resource allocation sharing technology something devops people get excited microservice software design pattern something developer get excited container microservices useful dependent use microservices idea behind microservices new decade software architect work trying decouple monolithic application reusable component benefit microservices numerous include easier automated testing rapid flexible deployment model higher overall resiliency another win adopting microservices ability pick best tool job part application benefit speed c others benefit increased productivity higher level language python javascript drawback microservices include need careful planning higher r investment front temptation overengineering application development team small enough workload challenging usually need throw additional engineering resource solving problem yet use microservices however starting see benefit microservices outweigh disadvantage specific design consideration separation computing storage need cpu power storage grow resource different scaling cost characteristic rely local storage beginning allow adapt future workload relative ease applies simple storage form like file system complex solution database asynchronous processing traditional approach gradually building application adding subroutine object call stop working workload grow application must stretched across multiple machine even data center rearchitecting application around eventdriven model required mean sending event waiting result instead calling function synchronously waiting result embrace message bus direct consequence implement asynchronous processing model monolithic application get broken event handler event emitter need robust performant flexible message bus required numerous option choice depends application scale complexity simple use case something like redis need application truly cloudnative scale may need ability process event multiple event source streaming pipeline like kafka infrastructure even monitoring event api versioning microservices using apis communicate via bus designing schema maintaining backward compatibility critical simply deploying latest version one microservice developer demanding everyone else upgrade code step backward towards monolith approach albeit separated across application domain development team must agree upon reasonable compromise supporting old apis forever keeping higher velocity development also mean api design becomes important skill frequent breaking api change one reason team fail productive developing complex microservices rethink security many developer realize migrating microservices creates opportunity much better security model every microservice specialized process good idea allow access resource need way vulnerability one microservice expose rest system attacker contrast large monolith tends run elevated privilege superset everyone need limited opportunity restrict impact breach kubernetes microservices kubernetes complex describe detail deserves overview since many people bring conversation microservices strictly speaking primary benefit kubernetes aka increase infrastructure utilization efficient sharing computing resource across multiple process kubernetes master dynamically allocating computing resource fill demand allows organization avoid paying computing resource using however side benefit make transition microservices much easier break monolithic application separate looselycoupled microservices team gain autonomy freedom however still closely cooperate interacting infrastructure microservices must run solve problem like predicting much computing resource service need requirement change load carve infrastructure partition divide microservices enforce resource restriction kubernetes solves problem quite elegantly provides common framework describe inspect reason infrastructure resource sharing utilization adopting kubernetes part microservice rearchitecture good idea kubernetes however complex technology learn even harder manage take advantage hosted kubernetes service provided cloud provider however always viable company need run kubernetes cluster across multiple cloud provider enterprise data center use case recommend trying gravity open source kubernetes packaging solution remove need kubernetes administration gravity work creating kubernetes cluster single image file kubernetes appliance downloaded moved created destroyed hundred making possible treat kubernetes cluster like cattle pet gravity useful using kubernetes single environment designed give huge advantage managing cloudnative application multicloud world conclusion summarize microservices new old software design pattern growing popularity due growing scale internet company small project shy monolithic design offer higher productivity smaller team kubernetes great platform complex application comprised multiple microservices kubernetes also complex system hard run consider using hosted kubernetes must run cluster need publish application downloadable appliance consider open source solution gravity learn useful multicloud environment related post kubernetes programming security  want stay informed subscribe newsletter get article product update
158,Lobsters,scaling,Scaling and architecture,Microservices after Two Years,https://georgestocker.com/2019/01/21/microservices-after-two-years/,microservices two year,microservices minimonoliths listen microservices require different way thinking problem solving domain boundary critical microservices success bounded context developer tooling support microservices well monolith deployment requires better tooling microservices microservices deliver promise objectoriented programming solid contract pattern practice code generated designing microservices architecture right way punt nonfunctional requirement event driven programming make microservices work disastrous sideeffects choosing rest event supporting microservices tougher may think fallacy distributed system circuitbreaker pattern eventually hover zero maintaining microservices requires strong organizational technical leadership microservices technical solution organizational problem closing,point two year experience microservices expert hardearned knowledge distilled working making lot mistake process learned wish known going microservices minimonoliths jim gaffigan rather funny skit american mexican food listen butcher punchline punchline skit mexican food basically consists tortilla cheese meat vegetable tend think deployable software way code wrapped deployment script sent production monolith independent complete application fulfill business function microservice independent complete application fulfills business function microservices minimonoliths answer come idea microservices collaborate monolith rely another monolith uptime data resiliency generally selfcontained view world due nature care anyone else exists company website wholly independent anything else critically though multiple team may work company website share code branch single production pipeline microservices hand independent complete application fulfill business function fulfill one monolith microservice understands independent possibly zero people interested say designed understanding mind monolith business eventually find wish monolith designed share information decoupled fashion often late anything easilymicroservices minimonoliths collaborator operate independently need microservices require different way thinking problem solving developer love write code enamored writing code write code even one need u write code solve nagging problem machine automate silly thing even write code solve problem household fact new side project set raspberry pi calendar viewer house probably unique software development though maybe plumber repipe house electrician rewire whim tenor overdone software development exhort new developer write code first ask work monolith monolith make writing code easy get point default state find problem write code ship without understanding whether problem best served bolton addon existing system small thing emergent issue small thing add become problem time instance ever tried add csv import existing system probably found within day desired csv import feature really csv domain specific logic import function almost harmful bulk method inserting part original requirement necessitating change api monolith really easy write code add functionality baked assumption clear potentially change api system expose present user ease writing code easy rush implementation without regard design writing code quickly job solving problem without causing problem job monolith make hard user want add stock microservices hand require upfront planning code written every time every new service change service may able coupled completely replacing service anything potential change contract system whether user service requires understanding upfront design change monolith go back csv import example potential way microservices new csv importer service stood take csv file domain specific formatting emits event sends http request correct service us existing api addingimporting information want add multiple csv service necessarily coupled though coupling go right direction since contract changed original service guarantee original service kept intact microservices make harder break existing consumer done well tradeoff upfront planning required designing solution microservices based topology domain boundary critical microservices success three general flow microservices may type escaping right microservices give new capability existing domain bounded context previous example adding csv import portfolio service separate microservice example several tradeoff depends constraint desire microservices represent stateless process viz validating credit card microservices represent stateful process interaction portfolio service notice said nothing size service depending speak size microservice mystery opinion course one invariant seen good microservices topology ensure line drawn domain bounded context fancy domain driven design phrase mean split model interaction mean sale customer interaction quite different model mode interaction customer interaction customer support splitting context boundary sale customer support software maintain independent idea interact customer depending context martin fowler illustration bounded context source http martinfowlercomblikiboundedcontexthtml microservices typically mean customer support portal different bounded context sale funnel even share property customer least demographically three way handle problem method set separate service independent customer model service sale customer support one created one system necessarily referenced elsewhere customerid customersupportid salesid method illustrated method set customer service sale service customer support service sale customer support get customer information customer service method set customer service sale service customer support service sale customer support duplicated data received event thing happen customer service maintain disparate model customer mean system perspective internal identifier used varies system system mean customer service demographic information sale service may may demographic information add sale context customer support service maintains duplicate information add customer support piece method tradeoff quickly see maintenance issue method three different representation customer potentially different state service sale person see customer signed dotted line customer support person always post sale view customer ok want sale customer support information need bit juggling ensure customer sale context indeed customer customer support context method allows one representation customer service either addon representation customer downstream service still beholden customer service context live also temporal coupling factor service get demographic information customer service method allows service decoupled customer service allows service add data mean customer allows service change independently since service emit event listen update model want also mean unified contract defines demographic customer ensuring service set listen event pertaining customer service appropriately handle customer event emitted event sourcing possible solution none method ideal easiest develop standpoint different level maintenance requirement one crucial decision team must make domain context thing dealing talked differently depending talk maintenance cost approach team chooses method lot distributed system problem easily solved made interacting system harder choose two service depend third really independent point added requestresponse dependency service may need exist harder debug choose approach quite bit upfront work defining contract defining pattern maintenance work reasoning service interacts another service debugging future expansion far easier developer tooling support microservices well monolith year experience industry creating tooling around building deploying software though really last year tooling accelerated even year experience pretty solid tooling around developing debugging monolith debugger ides take monolith forgranted likely write microservices depend microservices rest going bad time debugging service locally choice range standing part system collaborate mocking external dependency dockerizing system service stood independently course diving mixed networking land docker lot tooling make experience seamless service running outside docker debugging hard set work service running inside docker network vice versa frontend development even worse nodejs requirement building frontends day try livedebugging docker ui source kept locally fun team handle problem different way point problem exists solution mature debugging monolith use microservices need allocate sizable chunk time building tooling necessary allow people develop service deployment requires better tooling microservices deployment consideration key want fast moving organization respond change without able change software quickly even develop change quickly deploy quickly fastmoving organization continuous integration ci continuous delivery cd essential able respond change product reflect deployment view world monolithic nature source control built cicd system built around pretty much every commercial cd system built monolith mind several deployment model microservices used none good tooling microservices deploy onpremises packaged solutiondeploy cloud independentlydeploy cloud packaged solution sell product customer run data center deployment method often deal solution must packaged deployed together single unit necessitate develop monolith however microservices necessarily multiple deployable artifact whether contained monorepository service one source control repository microrepositories service source control repository separate matter cd pipeline must take account tradeoff change whether microrepository monorepository still exist problem solved current tooling instance tagging master release branch production promotion model different internal environment even local deployment need taken account tooling choose method combine continuous delivery tradeoff go away make rule latest master always pushed internal promotion environment tag happens particular commit pushed production tooling still lacking make seamless experience microservices deliver promise objectoriented programming understand hype object oriented programming understood fundamental encapsulation abstraction inheritance message dispatching polymorphism understand useful started perl moved java nothing compare java oo nature time seemed like work thing could perl ahh youth solid principle helped later always felt like hype oo actual benefit several job maintaining creating objectoriented solution convinced object oriented programming pipedream u expert programmer fad never make full use cause harm good started researching microservices fully independent object agency could collaborate others encapsulation ensured openclosed principle requirement single responsibility almost ensured nature service say micro tin inheritance far simpler consume service give modify suit need csv example share information unless common contract used sort message dispatching absolutely huge principle trying bring reality year codebases worked best downside oop practice really easy modifying code something break encapsulation business pressure make even easier microservices longer possible sure business induced pressure might cause problem alter contract service allowed system reasoned way oop promised perhaps best microservices put guard rail keep mistake oop happening better contract pattern practice code generated something manually twice write step manually third time automate producing even dozen service mean either manually enforcing structure contract format service communicate user pattern structure common infrastructural concern practice write software code generating commonality code generate entropy win even across feature service start thing different way find new pattern structuring event depending service could see different pattern untenable development maintenance perspective method show world customer service emits event customer added updated allowing interested service listen change update data store necessary without code generation would tedious process filled error code generation schema defined model viable development model imagine trying update modelcontract without code generation two sane path package commonality really done dependency utility function codegenerate everything packaging utility classesmodels like customer model event valid approach concern using taking dependency even internal one overhead internal infrastructure fact every service would required programming language latter path code generation exactly michael bryzek advocated talk designing microservices architecture right way coming trying path packaging common functionality manually see utility tradeoff course developing code generation tooling heavy investment time requires discipline develop tooling first without trying develop feature would likely result visible movement thing business care feature revenue etc also ensures long tooling support language implement model language like punt nonfunctional requirement lot nonfunctional requirement system never appear roadmap never spoken sale meeting tolerated product manager thing like user signed fifteen minute authorization system incorporate role location data transient part backup strategy data need backed every minute system must allow concurrent user time nonfunctional requirement quality system part userfacing feature developed monolith typically place go implement nonfunctional requirement discussed previously ide tooling built refactoring necessary ensure change take place everywhere needed statically typed language dynamic folk problem contend even implement new feature generally one place microservices implement authorization must implement across service implement timeout must implement across service unless microservices across host performance improvement must take account service may share host resource one service service using server instance ie every service us postgres share postgres server instance even separate database instance performance tuning backup must take account greatly complicates matter performance tuning dealing nonfunctional requirement system easily built nonfunctional requirement need known beginning every delay implementing nonfunctional requirement make likely disparate change need made across several service take much longer service built event driven programming make microservices work firmware programming finite state machine event got day peripheral separate state triggered event may happen user input peripheral instance seeing bluetooth advertisement whitelisted address may trigger connection since firmware byandlarge sits single core systemonchip limited use thread using event loop finitestatemachines one best way make firmware work finite state machine coupled event driven programming also nice property parlay well microservices event ensure service decoupled others direct requestresponses service finite state machine dictate happens based current state service plus input make debugging matter knowing state service input received greatly reduces complexity standing debugging service allows problem decomposed event state add event sourcing mix event stream record event occurred playing back issue simple replaying event possible microservices operate network boundary monolith forced debug entire monolith hope someone write code disastrous sideeffects impossible find normal mean easier find needle small jar needle giant haystack possible observable boundary microservices using pattern limit amount complexity allows arrive certain state going start writing microservices highly recommend going path eventdriven programming state machine sort event stream even decide event sourcing choosing rest event supporting microservices tougher may think read fallacy distributed system section almost writes microservices distributed system matter shake one major problem communicating across network boundary service network timeout using rest mean implementing circuitbreaker pattern sort timeout also mean service communicate service communicate service rest availability chain eventually hover zero video rightfully say go far say possible make call service rest need data service publish event consume event sound great decoupled resilient failure however service must mean publish bus consume event bus support whatever serialization scheme want use oh need able debug want runtime resiliency must sacrifice development simplicity get maintaining microservices requires strong organizational technical leadership business care topology system care architecture care easy maintain care whether use excel quickbooks forecasting business want two thing really n thing work increase revenuereduce cost believe feature increase revenue fair belief correlation imply causation feature also increase development cost business way solve problem reducing cost increasing revenue also fair good number case right path earlier mentioned microservices keep nasty shortcut cripple development team happening good thing business also bad thing see crippling shortcut may never happen adding feature way thinking increase revenue choose helping revenue possibly hurting future maintenance delaying feature several week helping future maintenance pick path fastest revenue every time person people keep happening hopefully organization cto engineering leadership vp director engineering architect senior leader team people cachet experience know going hurt future maintenance hopefully know enough know probably sure revenue bet either requires discipline trust part engineering leadership team must gained trust business delivering business want timeframe want must disciplined enough stick gun someone say well could week hooked service service b database failed microservices maintaining future monolith also lost advantage working microservices shortcut easy say yes shortcut greatly endanger maintainability health development team system microservices technical solution organizational problem developer consultant tend espouse microservices cloud scenario tend ignore microservices orthogonal deployment scenario orthogonal technology stack take away advantage microservices still left topology allows segment team along domain boundary team operate independently one another small enough scale could even individual service scale feature creation number people development organization mythical man month state adding people late project make later say people communicate could reduce amount communication needed ship feature microservices let fall firmly microrepository camp well conflate two purpose microservices development mean independent repository le issue merge conflict branching collaboration needing happen push particular feature also mean fewer avenue feature clash existing feature since definition service independent autonomous mean fewer part reason result faster development timemicroservices architected well let go faster otherwise could le need put organizational guardrail development team code review gated checkins code freeze resolve team performance issue minimizes effect single developer whole system great benefit organization hire well pay well every organization low turnover rate software development substitute technology human training improvement organization topnotch performer highperforming engineering organization high performing business turnover need microservices going make mistake microservices would fix however organization consists human fallible microservices provide benefit development monolith closing microservices another tool help make software development better make system easier maintain provide many benefit many tradeoff traditional monolith rarely clear whether system developed monolith microservices several factor steer choice towards one factor depend greatly individual organizational leadership business model constraint politics organization implementing service thing wish known started microservices wish known microservices working note special thanks adam mara spending part weekend giving feedback post
159,Lobsters,scaling,Scaling and architecture,Python + Memcached: Efficient Caching in Distributed Applications,https://julien.danjou.info/python-memcached-efficient-caching-in-distributed-applications/,python memcached efficient caching distributed application,leastrecently used algorithm redis memcached installing memcached available many platform linux explained macos homebrew window precompiled binary storing retrieving cached value using python install using pip automatically expiring cached data note warming cold cache thundering herd problem check set beyond caching scaling python,writing python application caching important using cache avoid recomputing data accessing slow database provide great performance boostpython offer builtin possibility caching simple dictionary complete data structure functoolslrucache latter cache item using leastrecently used algorithm limit cache sizethose data structure however definition local python process several copy application run across large platform using inmemory data structure disallows sharing cached content problem largescale distributed applicationstherefore system distributed across network also need cache running network nowadays plenty network server offer caching example redisas going see tutorial memcached another great option caching quick introduction basic memcached usage learn advanced pattern cache set using fallback cache avoid cold cache performance issuesinstalling memcachedmemcached available many platform run linux install using aptget install memcached yum install memcached install memcached prebuilt package alse build memcached source explained herefor macos using homebrew simplest option run brew install memcached installed homebrew package manageron window would compile memcached find precompiled binariesonce installed memcached simply launched calling memcached command memcachedbefore interact memcached pythonland need install memcached client library see next section along basic cache access operationsstoring retrieving cached value using pythonif never used memcached pretty easy understand basically provides giant networkavailable dictionary dictionary property different classical python dictionnary mainly key value byteskeys value automatically deleted expiration timetherefore two basic operation interacting memcached set get might guessed used assign value key get value key respectivelymy preferred python library interacting memcached recommend using simply install using pip pip install pymemcache following code show connect memcached use network cache python application pymemcacheclient import base nt forget run memcached running next line client baseclient localhost client instantiated access cache clientset somekey value retrieve previously set data clientget somekey valuememcached network protocol really simple implementation extremely fast make useful store data would otherwise slow retrieve canonical source data compute againwhile straightforward enough example allows storing keyvalue tuples across network accessing multiple distributed running copy application simplistic yet powerful great first step towards optimizing applicationautomatically expiring cached datawhen storing data memcached set expiration maximum number second memcached keep key value around delay memcached automatically remove key cachewhat set cache time magic number delay entirely depend type data application working could second might hourscache invalidation defines remove cache sync current data also something application handle especially presenting data old stale avoidedhere magical recipe depends type application building however several outlying case yet covered examplea caching server grow finite resource therefore key flushed caching server soon need space store thingssome key might also expired reached expiration time also sometimes called timetolive ttl case data lost canonical data source must queried againthis sound complicated really generally work following pattern working memcached python pymemcacheclient import base def dosomequery replace actual querying code database remote rest api etc return nt forget run memcached running code client baseclient localhost result clientget somekey result none cache empty need get value canonical source result dosomequery cache result next time clientset somekey result whether needed update cache point work data stored result variable print result note handling missing key mandatory normal flushout operation also obligatory handle cold cache scenario ie memcached started case cache entirely empty cache need fully repopulated one request timethis mean view cached data ephemeral never expect cache contain value previously wrote itwarming cold cachesome cold cache scenario prevented example memcached crash example migrating new memcached serverwhen possible predict cold cache scenario happen better avoid cache need refilled mean sudden canonical storage cached data massively hit cache user lack cache data also known thundering herd problem pymemcache provides class named fallbackclient help implementing scenario demonstrated pymemcacheclient import base pymemcache import fallback def dosomequery replace actual querying code database remote rest api etc return set ignoreexctrue possible shut old cache removing usage program ever necessary oldcache baseclient localhost ignoreexctrue newcache baseclient localhost client fallbackfallbackclient newcache oldcache result clientget somekey result none cache empty need get value canonical source result dosomequery cache result next time clientset somekey result print result fallbackclient query old cache passed constructor respecting order case new cache server always queried first case cache miss old one possible returntrip primary source dataif key set set new cache time old cache decommissioned fallbackclient replaced directed newcacheclientcheck setwhen communicating remote cache usual concurrency problem come back might several client trying access key time memcached provides check set operation shortened ca help solve problemthe simplest example application want count number user time visitor connects counter incremented using memcached simple implementation would def onvisit client result clientget visitor result none result else result clientset visitor result however happens two instance application try update counter time first call clientget visitor return number visitor let say add compute set number visitor number wrong result ie solve concurrency issue ca operation memcached handy following snippet implement correct solution def onvisit client true result ca clientgets visitor result none result else result clientcas visitor result ca break get method return value like get method also return ca valuewhat value relevant used next method ca call method equivalent set operation except fails value changed since get operation case success loop broken otherwise operation restarted beginningin scenario two instance application try update counter time one succeeds move counter second instance get false value returned clientcas call retry loop retrieve value time increment ca call succeed thus solving problemincrementing counter interesting example explain ca work simplistic however memcached also provides incr decr method increment decrement integer single request rather multiple getscas call realworld application get ca used complex data type operationsmost remote caching server data store provide mechanism prevent concurrency issue critical aware case make proper use featuresbeyond cachingthe simple technique illustrated article showed easy leverage memcached speed performance python applicationjust using two basic set get operation often accelerate data retrieval avoid recomputing result memcached share cache accross large number distributed nodesother advanced pattern saw tutorial like check set ca operation allow update data stored cache concurrently across multiple python thread process avoiding data corruptionif interested learning advanced technique write faster scalable python application check scaling python cover many advanced topic network distribution queuing system distributed hashing code profiling
160,Lobsters,scaling,Scaling and architecture,Azure in Plain English,https://www.expeditedssl.com/azure-in-plain-english,azure plain english,certsimple acquisition,certsimple acquisition early expedited security acquired certsimple domain site noncustomer asset
161,Lobsters,scaling,Scaling and architecture,Bottled Water: Real-time integration of PostgreSQL and Kafka [2015],https://www.confluent.io/blog/bottled-water-real-time-integration-of-postgresql-and-kafka/,bottled water realtime integration postgresql kafka,confluent martin kleppmann bottled water one talk blown away database replication work consistent snapshot realtime stream change much le problem change data capture databus wormhole central hub data stream getting realtime stream change goldengate mysql binlog mongodb oplog couchdb change feed rethinkdb firebase postgresql surprisingly fullfeatured logical decoding confluent bottled water introducing bottled water avro format kafka samza camus kafka log compaction feature replica identity replicated log abstraction avro advantage avro blog post confluent stream data platform guide confluent schema registry serializers logical decoding output plugin output plugin api client daemon related work decoderbufs xavier stevens pgkafka pgq skytools londiste bucardo sqoop writing kafka james cheng change capture project status bottled water readme berlin buzzword,summary confluent starting explore integration database event stream part first step exploration martin kleppmann made new open source tool called bottled water let transform postgresql database stream structured kafka event tremendously useful data integration writing database easy getting data surprisingly hard course want query database get result fine want copy database content system example make searchable elasticsearch prefill cache nice fast load data warehouse analytics want migrate different database technology data never changed would easy could take snapshot database full dump eg backup copy load system problem data database constantly changing snapshot already outofdate time loaded even take snapshot day still onedayold data downstream system large database snapshot bulk load become expensive really great want copy data several different system one option application socalled dual writes every time application code writes database also updatesinvalidates appropriate cache entry reindexes data search engine sends analytics system however explain one talk dualwrites approach really problematic suffers race condition reliability problem slightly different data get written two different datastores perhaps due bug race condition content datastores gradually drift apart become inconsistent time recovering gradual data corruption difficult rebuild cache index snapshot database advantage inconsistency get blown away rebuild new database dump however large database slow inefficient process entire database dump day frequently could make fast typically small part database change one snapshot next could process diff changed database since last snapshot would also smaller amount data could take diffs frequently could take diff every minute every second time second take extreme change database become stream every time someone writes database message stream apply message database exactly order original database committed end exact copy database think exactly database replication work replication approach data synchronization work much better dual writes first write data one database probably already anyway next extract two thing database consistent snapshot one point time realtime stream change point onwards load snapshot system example search index cache apply realtime change ongoing basis pipeline well tuned probably get latency le second downstream system remain almost uptodate since stream change provides ordering writes race condition much le problem approach building system sometimes called change data capture cdc though tool currently good however company cdc become key building block application example linkedin built databus facebook built wormhole purpose excited change capture allows unlock value data already feed data central hub data stream readily combined event stream data database realtime approach make much easier experiment new kind analysis data format allows gradual migration one system another minimal risk much robust data corruption something go wrong always rebuild datastore snapshot stream getting realtime stream change getting consistent snapshot database common feature need order take backup getting realtime stream change traditionally overlooked feature database oracle goldengate mysql binlog mongodb oplog couchdb change feed something like exactly easy use correctly recently database rethinkdb firebase oriented towards realtime change stream however today talk postgresql oldschool database good stable good performance surprisingly fullfeatured recently wanted get stream change postgres use trigger possible see fiddly requires schema change perform well however postgres released december introduced new feature change everything logical decoding explain detail logical decoding change data capture postgres suddenly becomes much appealing feature released set build change data capture tool postgres would take advantage new facility confluent sponsored work thank confluent today releasing alpha version tool open source called bottled waterintroducing bottled water logical decoding take database writeahead log wal give u access rowlevel change event every time row table inserted updated deleted event event grouped transaction appear order committed database abortedrolledback transaction appear stream thus apply change event order end exact transactionally consistent copy database postgres logical decoding well designed even creates consistent snapshot coordinated change stream use snapshot make pointintime copy entire database without locking continue writing database copy made use change stream get writes happened since snapshot bottled water us feature copy data database encodes efficient binary avro format encoded data sent kafka table database becomes kafka topic row database becomes message kafka data kafka easily write kafka consumer whatever need send elasticsearch populate cache process samza job load hdfs possibility endless kafka kafka messaging system best known transporting highvolume activity event web server log user click event kafka event typically retained certain time period discarded kafka really good fit database change event want database data discarded fact kafka perfect fit key kafka log compaction feature designed precisely purpose enable log compaction timebased expiry data instead every message key kafka retains latest message given key indefinitely earlier message given key eventually garbagecollected quite similar new value overwriting old value keyvalue store bottled water identifies primary key replica identity table postgres us key message sent kafka value message depends kind event insert update message value contains row field encoded avro deletes message value set null cause kafka remove message log compaction disk space freed log compaction need one system store snapshot entire database another system realtime message live perfectly well within system bottled water writes initial snapshot kafka turning every single row database message keyed primary key sending kafka broker snapshot done every row inserted updated deleted similarly turn message row frequently get updated many message key update turn message fortunately kafka log compaction sort garbagecollect old value waste disk space hand row never get updated deleted stay unchanged kafka forever never get garbagecollected full database dump realtime stream system tremendously powerful want rebuild downstream database scratch start empty database start consuming kafka topic beginning scan whole topic writing message database reach end uptodate copy entire database continue keeping uptodate simply continuing consume stream building alternative view onto data never easier idea maintaining copy database kafka surprise people familiar traditional enterprise messaging limitation actually use case exactly kafka built around replicated log abstraction make kind largescale data retention distribution possible downstream system reload reprocess data without impacting performance upstream database serving lowlatency query avro data extracted postgres could encoded json protobuf thrift number format however believe avro best choice gwen shapira written advantage avro schema management got blog post comparing protobuf thrift confluent stream data platform guide give reason avro good data integration bottled water inspects schema database table automatically generates avro schema table schema automatically registered confluent schema registry schema version embedded message sent kafka mean work stream data platform serializers work data postgres meaningful application object rich datatypes without writing lot tedious parsing code translation postgres datatypes avro already fairly comprehensive covering common datatypes providing lossless sensibly typed conversion intend extend support postgres builtin datatypes many effort worth good schema data tremendously important logical decoding output plugin interesting property postgres logical decoding feature define wire format change data sent network consumer instead defines output plugin api receives function call every insert update delete bottled water us api read data database internal format serializes avro output plugin must written c using postgres extension mechanism loaded database server shared library requires superuser privilege filesystem access database server something undertaken lightly understand many database administrator scared prospect running custom code inside database server unfortunately way logical decoding currently used moment logical decoding plugin must installed leader database principle would possible run separate follower impact client current implementation postgres allow limitation hopefully lifted future version postgres client daemon besides plugin run inside database server bottled water consists client program run anywhere connects postgres server kafka broker receives avroencoded data database forward kafka client also written c easiest use postgres client library way code shared plugin client fairly lightweight need write disk happens client crash get disconnected either postgres kafka problem keep track message published acknowledged kafka broker client restarts error replay message acknowledged thus message could appear twice kafka data lost related work various people working similar problem decoderbufs experimental postgres plugin xavier stevens decodes change stream protocol buffer format provides logical decoding plugin part story consistent snapshot client part xavier mention written client read postgres writes kafka open source pgkafka also xavier kafka producer client postgres function could potentially produce kafka trigger pgq postgresbased queue implementation skytools londiste developed skype us provide triggerbased replication bucardo another triggerbased replicator get impression triggerbased replication somewhat hack requiring schema change fiddly configuration incurring significant overhead also none project seems endorsed postgresql core team whereas logical decoding fully supported sqoop recently added support writing kafka knowledge sqoop take full snapshot database capture ongoing stream change also unsure transactional consistency snapshot using mysql james cheng put together list change capture project get data mysql kafka afaik focus binlog parsing piece consistent snapshot piece status bottled water present bottled water alphaquality software proof concept quite bit care gone design implementation yet tested realworld scenario definitely ready production use right testing tweaking hopefully become productionready future releasing open source hope getting feedback community also people heard working bugging release readme information get started please let u know get also talking bottled water berlin buzzword june hope see
163,Lobsters,scaling,Scaling and architecture,Interplanetary Internet,https://ahsan.io/2019-01-16-interplanetary-internet/,interplanetary internet,mar previous raft source,internet look like humanity reached interplanetary status company like spacex aspiring land first human mar lot time figure thankfully already development space one previous blogposts briefly mentioned internet become vital part modern society infrastructure supporting surely civilization conquers planet would want take internet mark watney without access instagram interplanetary communication hard delay interruption inherent communication scale roundtrip signal neighboring planet mar take minimum maximum minute unbroken connection major problem making internet interplanetary tcpip protocol powering internet today since confined short span earth assumes instantaneous uninterrupted strong connectivity throughout case delay interruption protocol fails miserably delay interruption inevitable galactic communication need new protocol one tolerate network partition long delay enter delaytolerant networking dtn also sometimes called disruptiontolerant networking dtn umbrella term encompasses project related networking robust face delay disconnectivity interplanet ipn example one project networking protocol developed specifically correct shortcoming internet protocol ip supported darpa nasa explored protocol communication interplanetary internet time take look side side application business logic one hand application like social medium somewhat tolerate delay thing like banking require strong immediate consistency therefore going challenging task deploy application galactic scale consistency across multiple planet going major problem one solution would treat planet edge network edge network drip cache primary source essence local internet oxymoron know specific planet consistency across planet new problem maybe solve old solution maybe form raft consensus algorithm could applied well nonetheless application architecture evolve simple technique cut mustard interesting ride take internet u exploration universe wait contribute evolution might even gcpaws region mar one day know however thing turn surely see lot cool technology backing interplanetary internet source
164,Lobsters,scaling,Scaling and architecture,Courier: Dropbox migration to gRPC,http://blogs.dropbox.com/tech/2019/01/courier-dropbox-migration-to-grpc/,courier dropbox migration grpc,scribebased log pipeline apache thrift openapi create new standard fbthrift courier brings grpc bandaid security service identity tl mutual authentication pfs optical character recognition ocr service secure production identity framework everyone observability stats tracing opentracing jaeger reliability deadline circuitbreaking grpc request includes deadline codel introspection debug endpoint heap cpu profile could exposed http grpc endpoint gcpercent malloc library dump internal stats channelz proposal grpc performance optimization tl handshake overhead modified grpccore grpcpython encryption expensive storage box change grpc high bandwidthdelay product link multiple data center connected backbone network upper bound bdp hardcoded grpcgo golang netserver v grpcserver netserver grpcserver golangprotobuf v gogoprotobuf gogoprotobuf caveat around using gogoprotobuf service description stub generation golang grpc library support tap interface appspecific error code pythonspecific change dropboxmypyprotobuf step freeze legacy rpc step common interface legacy rpc courier step migration new interface spare error budget step switch client use courier rpc step clean alt moving tl handshake separate process trafficruntimereliability team hiring swes sres wide variety engineering position san francisco new york seattle tel aviv office around world acknowledgment,dropbox run hundred service written different language exchange million request per second core service oriented architecture courier grpcbased remote procedure call rpc framework developing courier learned lot extending grpc optimizing performance scale providing bridge legacy rpc system note post show code generation example python go also support rust java courier dropbox first rpc framework even started break python monolith service earnest needed solid foundation interservice communication especially since choice rpc framework profound reliability implication previously dropbox experimented multiple rpc framework first started custom protocol manual serialization deserialization service like scribebased log pipeline used apache thrift main rpc framework legacy rpc protocol protobufencoded message new framework several choice could evolve legacy rpc framework incorporate swagger openapi could create new standard also considered building top thrift grpc settled grpc primarily allowed u bring forward existing protobufs use case multiplexing transport bidirectional streaming also attractive note fbthrift existed time may taken closer look thrift based solution courier brings grpc courier different rpc dropbox integrated grpc existing infrastructure example need work specific version authentication authorization service discovery also need integrate stats event logging tracing tool result work call courier support using bandaid grpc proxy specific use case majority service communicate proxy minimize effect rpc serving latency want minimize amount boilerplate write since courier common framework service development incorporates feature service need feature enabled default controlled commandline argument also toggled dynamically via feature flag security service identity tl mutual authentication courier implement standard service identity mechanism server client tl certificate issued internal certificate authority one identity encoded certificate identity used mutual authentication server verifies client client verifies server tl side control end communication enforce quite restrictive default encryption pfs mandatory internal rpcs tl version pinned also restrict symmetricasymmetric algorithm secure subset preferred identity confirmed request decrypted server verifies client proper permission access control list acls rate limit set service individual method also updated via distributed config filesystem afs allows service owner shed load matter second without needing restart process subscribing notification handling configuration update taken care courier framework service identity global identifier acls rate limit stats side bonus also cryptographically secure example courier aclratelimit configuration definition optical character recognition ocr service copy limit dropboxengineocr rpc method default maxconcurrency queuetimeoutms rateacls ocr client unlimited ocr nobody else get talk u authenticated unauthenticated considering adopting spiffe verifiable identity document svid part secure production identity framework everyone spiffe would make rpc framework compatible various open source project observability stats tracing using identity easily locate standard log stats trace useful information courier service code generation add perservice permethod stats client server server stats broken client identity box granular attribution load error latency courier service courier stats include clientside availability latency well serverside request rate queue size also various breakdown like permethod latency histogram perclient tl handshake one benefit code generation initialize data structure statically including histogram tracing span minimizes performance impact legacy rpc propagated requestid across api boundary allowed joining log different service courier introduced api based subset opentracing specification wrote client library serverside built top cassandra jaeger detail made tracing system performant warrant dedicated blog post tracing also give u ability generate runtime service dependency graph help engineer understand transitive dependency service also potentially used postdeploy check avoiding unintentional dependency reliability deadline circuitbreaking courier provides centralized location language specific implementation functionality common client timeouts time added many capability layer often action item postmortem deadline every grpc request includes deadline indicating long client wait reply since courier stub automatically propagate known metadata deadline travel request even across api boundary within process deadline converted native representation example go represented contextcontext result withdeadline method practice fixed whole class reliability problem forcing engineer define deadline service definition context travel even outside rpc layer example legacy mysql orm serializes rpc context along deadline comment sql query sqlproxy parse comment kill query deadline exceeded side benefit perrequest attribution debugging database query circuitbreaking another common problem legacy rpc client solve implementing custom exponential backoff jitter retries often necessary prevent cascading overload one service another courier wanted solve circuitbreaking generic way started introducing lifo queue listener workpool case service overload lifo queue act automatic circuit breaker queue bounded size critically also bounded time request spend long queue lifo downside request reordering want preserve ordering use codel also circuit breaking property mess order request introspection debug endpoint even though debug endpoint part courier widely adopted across dropbox useful mention couple example useful introspection security reason may want expose separate port possibly loopback interface even unix socket access additionally controlled unix file permission also strongly consider using mutual tl authentication asking developer present cert access debug endpoint esp nonreadonly one runtime ability get insight runtime state useful debug feature eg heap cpu profile could exposed http grpc endpoint planning using canary verification procedure automate cpumemory diffs old new code version debug endpoint allow modification runtime state eg golangbased service allow dynamically setting gcpercent library library author able automatically export libraryspecific data rpcendpoint may quite useful good example malloc library dump internal stats another example readwrite debug endpoint change logging level service fly rpc given troubleshooting encrypted binaryencoded protocol bit complicated therefore putting much instrumentation performance allows rpc layer right thing one example introspection api recent channelz proposal grpc application able view applicationlevel parameter also useful good example generalized application info endpoint buildsource hash command line etc used orchestration system verify consistency service deployment performance optimization discovered handful dropbox specific performance bottleneck rolling grpc scale tl handshake overhead service handle lot connection cumulative cpu overhead tl handshake become nonnegligible especially true mass service restarts switched rsa keypairs ecdsa get better performance signing operation boringssl performance example note rsa still faster signature verification rsa copy  bazel run bssl speed filter rsa rsa signing operation opssec rsa verify key operation opssec rsa verify fresh key operation opssec ecdsa copy  bazel run bssl speed filter ecdsa ecdsa signing operation opssec ecdsa verify operation opssec since rsa verification faster ecdsa one performance perspective may consider using rsa rootleaf cert security perspective though bit complicated since chaining different security primitive therefore resulting security property minimum performance reason also think twice using rsa higher cert rootleaf cert also found tl library choice compilation flag matter lot performance security example comparison macos x mojave libressl build v homebrewed openssl hardware libressl copy  openssl speed libressl sign verify sign verifys rsa bit openssl copy  openssl speed openssl nov sign verify sign verifys rsa bit fastest way tl handshake modified grpccore grpcpython support session resumption made service rollout way le cpu intensive encryption expensive common misconception encryption expensive symmetric encryption actually blazingly fast modern hardware desktopgrade processor able encrypt authenticate data rate single core copy  bazel run bssl speed filter aes byte seal operation mb nevertheless end tune grpc storage box learned encryption speed comparable memory copy speed reducing number memcpy operation critical addition also made change grpc authenticated encrypted protocol caught many tricky hardware issue example processor dma network data corruption even using grpc using tl internal communication always good idea high bandwidthdelay product link dropbox multiple data center connected backbone network sometimes node different region need communicate rpc eg purpose replication using tcp kernel responsible limiting amount data inflight given connection within limit r w mem though since grpc also flow control top tcp upper bound bdp hardcoded grpcgo become bottleneck single high bdp connection golang netserver v grpcserver go code initially supported grpc using netserver logical code maintenance perspective suboptimal performance splitting grpc path processed separate server switching grpc grpcserver greatly improved throughput memory usage courier service golangprotobuf v gogoprotobuf marshaling unmarshaling expensive switch grpc go code switched gogoprotobuf noticeably decreased cpu usage busiest courier server always caveat around using gogoprotobuf stick sane subset functionality fine starting going dig way deeper gut courier looking protobuf schema stub example different language example going use test service service use courier integration test service description let look snippet test service definition copy service test option rpccoreservicedefaultdeadlinems rpc unaryunary testrequest return testresponse option rpccoremethoddefaultdeadlinems rpc unarystream testrequest return stream testresponse option rpccoremethodnodeadline true mentioned reliability section deadline mandatory courier method set whole service following protobuf option copy option rpccoreservicedefaultdeadlinems method also set deadline overriding servicewide one present copy option rpccoremethoddefaultdeadlinems rare case deadline really make sense method watch resource developer allowed explicitly disable copy option rpccoremethodnodeadline true real service definition also expected extensive api documentation sometimes even along usage example stub generation courier generates stub instead relying interceptor except java case interceptor api powerful enough mainly give u flexibility let compare stub default one using golang example default grpc server stub look like copy func testunaryunaryhandler srv interface ctx contextcontext dec func interface error interceptor grpcunaryserverinterceptor interface error new testrequest err dec err nil return nil err interceptor nil return srv testserver unaryunary ctx info amp grpcunaryserverinfo server srv fullmethod testtestunaryunary handler func ctx contextcontext req interface interface error return srv testserver unaryunary ctx req testrequest return interceptor ctx info handler processing happens inline decoding protobuf running interceptor calling unaryunary handler let look courier stub copy func testunaryunarydbxhandler srv interface ctx contextcontext dec func interface error interceptor grpcunaryserverinterceptor interface error defer processorpanichandler impl srv dbxtestserverimpl metadata impltestunaryunarymetadata ctx metadatasetupcontext ctx clientid clientinfoclientid ctx stats metadatastatsmapgetorcreateperclientstats clientid statstotalcountinc req amp processorunaryunaryrequest srv srv ctx ctx dec dec interceptor interceptor rpcstats stats metadata metadata fullmethodpath testtestunaryunary req amp testtestrequest handler implunaryunaryinternalhandler clientid clientid enqueuetime timenow metadataworkpoolprocess req wait return reqresp reqerr lot code let go line line first defer panic handler responsible automatic error collection allows u send uncaught exception centralized storage later aggregation reporting copy defer processorpanichandler one reason setting custom panic handler ensure abort application panic default golangnet http handler behavior ignore continue serving new request potentially corrupted inconsistent state propagate context overriding value metadata incoming request copy ctx metadatasetupcontext ctx clientid clientinfoclientid ctx also create cache efficiency purpose perclient stats server side granular attribution copy stats metadatastatsmapgetorcreateperclientstats clientid dynamically creates perclient ie pertls identity stats runtime also permethod stats service since stub generator access method code generation time statically precreate avoid runtime overhead create request structure pas work pool wait completion copy req amp processorunaryunaryrequest srv srv ctx ctx dec dec interceptor interceptor rpcstats stats metadata metadata metadataworkpoolprocess req wait note almost work done point protobuf decoding interceptor execution etc acl enforcement prioritization ratelimiting happens inside workpool done note golang grpc library support tap interface allows early request interception provides infrastructure building efficient ratelimiters minimal overhead appspecific error code stub generator also allows developer define appspecific error code custom option copy enum errorcode option rpccorerpcerror true unknown notfound rpccoregrpccode notfound alreadyexists rpccoregrpccode alreadyexists staleread rpccoregrpccode unavailable shuttingdown rpccoregrpccode cancelled within service grpc app error propagated api boundary error replaced unknown avoids problem accidental error proxying different service potentially changing semantic meaning pythonspecific change python stub add explicit context parameter courier handler eg copy dropboxcontext import context import testrequest testresponse typingextensions import protocol class testcourierclient protocol def unaryunary self ctx type context request type testrequest type gt testresponse first looked bit strange time developer got used explicit ctx got used self note stub also fully mypytyped pay full largescale refactoring also integrates nicely ides like pycharm continuing static typing trend also add mypy annotation protos copy class testmessage message field int def init self field optional int gt none staticmethod def fromstring byte gt testmessage annotation prevent many common bug assigning none string field python code opensourced dropboxmypyprotobuf writing new rpc stack mean easy task term operational complexity still compared process infrawide migration assure success project tried make easier developer migrate legacy rpc courier since migration errorprone process decided go multistep process step freeze legacy rpc anything froze legacy rpc feature set longer moving target also gave people incentive move courier since new feature like tracing streaming available service using courier step common interface legacy rpc courier started defining common interface legacy rpc courier code generation responsible producing version stub satisfy interface copy type testserver interface unaryunary ctx contextcontext req testtestrequest testtestresponse error step migration new interface started switching service new interface continued using legacy rpc often huge diff touching method service client since errorprone step wanted derisk much possible changing one variable time low profile service small number method spare error budget migration single step ignore warning step switch client use courier rpc part courier migration also started running legacy courier server binary different port changing rpc implementation oneline diff client copy class myclient object def init self selfclient legacyrpcclient myservice selfclient courierrpcclient myservice note using model migrate one client time starting one lower slas like batch processing async job step clean service client migrated time prove legacy rpc used anymore done statically code inspection runtime looking legacy server stats step done developer proceed clean remove old code end day courier brings table unified rpc framework speed service development simplifies operation improves dropbox reliability main lesson learned courier development deployment observability feature metric breakdown outofthebox invaluable troubleshooting standardization uniformity important lower cognitive load simplify operation code maintenance try minimize amount boilerplate code developer need write codegen friend make migration easy possible migration likely take way time development also migration finished cleanup performed rpc framework place add infrastructurewide reliability improvement eg mandatory deadline overload protection etc common reliability issue identified aggregating incident report quarterly basis courier well grpc moving target let wrap runtime team reliability team roadmaps relatively near future wanted add proper resolver api python grpc code switch c binding pythonrust add full circuit breaking fault injection support later next year planning looking alt moving tl handshake separate process possibly even outside service container like runtimerelated stuff dropbox globally distributed edge network terabit traffic million request per second comfy small team mountain view san francisco trafficruntimereliability team hiring swes sres work tcpip packet processor load balancer httpgrpc proxy internal service mesh runtime couriergrpc service discovery afs thing also hiring wide variety engineering position san francisco new york seattle tel aviv office around world acknowledgment contributor ashwin amit berk guder dave zbarsky giang nguyen mehrdad afshari patrick lee ross delinger ruslan nigmatullin rus allbery santosh ananthakrishnan also grateful grpc team support
165,Lobsters,scaling,Scaling and architecture,Using LZ4 to speed up your cache,https://eng-doordash.com/2019/01/02/speeding-up-redis-with-compression/,using speed cache,speeding redis compression problem large value compression rescue leveldb using snappy similar technique snappy zlib zstandard brotli plethora benchmark already lzbench connecting dot conclusion,speeding redis compression one challenge face almost everyday keep api latency low problem sound simple surface get interesting sometimes one endpoint serf restaurant menu consumer high latency number since high traffic endpoint naturally use caching pretty intensively cache serialized menu redis avoid repeated call data base spread read traffic load end post present used compression improve latency also get space cache problem large value deep instrumentation inspection determined problem particular scenario menu almost half mb long instrumentation showed u reading large value repeatedly peak hour one reason high latency peak hour read redis took sometimes random took especially true restaurant chain really large menu running promotion happens surprise one reading writing many large payload network peak hour end causing network congestion delay compression rescue fix issue obviously wanted reduce amount traffic server node cache well aware technique like leveldb using snappy compress decrease ondisk size similarly friend cloudflare also used similar technique squeeze speed kafka wanted something similar ie use compression algorithm good speed decent compression ratio like folk benchmark found snappy two nice option also considered famous option like zlib zstandard brotli found decompression speed cpu load ideal scenario due specific nature endpoint found snappy favorable library goldilocks zone compressiondecompression speed cpu usage compression ratio plethora benchmark internet already comparing compression speed ratio without going detail repeating benchmark example summary finding byte chickfila menu serialized json compressed byte byte snappy byte cheesecake factory serialized json menu byte byte snappy overall observation average slightly higher compression ratio snappy ie compressing serialized payload average v snappy compression ratio compression speed snappy almost fractionally slower snappy hand faster snappy decompression case found faster snappy case curious comparing different compression technique use lzbench benchmark clearly showed favorable snappy due higher compression ratio almost compression time important fast decompression speed emerged favorite option connecting dot see thing action deploying production setup sandbox chose random menu sample contained good mix menu size ranging serialized getting setting entry redis without compression snappy yielded following number redis operation compression second snappy second second set get number confirmed hypothesis potential gain readwrite operation using compression result made appropriate change code slowly onboarded different store conclusion deployment production instrumentation confirmed drop latency also noticed reduced redis memory usage redis memory usage compression v without compression latency spike uncompressed value compared time value compressed latency spike uncompressed value compared time value compressed choice observed effect look obvious understanding diagnosing problem hard beginning reducing potential congestion peak hour allowing cache data redis doordash really important u make consumer experience positive possible look every possible opportunity improve optimize system particular scenario compression helped u improve system dealing large payload redis
166,Lobsters,scaling,Scaling and architecture,How Facebook Keeps Messenger From Crashing on New Year's Eve,https://spectrum.ieee.org/tech-talk/computing/software/how-facebooks-software-engineers-prepare-messenger-for-new-years-eve,facebook keep messenger crashing new year eve,,
167,Lobsters,scaling,Scaling and architecture,Runes Reforged: A Technical Retro,https://engineering.riotgames.com/news/runes-reforged-technical-retro,rune reforged technical retro,rune reforged goal changing design system responsible getting rune choice pregame ingame rune preforged game data server rune reforged game customization game customization microservice microservices platform make rune free rune retrospected hardwork montage sanitized improved aging tech tech debt dramatic system change reduced footprint platform monolith introduced new way bring feature game consistent standardized system emotes augment system odyssey event worth stuck client everyone delivered free rune free playing service telephone often found unfamiliar territory spiciest preseason,hey folk going take trip back time year decided finally time send one league legend long standing revered reviled feature depending ask meme graveyard right time retire rune mastery dave le member league legend gameplay system engineering team team tasked integrating many technical system required make rune mastery replacement rune reforged reality previously talked goal changing design pregame experience become streamlined impactful dust settled entire runefilled season like take look changed system responsible getting rune choice pregame ingame brace reforging coming rune preforged game league even begin every player already making pregame decision selecting champion picking summoner spell even banning potential option opponent rune mastery system designed give player additional interesting pregame decision make form pure stat bonus via rune situational bonus via mastery job past many year technical system powering aged poorly point become inflexible support anything new beyond simple quality life change rune mastery old predate game data server preferred system managing game content instead rune mastery content exists loose configuration file define name description item mastery involves logic beyond stat adjustment also accompanied script file executes conditional behavior game rune configuration file contains mod value every possible stat value modified rune system recall rune modified single stat yes going whole lot zero repeat zero file fun rune mastery designated unique id value stored sql database accessed platform large monolithic service house large number component including one responsible starting game platform contains business logic validate player choice rune mastery prevent illegal combination identifies rune mastery disabled deprecated case certain mastery season confirms access individual item ownership rune level restriction mastery unfortunately u business logic snaked across large number code file platform gained functionality bloat many season league rune mastery related logic attached file platform alone weighing hefty total codebase scary stuff finally player make pregame selection league client need way collect communicate game server ultimately host match game server instance allocated service running game server host called local server manager lsm lsm start new game server process receives gamestart message counterpart global server manager gsm gsm yet another component platform platform deems ready start game gsm component update pending game queue lsm continually watching queue spin new game server process new item appears hello middle management player make update selection league client client sends update platform gsm player ready go gsm sends gamestart message lsm includes blob data required start game blob known megapacket contains player said selection well information needed start game map eg summoner rift type game mode eg normal summoner rift v urf megapacket start json document ease reading ease modification ultimately serialized binary format transportation parsing downstream participant championid rune arrayofruneids summoner level mastery arrayofmasteryids spell summonerheal summonerhaste summonerid summonername teamid teamparticipantid platformid mapid decide alter rune mastery selection data beyond simple array id support new design paradigm would require changing megapacket data structure support also mean downstream system parse megapacket would also need updated every time something prefer avoid like flexibility modifying often enable new design space rune reforged ultimate goal retire rune mastery remiss consider approach future feature development rip floorboard house rune mastery built opportunity lay foundation enabling new feature customize game without start scratch time game customization getting rune selection game server reuse existing transport pipeline league client platform gsm lsm game server however still need alter megapacket payload way include selection existing megapacket structure contains section dedicated specifically rune mastery removing deprecated section introduce completely new section megapacket game customization section function similarly previous system array selected rune identifier rather tied specific content system add content identifier field game server use determine parse apply selected content participant championid gamecustomizationobjects category rune content rune primarypath secondarypath category summoneremotes content summoneremoteids summoner level spell summonerheal summonerhaste summonerid summonername teamid teamparticipantid platformid mapid small important change give u generic system featureagnostic define rune content section support rune reforged future define additional content type new feature without revisit megapacket generation system microservice decided structure game server receives rune reforged data via megapacket still question receives data decided previously adding code platform monolith may put u bad spot road luckily entire microservices platform disposal specifically designed running online service build microservice fill role dealing microservicebased implementation consider new detail first need modify orchestrated communication path platform monolith still league client first point contact microservice also ultimately responsible getting megapacket gsm rune reforged microservice involved updating content megapacket receiving request league client forwarding data platform rune talk keeping microservice functionally simple possible definitely favor mean much lower development cost easytounderstand implementation maintaining considerably easier process deploy horizontally scale last point especially worth noting league game supporting million player new system must flexibility scale end make microservice job simple validate rune choice player forward validated choice platform monolith constructing megapacket explicitly avoid microservice maintain state storing pending selection champ select try depend service aside platform monolith stateless service get u free win scaling million player easy spinning instance coordination involved making sure talking right instance microservice mean instance handle request recovery critical failure handled restart without needing save restore state going dependencyfree microservices leave u interesting problem validate whether player owns rune chosen involving player inventory certainly odds microservice technical design goal however revisiting product design goal present u bold opportunity make rune free rune available player get u win across board player get full suite rune day without grind ip purchase game designer free balance power rune stronger assumption player equal access implementation side microservice longer need access separate inventory datastore le work u le complicated service coordination le point failure whole chain easy peasy pretty scary picture finally endtoend picture player selecting rune game server receiving selection make rune pop ingame get deeper deeper implementation begin touching system requirement knowing understanding system involved becomes real challenge case unique opportunity turn challenge direct player value making new rune available every player side benefit making work implementing little easier rune retrospected congrats easy task bit effort brief movie hardwork montage finally reached present rune reforged original rune mastery system aside giving player new pregame toy play plenty victory celebrate sanitized improved aging tech amount tech debt code drove rune mastery huge deterrent altering system leaving u stuck cramped game design space whereas previously limited change shuffling around mastery introducing keystone flexibility dramatic system change much le effort reduced footprint platform monolith platform monolith still critical part getting player game every step towards reduction brings u closer goal reaching cleaner reasonably sized codebase introduced new way bring feature game consistent standardized system straightforward way develope new feature mean player hand sooner rather later proven path customization incredibly beneficial team explores different way bring fun new twist current experience emotes first test system technically launched rune reforged success paved way even highly impactful customizations augment system odyssey event worth say rune reforged freelo every project tradeoff certainly exception stuck league client decided pas air client meant binding league client release date already becoming challenging get door also make sure everyone migrated air client dead ground fun fact hoped get rune reforged preseason league client ship schedule give u much hope landing tiny preseason runway instead used opportunity additional season development make sure rune reforged delivered way wanted delivered free rune free deciding make rune free player fistbumps around design certainly gave engineering pause lot discussion several team design folk engineering folk finance folk free rune looked like u could deliver best player value possible end spin entire subproject dedicated rewarding player invested old rune economy meant collating purchase history million player worldwide constructing system would deliver reward time preseason start footing bill large hole old rune used fill bill pretty substantial cost company eat easily four dollar sign yelp ultimately decided right decision right player experience playing service telephone introducing new microservice good addressing separation concern decreasing code complexity resulted increased devops complexity deploying game every patch added additional microservice dependency account core game loop new potential point failure microservice subject matter expert addition platform subject matter expert necessary involve issue game start fewer certainly make juggling oncall rotation interesting often found unfamiliar territory core rune reforged engineering team small handful lol engineering folk one rioter put changing code business changing typical preseason update dipping code unfamiliar space owned completely different group riot tech stack big place solid case teaming many team discipline subject matter expert added overhead however teamwork certainly better solo queuing despite challenge cost happier tech state got much space game grow managed give player spiciest preseason league history worth
168,Lobsters,scaling,Scaling and architecture,Scaling ChristmasAn Illustrated Adventure,https://medium.com/square-corner-blog/scaling-christmas-an-illustrated-adventure-a2ea739f5451,scaling christmas illustrated adventure,wrapping,world fast enough get around entire world indigo friend escher cat escher ability bend space time indigo managed rig rocket sleigh occasion ahahahahahahahaha screamed indigo flying sky escher cat make christmas happen yet dear friend second star right straight ah right wrong fairytale duo went warping time make sure delivery made right timeas aside indigo would like consider top hat swears santa wearing one next year ragewrapping upas always hope enjoyed magical little christmas taleit noted santa helper lemur paid fair equitable wage work sustainable hour holiday season lemur rocket sleigh harmed injured story particularly motion sick cat keen againto one happy holiday whatever holiday may happy new year
169,Lobsters,scaling,Scaling and architecture,Dependency Abstraction,http://beza1e1.tuxen.de/dependency_abstraction.html,dependency abstraction,dependency abstraction dependency inversion modelviewcontroller generalizing design adapter generalizing beyond design,dependency abstraction principle dependency inversion useful design pattern flexible dependency helpful layered architecture highlevel package depend lowlevel package way round observer pattern used modelviewcontroller wellknown example visualize like imgdependencyinversionsvg left see class x us class directly package depends package b problem lower level depend b invert dependency package define interface iy b implement assuming java semantics package b depends package inverted relationship informally arrow package going good note package b implement package defines interface iy interface usually defined used implemented generalizing design package layer assume package c us b within c two component b shall interact also want b self contained neither depend c example could e f g like c use different version b direct dependency b lead problem scenario cheap solution introduce common base package imgbasecopoutsvg risk might scale well e f g need use version base probably update package without package b requires update base well think apply trick dependency inversion defines interface b vice versa c create adapter class interaction thus dependency b imgsamelayerdependencysvg e f g also create adapter depending version b use everything modifies adapter concede feel overengineered especially interface corresponding class nearly identical adapter class consists trivial method redirect member mean consider cost apply pattern generalization dependency inversion would name dependency abstraction since abstract dependency interface special case something external layer dependency inversion something external layer probably necessary well stub lower level testing might still useful something external layer define interface use adapter higher level generalizing beyond design apply dependency abstraction pattern also higher level software development highest level would requirement understand requirement engineering dependency abstraction design interface write code implement course requirement usually use natural language checking interface match manual errorprone activity principle requirement design software architecture even documented informally level use principle well example b could microservices using specify others interface c would orchestration actual adaptation happens b could plugins wordpress photoshop eclipse firefox outlook whatever usually plugins interact situation becomes suddenly lot complex b could microcontrollers car autosar component communicate well experience base solution usually used oem like volkswagen control coordinate communication component talking safetycritical realtime system communication bus bounded resource still dependency abstraction might reduce communication overhead company b could library used application c tricky thing architectural level clear definition interface like java design pattern generalizes dependency inversion also applied architectural level
170,Lobsters,scaling,Scaling and architecture,AWS re:Invent Breakout Sessions,https://reinventvideos.com/,aws invent breakout session,sorry aws hub nt work properly without javascript enabled please enable continue,sorry aws hub nt work properly without javascript enabled please enable continue
171,Lobsters,scaling,Scaling and architecture,Why Systolic Architectures? (1982),http://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf,systolic architecture,,obj endobj obj filterflatedecodeid index info rlength rsize stream b endstream endobj startxref eof obj stream  endstream endobj obj endobj obj endobj obj stream l endstream endobj obj stream g endstream endobj obj stream  e b
172,Lobsters,scaling,Scaling and architecture,Reasons to Scale Horizontally,https://blog.wallaroolabs.com/2018/11/horizontal-scaling-reasons/,reason scale horizontally,wallaroo scale application horizontally horizontal scaling handle throughput fault tolerance static website sharednothing need resource get single node wallaroo wrapping paper love san francisco talk pat helland beyond distributed transaction sign receive notification new post,wallaroo lab build wallaroo distributed stream processor designed make easy scale realtime python data processing application real mouthful mean critical part scale easy scale mean easily scale data processing application case wallaroo application mean easy scale application horizontally post going cover horizontal scaling different vertical scaling reason would horizontally scale application time finish post decent understanding horizontal scaling consider horizontal scaling computer science discussing scaling referring process add something system allow handle something else example might add memory laptop able run program one time form scaling computer key idea task want accomplish lack resource preventing accomplishing task laptop example resource lacking memory say constrained lack memory distinguish scaling two broad category vertical scaling horizontal scaling vertical scaling mean add resource single computer example add disk space memory cpu form vertical scaling eventually vertically scaling going hit limit limit amount memory cpu computer support varies computer computer limit eventually need scale need scale adding computer process adding computer horizontal scaling scaling vertically also known scaling whereas horizontal scaling known scaling vertical scaling adding resource single node system horizontal scaling process adding node system many reason might want horizontally scale system covering handle throughput fault tolerance need resource get single node handle throughput sometimes need able handle throughput get single node typical scenario run multiple web server handle traffic popular website get small website get single web server site get popular eventually single web server able handle incoming request web server need added handle load use case horizontally scaling throughput easy use case challenging increase throughput scaling horizontally difference scenario difficult easy primary difference one coordination add new node need know anything node relatively straightforward add new node allow handle throughput take web server example website great candidate scaling horizontally web server copy content site common many website add new node new node new node handle traffic often called shared nothing architecture new web server selfcontained talk share shared resource add coordination communication node depend shared resource scaling horizontally handle throughput start become difficult fault tolerance relatively common see people want add node system provide fault tolerance adding node better cope failure often called highavailability ha let say website served single web server web server go website longer available bummer one approach could take add web server one go still others available adding node work long new node operate independently others website simple static website probably add node get higher fault tolerance however achieve need webservers stateless allows individual node handle request system stateless sharednothing system easiest add fault tolerance even system stateless sharednothing system still possible add fault tolerance scaling horizontally distributed database provide fault tolerance running several node replicating data across node number replica also known replication factor allows u survive loss member system usually referred cluster higher replication factor node lose continue operate data need exists node even lose node still access somewhere cluster need resource get single node pursuing vertical scaling strategy eventually run limit able add memory add disk add something day come need find way scale application horizontally address problem imagine batch data processing system every night run churn bunch file create business output time input file keep getting larger larger increase input volume result data need hold memory generate important business output physical limit amount memory add machine processing upgrade machine hold still memory eventually going able keep getting beefier single node reach physical limit amount memory add given node going need add node keep adding memory let say machine maximum gig memory hold hit limit need memory need start adding additional node instead stuck machine gig memory grow machine gig memory scaling handle lack physical resource sound like relatively simple task however system grows big enough actually require kind transition may find painful engineering challenge talking scaling shared resource application run single node mean taking shared resource like memory stateful working across one machine depending particular problem might framework help like wallaroo important takeaway think decent chance eventually need scale horizontally need disk memory cpu plan ahead time know suddenly find needing couple day best case scenario measured week wrapping horizontal scaling deep complicated subject hope feeling little educated started post like explore recommend paper love san francisco talk pat helland beyond distributed transaction talk cover scale stateful application horizontally implemented wallaroo additionally diving different horizontal scaling related topic blog covering depth next scenario hard achieve throughput horizontally scaling sign receive notification new post know publish post series
173,Lobsters,scaling,Scaling and architecture,Cloud infrastructure for the Internet of Things: Kubernetes on solar plants,https://learnk8s.io/blog/kubernetes-on-solar-plants,cloud infrastructure internet thing kubernetes solar plant,cloud infrastructure internet thing kubernetes solar plant addition business government individual increasingly finding installing solar power array viable lower scale software eating world imagine controlling solar panel designing internet thing scale something small efficient communication encrypted everywhere never playing smart scaling cluster cloud infrastructure enter kubernetes kubernetes container orchestrator time however locked collection kubernetes security best practice container essentially archive similar zip file container also designed efficient resilience failover kubernetes want strict set rule deployment advanced deployment kubernetes deployment statefulset replicaset daemonset running kubernetes car toyota run kubernetes car beginning might sound odd could run kubernetes car folk aled james joe heck folk,cloud infrastructure internet thing kubernetes solar plantspublished december renewable energy growing steadily last decade solar power outstripping growth renewable form power generationsolar panel getting cheaper becoming economically viable source renewable energy many part worldin addition business government individual increasingly finding installing solar power array viable lower scalecapturing optimal amount energy solar panel however tricky businessin order solar panel operate efficiently need kept clean pointed optimal angle sun balance power generation prevents overheatingso keep everything control get notified something nt right requires attention software eating worldone solution capitalise small embedded device thatmeasure performance efficiency cellsmonitor environmentdrive actuator track sun daythe embedded computer collect data sends central location aggregated processed storedif solar panel dropping efficiency operator alerted take actionin larger plant solar array data passed wired network uncommon see embedded computer connecting wirelesslyimagine controlling solar panel beauty deal extra cable horror long response dropped connection timeoutsin setup like deploying managing application becomes real challengedesigning internet thing scaleif managing hundred thousand device practical attend every device person order install software firmware updatesyou design system updated remotelyideally design mechanism package software almost zero overhead without sacrificing portabilitysomething small efficientsomething last year come transmit data securely prevent malicious actor damaging infrastructurecommunication encrypted everywherebut designing secure cluster small feat particularly solar installation span wide area making hard protect perimeter trespasser could gain physical access device extract secretshow protect even security sorted rolled strategy take care software firmware update still long way goyou still create service aggregate process data design dashboard visualisation set alert monitoring control plane drive coordinated changeswhat initially seems like fun weekend project becomes major effort distributed system engineeringcompanies exist specialise designing installing software solar plant surrender buy prepackaged software neverso compete established business year experience playing smartscaling cluster cloud infrastructurebuilding internet thing scale scale solar plant share plenty commonality building cloud infrastructureelastic container service ec product amazon web service deploy application across several serversit designed install agent worker computer communicates master node charge scheduling workloadsyou tell ec deploy software master node instructs one agent download run itthat sound lot like want solar panelsyou want agent installed embedded computer want manage application central locationbut ec amazon ca nt take advantage hardwarewhat need open source version ecsenter kuberneteskubernetes container orchestratorkubernetes similar ec install agent called kubelet device communicates kubernetes master forming clusterfrom moment onwards device acting one schedule deployment manage applicationsthis time however locked inkubernetes major open source effort free download customise contribute tois secure communication kubelet master node secured using tlseach node provisioned certificate even one node compromised reject single certificate keeping rest cluster availableand even better community wealth shared resource regarding good practice secure cluster gleaned thousand realworld kubernetes deploymentsyou check collection kubernetes security best practice see meannow way rollout nt know deploy application written c java nodejsin fact nt know deploy application allthat useful kubernetes able deploy linux container also called container orchestratorcontainers essentially archive similar zip filesto run container unpack archive run application process hostyou probably nt want process interfere one another linux container nice feature process isolated othersso instead developing mechanism distribute application ask kubernetes schedule deploymentwait agent node kubelet pick tasklet kubelet download archive run isolated processcontainers also designed efficientwhen update package wish redistribute ship difference previous current containerwhen delta received new package recomputed diff unzipped run separate processlinux container kubernetes excellent platform run application internet thingsin fact installing kubernetes solar plant let benefit centralised scheduler issue deploymentssecure encrypted update delivered containersa proven technology able scale thousand devicesso happens solar panel break resilience failover kuberneteskubernetes continually watch infrastructure failing process agentswhen device fails kubernetes reschedule application deployed computer anotherif one application fails perhaps memory leak kubernetes restart app predetermined number timeskubernetes designed understanding node expected continue working forever design selfhealing always observing current state infrastructure take action whenever detects nt match desired state infrastructurewhere find discrepancy example nt enough capacity run apps ask cloud provider provision compute resourceskubernetes excellent deploying container way maximises efficiency infrastructurewhen deploy three instance application scheduled maximise efficiencythere guarantee three instance application end five different devicesthey could deployed node could deployed across two node depending physical resource availableparticularly embedded world resource scarce nt want deployment placed anywhereyou want strict set rule deploymentsas example solar panel one app running given timein case application responsible tilting solar panel track trajectory sun nt want two application deployed node trying drive motoradvanced deployment kuberneteskubernetes use several strategy allocate container nodesthe straightforward strategy deploymentin deployment specify number instance application kubernetes find space allocate themthis common deployment type useful cloud deployment nt care specific node running application care run something le useful embedded worldother strategy include statefulset replicaseteach come different tradeoff nt serve goal application run every available nodefor need daemonset strategy deploys one application per nodeif add new iotconnected solar panel cluster kubernetes automatically schedule deploy application embedded device node tooso far goodyou packaged application linux containersa centralised way distribute software securely incrementallystrategies deploy application across device prioritise hardware access across possible maximise efficiency reliable platform selfheals failuresconsidering started nothing pretty good placenow know kubernetes capable scale internet thing next running kubernetes carsin june redmonk wrote article suggesting toyota run kubernetes carsyou imagine small component car dashboard radio function side light computer kubernetes agent installedthe component connected communicate using internal networkthe kubernetes master charge making sure service always running well scheduling deploymentsneed replace dashboard replace component kubernetes schedule software run embedded computeradding gps problemjoin device cluster start streaming datain beginning might sound oddbut think make senseyou connect component orchestrate themyou ca nt front light disconnected dashboardbut turned nt righttoyota nt run kubernetes car used kubernetes part backend servicesonce connect car one diagnostic tool data extracted vehicle ingested kubernetes cluster designed run clouda day article publication redmonk amended article clarify thiseven story untrue result misunderstanding still make thinkwhat could run kubernetes car stopping using kubernetes solar plant internet thing device could solve hard challenge securing communication delivering incremental update centrally controlling fleetyou could best time market internet thingshundreds day development saved reuse proven toolwhile kubernetes initially designed run data centre application go well beyond cloud wo nt take long news come kubernetes used another internet thing setupand ca nt wait hear going build next itthat folk thanks aled james university bristol joe heck feedback folk enjoyed article might find following article interesting nt miss next article first notified new article kubernetes experiment published never share email address optout time
174,Lobsters,scaling,Scaling and architecture,How to Grow Neat Software Architecture out of Jupyter Notebooks,https://github.com/guillaume-chevalier/How-to-Grow-Neat-Software-Architecture-out-of-Jupyter-Notebooks,grow neat software architecture jupyter notebook,growing software notebook right way let start testdriven development tdd extract code external python file import machine learning project mine cleaner architecture cleaner code clean coder architecture domain driven design clean architecture document clustering machine learning consulting company recent school project mine pipe filter pipeline numpy oop visualization unit test research practical research research development note clean code highly likely conclusion license ccby extra link connect liked article help leave star fork share love related article,growing software notebook right way ever situation got jupyter notebook ipython notebook huge feeling stuck code even worse ever found duplicating notebook change ending lot badly named notebook well using notebook long enough code notebook first let see need careful notebook let see tdd inside notebook cell grow neat software architecture notebook also discus acceptance test unit test visualization test performance fitting test tradeoff want keep software clean research development research let start okay like notebook well feel easier say perhaps nice see output live writing code debugging time one way say go like allows faster feedback testcoderefactor loop testdriven development tdd wait giving tdd write notebook moment feedback would like testwriting real time really debugging application time writing thinking nice way program easy forget write test easy shoot foot writing notebook huge okay keep using notebook one solution grow app notebook yes first code basic thing need extract code external python file import easiest example machine learning project mine possible observe extracted data loading function external file notebook remains exercise tutorial serious project logic would ben extracted notebook test well test three part first prepare execute finally assert everything expected look code working notebook code look like little intro little interlude finale unit test well intro look time like test setup maybe loading data external data loading function already extracted little interlude logic currently implementing make class least something mere function finale mostly look like moment assert test ready create test three part notebook okay big test much like acceptance test often called functional test industry normally first test write well got written yet extracted test file reflex might extract execution interlude notebook function move better thing also extract test instead scrapping coded notebook let face time setup would either change much moment notebook would nt look would thrown away test need taken care want application stable extract test late okay understand big picture write viable software notebook already lot tdd realize scenario shown still flawed write test one reason tdd good make u think design upfront result cleaner architecture cleaner code speaking clean coder yet let rewind bit easier explain got data nt code yet core logic middle notebook would middle test code assertion first force think design front assertion need call code nt exist yet start creating code perspective person using tdd eureka architecture building app extracting function external file point application going grow ground nice moment setup layered architecture example could adhere principle domain driven design clean architecture example grew service domain layer notebook working recently document clustering project client machine learning consulting company side note also grew machinelearningoriented layer recent school project mine pipeline layer pipeline object layer nice pipe filter machine learning data pipeline pattern note test made time given school project probably useful reference one prolific pattern found machine learning project pipe filter pattern implemented scikitlearn pipeline object easy add new pipeline element class inherit base pipeline element class easy transform fit data multiple consecutive step pattern maybe building library like numpy need le architecture clear oop structure either way every medium big application need get notebook point easier grow code ground bottomup like topdown write code topdown would hard impossible within notebook visualization extract thing notebook might need notebook visualization purpose better extracted everything function class well notebook consists line visualization refactored extracting test many visualization might sign would needed notebook instead one big sake stable code ensuring visualization still work without error could probably extract visualization visualization test able run test unit test runner mute visual output ran within test suite example visualization notebook made monitor validate performance deep learning algorithm unit test still one thing nt talk unit test probably caught quick development cycle forget unit test instead code block run cell improve something tdd cycle way proper way tdd nt think design upfront well hardest kind test deal working notebook good way work would write acceptance test first unit test first unit test start cycling codingrefactoringtesting unit testing loop happen start writing test notebook test stack end cell chain somehow start burdensome extract everything notebook research practical research research development note clean code working notebook highly likely research development instead plain old coding got tradeoff research development keep might good idea skip unit test rewrite thing confident put discovery together test know nt ideal research development side research may skip test along trainvalidationtest set test performance fitting test wellknown research code hard reuse often dirty alternative would suggest keep thing gracious dealing unknown nt hesitate write open issue give suggestion conclusion hope satisfied reading sum learned grow software architecture notebook properly tdd loop important write test instead want increase chance good software architecture point notebook used visualization purpose everything extracted function already software finished remaining use notebook continue adding new functionnalities visualize thing write tutorial code work notebook however unit test remains hard would license ccby copyright c guillaume chevalier extra link connect liked article help leave star fork share love article seen related article
175,Lobsters,scaling,Scaling and architecture,An Uncoordinated Approach To Scaling (Almost) Infinitely,https://www.youtube.com/watch?v=7QEfphhZn3g,uncoordinated approach scaling almost infinitely,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature uncoordinated approach scaling almost infinitely andrew turley youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature uncoordinated approach scaling almost infinitely andrew turley youtube
176,Lobsters,scaling,Scaling and architecture,Building a Scalable and Language Agnostic Data Processing Pipeline,https://turbolent.com/data-processing-pipeline.html,building scalable language agnostic data processing pipeline,building scalable language agnostic data processing pipeline airflow airbnb luigi spotify pinball pinterest celery sidekiq splitting system component worker message example implementation scheduling system message queueing system kubernetes rabbitmq amqp containerized supervised worker amqpconsume amqppublish named pipe fifo ipc ipc using named pipe man page introduction interprocess communication using named pipe scheduling worker using kubernetes horizontal pod autoscaler exchange queue consumer using rabbitmq option used declaring shared work queue helpful tutorial work queue unique worker queue helpful tutorial routing multiple binding option tutorial publishsubscribe pattern queue need bound exchange channel prefetch setting qos consumer acknowledgment publisher confirms consumer prefetch option publish option queue worker using rabbitmq kubernetes recommendation amqp cony,building scalable language agnostic data processing pipeline minute last year worked project job involved building data processing pipeline integrated task like extraction information document using natural language processing nlp content recommendation using machine learning ml many task including example often heterogeneous need interoperable example domain artificial intelligence ai mlnlpdata science library written python java r commonly used furthermore task often reactive ie one computation triggered another larger task split smaller subtasks task frequently also performed parallel socalled workflow manager jobtask queuing scheduling system popular component data processing pipeline inconveniently system tailored towards one small set programming language also often assume task processed program short startup time ephemeral however program data processing pipeline may need longlived since long startup time instance might occur large ml model need loaded memory system also make assumption flow directed acyclic graph dag task batch nature continuously running initially looked popular workflow manager airflow airbnb luigi spotify pinball pinterest well popular jobtask queuing system like celery sidekiq unfortunately none existing system fit requirement mentioned since workflow manager heavily tied python support batch job jobtask queuing system also tied single language python ruby making difficult integrate code written language complex configure operate implement queuing system program scheduling feature simple yet powerful flexible solution implemented using offtheshelf component splitting system component let look architect system fit requirement breaking smaller component basically data processing pipeline defined two concept worker program accept incoming message perform single computation optionally generate output message message information flowing worker allowing communicate flowtask detail therefore processing pipeline constructed linking worker message though data streaming solution worker move data one message passed contain metadata specifically let define message consist event name payload pipeline coupled event name alone worker defines event listens event emits payload could formalized also loosely defined two type event allow construction almost pipeline event delivered one worker certain type allows parallelization timeconsuming task eg processing document generation recommendation distributing among multiple worker event delivered every worker certain type allows informing worker environment change eg updating state invalidation cache example task extract entity document would performed worker called entityextractor document processed parallel multiple worker type would look like example worker entity extractor entity extraction worker would listen event documentadded indicating document added data source emit event entitiesextracted finished processing document payload would id document one worker type would receive message every document processed another worker could listen emitted event perform operation document entity extraction worker would also listen event entityadded indicates new entity added data source worker like database storing information entity message event would delivered every worker type every worker need update state payload would id entity implementation build system based worker message requires two system scheduling system responsible scheduling worker message queueing system also known message broker responsible routing delivery message worker instead reinventing wheel building system scratch use established shelf software kubernetes platform automatic deployment scaling scheduling management containerized application rabbitmq message broker implementing amqp protocol support message queueing reliable delivery flexible routing also includes many useful extension delayed message delivery kubernetes rabbitmq production ready software reliable scalable run highly available configuration great support operation monitoring metric logging great tooling cli ui containerized supervised worker kubernetes allows u run program worker long containerized advantage worker written programming language avoid reimplement message queuing code every language want support decouple worker code message queuing environment extract logic implement single binary call process supervisor instead worker process communicating directly rabbitmq supervisor performs configuration rabbitmq exchange queue responsible receiving sending message rabbitmq contrast writing library every supported language would mean one would reimplement detail message consumption production potential creating inconsistency introducing bug chose implement supervisor go lends well generating small static binary since wellwritten robust rabbitmq library exist single binary idea inspired similar combination amqpconsume amqppublish longrunning process worker process communicates supervisor via named pipe fifo extension pipe used ipc simple linebased protocol using named pipe filebased ipc benefit supporting many programming language possible since receiving sending message simple reading fromwriting file even supported shell script addition worker process supervisor tested independently queuing system using simple filebased protocol might wonder named pipe used instead using standard input standard output pipe already provided every process want interfere since likely already used existing code base migration might expensive instance standard output frequently already used logging output message flow containerized worker using supervisor see supervisor listens message rabbitmq delivers named pipe supervised worker process message processed worker process report status back supervisor another named pipe output may also send message eg trigger worker ipc using named pipe let look worker process supervisor process interact depth container start supervisor started following manner supervisor creates two named pipe using syscallmkfifo supervisor start worker process wait exit configuration supervisor provided environment variable supervisorworkerprogram worker program supervisor start supervisorworkerarguments commaseparated list argument supervisor pas program launch given supervisorworkerprogram supervisorworkername name worker configuration worker process also provided supervisor environment variable ie path named pipe eg inputpath outputpath process running worker process may initialize receive message opened file opening file need performed specific order synchronize avoid deadlock opening pipe readonly writeonly blocking see part ononblock man page open worker process open input file reading mode osordonly block process supervisor opened yet supervisor open input file writing mode osowronly block worker process opened reading worker process open output file writing block worker process supervisor opened yet supervisor open output file reading block worker process opened writing nb since file named pipe need opened mode osmodenamedpipe file opened supervisor start sending message worker writing input file one messageline time wait worker processed message writing new message accomplished starting two goroutines one read decodes line output pipe worker process eg using bufioscanner second consumes message rabbitmq writes input pipe worker finished processing message need write line output file indicating supervisor whether message successfully processed addition worker emit multiple message single completion line json could suitable encoding purpose worker process exit supervisor us exit code determine last message processed properly supervisor also need forward signal receives eg sighup sigint sigterm importantly receives sigint sigterm stop consuming message rabbitmq try stop worker process sending termination signal waiting configurable grace period allow worker clean terminate allowed time finally sent kill signal one edge case worker terminated abnormally pipe successfully opened supervisor need perform opening opening pipe supervisor deadlock equivalent open call child process side process supervisor perform opening follows open input pipe readonly open output pipe writeonly close input pipe close output pipe blog post introduction interprocess communication using named pipe helped understand detail edgecases using named pipe bidirectional ipc scheduling worker using kubernetes worker scheduled using kubernetes manages container need containerize worker creating image consists supervisor worker binary bundled image kubernetes deployment concept used describe desired state worker instance image environment variable count applied kubernetes take care creating managing container easily scaled either manually specifying number pod automatically based cpu usage container using horizontal pod autoscaler automatic scaling could also extended based rabbitmq queue size supervisor need worry worker process crash simply terminates kubernetes supervises whole container restart needed exchange queue consumer using rabbitmq message queueing proceeds follows first supervisor declares single exchange message published following option used declaring type direct incoming message routed queue based routing key simply event name autodelete false ensures exchange deleted last queue unbound durable true ensures exchange survives rabbitmq restartscrashes recall two type messagesevents one delivered one worker certain type one delivered every worker certain type support two type messagesevents two type queue shared work queue distribute message among multiple worker one type rabbitmq documentation includes helpful tutorial work queue unique worker queue message need delivery every worker rabbitmq documentation includes helpful tutorial routing multiple binding therefore supervisor declares one queue type using following option exclusive false exclusive queue used consumed purged deleted etc declaring connection however shared work queue usable multiple worker autodelete shared work queue false queue never get deleted unique worker queue true queue get deleted single consumer unsubscribes ie worker stop restarts durable true ensures queue survives rabbitmq restartscrashes rabbitmq provides tutorial publishsubscribe pattern first seems like suitable approach implementing unique worker queue fanout exchange declared queue get autogenerated name using special empty name however queue survive rabbitmq restart instead use single direct exchange create unique queue name using uuids exchange queue declared queue need bound exchange routingkeys simply event name workersupervisor listen event name given supervisor environment variable supervisorsharedevents commaseparated list event name listen one worker instance receive message supervisoruniqueevents commaseparated list unique event listen worker instance receive message finally supervisor start consuming two queue message shared worker queue fairly distributed worker consuming ie new message dispatched next nonbusy worker new message dispatched worker processed acknowledged previous one behavior achieved rabbitmq setting channel prefetch setting qos particular prefetchcount option need set least rabbitmq detailed explanation consumer acknowledgment publisher confirms well consumer prefetch also consuming queue exclusive option set accordingly shared work queue false worker type also need able consume queue unique worker queue true worker never able consume queue message consumed successfully processed worker process delivered message acknowledged final piece message message never get lost also need persist durability queue also make message durable deliverymode publish option set persistent queue worker using rabbitmq kubernetes covered implementation worker scheduling message queuing part let look piece fit work together larger example full example queue worker interacting two type worker b two worker type two worker type b worker type listen message event type shared work event reason exchange route shared work queue queue message fairly distributed among worker worker worker worker also emit message event type worker type b listen message shared work event exchange route shared work queue queue b fairly distributed among worker worker worker however worker type b also listen message unique worker event incoming message type routed exchange unique worker queue queue queue worker receive message easily scale configuration speed processing message event type adding worker type recommendation mentioned earlier implemented supervisor binary go two library made easy straightforward amqp lowlevel amqp library support many rabbitmq extension cony highlevel wrapper lowlevel library allows declarative definition exchange queue binding handle reconnects backoff redeclaring everything properly
177,Lobsters,scaling,Scaling and architecture,4 ways to scale on Heroku,https://masteringheroku.substack.com/p/4-ways-to-scale-on-heroku,way scale heroku,columbus ruby brigade request queue time automate random routing adamlogic,week ago gave presentation columbus ruby brigade approach mental model scaling heroku apps attempt record talk failed rerecorded screencast cover video scaling option dynos see increased request queue time scout new relic need make app faster add dynos soon using one dyno automate instead playing guessing gamescaling option dyno size heroku random routing need concurrency within single dyno mean running web process consume memory may require larger dyno type aim least three web progressesscaling option process type limited web worker process type procfile consider multiple worker process type pull different job queue scaled independently control flexibilityscaling option app instance heroku pipeline make relatively easy deploy single codebase multiple production apps helpful isolate main app traffic traffic api admin endpoint example heroku route traffic correct app based request subdomain custom domain configured appmy general advice start simpleconfigure multiple web process per dyno increasing dyno size neededif need one web dyno autoscale itif certain background worker resource hog may require larger dyno size split process type dedicated job queue scaled independentlyif dedicated section web app api admin section split subdomain divert traffic separate app instance keep single app instance additional complexity absolutely necessarydid find video helpful anything add change let know happy scaling adam adamlogic
178,Lobsters,scaling,Scaling and architecture,Embracing failures and cutting infrastructure costs: Spot instances in Kubernetes,https://learnk8s.io/blog/kubernetes-spot-instances,embracing failure cutting infrastructure cost spot instance kubernetes,embracing failure cutting infrastructure cost spot instance kubernetes vertical scalability horizontal scalability flexible pricing logistics cost availability reliability cut bill payasyougo flexibility come price get billed amount time instance running get bulk discount reserved instance commit typically range trading flexibility cash spot instance cheap better reliable spot instance lowpriority vm preemptible vm promise horizontal scalability everyone need keep lot unutilised hardware case someone suddenly need additional compute unit however leaf lot resource unused flexibility reliability wager stability infrastructure account discount bill would impact customer likely lose node moment embracing failure eventually go replication redundancy depend currently unpopular kubernetes abstracting data centre single computer fixed predictable single cluster master worker selfhealing infrastructure kubernetes spot instance kubernetes match made heaven shift preventing failure embracing failure test failover chaos engineering availability reliability achieved test actively reduced bill chaos engineering winwin spot instance hot tip smart instance type maximum bid price aws reliability willing trade low bid bill stay low likelihood losing node high bid reduce node failure monitor everything spot termination notice handler kubernetes help prepare alternative mean summary abstract size node move component node spot instance scarcely le reliable cheap memory cpu nt need reliable hardware good enough software running application kubernetes nt make horizontally scalable duty critical actively test availability chaos engineering slashing bill folk,embracing failure cutting infrastructure cost spot instance kubernetespublished november last decade seen global shift onpremise data centre provisioning virtual machine vms mainstream cloud provider amazon web service azure google cloud platformrunning managing physical machine hard costly chance never successful efficient top cloud provider love leverage mature platform feature vertical scalability get instance different sizeshorizontal scalability get almost many instance wantflexible pricing pay uselogistics cost nt physically maintain server heat control electricity backup storage cost fire prevention availability provision vm separate data centresreliability pay instance keep done go immediately get replacementin article explore different pricing model typical cloud provider focus one strategy see could cut bill willing trade reliabilityfinally see kubernetes make lack reliability irrelevant allows run cheap yet highly available clusterpayasyougo flexibility come pricethe typical pricing model cloud provider based payasyougo schemecompute resource come different size ie memory cpu disk etc hourly cost get billed amount time instance runningthis flexibility pricing excellent fair careful consume leave instance running nt need anymore throwing money windowhowever let say foresee utilisation vm whole year nt able get bulk discount bill get bulk discount reserved instanceswith reserved instance need commit compute resource least whole year really love commitment five year may decide pay amount bill upfrontthe discount get depend long willing commit much pay upfront example amazon web service instance type discounted follow pricing model hourinstanceyeartotal yearpayasyougo year reserved upfront see discount offered reserved instance typically range reserved instance basically trading flexibility cashthough might sound like reasonable saving might always worth itare able forecast compute resource utilisation next year building cuttingedge startup cloud accurately predict traffic like year sound like gamble perhaps commitment upfront payment way could save cloud billspot instance cheap better reliableamazon call spot instance azure lowpriority vm google preemptible vmwe call spot instance seems common terminologythough inner working differ little stem rationalea typical cloud provider buy load powerful server organised large data centre maximise utilisation hardware divide computer smaller virtual machinesbecause promise horizontal scalability everyone need keep lot unutilised hardware case someone suddenly need additional compute unit however leaf lot resource unusedthe idea behind spot instance allow user tap extra resource much lower cost caveat might lose instance momentif running spot instance cloud provider suddenly need resource accommodate demand ondemand reserved customer immediately lose instancewhereas flexibility used bargaining chip reserved instance reliability given away saving benefit much significant though typically expect shave bill follows big question wager stability infrastructure account discount bill would impact customer likely lose node moment embracing failureobservations system scale proven application eventually go harddrives network jvms etc fail give enough time requestsyour primary weapon failure replication redundancyif run several copy component might resilient certain number failuresthe amount failure recover depend much redundancy willing put placedo nt forget redundancy mean compute resource compute resource lead higher billanother point consider dynamic aspect spot instance based idle resource size instance available depend currently unpopularin word beggar ca nt choosersperhaps week pick cheap memory instance great amount memory application requireswhat next week instance become unavailable buy instance memory starting memory course could use instance paying twice price extra memory would wasted waste waste waste app app spot instance excellent deal downside might acceptablehow cope random node disappearing without notice infrastructure handle node everchanging size need tool continually monitor node automatically manages redundancythis tool scale spread component application infrastructure node lost created infrastructure rebalancedit seems one would nt able manage consequent cloud infrastructure without toolchances someone already built ityou luck google faced issue year ago since opensourced solution problem kubernetesabstracting data centre single computerin traditional infrastructure say early fixed number server predictable amount resourcescloud infrastructure especially spot instance completely changed game kubernetes developed oversee increasing complexity managing everchanging compute resourceskubernetes provides layer abstraction compute resource regardless many irrespective size interact single entity clusteryour cluster could formed small virtual machine big bare metal server end result single point interaction manages scale workload nodeswhen install kubernetes infrastructure select one computer master node rest fleet join cluster worker nodesas add remove node cluster kubernetes keep track available memory cpu nodewhen cluster ready send deployment request master nodeupon receiving request kubernetes survey worker node available memory cpu find best candidate run applicationas user nt worry application running clusterif node running application dy workload immediately moved nodesinterestingly kubernetes nt care size worker node long offer memory cpuwhen worker node memory cpu register cluster master node keep track total available spare capacitiesit continually monitor current workload node decide given node enough available resource run applicationthis one kubernetes true beautyyou forget many individual node joined cluster big see single unified entitybut interested know big cluster sum memory cpu node tell much total capacity cluster hasif one one instance cluster vcpus master node node node cpu memory noteworthy feature kubernetes node monitored uptimeif node lost kubernetes remove memory cpu cluster migrate application worker nodesselfhealing infrastructure kubernetesthe master node run series synchronisation loop follow simple principle user specify desired state eg want instance application master node regularly check current state compare desired state make required adjustmentfor example master node notice instance application running requested three immediately start another onemost kubernetes component designed way control loop constantly regulates current state around desired stateimagine node replica application one running node node running spot instance reclaimed cloud provider application node lostkubernetes realises replica running instead immediately start another copy one two remaining node space available course spot instance kubernetes match made heavenin last decade industry massively adopted microservices architecturesome would argue fad others soa rebrandedi would say important revolution came microservices architecture nt decided write smaller application shift preventing failure embracing failuresthe significant insight like netflix google amazon scale thing go wrongeven best expensive hardware probability failure strictly larger zeroso design around test failover chaos engineeringchaos engineering recommends actively create failure infrastructure ensure resilient indeedif need remember single thing availability reliability achieved test activelyif want make sure application highly available need kill node regularlyand better way kill node random time spot instance saving cloud bill leveraging spare resource also continuously testing infrastructure resiliencethe precious reliability obtained reserved instance nt much important anymorein fact nt actually care cloud provider reclaims node unexpectedly modern resilient architecture wo nt even noticereduced bill chaos engineering winwinspot instance hot tipsif wish try spot instance kubernetes cluster thing could help optimise infrastructurebe smart instance typepick unpopular instance typesfor instance instance amazon web service cheap instance family recently releasedthis made instance go fashion meaning lower demand better price maximum bid price aws azure low priority vm google preemptible vms fixed price spot instance price determined bidding processas user specify maximum price per hour willing payif aws enough spare vms serve everyone everyone get instance low pricehowever price get higher demand increaseschoosing maximum bid price allows decide much reliability willing trade ina low bid ensure bill stay low increase likelihood losing nodea high bid example equal ondemand price reduce node failure mean might pay ondemand price ie discountyou may choose align bid price reserved instance price year typically cheaper ondemand way sure never pay reserved instance without actually reserve itthat also depends doingif want noncritical work lowest price may low bidding price hoping get cheap resource nothing otherwisemonitor everythingappropriate monitoring tell system recovered properly node failureif application cope losing spot instance cluster need know pretty quicklyat beginning journey spot instance could set email notification cloud provider reclaims one nodesas system get robust get confident probably wo nt need anymoreon aws could try spot termination notice handler notified spot instance termination happens leaving time reschedule app onto node gracefullyon aws also monitor much pay bid price end close ondemand price probably find another instance typeremember kubernetes help thathaving instance memory almost identical instance smart prepare alternative meanshave alternative mean providing instancesfor example provisioning cluster amazon web service prepare additional node pool payasyougo model set desired quantity available spot instance available always let kubernetes switch node poolssummarykubernetes designed abstract size node seamlessly move component nodesthis make perfect candidate work spot instance cluster built top spot instance scarcely le reliable cluster built reserved virtual machineswhen shopping node kubernetes cluster reliability primary concernyou focus cheap memory cpu echo one fundamental principle google nt need reliable hardware good enough software remember running application kubernetes nt make horizontally scalable duty ensure multiple copy application run time gracefully shut without dropping connection importantly critical actively test availabilitychoosing spot instance force practice degree chaos engineering slashing bill bandwagon try folk enjoyed article might find following article interesting nt miss next article first notified new article kubernetes experiment published never share email address optout time
179,Lobsters,scaling,Scaling and architecture,The Treacherous Tangle of Redundant Data: Resilience for Wallaroo,https://blog.wallaroolabs.com/2018/11/the-treacherous-tangle-of-redundant-data-resilience-for-wallaroo/,treacherous tangle redundant data resilience wallaroo,introduction need data redundancy exactly wallaroo option partial failure distributed stream processing wallaroo checkpointing algorithm data lost forever example figure figure figure figure bos say sorry recovery data infinitely sized arrives unpredictable time appendonly file single producer single consumer immutability change everything reduce complexity much feasible avoid vendor lockin almost good enough ftp file transfer protocol rfc rfc wallaroo choice do dumb object service almost ftp example wallaroo cluster do server providing data redundancy figure figure figure figure figure implementation overview client side inside wallaroo serialized io operation file io journal appendonly journal io journal file size logical time chandylamport snapshot consistency across wallaroo worker checkpointing consistent recovery line handle failure wallaroo assume byzantine failure data corruption inside io journal file next data redundancy wallaroo give try get started wallaroo,introduction need data redundancy exactly distributed system production congratulation cluster starting six machine expected grow quickly assigned work cluster main application stateful problem lose local disk drive sysadmin run rm rf wrong directory else entire machine reboot due power failure administrator error destroys entire virtual machine today application writing critical data file local file system machine tomorrow data need redundant sooner later hardware fail administrator something wrong add data redundancy know need ask advice answer get scattered map tech store file nfs na perhaps san iscsi service aws elastic block store eb volume store file nontraditional shared file service like hadoop ceph switch file relational database like replicated mysql postgresql azure cosmosdb switch file nonrelational database like cassandra amazon dynamodb none answer could make sense depending application company company broader goal article explores wallaroo lab chose data redundancy technique fit need product wallaroo wallaroo wallaroo framework designed make easier developer build operate highperformance application handle complexity building distributed data processing application need worry domain logic wallaroo streaming data processing framework one property streaming data total size input unknown last item process may never arrive streaming platform also know much data arrive given timeframe many event arrive next minute zero one thousand ninety million every minute certain much data arrive processing option partial failure distributed stream processing machine distributed stream processor fails two basic option recovery start process stream data starting beginning restore system state snapshot backup made recent past resume processing option streaming system based apache spark kafka store input event data resend event component rebooted option wallaroo colleague john mumm recently wrote blog article wallaroo checkpointing algorithm article explains wallaroo recovery process worker restarted however blog post cover different scenario happens wallaroo worker data lost data lost forever example let look concrete example wallaroo cluster six worker process different computer virtual machine container figure six worker wallaroo cluster state storage via local disk figure data redundancy local disk fails recovery impossible entire machine vm container fails administrator error recovery impossible alternative cluster deployed amazon web service aws cluster using elastic block store eb volume create data redundancy figure depicts general principle applies file blockdevicecentric scheme mentioned introduction including na san iscsi figure six worker wallaroo cluster state storage via eb let assume machine caught fire data lost see figure also let make diagram bit accurate figure placement eb service relative rest cluster figure wallaroo process restarts replacement machinevmcontainer accessing eb volume original machinevm fails eb volume disconnected dead vm attached replacement machinevm restarted machine participates wallaroo internal snapshot recovery protocol finish recovery process bos say sorry point bos perhaps start adding annoying restriction app use amazon aws exclusively app use onpremise data center app emc san phased soon app use small sample deployment restriction might work let look property restriction priority wallaroo lab considered creating data redundancy strategy wallaroo recovery data infinitely sized arrives unpredictable time wallaroo streaming data system two fundamental property streaming data system following input data infinite truly infinite data impossible input data may large many network data protocol assume amount data transfer known advance ideal data replication system would require wallaroo specify exact data size advance example amazon object store requires multipart upload api complicated move problem different place input data arrival time unknown every data streaming system receives least one byte per second system may receive one byte every day every week large gap data input event wallaroo could run data replication problem related timeouts writing data ftp server amazon service sql server server disconnect due idletimeout event appendonly file single producer single consumer wallaroo property simplify choosing redundant data storage scheme single producer single consumer general purpose database file system usually support kind shared read write access data client want read write database row key file support contrast every wallaroo worker recovery data shared worker private state use single worker single worker process run one machine given time worker recovery data accessed single producer single consumer manner industry open source community many solution kind problem including type storage area network sans fibre channel shared disk fcoe fibre channel ethernet aoe ata ethernet exclusive use shared block device aws eb elastic block store linux nbd network block device however technique make assumption hardware andor network service available public cloud computing service provider private data center want wallaroo data redundancy management steer u vendor lockin problem furthermore wallaroo worker data file appendonly intentionally made choice make file replication easier appendonly file data written modified later writes written appendonly file content immutable immutability make task replicating file much much easier reading see pat helland excellent article punny name immutability change everything reduce complexity much feasible many open source system today support wallaroo recovery data storage requirement apache kafka bookkeeper two bestknown one wallaroo use case fit kafka bookkeeper feature set almost exactly use kafka bookkeeper hadoop another excellent fit wallaroo need use hadoop ala kafka bookkeeper hadoop complex piece software right wallaroo need feature let try avoid requiring wallaroo administrator familiar detail running service development testing production environment kafka bookkeeper hadoop another complex dependency zookeeper consensus system nobody wallaroo lab wish make zookeeper mandatory development work production system let try avoid requiring customer use maintain zookeeper avoid vendor lockin wallaroo lab want restrict wallaroo deployment single cloud provider like azure google compute engine amazon web service could avoid lockin supporting many cloud service however company size resource quickly develop code needed supporting major player cloud storage market similarly traditional onpremise data center want require use na san similar storage device almost good enough ftp file transfer protocol rfc defines file transfer protocol ftp ftp service old welltested widely available ftp however perfect fit wallaroo use case following condition wallaroo data arrives unpredictable time could tweak ftp server use infinite timeouts wished single writer restriction available could tweak ftp server enforce restriction wished periodic feedback recevied data available adding feedback received fsync ed data eg per second would significant protocol change simple tweak ftp provides status acknowledgment end file received also rfc explicitly require durable stable storage file content wallaroo choice do dumb object service almost ftp last section listed three problem ftp protocol defined rfc problem possible tweak mentioned accident ftp protocol tweaked three way result would fit wallaroo need exactly rather adapt ftp protocol chose create implement new file transfer protocol similar ftp quite ftp name do dumb object service do protocol inspired small subset ftp command without asciionly nature ftp command small binary protocol specify ftplike command cwd change working directory feature used allow single do server store data multiple do client do client writes data file separate directory append append data file similar http put command ftp command combination pasv stor append command includes argument do server file size size file yet exist do server allow append operation proceed server file size exactly equal size argument provided do client also server prohibits multiple client writing file simultaneously get retrieve file content similar http get command ftp command combination pasv retr do server get command permit partial read file specifying starting offset maximum number byte send client list list file current directory feature mentioned requirement list useful wallaroo worker client discover name size file stored private directory quit terminates do client session example wallaroo cluster do server providing data redundancy let see happens add two do server sixworker wallaroo cluster worker configured replicate io journal file do server do server us local file system local disk store io journal six wallaroo worker figure six worker wallaroo cluster plus two do server wallaroo worker fails catastrophically local recovery file lost must retrieve copy remote do server restart worker procedure figuring do server date described later article figure suggests different machine virtual machine container required six wallaroo worker process two do server figure show happens collocate do server box also run two wallaroo worker figure six worker wallaroo cluster plus two do server sharing hardware last example another option us eb provide redundant storage single do server instead six worker server figure system figure may costeffective option provisioned iop cost eb volume added system do server fails replacement procedure similar failover scenario shown figure figure six worker wallaroo cluster plus one do server plus eb implementation overview client side inside wallaroo wallaroo worker creates number recovery data file inside resiliencedir directory specified command line file listing directory look like rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct rwr r ubuntu ubuntu oct wallaroo application intentionally restricted three filemutating io operation append file data file truncate file delete file io operation io call argument written journal file io journal file strictly appendonly journal file data written via do protocol one do server describe serialization io operation next section serialized io operation myappjournal file file file writeappend event nonjournal file eg file serialized single generic function pony type encoderequest optag usize op ints array usize bs array byteseq array byteseq iso usize request operation tag number operation specifying byte two array argument one pony language type usize integer one pony type byteseq stringsbyte array depending operation byte op caller includes appropriate number element respective array view serialized byte returned encoderequest size type description protocol version op type append delete number int args number stringbyte args usize op tag usize int arg usize nth int arg usize size stringbyte arg usize size nth stringbyte arg x stringbyteseq content stringbyte arg x stringbyteseq content nth stringbyte arg binary content myappjournal file translated humanreadable form look something like ints string localhostn ints string ints string file write operation first integer ints list file offset first string string list path file write second string string list data written offset file truncate operation first integer ints list file size truncate first string string list path file truncate io journal appendonly journal io journal file size logical time filemutating io operation resiliencedir file serialized appended local io journal file asynchronously local io journal file data written via do protocol one remote do server redundancy consider file size io journal file logical time given set f file whose io ops written io journal single file f set f reconstructed logical time extracting f io event journal file offset io event le example want reconstruct roll back content file existed wall clock time midnight new year day procedure determine size io journal file midnight new year day let call offset maxoffset open io journal file reading seek beginning journal read serialized io operation file reach offset maxoffset serialized operation involves file path apply op ie writefile truncatefile delete file path chandylamport snapshot consistency across wallaroo worker logical time construct mentioned article applicable single wallaroo worker valid comparison wallaroo worker wallaroo release introduced snapshot algorithm responsible managing logical time consistency across wallaroo worker algorithm variation chandylamport snapshot algorithm us separate notion logical time logical time discussed please see article checkpointing consistent recovery line handle failure wallaroo overview wallaroo adaptation chandylamport work assume byzantine failure data corruption inside io journal file do protocol larger wallaroo system operate correctly byzantine failure assume byzantine failure including software bug would bad reorder record within journal assume data corruption reason inside journal file case data corruption inside log remedy available best remedy add robust checksum data written journal today wallaroo yet use checksum next data redundancy wallaroo wallaroo version released october raw component data redundancy date component lightly tested performance do client written purely pony inside wallaroo performance analysis performed yet do server written python seems odd choice highperformance server early test shown do server write mbytessecond recovery data without significant latency later wallaroo release may reimplement do server pony higher efficiency longterm goal handsfree turnkey system react failure wallaroo component restart gracefully need choose environment system kubernetes mesos gce azure aws onprem data center wrestle choice confident building solid foundation data redundancy component give try hopefully gotten excited wallaroo data redundancy story love give try give u feedback feedback help u drive product forward thank everyone contributed feedback far thank everyone reading blog post get started wallaroo
180,Lobsters,scaling,Scaling and architecture,A Large Scale Study of Data Center Network Reliability,https://people.inf.ethz.ch/omutlu/pub/data-center-network-errors-at-facebook_imc18.pdf,large scale study data center network reliability,,obj length filter flatedecode stream p jbd c e endstream endobj obj length filter flatedecode stream
181,Lobsters,scaling,Scaling and architecture,October 21 post-incident analysis,https://blog.github.com/2018-10-30-oct21-post-incident-analysis/,october postincident analysis,incident background data center facility mysql store github metadata mysql high availability orchestrator raft incident timeline october utc october utc october utc yellow status status red october utc october utc october utc updated status october utc october utc updated october utc blog post october utc october utc october utc status red october utc october utc updated green next step resolving data inconsistency communication technical initiative organizational initiative conclusion,last week github experienced incident resulted degraded service hour minute portion platform affected incident multiple internal system affected resulted displaying information date inconsistent ultimately user data lost however manual reconciliation second database writes still progress majority incident github also unable serve webhook event build publish github page site u github would like sincerely apologize impact caused every one aware trust place github take pride building resilient system enable platform remain highly available incident failed deeply sorry undo problem created github platform unusable extended period time explain event led incident lesson learned step taking company better ensure happen background majority userfacing github service run within data center facility data center topology designed provide robust expandable edge network operates front several regional data center power compute storage workload despite layer redundancy built physical logical component design still possible site unable communicate amount time utc october routine maintenance work replace failing optical equipment resulted loss connectivity u east coast network hub primary u east coast data center connectivity location restored second brief outage triggered chain event led hour minute service degradation past discussed use mysql store github metadata well approach mysql high availability github operates multiple mysql cluster varying size hundred gigabyte nearly five terabyte dozen read replica per cluster store nongit metadata application provide pull request issue manage authentication coordinate background processing serve additional functionality beyond raw git object storage different data across different part application stored various cluster functional sharding improve performance scale application direct writes relevant primary cluster delegate read request subset replica server vast majority case use orchestrator manage mysql cluster topology handle automated failover orchestrator considers number variable process built top raft consensus possible orchestrator implement topology application unable support therefore care must taken align orchestrator configuration applicationlevel expectation incident timeline october utc network partition described orchestrator active primary data center began process leadership deselection according raft consensus u west coast data center u east coast public cloud orchestrator node able establish quorum start failing cluster direct writes u west coast data center orchestrator proceeded organize u west coast database cluster topology connectivity restored application tier immediately began directing write traffic new primary west coast site database server u east coast data center contained brief period writes replicated u west coast facility database cluster data center contained writes present data center unable fail primary back u east coast data center safely october utc internal monitoring system began generating alert indicating system experiencing numerous fault time several engineer responding working triage incoming notification utc engineer first responder team determined topology numerous database cluster unexpected state querying orchestrator api displayed database replication topology included server u west coast data center october utc point responding team decided manually lock internal deployment tooling prevent additional change introduced utc responding team placed site yellow status action automatically escalated situation active incident sent alert incident coordinator utc incident coordinator joined two minute later made decision change status red october utc understood time problem affected multiple database cluster additional engineer github database engineering team paged began investigating current state order determine action needed taken manually configure u east coast database primary cluster rebuild replication topology effort challenging point west coast database cluster ingested writes application tier nearly minute additionally several second writes existed east coast cluster replicated west coast prevented replication new writes back east coast guarding confidentiality integrity user data github highest priority effort preserve data decided minute data written u west coast data center prevented u considering option failingforward order keep user data safe however application running east coast depend writing information west coast mysql cluster currently unable cope additional latency introduced crosscountry round trip majority database call decision would result service unusable many user believe extended degradation service worth ensuring consistency user data october utc clear querying state database cluster needed stop running job write metadata thing like push made explicit choice partially degrade site usability pausing webhook delivery github page build instead jeopardizing data already received user word strategy prioritize data integrity site usability time recovery october utc engineer involved incident response team began developing plan resolve data inconsistency implement failover procedure mysql plan restore backup synchronize replica site fall back stable serving topology resume processing queued job updated status inform user going executing controlled failover internal data storage system mysql data backup occur every four hour retained many year backup stored remotely public cloud blob storage service time required restore multiple terabyte backup data caused process take hour significant portion time consumed transferring data remote backup service process decompress checksum prepare load large backup file onto newly provisioned mysql server took majority time procedure tested daily minimum recovery time frame well understood however incident never needed fully rebuild entire cluster backup instead able rely strategy delayed replica october utc backup process affected mysql cluster initiated time engineer monitoring progress concurrently multiple team engineer investigating way speed transfer recovery time without degrading site usability risking data corruption october utc several cluster completed restoration backup u east coast data center begun replicating new data west coast resulted slow site load time page execute write operation crosscountry link page reading database cluster would return uptodate result read request landed newly restored replica larger database cluster still restoring team identified way restore directly west coast overcome throughput restriction caused downloading offsite storage increasingly confident restoration imminent time left establishing healthy replication topology dependent long would take replication catch estimate linearly interpolated replication telemetry available status page updated set expectation two hour estimated time recovery october utc github published blog post provide context use github page internally build paused several hour earlier publishing took additional effort apologize delay intended send communication much sooner ensuring publish update future constraint october utc database primary established u east coast resulted site becoming far responsive writes directed database server colocated physical data center application tier improved performance substantially still dozen database read replica multiple hour delayed behind primary delayed replica resulted user seeing inconsistent data interacted service spread read load across large pool read replica request service good chance hitting read replica multiple hour delayed reality time required replication catch adhered power decay function instead linear trajectory due increased write load database cluster user woke began workday europe u recovery process took longer originally estimated october utc approaching peak traffic load githubcom discussion incident response team proceed clear replication delay increasing instead decreasing towards consistent state begun provisioning additional mysql read replica u east coast public cloud earlier incident became available became easier spread read request volume across server reducing utilization aggregate across read replica allowed replication catch october utc replica sync conducted failover original topology addressing immediate latencyavailability concern part conscious decision prioritize data integrity shorter incident window kept service status red began processing backlog data accumulated october utc phase recovery balance increased load represented backlog potentially overloading ecosystem partner notification getting service back quickly possible five million hook event thousand page build queued reenabled processing data processed webhook payload outlived internal ttl dropped upon discovering paused processing pushed change increase ttl time avoid eroding reliability status update remained degraded status completed processing entire backlog data ensured service clearly settled back normal performance level october utc pending webhooks page build processed integrity proper operation system confirmed site status updated green next step resolving data inconsistency recovery captured mysql binary log containing writes took primary site replicated west coast site affected cluster total number writes replicated west coast relatively small example one busiest cluster writes affected window currently performing analysis log determining writes automatically reconciled require outreach user multiple team engaged effort analysis already determined category writes since repeated user successfully persisted stated analysis primary goal preserving integrity accuracy data store github communication desire communicate meaningful information incident made several public estimate time repair based rate processing backlog data retrospect estimate factor variable sorry confusion caused strive provide accurate information future technical initiative number technical initiative identified analysis continue work extensive postincident analysis process internally expect identify even work need happen adjust configuration orchestrator prevent promotion database primary across regional boundary orchestrator action behaved configured despite application tier unable support topology change leaderelection within region generally safe sudden introduction crosscountry latency major contributing factor incident emergent behavior system given previously seen internal network partition magnitude accelerated migration new status reporting mechanism provide richer forum u talk active incident crisper clearer language many portion github available throughout incident able set status green yellow red recognize give accurate picture working future displaying different component platform know status service week prior incident started companywide engineering initiative support serving github traffic multiple data center activeactiveactive design project goal supporting redundancy facility level goal work tolerate full failure single data center failure without user impact major effort take time believe multiple wellconnected site geography provides good set tradeoff incident added urgency initiative take proactive stance testing assumption github fast growing company built fair share complexity last decade continue grow becomes increasingly difficult capture transfer historical context tradeoff decision made newer generation hubbers organizational initiative incident led shift mindset around site reliability learned tighter operational control improved response time insufficient safeguard site reliability within system service complicated bolster effort also begin systemic practice validating failure scenario chance affect work involve future investment fault injection chaos engineering tooling github conclusion know much rely github project business succeed one passionate availability service correctness data continue analyze event opportunity serve better earn trust place u
182,Lobsters,scaling,Scaling and architecture,Polylith: A software architecture based on Lego-like blocks,https://www.youtube.com/watch?v=Y3FfLq8QATY,polylith software architecture based legolike block,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature polylith software architecture based legolike block youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature polylith software architecture based legolike block youtube
183,Lobsters,scaling,Scaling and architecture,Black Friday Infrastructure Readiness Checklist,https://fuzzy-logic.org/file/FuzzyLogic-ReadinessChecklist.pdf,black friday infrastructure readiness checklist,, obj stream endstream endobj obj endobj obj stream endstream endobj obj endobj obj stream x h g  k z p c p n v hj th  e en
185,Lobsters,scaling,Scaling and architecture,"Zero Downtime, Instant Deployment and Rollback",https://www.ebayinc.com/stories/blogs/tech/zero-downtime-instant-deployment-and-rollback/,zero downtime instant deployment rollback,problem analysis proposed solution ab switch software load balancer zero downtime parallel deployment apache tomcat context container delayed port binding advanced port binding breaking myth address already use soreuseport link post article conclusion,deployment cloud evolving area many tool available deploy application node machine cloud zero deployment downtime rare nonexistent post take look problem propose solution focus post web serverside application run port shared resource traditional deployment environment switching node cloud current version new version window time node unusable term serving traffic window node taken traffic switch brought back traffic production environment downtime trivial capacity planning advance usually accommodates loss node adding machine however problem becomes magnified principle like continuous delivery deployment adopted provide effective nondisruptive deployment rollback platform service paas posse two characteristic best utilization resource minimize deployment downtime much possible instantaneous deployment rollback problem analysis suppose node running deploying node lifecycle would look every machine pool undergoes lifecycle machine stop serving traffic right first step resume serving traffic last step time node effectively offline ebay deployment lifecycle take reasonably sized application minute organization size many day availability lost every node must go offline phase deployment minimize offtraffic time closer get instantzerodowntime deploymentrollback proposed solution let look option achieving goal ab switch approach set node standing deploy new version node switch traffic instantly keep old node original state could instant rollback well load balancer front application responsible switch upon request disadvantage approach node idle unless true elasticity amplify node wastage lot deployment occurring time may end needing double capacity handle load software load balancer approach configure software load balancer fronting application one end point effectively route traffic one another solution elegant offer much control software level however application designed approach mind particular load balancer contract application critical successful implementation resource standpoint previous approach similar use additional resource like memory cpu first approach need whole node whereas one accommodated inside node zero downtime approach keep set machine rather delay port binding shared resource acquisition delayed application start port switched application start old version also kept running without access point roll back instantly needed similar solution exist already common server parallel deployment apache tomcat apache tomcat added parallel deployment feature version release let two version application run time take latest version default achieve capability context container versioning pretty simple straightforward appending war name example webapp webapp coexist within context rolling back webapp required delete webapp although feature might appear trivial solution apps need take special care shared file cache much writethrough possible lowerlayer socket usage delayed port binding solution available web server currently typical server first bind port start service apache let delay binding extent overriding bindoninit still binding occurs connector started propose ability start server without binding port essentially without starting connector later separate command start bind connector version software deployed version running already bound version started later unbind version bind version approach node effectively offline second lifecycle delayed port binding would look like however still fewsecond glitch look next solution advanced port binding minimized window unavailability second see reduce zero way would bring version version go first breaking myth address already use used server run application sure seen exception least let consider scenario start server bind port try start another instance another server port process fails error address already use kill old server start work ever given thought two process listening port could preventing answer nothing indeed possible two process listening port soreuseport reason see error typical environment server bind soreuseport option option let two process bind port provided application bound first process option set binding option o interprets setting mean port shared block subsequent process binding port soreuseport option also provides fair distribution request important since threading suffers bottleneck multicores threading thread listening dispatching well multiple thread underover utilization cycle additional advantage soreuseport take care sending datagram client server process however shortcoming packet might dropped new process added removed fly shortcoming addressed find good article soreuseport link lwnnet want try see post freeprogrammer blog soreuseport option address two issue small glitch application version switching node serve traffic time effectively giving u zero downtime improved scheduling data indicates see article lwnnet thread scheduling fair ratio busiest thread versus one least connection please note soreuseport soreuseaddress available java operating system support conclusion application successfully serve traffic deployment carefully design manage application combining late binding port reuse effectively achieve zero downtime keep standby process around able instant rollback well
186,Lobsters,scaling,Scaling and architecture,Is Serverless the future of Cloud Computing?,https://blog.avanscoperta.it/it/2018/08/31/is-serverless-the-future-of-cloud-computing/,serverless future cloud computing,interview luciano mammino gojko adzic gojko adzic serverless one revolutionary disruptive technology luciano gojko luciano word serverless always sound bit weird first serverless billing model like see differently gojko billing model wrote serverless finally good implementation platformasaservice luciano research ended serverless make special gojko amazon really good mining metadata client workflow system utility computing computing become public utility serverless next step trend gojko mindmupcom cut hosting cost embraced aws platform vulnerability amazon turn already fix lambda morning make product better luciano gojko lambda typical web apis perfectly fine impractical longpolling luciano serverless silver bullet war story gojko luciano gojko easiest thing get started ton simple example project common task http claudiajscom check one organising avanscoperta called serverless development aws lambda currently demand luciano hard find example tutorial build real productionlike application many component gojko lambda best used glue service luciano asking believe serverless future cloud computing gojko general trend getting towards paying computing public utility continue difficult predict future hold luciano gojko learn gojko adzic serverless development aws lambra workshop,interview luciano mammino gojko adzicgojko adzic name already know following u past raise hand knew mr adzic also goto serverless expert asked luciano mammino author nodejs design pattern second edition serverless enthusiast international speaker interview gojko thing serverlesswhat follows indepth conversation among expert one revolutionary disruptive technology feeling curious read luciano hi gojko really pleasure share time someone influential sphere agile testing serverless thanks lot today ready chat serverless gojko sure always pleasure talk people similar interestsluciano brilliant would like start definitionsthe word serverless always sound bit weird first people come sort definition itthe last one heard serverless billing model unconventional provocative definition also interesting one course also like see differentlywhat think would good definition serverless gojko revolutionary thing serverless space billing modeli wrote still think thattechnically spinning container quickly scaling nice already existed amazon offered rent tap bill individual requestswhether whole definition knowi kind like simon wardley idea serverless finally good implementation platformasaservice lambda function really become magic people use universal glue ton platform service amazon providesinstead thinking lambda web server use store file process client request using kinesis push message client using aws iotinstead implementing authorisation use cognito iamthat dropped infrastructure code saved lot cash moving mindmupcom heroku lambdarunning serverless allowed u effectively rent platform amazon pay everything per request without thinking reserving capacity scalingjust running app inside lambda paying differently given u result goodluciano one thing generally tend curious discussing trend serverless understand principle model went mainstream recently little research history serverless trying understand model got much attention evolved cloud modelsi know ever asked ended serverless make specialif really curious know conclusion drawgojko ended serverless amazon really good mining metadata client workflow system spotting trend building product around trendsin worked editor local computer magazine remember reviewing article utility computing writeup conference people hp presented vision internet trendsthey talked infrastructure become commoditised company make sense pay provider similar people paying electricity waterif really big really special company may electricity production water sourcing company even consider running power plantthe hp people argued computing going direction computing become public utilityin many way turning true least mei think serverless next step trendcloud computing still much vendor controlled day similar westinghouse edison offering different type electricity available tapone day able say glass cloud computing tap please luciano let take practical angle moment tell something serverless application contributed building like serverless model good fit given project particular challenge team face constraint serverless model gojko working mindmupcom collaborative mind mapping applicationin started gradually moving service heroku combination factor tried lambda one potential solution impressed capability next year pretty much everything moved thereit great fit application need scale massively throughput bound user generally care dozen millisecond latency moreby internal metric migration period actually noticeable difference enduser latency able cut hosting cost two third remove lot infrastructure code benefit amazon autoscaling capability handle loadall lot productive waywe really embraced aws platform write almost infrastructure code meaning focus building important business feature insteadover time one biggest advantage u aws tends patch thing without u even caring thata recent example vulnerabilityi learnt user asked forum mindmup impacted hour warning started appear major news sitesamazon turn already fix lambda morningif running container waste lot time patching restarting servicesthis way spend time working make product better chasing infrastructure problemsluciano think serverless mature enough adopted default developing new cloud based service think missing gojko always depends contexti think lambda pretty solid service application require lot throughput autoscaling need squeeze millisecond average response time good platformso build algorithmic trading app need win competitor millisecond faster typical web apis perfectly finelikewise current limit five minute per execution make impractical longpolling case need keep open socket twitter clientluciano course serverless silver bulletpersonally try adopt serverless default work application due progress realized serverless going good fit go back classical architecturesi like hear war story giving serverless onegojko anything direction generally able move everything lamda use glue associated service mindmupluciano think easy get started serverless would suggest team start embracing serverless paradigm gojko know infrastructure provider google m lambda requires heavy lifting understand configuration universal glue people incredibly complicated thing easiest thing get started withwhen started mindmup lambda first function twentythirty line long accompanied twohundred line shell script deploywe built internal tool take care boilerplate enable u work way used lightweight web server express ruby railsthe tool opensource make simple thing aws really easy doi suggest people try first learning lambda perhaps move complicated stuff later need heavy supportit ton simple example project common task easy copy paste stuff people experimentcheck http claudiajscomi also run workshop programmer help get started lambda based experience mindmupcheck one organising avanscoperta called serverless development aws lambda currently demand luciano seen many many tutorial building hello world application trivial application like webhooks slack command image manipulation backup systemsit hard find example tutorial build real productionlike application many componentsis serverless good simple apps community mature enough produce realistic example gojko think lambda best used glue servicesif lambda function complex probably trying much lot lambda function mindmup singlepurpose singletask actionsrather trying find complex example lambda suggest people look service platform offer cognito aws iot kinesis cloudwatch see need benefit service wire part lambda functionsluciano like finish wonderful chat asking believe serverless future cloud computing whywill live breath serverless upcoming year something different rule day cloud developer gojko think general trend getting towards paying computing public utility continue see commoditization spacethere still thing like longrunning workflow lowlatency application current serverless architecture apply assume see development areait difficult predict future hold even though someone like hp might talked utility computing fifteen year ago amazon still trying figure sell book aws beat everyone reach stage gameluciano thanks gojko chat really illuminating sure reader thanks joining shedding light serverless trend bestgojko thanks lovely chat pic credit adam wilson imani imgixlearn gojko adzicgojko adzic trainer serverless development aws lambra workshop
187,Lobsters,scaling,Scaling and architecture,Scaling AFL to a 256 thread machine,https://gamozolabs.github.io/fuzzing/2018/09/16/scaling_afl.html,scaling afl thread machine,changelog performance disclaimer intro target selection libjpegturbo fuzzing using core afllaunch actually using core optimizing target maxium cpu time afl persistent mode conclusion gamozolabs shoutouts marcograss,changelog date info initial changed custom jpeg test program little save syscalls bringing u per fuzz case fuzz case used flag build neither noticeable impact performance thus performance number updated performance disclaimer performance critical work researching specifically fuzzer performance scaling past year important performance number accurate use tooling fullest please let know suggestion could make number better still using unmodified afl also using stock libjpeg want make internal mod jpeg increase risk invalid result machine testing done single socket intel xeon phi machine yes hw thread per core clocked ghz core effectively atom core making much weaker conventional one single ghz phi core usually run identical code slower conventional modern xeon ghz mean number might seem unreasonably low compared may expect weak core intro trying get afl scale correctly past day turn fairly hard afl really provide built way spinning multiple core using something like aflfuzz machine trying quite exotic much scale correctly anyways let hop right give go testing actually done upcoming blog series demonstrating neat new way fuzzing harnessing call vectorized emulation wanted get best performance number afl reasonable comparison would make tech bit relatable stay tuned post hopefully within week blog going talk major thing keep eye trying get every drop performance using core scaling well spending time actual target thing like kernel spinning one process per core possible true go real example process easy fall trap effectively using core target selection first need good target try fuzz fast possible something common reasonably small easy convert use afl persistent mode decided look libjpegturbo common target many people probably already familiar quite simple throw loop found bug pretty good day always fun play real target fuzzing first thing going try almost new target look find tool already come source way par image case libjpegturbo actually use tool come called djpeg simple command line utility take file stdin via command line argument produce another output file potentially another format since know going use afl let get basic afl environment set pretty simple case using latest time writing blog using asan looking best case performance number pleb debphi blogging wget http lcamtufcoredumpcxaflreleasesafllatesttgz http lcamtufcoredumpcxaflreleasesafllatesttgz resolving lcamtufcoredumpcx lcamtufcoredumpcx connecting lcamtufcoredumpcx lcamtufcoredumpcx connected http request sent awaiting response ok length applicationxgzip saving afllatesttgz afllatesttgz kb afllatesttgz saved pleb debphi blogging tar xf afllatesttgz pleb debphi blogging cd pleb debphi make checking ability compile code everything seems working ready compile cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin aflgccc aflgcc ldl set e aflg aflclang aflclang ln sf aflgcc done cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin aflfuzzc aflfuzz ldl cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin aflshowmapc aflshowmap ldl cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin afltminc afltmin ldl cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin aflgotcpuc aflgotcpu ldl cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin aflanalyzec aflanalyze ldl cc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin aflasc aflas ldl ln sf aflas testing cc wrapper instrumentation output unset afluseasan aflusemsan aflpath aflgcc funrollloops wall g wnopointersign daflpath usrlocallibafl ddocpath usrlocalsharedocafl dbinpath usrlocalbin testinstrc testinstr ldl echo aflshowmap none q testinstr echo aflshowmap none q testinstr right instrumentation seems working llvm user see llvmmodereadmellvm faster alternative aflgcc done sure review readme pretty short useful box afl give u compiler wrapper gcc clang aflgcc aflclang respectively use build libjpegturbo source afl add instrumentation used coverage feedback critical modern fuzzer operation especially byte flipping like afl let grab libjpegturbo build pleb debphi blogging git clone http githubcomlibjpegturbolibjpegturbo cloning libjpegturbo remote counting object done remote compressing object done remote total delta reused delta packreused receiving object mib mib done resolving delta done pleb debphi blogging mkdir buildjpeg pleb debphi blogging cd buildjpeg pleb debphi bloggingbuildjpeg export path path pleb debphi bloggingbuildjpeg cmake g unix makefiles dcmakeccompileraflgcc homeplebblogginglibjpegturbo pleb debphi bloggingbuildjpeg make built target tjunitteststatic pleb debphi bloggingbuildjpeg l cjpeg cmakefiles ctesttestfilecmake jconfigh jpegtran libjpegmap sharedlib tjbenchstatic tjexampletest wrjpgcom cjpegstatic cmakeinstallcmake djpeg jconfiginth jpegtranstatic libjpegso libturbojpega pkgscripts simd tjbenchtest tjunittest cmakecachetxt cmakeuninstallcmake djpegstatic jcstest libjpega libturbojpegso makefile rdjpgcom tjbench tjexample tjunitteststatic woo libjpegturbo built afl instrumented djpeg going use fuzz need test input corpus jpegs however since benchmarking picked single jpeg kib size set afl fuzz frill pleb debphi blogging mkdir fuzzing pleb debphi blogging cd fuzzing pleb debphi bloggingfuzzing mkdir input copy input pleb debphi bloggingfuzzing mkdir output pleb debphi bloggingfuzzing aflfuzz h pleb debphi bloggingfuzzing aflfuzz input output buildjpegdjpeg hacking important keep eye exec speed flutter around different pass case seemed hover around average using number picture baseline first test using core got sad news though using anything single core thread machine using fuzz waste silicon sadly trivial way spin afl let cheat grab something u afllaunch requires go instruction pretty clear get set running take effectively exact args aflfuzz take n parameter spin multiple job u let also switch ramdisk decrease thrashing disk matter much anyways due f caching pleb debphi bloggingfuzzing rm rf mntramoutputs pleb debphi bloggingfuzzing gobinafllaunch n mntraminputs mntramoutputs buildjpegdjpeg expect roughly number execsec single core number physical core execssec reality expect even due hyperthreading pleb debphi bloggingfuzzing p grep aflfuzz grep v grep wc l pleb debphi bloggingfuzzing aflwhatsup mntramoutputs status check tool aflfuzz lcamtuf googlecom summary stats fuzzers alive total run time day hour total exec million cumulative speed execssec pending path faves total pending per fuzzer faves total average crash found locally unique hmm running instance aflwhatsup confirms getting execssec speedup running thread hmm good even switched ramdisk even advantage single threaded run let check cpu utilization actually using core okay apparently using thread even though process running linux evenly distribute thread afl must something special look around affinity afl codebase stumble across build list process bound specific core return nothing found assumes upper bound cpu static void bindtofreecpu void dir struct dirent de cpusett c cpuused cpucorecount return getenv aflnoaffinity warnf binding cpu core aflnoaffinity set return opendir proc warnf unable access proc ca nt scan free cpu core return actf checking cpu core loadout introduce jitter case multiple afl task thing time usleep r scan proc pid status entry checking cpusallowedlist flag process bound specific cpu using cpuused fail exotic binding setup likely good enough almost realworld use case de readdir fn file f tmp maxline hasvmsize isdigit de dname continue fn allocprintf proc sstatus de dname f fopen fn r ckfree fn continue fgets tmp maxline f hval process without vmsize probably kernel task strncmp tmp vmsize hasvmsize strncmp tmp cpusallowedlist strchr tmp strchr tmp sscanf tmp u hval hval sizeof cpuused hasvmsize cpuused hval break ckfree fn fclose f closedir cpucorecount cpuused break cpucorecount sayf n clrd crst uhoh look like u cpu core system allocated ton instance aflfuzz similar cpulocked task startingn another fuzzer machine probably bad plan absolutely sure set aflnoaffinity try againn cpucorecount fatal free cpu core okf found free cpu core binding u cpuaff cpuzero c cpuset c schedsetaffinity sizeof c c pfatal schedsetaffinity failed endif haveaffinity see code interesting processing procfs find processor use pin interestingly never get uhoh message saying cpu instance running way possible afl binding multiple process core possible due race procfs cpu mask getting updated right away delay added spinning afl instance better see top function functionality turned entirely setting aflnoaffinity environment variable let manage affinity also going drop afllaunch tool import subprocess threading time shutil o numcpus inputdir mntramjpegs outputdir mntramoutputs def dowork cpu masterarg cpu masterarg restart dy happens startup bit true try sp subprocesspopen taskset c cpu aflfuzz inputdir outputdir masterarg fuzzer cpu buildjpegdjpeg stdoutsubprocesspipe stderrsubprocesspipe spwait except pas print cpu aflfuzz instance died cpu backoff fail run timesleep assert ospathexists inputdir invalid input directory ospathexists outputdir print deleting old output directory shutilrmtree outputdir print creating output directory osmkdir outputdir disable afl affinity better osenviron aflnoaffinity cpu range numcpus threadingtimer dowork args cpu start let master stabilize first cpu timesleep threadingactivecount timesleep try subprocesscheckcall aflwhatsup outputdir except pas using taskset spawn afl process manually control core rather afl trying figure used know used since launching everything osenviron aflnoaffinity make sure afl get control affinity manage got thing like give second delay master instance automatically clean ouput directory call aflwhatsup loop also restart dead aflfuzz instance observed happen sometimes spawning everything status check tool aflfuzz lcamtuf googlecom summary stats fuzzers alive total run time day hour total exec million cumulative speed execssec pending path faves total pending per fuzzer faves total average crash found locally unique well gave u speedup look cpu utilization optimizing target maxium cpu time using core htop master might know red mean kernel time bar mean eyeballing spending cpu time kernel time kernel time spent fuzzing jpegs point got afl everything gon na get creative target telling u must able find least speedup target moving goal execssec possible kernel usage unavoidable something like libjpegturbo would unreasonable spend large amount time kernel anyways let use everything afl give u using afl persistent mode effectively allows run multiple fuzz case single instance program rather reverting program state back every fuzz case via clone fork reduce kernel overhead worried let set persistent mode environment building aflclangfast pleb debphi blogging cd pleb debphi make checking working llvmconfig checking working clang checking aflshowmap set ready build clang funrollloops wall g wnopointersign daflpath usrlocallibafl dbinpath usrlocalbin dversion aflclangfastc aflclangfast ln sf aflclangfast aflclangfast clang llvmconfig cxxflags fnortti fpic funrollloops wall g wnopointersign dversion wnovariadicmacros shared aflllvmpasssocc aflllvmpassso llvmconfig ldflags warning unknown warning option wnomaybeuninitialized mean wnouninitialized wunknownwarningoption warning generated clang funrollloops wall g wnopointersign daflpath usrlocallibafl dbinpath usrlocalbin dversion fpic c aflllvmrtoc aflllvmrto building variant runtime success building variant runtime success testing cc wrapper instrumentation output unset afluseasan aflusemsan aflinstratio aflpath aflccclang aflclangfast funrollloops wall g wnopointersign daflpath usrlocallibafl dbinpath usrlocalbin dversion testinstrc testinstr echo aflshowmap none q testinstr echo aflshowmap none q testinstr right instrumentation seems working done use aflclangfast compile program aflclangfast binary folder let rebuild libjpegturbo using pleb debphi bloggingbuildjpeg cmake g unix makefiles dcmakeccompileraflclangfast homeplebblogginglibjpegturbo pleb debphi bloggingbuildjpeg make built target jpegtranstatic aflclangfast lszekeres googlecom built target jpegtran libjpegturbo library meaning designed used program also one popular library image compression surely relatively easy use let quickly write barebones application load image provided argument include stdlibh include stdioh include unistdh include fcntlh include jpeglibh include setjmph struct myerrormgr struct jpegerrormgr pub public field jmpbuf setjmpbuffer return caller typedef struct myerrormgr myerrorptr longjmp error methoddef void myerrorexit jcommonptr cinfo myerrorptr myerr myerrorptr cinfo err longjmp myerr setjmpbuffer eat warning methoddef void emitmessage jcommonptr cinfo int msglevel global int readjpegfile char filename unsigned char filebuf sizet filebuflen struct jpegdecompressstruct cinfo struct myerrormgr jerr jsamparray buffer output row buffer int rowstride physical row width output buffer int fd ssizet flen fd open filename ordonly fd return flen read fd void filebuf filebuflen close fd flen return cinfoerr jpegstderror jerrpub jerrpuberrorexit myerrorexit jerrpubemitmessage emitmessage establish setjmp return context myerrorexit use setjmp jerrsetjmpbuffer jpegdestroydecompress cinfo return jpegcreatedecompress cinfo jpegmemsrc cinfo filebuf flen void jpegreadheader cinfo true void jpegstartdecompress cinfo rowstride cinfooutputwidth cinfooutputcomponents buffer cinfomem allocsarray jcommonptr cinfo jpoolimage rowstride cinfooutputscanline cinfooutputheight void jpegreadscanlines cinfo buffer void jpegfinishdecompress cinfo jpegdestroydecompress cinfo return int main int argc char argv void filebuf null const sizet filebuflen argc fprintf stderr nice usage noobn return filebuf malloc filebuflen filebuf return aflloop readjpegfile argv filebuf filebuflen built aflclangfast examplec ihomeplebbloggingbuildjpeg ihomeplebblogginglibjpegturbo homeplebbloggingbuildjpeglibjpega see code derived comment modified specific need removed almost comment keep code small possible also relatively simple read based function name made change code removed output code print screen warning error save file parse input correctly return via setjmp longjmp error allow u quickly move next case see introduced aflloop special meaning running code aflfuzz running us signal notify done fuzz case need new one loop set limit iteration tearing child restarting pretty simple pretty clean hopefully syscall usage let check first going run new single threaded verify running persistent pleb debphi bloggingjpegfuzz aflfuzz mntramjpegs mntramoutputs aout persistent mode binary detected woo woo little faster initial single threaded djpeg running one ramdisk verified relevant running faster le misc thing kernel code afl tell u persistent let triple check running strace fuzz process p aux grep r aout grep v grep awk print p xargs strace bit ugly strace actively running aout task since crude might take try get attached right one bash pro see get repeating pattern openat atfdcwd mntramoutputscurinput ordonly read close rtsigprocmask sigblock rtmin getpid gettid tgkill sigstop sigstop sisignosigstop sicodesitkill stopped sigstop rtsigprocmask sigsetmask null sigcont sisignosigcont sicodesiuser openat atfdcwd mntramoutputscurinput ordonly read close rtsigprocmask sigblock rtmin getpid gettid tgkill sigstop sigstop sisignosigstop sicodesitkill stopped sigstop rtsigprocmask sigsetmask null sigcont sisignosigcont sicodesiuser see openat open read read file close done parsing rtsigprocmask beyond well persistent mode afl us communicate fuzz case done actually find code descriptive comment persistent mode child stop sigstop indicate successful run case want wake without forking mean rtsigprocmask beyond control bare minimum read file open read close nothing else running ten thousand fuzz case single instance program without exit fork alright let put together fuzz new binary core status check tool aflfuzz lcamtuf googlecom summary stats fuzzers alive total run time day hour total exec million cumulative speed execssec pending path faves total pending per fuzzer faves total average crash found locally unique woo per second expecting custom written target save another htop image tell cpu time spent kernel given syscalls per fuzz case mean per second still fairly high syscalls due afl u control conclusion pretty easy get stuck thinking tool work box people usually worry scaling level possible rather actually work important note easy run instance tool end getting little performance gain world fuzzing usually able scale linearly core getting efficiency probably time settle figure control able go naive singlecore afl usage execssec run afl optimization get u within hour work shame would taken run would wasting almost cpu first blog please let know anything want le follow gamozolabs twitter want notification new blog come think use r something still one people shoutouts shoutouts marcograss twitter giving afl tip trick getting number
188,Lobsters,scaling,Scaling and architecture,System Reliability and Availability Calculation,https://eventhelix.com/RealtimeMantra/FaultHandling/system_reliability_availability.htm,system reliability availability calculation,system reliability availability reliability availability basic system availability availability series component availability downtime availability parallel component availability downtime hardware fault tolerance partial operation availability xenon switching system availability computation example understanding system reliability modeling system calculating availability individual component reliability availability basic component mtbf mttr availability downtime calculating system availability component availability downtime,system reliability availability already discussed reliability availability basic previous article article focus technique calculating system availability availability information component following topic discussed detail system availabilitysystem availability calculated modeling system interconnection part series parallel following rule used decide component placed series parallel failure part lead combination becoming inoperable two part considered operating series failure part lead part taking operation failed part two part considered operating parallel availability series stated two part x considered operating series failure either part result failure combination combined system operational part x part available follows combined availability product availability two part combined availability shown equation ax ay implication equation combined availability two component series always lower availability individual component consider system figure part x connected series table show availability downtime individual component series combination component availability downtime x daysyear minutesyear x combined daysyear table clear even though high availability part used overall availability system pulled low availability part x prof saying chain strong weakest link specifically chain weaker weakest link availability parallel stated two part considered operating parallel combination considered failed part fail combined system operational either available follows combined availability part unavailable combined availability shown equation implication equation combined availability two component parallel always much higher availability individual component consider system figure two instance part x connected parallel table show availability downtime individual component parallel combination component availability downtime x daysyear two x component operating parallel minutesyear three x component operating parallel second year table clear even though low availability part x used overall availability system much higher thus parallel operation provides powerful mechanism making highly reliable system low reliability reason mission critical system designed redundant component different redundancy technique discussed hardware fault tolerance article partial operation availabilityconsider system like xenon switching system xenon xen card handle call processing digital trunk connected xen card system designed incrementally add xen card handle subscriber load consider case xenon switch configured xen card consider system unavailable one xen card fails nt seem right subscriber still served system failure component lead user losing service system availability defined considering percentage user affected failure example xenon system might considered unavailable subscriber affected translates xen card failing need formula calculate availability system xen card considered available consider system n component system considered available least nm component available ie component fail availablility system denoted calculated availability computation examplein section compute availability simple signal processing system understanding systemas first step prepare detailed block diagram system system consists input transducer receives signal convert data stream suitable signal processor output fed redundant pair signal processor active signal processor act input standby signal processor ignores data input transducer standby monitor sanity active signal processor output two signal processor board combined fed output transducer active signal processor drive data line standby keep data line tristated output transducer output signal external world input output transducer passive device microprocessor control signal processor card run realtime operating system signal processing application also note system stay completely operational long least one signal processor operation failure input output transducer lead complete system failure reliability modeling systemthe second step prepare reliability model system stage decide parallel serial connectivity system complete reliability model example system shown important point note signal processor hardware software modeled two distinct entity software hardware operating series signal processor function hardware software operational two signal processor software hardware combine together form signal processing complex within signal processing complex two signal processing complex placed parallel system function one signal processor fails input transducer signal processing complex output transducer placed series failure three part lead complete failure system calculating availability individual componentsthird step involves computing availability individual component mtbf mean time failure mttr mean time repair value estimated component see reliability availability basic article detail hardware component mtbf information obtained hardware manufacture data sheet hardware developed house hardware group would provide mtbf information board mttr estimate hardware based degree system monitored operator estimate hardware mttr around hour mtbf mttr known availability component calculated using following formula estimating software mtbf tricky task software mtbf really time subsequent reboots software interval may estimated defect rate system estimate also based previous experience similar system estimate mtbf around hour mttr time taken reboot failed processor processor support automatic reboot estimate software mttr around minute note minute might seem higher side mttr include following time wasted activity aborted due signal processor software crash time taken detect signal processor failure time taken failed processor reboot come back service component mtbf mttr availability downtime input transducer hour hour minutesyear signal processor hardware hour hour hoursyear signal processor software hour minute minutesyear output transducer hour hour minutesyear thing note table availability software higher even though hardware mtbf higher main reason software much lower mttr word software fail often recovers quickly thereby le impact system availability input output transducer fairly high availability thus fairly high availability achieved even without redundant component calculating system availabilitythe last step involves computing availability entire system calculation based serial parallel availability calculation formula component availability downtime signal processing complex software hardware hoursyear combined availability signal processing complex operating parallel secondsyear complete system minutesyear
189,Lobsters,scaling,Scaling and architecture,RabbitMQ at Tajawal,https://medium.com/tech-tajawal/rabbitmq-at-tajawal-c4eeccddf458,rabbitmq tajawal,problem higher resolution time crons permachine task error handling good way monitor system level task rather application level task difficult debug stackexchange thread choosing rabbitmq apache kafka rabbitmq easier management monitoring wide variety available tool plugins write extend functionality better developer experience using rabbitmq article defining virtual host exchange queue policy user http api standardizing message format consuming message dispatch async request using fpm piece combined,problemsthe mechanism worked pretty well time system kept growing http call cronjobs started show age common issue encountered higher resolution time ie task picked immediately crons could running dry data processcrons permachine task ie going run multiple server going need sort locking mechanism return going need maintenance example cleaning stale lock abnormally failed crons etc ended lot boilerplate code manage even small taskserror handling ie case command fails process item never picked upno good way monitor ie thing going track load system point timesystem level task rather application level task developer care application run crons system level task difficult developer debug testdifficult debug look stackexchange threadchoosing rabbitmqkeeping view issue started looking alternative least listed item wishlistlower resolution timedeveloper friendly powerfuleasy test monitoreasy scale without boilerplate codeeasy handle errorsthe idea make system easier scale converting mainly stateflow key functionality handled queuing mechanism instead cronjobswe research looked available alternative comparison thorough research came either using apache kafka go rabbitmq finally decided go rabbitmq mainly reason listed management monitoring available http api command line tool admin panelwide variety available tool plugins large number tool plugins available extend rabbitmq doesability write extend functionality using custom pluginsbetter developer experienceflexible routing multiple messaging protocol etcusing rabbitmqbefore go explain rabbitmq fit architecture let put rough diagram rabbitmq workshave look article would like understand different part shown diagram abovein next section discus put together make work u first discussing defining required structure discus publisher set dispatch message discussing consumer consume message end article bigger picture put togetherdefining virtual host exchange queue policy usersbefore start using rabbitmq need define key piece ie vhosts exchange queue policy user multiple way define themdo admin panel rabbitmqdo cliuse http api provided rabbitmqwe wanted something flexible ended using api internal tool called opstool defined vhosts exchange queue form folder structure whenever developer update virtual host user exchange queue policy open tool modify add folder structure run command automatically necessary modification using api diagram show part directory structure turned vhosts exchange queuesthe command idempotent evaluates difference applies provided change without affecting existing structure plus registered docker machine used developer change replicated developer machine soon madestandardizing message formatswith lot application message coming many different publisher paramount importance standardize message format order created standard package called rabbit help publisher connect broker generate standard message sent message defined form class package message must dispatched creating instance class application want connect rabbitmq must package installed message format definedconsuming messageson consumer side decided split command actual project thus created associated cli application product example flightapi divided two application ie flightapi flightapicli hotelapi divided hotelapi hotelapicli oncli application consumer simple command receive message dispatch async request using fpm relevant handler messageall piece combinedthe image show piece put together
191,Lobsters,scaling,Scaling and architecture,Amazon Redshift - Fundamentals,https://www.jefclaes.be/2018/05/amazon-redshift-fundamentals.html,amazon redshift fundamental,amazon redshift github,late set replace upgrade existing reporting analytics infrastructure something would better fit workload keeping cost required maintenance minimum would nice plus making easy sell bit research obvious amazon redshift potential tick right box steadily porting problematic workload away existing infrastructure started writing investigative article fundamental concept amazon redshift learned lot studying individual building block allowing make small impactful change setup along way outcome word document hour reading time covering topic storage distribution importing data table maintenance exporting data query processing workload management text available three format project open source available github thanks everyone proofread earlier iteration provided indispensable feedback hope work teach much thought looking forward feedback
192,Lobsters,scaling,Scaling and architecture,Blobstore: Twitters in-house photo storage system (2012),https://blog.twitter.com/engineering/en_us/a/2012/blobstore-twitter-s-in-house-photo-storage-system.html,blobstore twitter inhouse photo storage system,filter meheranand asdf thinkingkiddo jg sjlee armondbigian,million people turn twitter share discover photo make possible upload photo attach tweet directly twitter partnered photobucket soon photo became native part twitter experience people began using feature share photo order introduce new feature functionality filter continue improve photo experience core storage team began building inhouse photo storage system september began use new system called blobstorewhat blobstore blobstore lowcost scalable storage system built store photo binary large object also known blob set build blobstore three design goal mind low cost reduce amount money time twitter spent storing tweet photo high performance serve image low ten millisecond maintaining throughput hundred thousand request per second easy operate able scale operational overhead continuously growing infrastructure work user tweet photo send photo one set blobstore frontend server frontend understands given photo need written forward server responsible actually storing data storage server call storage node write photo disk inform metadata store image written instruct record information required retrieve photo metadata store nonrelational keyvalue store cluster automatic multidc synchronization capability span across data center providing consistent view data blobstore brain blobstore blob manager run alongside frontends storage node index cluster blob manager act central coordinator management cluster source knowledge file stored responsible updating mapping coordinating data movement storage node added removed due failure finally rely kestrel existing asynchronous queue server handle task replicating image ensuring data integrity across data center guarantee image successfully uploaded twitter immediately retrievable data center initially received image within short period time image replicated data center retrievable well rely multidatacenter metadata store central index file within blobstore aware short amount time whether image written original data center route request kestrel queue able replicate datablobstore component data found image requested blobstore need determine location order access data approach solving problem pro con one approach map hash image individually given server method method fairly major downside make managing movement image much complicated example add remove server blobstore would need recompute new location individual image affected change add operational complexity would necessitate rather large amount bookkeeping perform data movement instead created fixedsized container individual blob data called map image container map container individual storage node keep total number virtual bucket unchanged entire lifespan cluster order determine virtual bucket given image stored perform simple hash unique id long number virtual bucket remains hashing remain stable advantage stability reason movement data much coarsely grained level individual imagehow place data mapping virtual bucket physical storage node keep rule mind make sure lose data lose server hard drive example put copy given image single rack server losing rack would mean particular image would unavailable completely mirror data given storage node another storage node would unlikely would ever unavailable data likelihood losing node fairly low however whenever lose node would single node source rereplicate data would recover slowly impact performance single remaining node take opposite approach allow server cluster share range data server would avoid bottleneck recovering lost replica would essentially able read entire cluster order rereplicate data however would also high likelihood data loss lose replication factor cluster two per data center chance two node would share piece data would high optimal approach would somewhere middle given piece data would limited number machine could share range data replica one le entire cluster took thing account determined mapping data storage node result built library called understands various data placement rule rackawareness understands replicate data way minimizes risk data loss also maximizing throughput data recovery attempt minimize amount data need moved upon change cluster topology node added removed also give u power fully map network topology data center storage node better data placement take account rack awareness placement replica across pdu zone router keep eye blog post information libcrunchhow data stored know given piece data located need able efficiently store retrieve relatively high storage density using standard hard drive inside storage node rpm disk since mean disk seek expensive attempted minimize number disk seek per read write preallocate file storage node disk using fallocate around store blob data sequentially within fat file along small header offset length data stored metadata store us ssds internally access pattern index read writes wellsuited solid state medium furthermore splitting index data save u needing scale memory storage node need keep local index ram fast lookup time end hitting disk storage node already fat file location byte offset given piece data mean generally guarantee single disk seek read topology management number disk node increase rate failure increase capacity need added disk node need replaced failure server need moved make blobstore operationally easy put lot time effort libcrunch tooling associated making cluster change storage node fails data hosted node need copied surviving replica restore correct replication factor failed node marked unavailable cluster topology libcrunch computes change mapping virtual bucket storage node mapping change storage node instructed copy migrate virtual bucket new locationszookeeper topology placement rule stored internally one zookeeper cluster blob manager deal interaction us information stored zookeeper operator make change system topology change consist adjusting replication factor adding failing removing node well adjusting input parameter libcrunch replication across data center kestrel used cross data center replication kestrel durable queue use asynchronously replicate image data across data center data centeraware routing tfe twitter frontend one core component routing wrote custom plugin tfe extends default routing rule metadata store span multiple data center metadata stored per blob small byte typically replicate information much faster blob data user try access blob replicated nearest data center routed look metadata information proxy request nearest data center blob data stored give u property replication get delayed still route request data center stored original blob serving user image cost little higher latency replicated closer data centerfuture work shipped first version blobstore internally although blobstore started photo adding feature use case require blob storage blobstore also continuously iterating make robust scalable easier maintainacknowledgments blobstore group effort following folk contributed project meher anand meheranand ed ceaser asdf harish doddi thinkingkiddo chris goffinet jack gudenkauf jg sangjin lee sjlee posted armond bigian armondbigian engineering director core storage database engineering
193,Lobsters,scaling,Scaling and architecture,My arsenal of AWS security tools,https://blyx.com/2018/07/18/my-arsenal-of-aws-security-tools/,arsenal aws security tool,,opinion expressed solely express view opinion employer organization member la opiniones expresadas aqu son mi propias opiniones personales representan la opinin de nadie ms tampoco representa la opinin la de la empresa para la que trabajo u organismos los que pertenezco
194,Lobsters,scaling,Scaling and architecture,Choose Wisely,http://jmoiron.net/blog/choose-wisely/,choose wisely,choose sql different language timeseries database popular logical fallacy structure central orms flake id size querying nearly citation strength weakness invaluable,choose sql seems struck something nerve agree underlying premise written popular blog post using sql sql focused library different language also written distributed keyvalue timeseries database easymode based preexisting embedded storage library nearly antipodal guarantee semantics relational database yes starting zero relational database almost certainly right flexible still despite fact basically agree little uncomfortable choose sql nt much substance pretty much every popular logical fallacy well represented straw man appeal authority bandwagoning etc classic problem relational database waved away yagni pay someone else solve instead redressing quality research reasoned argument going share observation admittedly without data made large data system struggle seen relational nonrelational database lot sound equivocal even though nearly always caveat generally consider advice either wrong b applying anyone nt already realize nt apply structure promise schemaless fiction data structure central programming way structure data determine factor relationship bidirectional need data usually also end influencing structure data set schema nt building schema likely core product rdbms index keyvalue store transactional read writes queried via sql else equal incredibly powerful mix feature hard time finding nonrelational database get level performance data integrity tooling query flexibility key make break schema real world relational schema still require id type key record relational database need referenced somewhere else bit key verboten nt save meaningful resource small datasets nt enough keyspace larger one nt matter never billion foobars bunch way contention could force distribution avoid autoincrementserial key generated insert default lot orms another impediment distribution ordered key requirement use flake id nt generate key based natural uniqueness requirement schema coordination difficult fraught attempt avoid worth size confusion constitutes big data become vacuous marketing term definition big shift hardware software system evolve also different kind bigness fine grain high cardinality generally hurt quicker volume answer weighs youtube video tweet tweet likely result read write ops data generated human nt million user data probably nt big scale know far scale platform top cost effectiveness instance writing scale memory ssd pricy may worth cash also something better top engineer still nt sure big dataset smaller entire web ca probably meet threshold making call edge bigness difficult making start nearly impossible schema access pattern yet fully revealed lot problem solved single laptop good mr cluster gnu parallel worker box good mr cluster particularlly effective shorthand nnode mr cluster eventually overhead ca nt avoided natively clustered solution start pay querying json style query language mongodb major example upside compose naturally tree mean created modified analyzed serialized type ized easily without special purpose tool also insideout like cstyle declaration make awful repl sql composition much linear far superior ad hoc reporting poking around nt realize groundswell backlash tarring mapreduce hipster faff real thing wrong hipster faff seminal academic paper nearly citation reason subtitled simplifying data processing large cluster though labour intensive way process data benefit must offset cost due diligence probably start feeling pain go overhead running system high generally distributed computation data size must physicalasinphysics requirement result query many order magnitude smaller amount data access dynamostyle free distribution ai nt free nt mean potentially cheaper easier rdbms distribution technique generally sharding handle drawback restricted queryaccess pattern varying degree throughputconsistency tradeoff locality opacity upside significant strength weakness primary strength relational database safety storage efficiency application flexibility nonrelational database tend vary relational one strength generally one simplicity native clustering scaling availability distribution note speed missing speed datastore implement schema natural quality datastore keyvalue access pretty much btree implementation going look similar whether lmdb postgresql index guarantee cost feel warm fuzzy acid guarantee nt mean problem requires significant advantage dropping appropriate writes read relational database designed writeoncereadmany type load increasingly machinegenerated data writeoncereadmaybeduringanoutageorsomething sadly abbreviation cassandra lsm style database specifically designed writes lsm stand technique delay write amplification cause outage suitability scalability something overemphasized industry fetishizes large solution regardless quality sound really cool run node cluster actually quite difficult thing anyone probably thing two teach even system large inefficient hard manage large system inefficient handle large one efficient experience nothing get box going save issue start scale cluster large number machine open source relational database native clustering failover high availability meaning even safe standby replica primary outage still going cause issue violate singleprimary consistency guarantee sacrifice latency know multiprimary get ha readwrite scalability dataset scalability database type eventually exhibit difficult diagnose slow behaviour regardless type distributed database like dynamo slow behaviour spread across cluster impact availability way seem obvious database written managed memory language inevitably cause slow due gc unless manage memory sly generally via system mapped memory custom allocator everything else compaction strategy eventually go awry much thing whether vacuum lsm merge finally find someone built type system want build ask question listen answer spoken people worked publically available metric product compete one work people created inhouse metric product enormous company definitely know one sounded slightest bit alien kind experience invaluable mar
195,Lobsters,scaling,Scaling and architecture,Production Ready Microservices at Scale in Ruby,https://www.youtube.com/watch?v=SJpKo2hz6ws,production ready microservices scale ruby,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature deccan ruby conference production ready microservices scale rajeev bharshetty youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature deccan ruby conference production ready microservices scale rajeev bharshetty youtube
196,Lobsters,scaling,Scaling and architecture,The Evolutionary Origins of Hierarchy (2016),http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004829,evolutionary origin hierarchy,abstract author summary citation editor received accepted published copyright creative common attribution license data availability http funding competing interest introduction fig http fig method b c e fig f g http method fig table fig http result fig fig fig fig fig fig fig method fig fig http fig fig http fig method fig fig http fig fig fig fig fig fig fig fig fig fig fig fig fig b c eg e f fig g http http fig fig method http method fig fig fig fig fig fig method fig fig fig fig fig discussion method experimental setup statistic evolutionary algorithm fig behavioral diversity connection cost fig network model biological relevance fig fig mutation modularity default hierarchy metric alternate hierarchy metric functional hierarchy supporting information fig addition connection cost lead evolution hierarchical modular functionally hierarchical network method bc http result main experiment qualitatively second different hierarchical problem andequand b c http fig result main experiment qualitatively third different hierarchical problem orxorequequ http fig evolving network connection cost additional explicit pressure nonmodular produce network hierarchical nonmodular b http part network pa treatment first evolvability experiment network first evolved perfect fitness andxorand problem transferred black arrow andequand problem b http fig part second andxorand orxorand evolvability experiment network pa treatment experiment b fig http fig part third evolvability experiment orxorequequ andequand network pa treatment experiment b fig http fig evolvability improved even network hierarchical nonmodular demonstrating property hierarchy conveys evolvability independent modularity b fig http fig strong linear positive correlation network hierarchy modularity http fig addition main experimental problem p cc treatment also evolved highperforming network faster pa treatment two different problem andequand left orxorequequ right pictured fig main text http fig addition cost network connection also promotes evolution modularity hierarchy default model modified allow connection skip layer removing traditional constraint connection allowed adjacent layer method b fig c http fig result final generation experiment allows connection skip layer removing traditional constraint connection allowed adjacent layer method http fig adding second output change result p cc network evolve significantly hierarchical modular solve higher percent subproblems see fig pvalues fig b c http fig main finding paper qualitatively unchanged different hierarchy measure employed method b c http fig result qualitatively unchanged initializing network sparsely connected network method b c http fig detail evolutionary algorithm figure adapted b c http author contribution reference http,abstracthierarchical recursive composition ubiquitous biological network including neural metabolic ecological genetic regulatory network humanmade system large organization internet date research hierarchy network limited quantifying property however open important question evolutionary biology hierarchical organization evolves first place recently shown modularity evolves presence cost network connection investigate whether connection cost also tend cause hierarchical organization module computational simulation find network without connection cost evolve hierarchical even task hierarchical structure however connection cost network evolve modular hierarchical network exhibit higher overall performance evolvability ie faster adaptation new environment additional analysis confirm hierarchy independently improves adaptability controlling modularity overall result suggest cost evolution hierarchy modularity property important driver network performance adaptability addition shedding light emergence hierarchy across many domain appears finding also accelerate future research evolving complex intelligent computational brain field artificial intelligence robotics author summary hierarchy ubiquitous organizing principle biology key reason evolution produce complex evolvable organism yet origin poorly understood demonstrate first time hierarchy evolves result cost network connection confirm previous finding connection cost drive evolution modularity show also cause evolution hierarchy confirm hierarchy promotes evolvability addition evolvability caused modularity many biological humanmade phenomenon represented network hierarchy critical network property finding immediately relevant wide array field biology sociology medical research harnessing evolution engineering citation mengistu h huizinga j mouret jb clune j evolutionary origin hierarchy plo comput biol http olaf sporns indiana university united state received december accepted february published june mengistu et al open access article distributed term creative common attribution license permit unrestricted use distribution reproduction medium provided original author source crediteddata availability data found http jc supported nsf career award career jbm supported anr creadapt project european research council erc european union horizon research innovation programme grant agreement number funders role study design data collection analysis decision publish preparation manuscriptcompeting interest author declared competing interest exist introductionhierarchy important organizational property many biological manmade system ranging neural ecological metabolic genetic regulatory network organization company city society internet many type hierarchy one relevant biological network especially neural network refers recursive organization module module defined highly connected cluster entity sparsely connected entity cluster hierarchy long recognized ubiquitous beneficial design principle natural manmade system example complex biological system hierarchical composition module thought confer greater robustness adaptability whereas engineered design hierarchical organization simple structure accelerates design production redesign artifact study hierarchy focus producing method quantify instead examined hierarchy emerges various system domain emergence hierarchy well understood eg complex system social network ecosystem road network emergence hierarchy explained resource constraint local decision interaction biological system evolution hierarchy shaped natural selection hierarchy evolves whether evolution due direct indirect selection open interesting question nonadaptive theory state hierarchy type biological network may emerge byproduct random process adaptive explanation claim hierarchy directly selected confers evolvability ability population quickly adapt novel environment yet computational experiment simulate natural evolution hierarchy rarely ever evolves suggesting alternate explanation required explain evolutionary origin hierarchy moreover even hierarchy present directly selected evolvability confers explanation still required hierarchy emerges first place paper investigate one hypothesis existence cost network connection creates indirect selection evolution hierarchy hypothesis based two line reasoning first hierarchy requires recursive composition module second hierarchy includes sparsity recent study demonstrated modularity sparsity evolve presence cost network connection connection cost may therefore promote modularity sparsity thus may also promote evolution hierarchy realistic incorporate connection cost biological network model known cost create connection maintain transmit information along additionally evidence support existence selection pressure biological network reduce net cost connection remains open question strong selection multiple study shown biological neural network hierarchical organized reduce amount wiring fewer long connection locating neuron optimally reduce wiring relationship hierarchy connection cost also observed variety different manmade system example large scale integrated circuit vlsis designed minimize wiring hierarchically organized organization military company hierarchical communication model shown ideal configuration cost communication link organization member connection cost promote hierarchy humanmade system suggests might true evolved system test hypothesis computational simulation evolution experiment confirm hierarchy indeed evolve cost network connection fig fig main hypothesisevolution selection performance result nonhierarchical nonmodular network take longer adapt new environment evolving network connection cost however creates hierarchical functionally modular network solve overall problem recursively solving subproblems network also adapt new environment faster http also investigate hypothesis hierarchy confers evolvability long argued previously extensively tested experiment confirm hierarchical network evolved response connection cost exhibit enhanced ability adapt experimentally investigating evolution hierarchy biological network impractical natural evolution slow currently possible vary cost biological connection therefore conduct experiment computational simulation evolving network computational simulation evolution shed substantial light open important question evolutionary biology including evolution modularity structural property closely related hierarchy simulation randomly generated individual recombine mutate reproduce based fitness function evaluates individual according well performs task task analogous efficiently metabolizing resource performing required behavior process evolution cycle predetermined number generation evolved computational abstraction animal brain called artificial neural network anns solve hierarchical boolean logic problem fig addition abstracting animal brain anns also used abstraction gene regulatory network abstract sense environment input produce output either interpreted regulating gene moving muscle method experiment evolve anns without cost network connection specifically experimental treatment selects maximizing performance minimizing connection cost performance connection cost p cc whereas control treatment selects performance performance alone pa fig cost network connection produce network significantly hierarchical modular highperforming likely functionally decompose problemthe algorithm quantifying hierarchy modularity described method bar plot indicate generation significant difference exists two treatment hierarchical andxorand problem default experiment top eight node input problem bottom node output b p cc network significantly hierarchical pa network pvalues mannwhitneywilcoxon ranksum test default statistical test throughout paper unless otherwise stated c p cc network also significantly modular pa network confirming previous finding p cc network evolve solution problem significantly faster e evolved network highestperforming replicates pa treatment network nonhierarchical nonmodular tend decompose problem network panel report fitnessperformance f hierarchy h modularity node colored solve one logic subfunctions fig show network replicates treatment f evolved network highestperforming replicates p cc treatment network hierarchical modular decompose problem g comparison p cc pa network final generation p cc network significantly hierarchical modular solve significantly subproblems http previous work evolution modularity default treatment evolving network layered feedforward ie connection allowed neuron one layer neuron next layer eight input single output method evaluation network tested possible input pattern zero one network output checked hierarchical boolean logic function provided input fig table ann output considered true output considered false network performance fitness percent correct answer input pattern table main problem pictured fig network receive vector input shown successful network could adjacent input pair xor resulting pair result performance function final output thus depend network solves problem nonhierarchical solution also exist http resultson main experimental problem fig addition connection cost lead evolution significantly hierarchical network fig confirming previous finding different problem addition connection cost also significantly increase modularity fig reduces number generation required evolve solution fig importantly final performance level performance connection cost p cc treatment similar performance alone pa treatment qualitative difference network solve problem p cc network exhibit functional hierarchy solve overall problem recursively combining solution subproblems fig whereas pa network tend combine input information first layer process monolithic fashion fig functional hierarchy quantified percent subproblems network solves eg xor gate fig subproblem considered solved possible network input exists threshold neuron network exists output abovethreshold value whenever answer subproblem true subthreshold value answer false viceversa method measure reveals evolved p cc network solve significantly subproblems pa counterpart fig p via fisher exact test investigate ability solve subproblems related hierarchy modularity plotted percent subproblems solved v hierarchy modularity fig plot show significant strong positive correlation ability solve subproblems hierarchy modularity fig solving subproblems correlated hierarchy left modularity right shape size enclosed number indicate number network coordinate empty shape indicates one network present pearson correlation coefficient hierarchy modularity indicating strong linear positive relationship correlation significant p according ttest correlation zero null hypothesis http hypothesized one advantage network hierarchy confers evolvablity test hypothesis first evolving network solve one problem base environment evolving network solve different problem target environment isolate evolvability keep initial performance equal taking first run treatment pa p cc evolve perfectlyperforming network base environment network seed run target environment total replicates per treatment base target problem hierarchical share subproblems fig evolution target environment continues new problem solved generation elapse quantify evolvability number generation required solve target problem performed three experiment different base target problem experiment p cc network take significantly fewer generation adapt new environment pa network also solve significantly target problem subproblems fig fig p cc network adapt significantly faster solve significantly subproblems new environmentsin experiment network first evolve solve problem perfectly base environment left placed target environment right continue evolving solve different problem evolvability pa p cc network quantified number generation take solve new problem perfectly pair evolved network shown treatment left one show network median hierarchy elsewhere rounding replicates base environment right one show median hierarchy network run target environment started network left fig show network pair http possible reason fast adaptation p cc network modular structure allows solution subproblems reused different context hierarchical structure may also beneficial problem hierarchical even computation point structure different example module solve xor gate quickly rewired solve equ gate fig another reason faster p cc adaptation could network sparser meaning fewer connection need optimized understand connection cost increase hierarchy generated random valid network number connection network could method network valid path input node output node network neither evolved evaluated performance network lowcost tend high hierarchy high cost low hierarchy fig left inherent association low connection cost high hierarchy suggests selecting lowcost network promotes evolution hierarchical network also suggests network evolve nonhierarchical without pressure minimize connection cost indeed evolved pa network reside highcost lowhierarchy region whereas p cc network occupy lowcost highhierarchy region fig left fig lower cost network hierarchical modularthe hierarchy left modularity right randomly generated ie nonfunctional network shown cost normalized per cost value smoothed gaussian kernel density estimation function color indicate probability network generated location heat map network evolved either p cc pa treatment overlaid green circle blue triangle respectively circle triangle size enclosed number indicate number network coordinate number mean evolved p cc network highhierarchy lowcost region evolved pa network highcost lowhierarchy region http also inverse relationship cost modularity random network fig right shown evolved p cc network found lowcost highmodularity region pa network spread lowmodularity highcost region fig right fig suggest network modularity hierarchy highly correlated fig pearson correlation coefficient p based ttest correlation zero null hypothesis thus unclear whether hierarchy evolves direct consequence connection cost evolution byproduct evolved modularity address issue ran additional experiment evolutionary fitness function network lowcost p cc treatment highperforming treatment nonmodular achieved selecting low modularity score call treatment p ccnonmod result reveal p ccnonmod network low level modularity pa network fig p significantly higher hierarchy fig p solve significantly subproblems pa network fig result reveal independent modularity connection cost promotes evolution hierarchy additionally p ccnonmod network significantly evolvable pa network fig revealing hierarchy promotes evolvability independently known evolvability improvement caused modularity gain better insight relationship modularity hierarchy performance searched highestperforming network possible level modularity hierarchy performed search multidimensional archive phenotypic elite mapelites algorithm result show network modularity wide range different level hierarchy viceversa indicates network property vary independently fig additionally highhierarchy highmodularity region evolved p cc network reside contains highperforming solution lowhierarchy lowmodularity region pa network reside fig suggesting explanation p cc network find highperforming solution faster fig test generality hypothesis ran three additional experiment first repeated main experiment two different booleanlogic problem different logic gate number input output problem variant p cc network significantly hierarchical modular solve subproblems pa network fig p cc treatment also evolved highperforming network significantly faster additional problem fig fig evolving lowcost highperforming network nonmodular reveals independent modularity connection cost promotes evolution hierarchy network highestperforming p ccnonmod replicates fig show network trial network hierarchal highly modular b significant difference modularity p ccnonmod pa network p ccnonmod network significantly hierarchical c solve significantly subproblems pa network eg p ccnonmod network also adapt significantly faster new environment pa network suggesting hierarchy promotes evolvability independently modularity e base target problem evolvability experiment f perfectperforming network evolved base problem left descendant network evolved target problem right example network median hierarchy fig show pair g p ccnonmod network adapt significantly faster new problem http network modularity hierarchy independently vary highperforming network exist wide range modularity hierarchy scoresthe highestperforming network evolution discovered mapelites algorithm combination modularity hierarchy example network shown along fitness f hierarchy h modularity best network pa p cc treatment also overlaid blue triangle green circle respectively size circle triangle enclosed number indicate number network coordinate number mean http result qualitatively unchanged different problemsthe p cc network significantly modular hierarchical solve subproblems pa network different hierarchical booleanlogic problem problem example evolved network specifically one median hierarchy treatment shown fig fig show final evolved network replicate treatment problem note problem right extra layer hidden node added due complexity problem method http known biological network neural system longrange connection equivalent model connection skip number layer connection could affect evolution network modularity hierarchy conducted second experiment investigates whether result reported hold connection layer allowed method result qualitatively unchanged connection cost still promoted evolution modularity hierarchy fig fig also conducted third experiment similar main experiment problem eight input two output see fig experiment investigates whether hierarchy modularity evolve presence problem many input single output main experimental problem fig result qualitatively unchanged fig fig additionally result qualitatively unchanged measuring hierarchy via different independently created hierarchy metric specifically czgel palla method default metric developed mones et al alternate metric finalgeneration p cc network significantly hierarchical finalgeneration pa network main treatment fig p treatment allows connection skip layer fig p twooutput treatment fig p new metric also enabled u verify p cc network significantly hierarchical pa network problem three output fig p problem shown fig mones et al hierarchy metric returned pathological result threeoutput case overall result additional experiment suggest hypothesis connection cost promote evolution modularity hierarchy robust varying key assumption default model discussionthe evolution hierarchy likely caused multiple factor result suggest one factor indirect selection reduce net cost network connection adding cost connection previously shown evolve modularity result paper confirm finding show cost connection also lead evolution hierarchy moreover hierarchy evolves functional involves solving problem recursively combining solution subproblems likely force encourage evolution hierarchy connection cost force operates conjunction identifying driver evolution hierarchy relative contribution interesting area future research result also reveal like modularity hierarchy improves evolvability modularity hierarchy correlated experiment conducted explicitly select network hierarchical nonmodular reveal hierarchy improves evolvability even modularity discouraged additional factor present modular hierarchical network sparsity term meaning connection exist total could possible property explains even evolvability benefit modular hierarchical network future work needed address difficult challenge experimentally teasing apart related property study suggests biologically plausible selection cost lead evolution sparse hierarchical network finding thus consistent previous study found sparse network tend hierarchical however previous work investigate whether sparsity tends evolve force encourage evolution investigate whether sparsity lead hierarchy evolutionary context thus work reaffirms relationship sparsity hierarchy provides insight evolutionary origin pointed evolution modularity even hierarchy present directly selected increase evolvability explain evolutionary origin enough hierarchy present first place evolvability gain selected paper offer one explanation sufficient hierarchy emerge first place provide evolvability benefit selected paper show effect connection cost evolution hierarchy via experiment many variant one class problem hierarchical logic problem many input one output future work interesting test generality result across different class problem including nonhierarchical problem data paper suggest connection cost always make likely hierarchy evolve remains open interesting question wide range problem hierarchy evolve even connection cost addition shedding light biological network evolve hierarchical work also lends additional support hypothesis connection cost may also drive emergence hierarchy humanconstructed network company organization road system internet furthermore knowing evolve hierarchical network improve medical research benefit biologically realistic thus hierarchical network model ability evolve hierarchy also aid field harness evolution automatically generate solution challenging engineering problem known hierarchy beneficial property engineered design fact artificial intelligence researcher long sought evolve computational neural model property modularity regularity hierarchy key enablers intelligence animal brain recently shown combining technique known produce regularity connection cost produce network modular regular work suggests produce network three property hypothesis confirm future work able create network three property improve effort study evolution natural intelligence accelerate ability recreate artificially method experimental setup trial per treatment trial independent evolutionary process initiated different random seed meaning sequence stochastic event drive evolution eg mutation selection different trial lasted generation population size unless otherwise stated analysis visualization based highestperforming network per trial tie broken randomly statistic test statistical significance mannwhitneywilcoxon ranksum test unless otherwise stated report plot median bootstrapped confidence interval median calculated resampling data time visual clarity reduce resampling noise inherent bootstrapping smoothing confidence interval median filter window size evolutionary algorithm network evolve via multiobjective evolutionary algorithm called nondominated sorting genetic algorithm version ii nsgaii first introduced original nsgaii weight objective equally explore consequence performance objective important connection cost objective clune et al created stochastic version nsgaii called probabilistic nsgaii pnsga implemented via framework objective considered selection certain probability detailed explanation found supp fig specifically performance factor selection time connection cost objective factor p percent time preliminary experiment paper demonstrated value p led qualitatively similar result however simplicity largest difference p cc pa treatment resulted p chose value default paper note p nsgaii pnsga identical behavioral diversity evolutionary algorithm notoriously get stuck local optimum locally globally highfitness area part limited computational resource require smaller population size found nature make computational evolution representative natural evolutionary population exhibit diversity adopt common technique promoting behavioral diversity adding another independent objective reward individual behaving differently individual population diversity objective factored selection time preliminary experiment confirmed diversitypromoting technique necessary without evolution reliably produce functional network either treatment previously shown evolving network property modularity calculate behavioral diversity network input store network output response binary vector output value stored otherwise different network behavior population calculated computing hamming distance network output vector output vector network normalizing result get behavioral diversity measure connection cost following connection cost network computed finding optimal node placement internal hidden node input output node fixed location minimizes overall network connection cost location computed exactly optimizing location internal node biologically motivated evidence location neuron animal nervous system optimized minimize summed length connection network visualization show optimal neural placement overall network connection cost cc computed summed squared length connection aij node node j connected otherwise dist j euclidean distance node j moving optimal position network node located twodimensional cartesian space x location input output node fixed location hidden node vary according optimal location described problem input x coordinate coordinate output located except problem fig right output located extra layer hidden neuron network model biological relevance default network model multilayered feedforward meaning node layer n receives incoming connection node layer n outgoing connection node layer n network model common investigating question system biology including study evolution modularity experiment allow longrange connection skip layer node layer n receive incoming connection node layer n outgoing connection node layer n layered feedforward nature network may contribute elevated hierarchy network architecture across treatment interested difference level hierarchy occur without connection cost main problem andxorand network form mean input node hidden layer node respectively output node integer possible value connection weight whereas possible value bias information flow network discrete time step one layer time output yj node j result function ij set node connected node j wij connection strength node node j yi output value node bj bias bias determines input value output change negative positive function tanh x activation function guarantee output node range slope transition two extreme output value determined  set fig consistent treatment initial number connection network randomly chosen chosen number initial connection maximum number connection network network fully connected random valid network generated initial number connection range minimum number needed maximum number possible connection assigned random weight selected possible value connection weight placed randomly chosen unconnected neuron residing two consecutive layer result qualitatively unchanged initial number connection smaller default ie evolution start sparse network fig mutation create offspring parent network copied randomly mutated network chance single connection added default experiment candidate new connection pair unconnected node reside two consecutive layer experiment allow connection skip layer two different layer randomly selected node within layer selected randomly node joined new connection network also chance randomly chosen connection removed node network chance bias incremented decremented option equally probable five value available mutation produce value higher lower value ignored connection network chance weight incremented decremented n total number connection network weight must set possible value mutation produce value higher lower four value ignored modularity network modularity calculated finding partition network module maximizes commonly used q modularity metric directed network resulting q score best estimate true modularity network metric extensively described describe briefly q metric defines network modularity particular division network number withinmodule edge minus expected number edge equivalent network edge placed randomly formula metric outdegree node j respectively total number edge network aij entry connectivity matrix edge node j otherwise ci cj function whose value node j belong module otherwise iterating possible network partition prohibitively expensive even small network presented adopt widely used computationally efficient spectral method approximating true q score described default hierarchy metric default hierarchy measure come rank node based influence node influence network equal portion network reachable node respecting fact edge directed based metric larger proportion network node reach via outgoing edge influential example root node influence path traced every node network whereas leaf node influence metric calculates network hierarchy computing heterogeneity influence value node network intuitively nodeinfluence heterogeneity high hierarchical network node great deal influence others none low nonhierarchical network eg fully connected network influence node perfectly homogeneous nonlinear function map node input output even small change input node change whether fire reason difficult determine influence one node another based strength connection thus calculate hierarchy score looking presence connection node ignoring strength connection score weighted directed network calculated highest influence value v represents set node network n number node network node network represented cr influence value node given dout j length path go node node j meaning number outgoing connection along path alternate hierarchy metric section give brief description second alternate hierarchy metric employed verify main result complete description method found czgel palla metric rank network node based influence node network influence measured step algorithm starting random walker node network allowing randomly traverse single network edge reverse order based relative importance edge distribution node visited normalized step aggregated across many step limit unique stable distribution reached reflects influence node network node hierarchy quantified relative standard deviation distribution heterogenous distribution reveal hierarchical structure node high influence node whereas little influence conversely highly homogenous distribution indicates nonhierarchical structure influence node others similar functional hierarchy proxy quantifying functional hierarchy measure percent logic subproblems overall problem solved part network note overall logic problem solved without solving specific subproblem eg extreme entire problem computed one step determine whether logic subproblem solved follows possible input network neuron solves logic gate subproblem output exists threshold value correctly separate true answer false answer logic gate question also consider subproblem solved solved group neuron layer check case consider possible grouping neuron layer group size checked sum output neuron group see threshold correctly separate true false logic subproblem possible network input prevent counting solution multiple time subproblem considered ie algorithm stop searching subproblem found solved supporting fig addition connection cost lead evolution hierarchical modular functionally hierarchical networksin visualization network first sorted fitness f hierarchy h finally modularity network node colored solve logic subproblem overall problem method main experimental problem paper andxorand bc highestperforming network end trial performance alone pa treatment left le hierarchical modular functionally hierarchical network performance connection cost p cc treatment right http eps result main experiment qualitatively second different hierarchical problem andequand highestperforming network end trial performance alone pa treatment b le hierarchical modular functionally hierarchal network performance connection cost p cc treatment c network first sorted fitness f hierarchy h finally modularity http eps fig result main experiment qualitatively third different hierarchical problem orxorequequsee previous caption lengthier explanation network extra layer hidden node v default network model owing extra complexity last logic gate equ http eps fig evolving network connection cost additional explicit pressure nonmodular produce network hierarchical nonmodularthese result show connection cost promotes hierarchy independent modularityinducing effect connection cost problem experiment default experiment paper andxorand b almost endofrun network p ccnonmod treatment hierarchical yet low modularity http eps part network pa treatment first evolvability experiment network first evolved perfect fitness andxorand problem transferred black arrow andequand problem highestperforming network replicate base environment seed independent run target environment leading total replicates per treatment target environment b visualization bestperforming network original environment left side arrow right side example descendant network target environment specifically network median hierarchy http eps fig part second andxorand orxorand evolvability experiment network pa treatment experiment b except different target environment experiment setup evolvability experiment supp fig http eps fig part third evolvability experiment orxorequequ andequand network pa treatment experiment b except different base environment experiment setup evolvability experiment shown supp fig http eps fig evolvability improved even network hierarchical nonmodular demonstrating property hierarchy conveys evolvability independent modularity base problem network originally evolved left new target problem network transferred evolved right b pair left perfectperforming network evolved base problem right example descendant network evolved target problem specifically descendant network median hierarchy except p ccnonmod treatment evolvability experiment setup evolvability experiment supp fig http eps fig strong linear positive correlation network hierarchy modularitythe pearson correlation coefficient correlation significant p calculated ttest correlation zero null hypothesis larger circle triangle indicate presence one network location number describes many http eps fig addition main experimental problem p cc treatment also evolved highperforming network faster pa treatment two different problem andequand left orxorequequ right pictured fig main text bar plot indicates significant difference exists two treatment http eps fig addition cost network connection also promotes evolution modularity hierarchy default model modified allow connection skip layer removing traditional constraint connection allowed adjacent layer method problem experiment main experimental problem andxorand evolved pa network b significantly le hierarchal le modular fig evolved p cc network c http eps fig result final generation experiment allows connection skip layer removing traditional constraint connection allowed adjacent layer method change model change result p cc network significantly hierarchical modular solve subproblems pa network http eps fig adding second output change result p cc network evolve significantly hierarchical modular solve higher percent subproblems see fig pvalues eightinput twooutput andxorandor problem experiment similar main experimental problem fig main paper except output final evolved network performance alone pa treatment b le hierarchical modular functionally hierarchical network performance connection cost p cc treatment c http eps fig main finding paper qualitatively unchanged different hierarchy measure employedwith new metric method hierarchy finalgeneration p cc network significantly higher pa network main experimental problem b experiment connection allowed skip layer c twooutput experiment threeoutput experiment http eps fig result qualitatively unchanged initializing network sparsely connected networksin experiment minimum maximum number initial connection network start generation respectively due fact least connection needed solve experimental problem network initial number connection within range considered sparse note default range initial number connection method hierarchy modularity b percent subproblems solved c significantly higher endofrun p cc network indicating regardless initial connectivity network connection cost promotes evolution trait http eps fig detail evolutionary algorithm figure adapted graphical depiction multiobjective evolutionary algorithm study called nondominated sorting genetic algorithm version ii nsgaii nsgaii evolution start population n randomly generated network n offspring generated randomly mutating best individual determined tournament selection wherein best organism randomly selected organism chosen produce offspring asexually combined pool offspring current population ranked based pareto dominance best n network selected form next generation process continues fixed number generation network desired performance property evolve b example network model network typically used researcher abstract activity many biological network gene regulatory network neural network node analogous neuron gene represent processing unit receive input neighbor external source process compute output signal propagated node example node input layer activated environmental stimulus output passed internal node figure arrow indicate connection two node thus illustrate pathway information flow connection weight number control strength interaction two node information flow network ultimately determining firing pattern output node firing pattern output node considered command activate gene gene regulatory network move muscle animal body output value node function weighted input bias paper specific activation function tanh x wiii b ii ith input wi associated synaptic weight b bias like weight vector evolved specific function depicted c multiplying input make function like step function output range http eps author contributionsconceived designed experiment hm jh jbm jc performed experiment hm analyzed data hm jh jbm jc contributed reagentsmaterialsanalysis tool hm jh jbm wrote paper hm jh jbm meunier lambiotte r bullmore et modular hierarchically modular organization brain network frontier neuroscience dec meunier lambiotte r fornito ersche kd bullmore et hierarchical modularity human brain functional network frontier neuroinformatics miller w iii hierarchical structure ecosystem connection evolution evolution education outreach jan ravasz e somera al mongru da oltvai zn barabsi al hierarchical organization modularity metabolic network science aug yu h gerstein genomic analysis hierarchical structure regulatory network proceeding national academy science oct rowe r creamer g hershkop stolfo sj automated social hierarchy detection email network analysis proceeding webkdd snakdd workshop web mining social network analysis aug pp acm krugman p confronting mystery urban hierarchy journal japanese international economy dec guimera r danon l diazguilera giralt f arena selfsimilar community structure network human interaction physical review e dec vzquez pastorsatorras r vespignani largescale topological dynamical property internet physical review e jun ravasz e barabsi al hierarchical organization complex network physical review e feb mones e vicsek l vicsek hierarchy measure complex network plo one mar pumain hierarchy natural social science ed springer http lane hierarchy complexity society hierarchy natural social science pp springer netherlands salespardo guimera r moreira aa amaral la extracting hierarchical organization complex system proceeding national academy science sep lorenz dm jeng deem mw emergence modularity biological system physic life review jun bassett d greenfield dl meyerlindenberg weinberger dr moore sw bullmore et efficient physical embedding topologically complex information processing network brain computer circuit plo comput biol apr clune j mouret jb lipson h evolutionary origin modularity proceeding royal society london b biological science mar verbancsics p stanley ko constraining connectivity encourage modularity hyperneat proceeding annual conference genetic evolutionary computation jul pp acm lipson h principle modularity regularity hierarchy scalable system journal biological physic chemistry dec wagner gp pavlicev cheverud jm road modularity nature review genetics dec kaiser hilgetag cc ktter r hierarchy dynamic neural network frontier neuroinformatics aug suh np principle design new york oxford university press feb ozaktas hm paradigm connectivity computer circuit network optical engineering jul trusina maslov minnhagen p sneppen k hierarchy measure complex network physical review letter apr corominasmurtra b rodrguezcaso c goi j sol r measuring hierarchy feedforward network chaos interdisciplinary journal nonlinear science mar dehmer borgert emmertstreib f entropy bound hierarchical molecular network plo one aug song c havlin makse ha origin fractality growth complex network nature physic apr ryazanov ai dynamic hierarchical system physicsuspekhi corominasmurtra b goi j sol rv rodrguezcaso c origin hierarchy complex network proceeding national academy science aug neill rv hierarchical concept ecosystem princeton university press wu j david jl spatially explicit hierarchical approach modeling complex ecological system theory application ecological modelling jul flack jc erwin elliot krakauer dc timescales symmetry uncertainty reduction origin hierarchy biological system evolution cooperation complexity feb salthe sn evolving hierarchical system structure representation columbia university press aug sun j deem mw spontaneous emergence modularity model evolving individual physical review letter nov pigliucci evolvability evolvable nature review genetics jan clune j beckmann mckinley pk ofria c investigating whether hyperneat produce modular neural network proceeding annual conference genetic evolutionary computation jul pp acm paine rw tani j hierarchical control selforganizes artificial adaptive system adaptive behavior sep huizinga j mouret jb clune j evolving neural network modular regular hyperneat plus connection cost technique proceeding annual conference genetic evolutionary computation cherniak c mokhtarzada z rodriguezesteban r changizi k global optimization cerebral cortex layout proceeding national academy science chen bl hall dh chklovskii db wiring optimization relate neuronal structure function proceeding national academy science mar sporns network brain mit press raj chen yh wiring economy principle connectivity determines anatomy human brain plo one sep ahn yy jeong h kim bj wiring cost organization biological neuronal network physica statistical mechanic application jul laughlin sb sejnowski tj communication neuronal network science sep guimera r arena diazguilera communication optimal hierarchical network physica statistical mechanic application oct simon ha architecture complexity proceeding american philosophical society lenski ofria c collier tc adami c genome complexity robustness genetic interaction digital organism nature aug lenski ofria c pennock rt adami c evolutionary origin complex feature nature may wilke co wang jl ofria c lenski adami c evolution digital organism high mutation rate lead survival flattest nature jul espinosasoto c wagner specialization drive evolution modularity plo comput biol mar kashtan n alon u spontaneous evolution modularity network motif proceeding national academy science sep kashtan n noor e alon u varying environment speed evolution proceeding national academy science aug alon u introduction system biology design principle biological circuit crc press jul trappenberg fundamental computational neuroscience oxford university press geard n wile j gene network model developing cell lineage artificial life mouret jb clune j illuminating search space mapping elite arxiv preprint cully clune j tarapore mouret jb robot adapt like animal nature kaiser hilgetag cc nonoptimal component placement short processing path due longdistance projection neural system plo comput biol jul louf r jensen p barthelemy emergence hierarchy costdriven growth spatial network proceeding national academy science may zhang huang z zhu z liu j zheng x zhang network analysis chipseq data reveals key gene prostate cancer european journal medical research sep shmulevich dougherty er kim zhang w probabilistic boolean network rulebased uncertainty model gene regulatory network bioinformatics feb albert r scalefree network cell biology journal cell science nov koza jr keane streeter mj mydlowec w yu j lanza g genetic programming iv routine humancompetitive machine intelligence springer science business medium mar floreano mattiussi c bioinspired artificial intelligence theory method technology mit press aug stanley ko miikkulainen r taxonomy artificial embryogeny artificial life hornby g measuring enabling comparing modularity regularity hierarchy evolutionary design proceeding annual conference genetic evolutionary computation jun pp acm clune j stanley ko pennock rt ofria c performance indirect encoding across continuum regularity evolutionary computation ieee transaction jun gruau f automatic definition modular neural network adaptive behavior sep nolfi floreano evolutionary robotics biology intelligence technology selforganizing machine mit press striedter gf principle brain evolution sinauer associate wagner gp altenberg l perspective complex adaptation evolution evolvability evolution jun stanley ko ambrosio db gauci j hypercubebased encoding evolving largescale neural network artificial life mar deb k multiobjective optimization using evolutionary algorithm vol john wiley son mouret jb doncieux overcoming bootstrap problem evolutionary robotics using behavioral diversity evolutionary computation cec ieee congress may pp ieee doncieux mouret jb behavioral diversity measure evolutionary robotics evolutionary computation cec ieee congress jul pp ieee mouret jb doncieux encouraging behavioral diversity evolutionary robotics empirical study evolutionary computation mar risi vanderbleek sd hughes ce stanley ko novelty search escape deceptive trap learning learn proceeding annual conference genetic evolutionary computation jul pp acm chklovskii db exact solution optimal neuronal layout problem neural computation oct lehman j stanley ko abandoning objective evolution search novelty alone evolutionary computation may karlebach g shamir r modelling analysis gene regulatory network nature review molecular cell biology oct leicht ea newman community structure directed network physical review letter mar newman modularity community structure network proceeding national academy science jun czgel palla g random walk hierarchy measure hierarchical chain tree star scientific report mouret jb doncieux sferes evolvin multicore world evolutionary computation cec ieee congress ieee
197,Lobsters,scaling,Scaling and architecture,What's in a Production Web Application?,https://stephenmann.io/post/whats-in-a-production-web-application/,production web application,production getting started figure figure launch day figure performance problem figure growing team figure figure figure smooth sailing outro brief introduction infrastructure automation,second post series want start beginning production early career worked company built web content management system product helped marketing department selfmanage website instead relying developer make every change product helped customer reduce operational expense helped learn build web application product general purpose customer tended use solve specific problem problem pushed product limit every imaginable way engineering ultimately provide solution working environment ten year gave thorough appreciation wide variety way production web application break discus post one lesson learned year individual engineer tend learn deeply interest learn enough supporting piece dangerous work well team engineer good communication since combined knowledge overlap fill individual gap team little experience industry individual engineer possible started environment like set build deploy entire web application scratch might find quickly mean dangerous industry provided number solution aimed addressing problem managed web application beanstalk appengine hosted container management kubernetes ec many others work well get running think excellent job solving problem hide lot complexity required get web application running tend work unfortunately work come time make nuanced decision around specific production issue may find wishing understood bit ominous black box post going take unreliable system evolve one reasonable level reliability step along way use real world problem motivation move onto next step rather discussing piece final design find incremental approach help provide better context order make certain decision end built scratch basic structure managed web application hosting service provides hopefully provided ample context around piece exists getting started let pretend hosting budget year decided rent single server amazon aws time writing cost per year know front login session need store user information need database due constrained budget let host server end infrastructure look like figure suffice fact probably work quite small point probably handle visit per day small instance may sufficed since optimistic company growth made good choice instance value business stored database pretty important make sure server go lose data probably good time make sure stored database content ephemeral disk instance get deleted going lose data scary thought also make sure backup going external storage seems like good place put relatively cheap let set well definitely test working restoring backup every setup look something like figure increased reliability database decide prepare massive hacker news traffic spike running load test server everything seems going well error start showing followed stream investigate figure happened turn clue failed writing log console piping console output log file also see process running safely assume got mild wave relief wash forsight run local load test instead using hacker news load test fix autorestart issue creating systemd service run web server end solving logging problem well run another load test make sure solved everything see error thankfully check log see went wrong discover saturated database connection pool set unfortunately low limit connection update limit restart database run load test everything go well decide promote site hacker news launch day great scott service instant hit hit front page hacker news get hit first minute see comment pouring say getting check archived version page link anyone need nothing show also javascript disabled people assume want mb javascript homepage take second load traceroute live australia show server hosted somewhere texas also landing page require mb javascript mad scramble set nginx server reverse proxy application configure server static page also update deployment process push static file necessary cloudfront cdn help reduce load time australia figure addressed immediate problem go onto server check log ssh connection curiously laggy inspection discover log file completely used disk space crashed process prevented starting create much larger disk mount log also setup logrotate prevent log file getting huge performance problem month pas userbase grow site begin slow notice cloudwatch monitoring seems happen hour midnight noon utc due consistent start end time slowdown guess due scheduled task server check crontab realize one job scheduled midnight backup sure enough backup take twelve hour overload database causing significant site slowdown read decide run backup slave database remember slave database need create one make much sense run slave database server decide time expand create two new server one master database one slave database change backup run slave database figure growing team everything run smoothly quite time month pas hire larger development team one new developer check bug take production server developer blame environment differing production truth said since understanding person good temperament treat occasion learning opportunity time build environment staging qa development fortunately automated creation infrastructure day one easy also used good continuous delivery practice day one easily build pipeline new branch marketing want launch sure go anyway time prepare another traffic spike running close peak utilization web server decide time start load balancing traffic amazon elb make easy around time also discover layered diagram blog post show layer top bottom instead left right figure confident able handle load post site hacker news lo behold hold traffic great success seems well go check log take hour due twelve server check four environment hassle fortunately making enough money point implement elk stack elasticsearch logstash kibana build one point environment figure read log take look notice something odd full get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt get wploginphp window nt running php wordpress matter pretty concerning notice similar suspect log database server wonder ever exposed internet time public private subnets figure check log still hacking attempt limited port load balancer eas mind bit since application server database server elk stack longer exposed internet despite centralized logging tire discover outage manually checking log use amazon cloudwatch setup disk cpu network alarm send email hit capacity wonderful smooth sailing kidding thing smooth sailing software something go wrong fortunately lot tooling place make handling problem easier built scalable web application backup rollback using bluegreen deployment production staging centralized logging monitoring alerting good point stop since growth tends depend applicationspecific need industry provided number hosted option handle instead building rely beanstalk appengine gke ec etc service setup sensible permission load balancer subnets automatically take lot hassle getting application running quickly reliability site need run long time regardless think useful understand functionality platform provides provide make easier select platform based need everything running platform already figured important aspect tool work something go wrong help know necessary tool solve problem outro post omits lot detail cover automate creation infrastructure provision server configure server cover create development environment setup continuous delivery pipeline execute deployment rollback cover network security secret sharing principle least privilege cover importance immutable infrastructure stateless server migration topic requires post article purpose mostly provide high level overview reasonable production web application ought look like future post may reference one expand thanks reading happy coding second post series next brief introduction infrastructure automation edit take exact number used illustrative story literally individually event happened different time completely different environment different type load
198,Lobsters,scaling,Scaling and architecture,One process programming notes,https://crawshaw.io/blog/one-process-programming-notes,one process programming note,one process programming note go sqlite go northwest brief introduction sqlite handson sqlite command line tool provided csv file email protected using sqlite go standard databasesql lowlevel wrapper crawshawiosqlite session extension shared cache cgo go sqlite client io app session extension nested transaction go sqlite cloud process programming nt use n computer shared cache wal incremental blob api incremental blob api golike designing one process programming really need n computer indie dev technique corporate programmer,one process programming note go sqlite july blogified version talk gave go northwest content cover recent exploration writing internet service io apps macos program indie developer several topic blog post lot programming going put note split material time later focus adapt lesson learned working team google single programmer building small business work many great engineering practice silicon valley big company wellcapitalized vc firm one person enough bandwidth use write software exercise keep must go right technology technique described sound easy fit head enough capacity left write software people want every extra thing great cost especially rarely touched software come back bite middle night six month later two key technology decided use go sqlite brief introduction sqlite sqlite implementation sql unlike traditional database implementation like postgresql mysql sqlite selfcontained c library designed embedded program built richard hipp since release past year open source contributor helped point around time programming core part programming toolbox handson sqlite command line tool rather talk sqlite abstract let show kind person kaggle provided csv file play shakespeare let build sqlite database head shakespearedatacsv dataline play playerlinenumber actsceneline player playerline henry iv act henry iv scene london palace henry iv enter king henry lord john lancaster earl westmoreland sir walter blunt others henry iv king henry iv shaken wan care henry iv king henry iv find time frighted peace pant henry iv king henry iv breathe shortwinded accent new broil henry iv king henry iv commenced strand afar remote henry iv king henry iv thirsty entrance soil henry iv king henry iv shall daub lip child blood first let use sqlite command line tool create new database import csv shakespearedb sqlite mode csv sqlite import shakespearedatacsv import done couple selects let u quickly see worked sqlite select count import sqlite select import limit henry iv act henry iv scene london palace henry iv enter king henry lord john lancaster earl westmoreland sir walter blunt others henry iv king henry iv shaken wan care henry iv king henry iv find time frighted peace pant henry iv king henry iv breathe shortwinded accent new broil henry iv king henry iv commenced strand afar remote henry iv king henry iv thirsty entrance soil henry iv king henry iv shall daub lip child blood look good little cleanup original csv contains column called acesceneline us dot encode act number scene number line number would look much nicer column sqlite create table play rowid integer primary key play linenumber act scene line player text sqlite schema create table import rowid primary key play playerlinenumber actsceneline player playerline create table play rowid primary key play linenumber act scene line player text sqlite insert play select row rowid play playerlinenumber linenumber substr actsceneline act substr actsceneline scene substr actsceneline line player playerline text import substr improved using instr find character exercise left reader used insert select syntax build table another table actsceneline column split apart using builtin sqlite function substr slice string result sqlite select play limit henry iv act henry iv scene london palace henry iv enter king henry lord john lancaster earl westmoreland sir walter blunt others henry iv king henry iv shaken wan care henry iv king henry iv find time frighted peace pant henry iv king henry iv breathe shortwinded accent new broil henry iv king henry iv commenced strand afar remote henry iv king henry iv thirsty entrance soil henry iv king henry iv shall daub lip child blood data let u search something sqlite select play text like whether ti nobler sqlite work hamlet definitely say perhaps text formatting slightly sqlite rescue ship full text search extension compiled let u index shakespeare sqlite create virtual table playsearch using playsrowid text sqlite insert playsearch select rowid text play search soliloquy sqlite select rowid text playsearch text match whether ti nobler nobler mind suffer success act scene acquired joining original table sqlite select play act scene line player playstext playsearch inner join play playsearchplaysrowid playsrowid playsearchtext match whether ti nobler nobler mind suffer let clean sqlite drop table import sqlite vacuum finally look like file system l l email protected crawshaw staff apr shakespearedatacsv rwr r crawshaw staff jul shakespearedb sqlite database contains two full copy play shakespeare one full text search index store twice space take original csv file store one bad give feel ite sqlite scene using sqlite go standard databasesql number cgobased databasesql driver available sqlite popular one appears get job done probably want using databasesql package straightforward open sqlite database execute sql statement example run ft query earlier using go code package main import databasesql fmt log func main db err sqlopen shakespearedb err nil logfatal err defer dbclose stmt err dbprepare select play act scene playstext playsearch inner join play playsearchplayrowid playsrowid playsearchtext match err nil logfatal err var play text string var act scene int err stmtqueryrow whether ti nobler scan play act scene text err nil logfatal err fmtprintf qn play act scene text executing yield hamlet whether nobler mind suffer lowlevel wrapper crawshawiosqlite sqlite step beyond basic select insert update delete fulltext search several interesting feature extension accessed sql statement alone need specialized interface many interface supported existing driver wrote get crawshawiosqlite particular support streaming blob interface session extension implement necessary sqliteunlocknotify machinery make good use shared cache connection pool going cover feature two use case study client cloud cgo approach rely cgo integrating c go straightforward add operational complexity building go program using sqlite requires c compiler target practice mean develop macos need install crosscompiler linux typical concern impact software quality adding c code go apply sqlite extraordinary degree testing quality code exceptional go sqlite client building io app almost code written go ui provided web view app full copy user data thin view onto internet server mean storing large amount local structured data ondevice full text searching background task working database way disrupt ui syncing db change backup cloud lot moving part client want write javascript want write swift promptly rewrite ever manage build android app importantly server go one independent developer absolutely vital reduce number moving piece development environment smallest possible number hence effort build big bit client using exact technology server session extension session extension let start session sqlite connection change made database connection bundled patchset blob extension also provides method applying generated patchset table func conn conn createsession db string session error func session changeset w iowriter error func conn conn changesetapply r ioreader filterfn func tablename string bool conflictfn func conflicttype changesetiter conflictaction error used build simple clientsync system collect change made client periodically bundle changeset upload server applied backup copy database another client change database server advertises client downloads changeset applies requires bit care database design reason kept ft table separate shakespeare example keep ft table separate attached database sqlite mean different file cloud backup database never generates ft table client free generate table background thread lag behind data backup another point care minimizing conflict biggest one autoincrement key default primary key rowid table incremented mean multiple client generating rowids see lot conflict trialing two different solution first client register rowid range server allocate range work second randomly generating value relying low collision rate far work strategy risk nt decided better practice found limit db update single connection keep changeset quality high changeset see change made connection maintain readonly pool connection single guarded readwrite connection pool code grab readwrite connection need readonly connection enforced readonly bit sqlite connection nested transaction databasesql driver encourages use sql transaction tx type appear play well nested transaction concept implemented savepoint release sql make surprisingly composable code function need make multiple statement transaction open savepoint defer call release function produce go return error instead call rollback return error func f conn sqliteconn err error conn savepoint defer func err nil conn release else conn rollback transactional function f need call another transactional function g g use exactly strategy f call traditional go way err g conn err nil return err change f rolled back defer function g also perfectly safe use right transaction using savepoint defer release return error semantics several month find invaluable make easy safely wrap code sql transaction example however bit bulky edge case need handled example release fails error need returned wrapped utility func f conn sqliteconn err error defer sqlitexsave conn err code transactional stacked function call sqlitexsave first time see sqlitexsave action little offputting least first created quickly got used lot heavy lifting first call sqlitexsave open savepoint conn return closure either release rollback depending value err set err necessary go sqlite cloud spent several month redesigning service encountered designing service problem would like work going forward process led general design work many problem quite enjoy building summarized vm zone process programming sound ridiculously simplistic think good simple meet sort requirement would like modern fancy cloud service meet serverless mean service extremely small run free service grows automatically scale indeed explicit scaling limit right best server get amazon roughly cpu thread ram gbit ethernet gbps na hour yearly downtime huge potential downside one process programming however claim livable limit claim typical service hit scaling limit building small business product grow become profitable well limit year see limit approaching next year two business revenue hire one engineer new team face radically changing business requirement rewrite service reaching limit good problem come plenty time deal human resource need solve well early life small business nt every hour spend trying work beyond scaling limit hour would better spent talking customer need principle work nt use n computer go bit technical detail run single vm aws single availability zone vm three eb volume amazon name na first hold o log temporary file ephemeral sqlite database generated main database eg ft table second primary sqlite database main service third hold customer sync sqlite database system configured periodically snapshot system eb volume customer eb volume amazon georedundant blob store relatively cheap operation scripted block change copied main eb volume backed regularly custom code flush wal cache explain bit service single go binary running vm machine plenty extra ram used linux disk cache used second copy service spinning low downtime replacement result service ten hour downtime year much change suffering block loss physical computer array active offsite backup made every minute distributed system built maintained large team system astonishingly simple shell one machine linux machine deploy script service ten line long almost performance work done pprof medium sized vm clock thousand concurrent request hour performance tuning largest machine aws ten thousand talk little particular stack shared cache wal make server extremely concurrent two important sqlite feature use first shared cache let allocate one large pool memory database page cache many concurrent connection use simultaneously requires support driver sqliteunlocknotify user code nt need deal locking event transparent end user code second write ahead log mode sqlite knocked beginning connection change way writes transaction disk instead locking database making modification along rollback journal appends new change separate file allows reader work concurrently writer wal flushed periodically sqlite involves locking database writing change default setting override execute wal flush manually package done also trigger snapshot package called reallyfsync work test properly make open source incremental blob api another smaller important particular server feature sqlite incremental blob api allows field byte read written db without storing byte memory simultaneously matter possible request working hundred megabyte want ten thousand potential concurrent request one place driver deviate closetocgo wrapper golike type blob func blob blob close error func blob blob read p byte n int err error func blob blob readat p byte n int err error func blob blob seek offset whence int error func blob blob size func blob blob write p byte n int err error func blob blob writeat p byte n int err error look lot like file indeed used like file one caveat size blob set created still find temporary file useful designing one process programming start really need n computer problem really example build lowlatency index public internet ram need lot problem great fun like talk lot relatively small amount code written far project developing postgoogle fit computer also common subproblems hard solve one computer global customer base need lowlatency server speed light get way many problem solved relatively straightforward cdn product another great solution speed light geosharding complete independent copy service multiple datacenters move user data service near easy one small global redirect database maybe sqlite georedundant nfs redirecting user specific dns name like useast uswest mservicecom problem fit one computer point spend time determining point year away good chance one computer indie dev technique corporate programmer even write code particular technology stack independent developer value use one big vm one zone one process go sqlite snapshot backup stack hypothetical tool test design add hypothetical step design process solved problem stack one computer far could get many customer could support size would need rewrite software indie mini stack would last business year might want consider delaying adoption modern cloud software programmer wellcapitalized company may also want consider development look like small internal experimental project coworkers use large complex distributed system policy reason many project never need scale beyond one computer need rewrite deal shifting requirement case find way make indie stack linux vms file system available prototyping experimentation
199,Lobsters,scaling,Scaling and architecture,Node.js 8 and Python 3.7 runtimes now available in Google Cloud Functions,https://cloudplatform.googleblog.com/2018/07/bringing-the-best-of-serverless-to-you.html,nodejs python runtimes available google cloud function,learn,demonstrate proficiency design build manage solution google cloud platform learn
200,Lobsters,scaling,Scaling and architecture,The basic architecture concepts I wish I knew when I was getting started as a web developer,https://engineering.videoblocks.com/web-architecture-101-a3224e126947,basic architecture concept wish knew getting started web developer,web architecture basic architecture concept wish knew getting started web developer first result dns load balancer stackoverflow post web application server database server industry aligning sql interface even nosql database caching service job queue server fulltext search service fulltext search inverted index mysql support fulltext search elasticsearch sphinx apache solr service account service content service payment service html pdf service data cloud storage according aws cdn source check article parting thought,web architecture basic architecture concept wish knew getting started web developermodern web application architecture overviewthe diagram fairly good representation architecture storyblocks experienced web developer likely find complicated walk make approachable dive detail componenta user search google strong beautiful fog sunbeam forest first result happens storyblocks leading stock photo vector site user click result redirects browser image detail page underneath hood user browser sends request dns server lookup contact storyblocks sends requestthe request hit load balancer randomly chooses one web server running site time process request web server look information image caching service fetch remaining data database notice color profile image computed yet send color profile job job queue job server process asynchronously updating database appropriately resultsnext attempt find similar photo sending request full text search service using title photo input user happens logged storyblocks member look account information account service finally fire page view event data firehose recorded cloud storage system eventually loaded data warehouse analyst use help answer question businessthe server render view html sends back user browser passing first load balancer page contains javascript cs asset load cloud storage system connected cdn user browser contact cdn retrieve content lastly browser visibly render page user seenext walk component providing introduction give good mental model thinking web architecture going forward follow another series article providing specific implementation recommendation based learned time dnsdns stand domain name system backbone technology make world wide web possible basic level dns provides keyvalue lookup domain name eg googlecom ip address eg required order computer route request appropriate server analogizing phone number difference domain name ip address difference call john doe call like needed phone book look john number old day need dns look ip address domain think dns phone book internetthere lot detail could go skip critical load balancerbefore diving detail load balancing need take step back discus horizontal v vertical application scaling difference simply put stackoverflow post horizontal scaling mean scale adding machine pool resource whereas vertical scaling mean scale adding power eg cpu ram existing machinein web development almost always want scale horizontally keep simple stuff break server crash randomly network degrade entire data center occasionally go offline one server allows plan outage application continues running word app fault tolerant secondly horizontal scaling allows minimally couple different part application backend web server database service x etc run different server lastly may reach scale possible vertically scale computer world big enough app computation think google search platform quintessential example though applies company much smaller scale storyblocks example run aws instance given point time would challenging provide entire compute power via vertical scalingok back load balancer magic sauce make scaling horizontally possible route incoming request one many application server typically clone mirror image send response app server back client one process request way matter distributing request across set server none overloadedthat conceptually load balancer fairly straight forward hood certainly complication need dive web application serversat high level web application server relatively simple describe execute core business logic handle user request sends back html user browser job typically communicate variety backend infrastructure database caching layer job queue search service microservices datalogging queue mentioned typically least two often time many plugged load balancer order process user requestsyou know app server implementation require choosing specific language nodejs ruby php scala java c net etc web mvc framework language express nodejs ruby rail play scala laravel php etc however diving detail language framework beyond scope database serversevery modern web application leverage one database store information database provide way defining data structure inserting new data finding existing data updating deleting existing data performing computation across data case web app server talk directly one job server additionally backend service may database isolated rest applicationwhile avoiding deep dive particular technology architecture component disservice mention next level detail database sql nosqlsql stand structured query language invented provide standard way querying relational data set accessible wide audience sql database store data table linked together via common id typically integer let walk simple example storing historical address information user might two table user useraddresses linked together user id see image simplistic version table linked userid column useraddresses foreign key id column user tableif know much sql highly recommend walking tutorial like find khan academy ubiquitous web development least want know basic order properly architect applicationnosql stand nonsql newer set database technology emerged handle massive amount data produced large scale web application variant sql scale horizontally well scale vertically certain point know anything nosql recommend starting high level introduction like would also keep mind large industry aligning sql interface even nosql database really learn sql know almost way avoid caching servicea caching service provides simple keyvalue data store make possible save lookup information close time application typically leverage caching service save result expensive computation possible retrieve result cache instead recomputing next time needed application might cache result database query call external service html given url many example real world application google cache search result common search query like dog taylor swift rather recomputing timefacebook cache much data see log post data friend etc read detailed article facebook caching tech herestoryblocks cache html output serverside react rendering search result typeahead result morethe two widespread caching server technology redis memcache go detail another job queue serversmost web application need work asynchronously behind scene directly associated responding user request instance google need crawl index entire internet order return search result every time search instead crawl web asynchronously updating search index along waywhile different architecture enable asynchronous work done ubiquitous call job queue architecture consists two component queue job need run one job server often called worker run job queuejob queue store list job need run asynchronously simplest firstinfirstout fifo queue though application end needing sort priority queuing system whenever app need job run either sort regular schedule determined user action simply add appropriate job queuestoryblocks instance leverage job queue power lot behindthescenes work required support marketplace run job encode video photo process csvs metadata tagging aggregate user statistic send password reset email started simple fifo queue though upgraded priority queue ensure timesensitive operation like sending password reset email completed asapjob server process job poll job queue determine work pop job queue execute underlying language framework choice numerous web server dive detail fulltext search servicemany web apps support sort search feature user provides text input often called query app return relevant result technology powering functionality typically referred fulltext search leverage inverted index quickly look document contain query keywordsexample showing three document title converted inverted index facilitate fast lookup specific keyword document keyword title note common word etc called stop word typically included inverted indexwhile possible fulltext search directly database eg mysql support fulltext search typical run separate search service computes store inverted index provides query interface popular fulltext search platform today elasticsearch though option sphinx apache servicesonce app reach certain scale likely certain service carved run separate application exposed external world app service interact storyblocks example several operational planned service account service store user data across site allows u easily offer crosssell opportunity create unified user experiencecontent service store metadata video audio image content also provides interface downloading content viewing download historypayment service provides interface billing customer credit cardshtml pdf service provides simple interface accepts html return corresponding pdf datatoday company live die based well harness data almost every app day reach certain scale leverage data pipeline ensure data collected stored analyzed typical pipeline three main stage app sends data typically event user interaction data firehose provides streaming interface ingest process data often time raw data transformed augmented passed another firehose aws kinesis kafka two common technology purposethe raw data well final transformedaugmented data saved cloud storage aws kinesis provides setting called firehose make saving raw data cloud storage extremely easy configurethe transformedaugmented data often loaded data warehouse analysis use aws redshift large growing portion startup world though larger company often use oracle proprietary warehouse technology data set large enough hadooplike nosql mapreduce technology may required analysisanother step pictured architecture diagram loading data app service operational database data warehouse example storyblocks load videoblocks audioblocks storyblocks account service contributor portal database redshift every night provides analyst holistic dataset colocating core business data alongside user interaction event cloud storage cloud storage simple scalable way store access share data internet according aws use store access le anything store local file system benefit able interact via restful api http amazon offering far popular cloud storage available today one rely extensively storyblocks store video photo audio asset cs javascript user event data much cdncdn stand content delivery network technology provides way serving asset static html cs javascript image web much faster serving single origin server work distributing content across many edge server around world user end downloading asset edge server instead origin server instance image user spain request web page site origin server nyc static asset page loaded cdn edge server england preventing many slow crossatlantic http requestssourcecheck article thorough introduction general web app always use cdn serve cs javascript image video asset apps might also able leverage cdn serve static html pagesparting thoughtsand wrap web architecture hope found useful hopefully post series article provide deep dive component course next year two
201,Lobsters,scaling,Scaling and architecture,The Software Architecture Chronicles,https://herbertograca.com/2017/07/03/the-software-architecture-chronicles/,software architecture chronicle,series post software architecture importance knowing history post software architecture premise programming language evolution architectural style v architectural pattern v design pattern monolithic architecture layered architecture mvc alternative modelviewcontroller pac hierarchical modelviewcontroller modelviewpresenter model model modelviewviewmodel modelviewpresenterviewmodel resourcemethodrepresentation actiondomainresponder ebi architecture packaging namespacing domaindriven design port adapter architecture aka hexagonal architecture onion architecture clean architecture eventdriven architecture cqs cqrs service oriented architecture soa explicit architecture ddd hexagonal onion clean cqrs put together explicit architecture concentric layer explicit architecture reflecting architecture domain code timeline programming paradigm page wikipedia series post nonstructured programming structured programming layering tier procedural functional programming modelviewcontroller object oriented programming layering tier corba hierarchical modelviewcontroller lsp l layering tier message bus entityboundaryinteractor modelviewpresenter ocp isp dip id aspect oriented programming web service esb srp domaindriven design modelviewviewmodel port adapter architecture cqrs e onion architecture microservices datacontextinteraction architecture clean architecture translation chinese like published,post first series post software architecture write learned software architecture think use knowledge calling series post software architecture chronicle think great writer find name rather corny funny way first post going talk writing series post come importance knowing history fail learn history doomed repeat itwinston churchil find important learn history teach u lot personal level eventually hopefully learn personal mistake country history help model culture help create idea group idea u national identity also help u learn ancestor mistake like trusting people weird idea think u developer help u build predecessor developer knowledge help u learn mistake path experience knowledge help u achieve higher ground standing shoulder giant path better developer read lot article watched lot conference talk read plenty book best stand shoulder giant one thing puzzle though many opinion based opinion based work like chinese whispering game end distorted idea paper article book really state set scavenge internet original paper article book state concept find importance work reason post result reasoning try understand concept came somewhat chronological way writing post forced read reason lot subject helped understand technique used today software development hope post helpful fellow developer nevertheless read something understand disagree please let know perfectly open discussing subject learn people change point view proven wrong post software architecture chronicle software architecture chroniclessoftware architecture premisesprogramming language evolutionarchitectural style v architectural pattern v design patternsmonolithic architecturelayered architecturemvc alternative pac hierarchical model model modelviewviewmodel actiondomainresponder ebi architecturepackaging namespacingdomaindriven designports adapter architecture aka hexagonal architecture onion architectureclean architectureeventdriven architecturefrom cqs cqrsservice oriented architecture soa explicit architecture ddd hexagonal onion clean cqrs put togetherexplicit architecture concentric layersexplicit architecture reflecting architecture domain code explicit architecture documenting architecture evolving project mvp architecture view modelarchitecture quality attribute timeline rough timeline software development evolution perceive read article book subject added link found date reference put wherever sure date meaning approximately also find plethora information thing main programming paradigm page wikipedia talk subject series post nonstructured programming assembly structured programming layering tier ui business logic data storage algol procedural functional programming pascal c modelviewcontroller object oriented programming first thought late though layering tier tier ui tier business logic data storage c corba common object request broker architecture though first stable version first usage erlang perl pac aka hierarchical modelviewcontroller lsp solid layering tier tier ui tier business logic ui presentation logic case browser client tier data storage message bus python entityboundaryinteractor architecture aka ebc aka eic ruby delphi java javascript php modelviewpresenter ocp isp dip solid rep crp ccp adp sdp sap aspect oriented programming web service esb enterprise service bus although book coined term published concept already used srp solid domaindriven design modelviewviewmodel port adapter architecture aka hexagonal architecture cqrs e command query responsibility segregation event sourcing onion architecture microservices netflix datacontextinteraction architecture clean architecture model translation chinese qinyusuain like like loading related published july
202,Lobsters,scaling,Scaling and architecture,A New Golden Age for Computer Architecture (2017),https://www.youtube.com/watch?v=3LVeEjsn8Ts,new golden age computer architecture,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature john hennessy david patterson acm turing award lecture youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature john hennessy david patterson acm turing award lecture youtube
203,Lobsters,scaling,Scaling and architecture,Monorepo Architecture Simplified With Bit and NPM,https://blog.bitsrc.io/monorepo-architecture-simplified-with-bit-and-npm-b1354be62870,monorepo architecture simplified bit npm,zero refactoring repository monorepo bit example react app available npm package set file package abstraction smallest function automatic dependency management lerna bit packagespecific buildtest configuration premade collection createextend code discoverability,zero refactoring repository monorepobit used publish set file repository package without refactor repository source code file structure allinstead bit pointed relevant package component module etc repository seamlessly isolate tag version wrap unique environmentthe isolated component module installed npmyarn project imported using bit developmenthere example react app githubyou might missed bit added project used make ui component repo available npm packagesthe repository effectively became multipackage monorepo even though single file codeline changed repository save lot refactoring time effort also useful shared library containing multiple shared componentsmodulesoriginal repository shared set file package abstraction another useful aspect bit abstraction using bit define code found path repository reusable componentso bound package specific directory structure easily turn module core ingredient smallest function package repository structure remain intactthis ability useful sharing many component single repo make bit flexible enough play well tool ecosystemfor example combine bit lerna handle core package repo also easily publish manage hundred smaller automatic dependency managementa basic lerna monorepo structure look like mylernarepo packagejson package packagejson packagejsonas see every package requires packagejson file dependency package defined managedbit us automatic dependency definition mechanism save lot time effort work define set file component using bit add bit run code identify file package dependenciesif detects file dependency either add component turn new dependent component dependency seamlessly managed component bitjson fileif bit detects package dependency component create packagejson file component manage thereyou learn herewhen working inside project new componentpackage version available bit prompt bit status command decide near future able define proactive eventdriven update strategy either way time work savedwith bit constantly define configure dependency chain package repo bit save packagespecific buildtest configurationsone major overheadgenerators around multipackage repo environment build test etc configuration needed every packagebit eliminates overhead letting import premade environment project workspace publishing component choose environment premade collection createextend build test environmentsbit us environment build test every componentmodule individually isolation project result presented component screen bit web uireact spinner shared single repobased process even add cool feature like live playground play component using code discoverability
204,Lobsters,scaling,Scaling and architecture,Goodbye Microservices: From 100s of problem children to 1 superstar,https://segment.com/blog/goodbye-microservices/,goodbye microservices problem child superstar,adopted best practice segment product microservices work worked one hundred type destination headofline blocking case individual repos scaling microservices repos ditching microservices queue centrifuge moving monorepo building resilient test suite yakbak monolith work trade offs conclusion stephen mathieson rick branson achille roussel tom holmes rick branson see segment sign free workspace,unless living rock probably already know microservices architecture du jour coming age alongside trend segment adopted best practice earlyon served u well case soon learn well othersbriefly microservices serviceoriented software architecture serverside application constructed combining many singlepurpose lowfootprint network service touted benefit improved modularity reduced testing burden better functional composition environmental isolation development team autonomy opposite monolithic architecture large amount functionality life single service tested deployed scaled single unitin early reached tipping point core piece segment product seemed falling microservices tree hitting every branch way instead enabling u move faster small team found mired exploding complexity essential benefit architecture became burden velocity plummeted defect rate explodedeventually team found unable make headway fulltime engineer spending time keeping system alive something change post story took step back embraced approach aligned well product requirement need teamwhy microservices work workedsegment customer data infrastructure ingests hundred thousand event per second forward partner apis refer serverside destination one hundred type destination google analytics optimizely custom webhook year back product initially launched architecture simple api ingested event forwarded distributed message queue event case json object generated web mobile app containing information user action sample payload look like following event consumed queue customermanaged setting checked decide destination receive event event sent destination api one another useful developer need send event single endpoint segment api instead building potentially dozen integration segment handle making request every destination endpointif one request destination fails sometimes try sending event later time failure safe retry others retryable error could potentially accepted destination change example http rate limit timeouts nonretryable error request sure never accepted destination example request invalid credential missing required fieldsat point single queue contained newest event well may several retry attempt across destination resulted headofline blocking meaning particular case one destination slowed went retries would flood queue resulting delay across destinationsimagine destination x experiencing temporary issue every request error timeout create large backlog request yet reach destination x also every failed event put back retry queue system would automatically scale response increased load sudden increase queue depth would outpace ability scale resulting delay newest event delivery time destination would increase destination x momentary outage customer rely timeliness delivery afford increase wait time anywhere pipelineto solve headofline blocking problem team created separate service queue destination new architecture consisted additional router process receives inbound event distributes copy event selected destination one destination experienced problem queue would back destination would impacted microservicestyle architecture isolated destination one another crucial one destination experienced issue often dothe case individual reposeach destination api us different request format requiring custom code translate event match format basic example destination x requires sending birthday traitsdob payload whereas api accepts traitsbirthday transformation code destination x would look something like many modern destination endpoint adopted segment request format making transforms relatively simple however transforms complex depending structure destination api example older sprawling destination find shoving value handcrafted xml payloadsinitially destination divided separate service code lived one repo huge point frustration single broken test caused test fail across destination wanted deploy change spend time fixing broken test even change nothing initial change response problem decided break code destination repos destination already broken service transition naturalthe split separate repos allowed u isolate destination test suite easily isolation allowed development team move quickly maintaining destinationsscaling microservices reposas time went added new destination meant new repos ease burden developing maintaining codebases created shared library make common transforms functionality http request handling across destination easier uniformfor example want name user event eventname called destination code shared library check event property key name name exist check first name checking property firstname firstname firstname last name checking case combining two form full namethe shared library made building new destination quick familiarity brought uniform set shared functionality made maintenance le headachehowever new problem began arise testing deploying change shared library impacted destination began require considerable time effort maintain making change improve library knowing test deploy dozen service risky proposition pressed time engineer would include updated version library single destination codebaseover time version shared library began diverge across different destination codebases great benefit reduced customization destination codebase started reverse eventually using different version shared library could built tool automate rolling change point developer productivity suffering began encounter issue microservice architecturethe additional problem service distinct load pattern service would handle handful event per day others handled thousand event per second destination handled small number event operator would manually scale service meet demand whenever unexpected spike loadwhile autoscaling implemented service distinct blend required cpu memory resource made tuning autoscaling configuration art sciencethe number destination continued grow rapidly team adding three destination per month average meant repos queue service microservice architecture operational overhead increased linearly added destination therefore decided take step back rethink entire pipelineditching microservices queuesthe first item list consolidate service single service overhead managing service huge tax team literally losing sleep since common oncall engineer get paged deal load spikeshowever architecture time would made moving single service challenging separate queue per destination worker would check every queue work would added layer complexity destination service comfortable main inspiration centrifuge centrifuge would replace individual queue responsible sending event single monolithic servicemoving monorepogiven would one service made sense move destination code one repo meant merging different dependency test single repo knew going messyfor unique dependency committed one version destination moved destination check dependency using update latest version fixed anything destination broke newer versionswith transition longer needed keep track difference dependency version destination using version significantly reduced complexity across codebase maintaining destination became le time consuming le riskywe also wanted test suite allowed u quickly easily run destination test running test one main blocker making update shared library discussed earlierfortunately destination test similar structure basic unit test verify custom transform logic correct would execute http request partner endpoint verify event showed destination expectedrecall original motivation separating destination codebase repo isolate test failure however turned false advantage test made http request still failing frequency destination separated repos little motivation clean failing test poor hygiene led constant source frustrating technical debt often small change taken hour two would end requiring couple day week completebuilding resilient test suitethe outbound http request destination endpoint test run primary cause failing test unrelated issue like expired credential fail test also knew experience destination endpoint much slower others destination took minute run test destination test suite could take hour runto solve created traffic recorder traffic recorder built top yakbak responsible recording saving destination test traffic whenever test run first time request corresponding response recorded file subsequent test run request response file played back instead requesting destination endpoint file checked repo test consistent across every change test suite longer dependent http request internet test became significantly resilient musthave migration single repoi remember running test every destination first time integrated traffic recorder took millisecond complete running test destination past one destination could taken couple minute complete felt like magicwhy monolith worksonce code destination lived single repo could merged single service every destination living one service developer productivity substantially improved longer deploy service change one shared library one engineer deploy service matter minutesthe proof improved velocity microservice architecture still place made improvement shared library year made improvement made improvement library past month change also benefited operational story every destination living one service good mix cpu memoryintense destination made scaling service meet demand significantly easier large worker pool absorb spike load longer get paged destination process small amount loadtrade offsmoving microservice architecture monolith overall huge improvement however tradeoff fault isolation difficult everything running monolith bug introduced one destination cause service crash service crash destination comprehensive automated testing place test get far currently working much robust way prevent one destination taking entire service still keeping destination monolithinmemory caching le effective previously one service per destination low traffic destination handful process meant inmemory cache control plane data would stay hot cache spread thinly across process much le likely hit could use something like redis solve another point scaling account end accepted loss efficiency given substantial operational benefitsupdating version dependency may break multiple destination moving everything one repo solved previous dependency mess mean want use newest version library potentially update destination work newer version opinion though simplicity approach worth tradeoff comprehensive automated test suite quickly see break newer dependency versionconclusionour initial microservice architecture worked time solving immediate performance issue pipeline isolating destination however set scale lacked proper tooling testing deploying microservices bulk update needed result developer productivity quickly declinedmoving monolith allowed u rid pipeline operational issue significantly increasing developer productivity make transition lightly though knew thing consider going workwe needed rock solid testing suite put everything one repo without would situation originally decided break apart constant failing test hurt productivity past want happening againwe accepted tradeoff inherent monolithic architecture made sure good story around comfortable sacrifice came changewhen deciding microservices monolith different factor consider part infrastructure microservices work well serverside destination perfect example popular trend actually hurt productivity performance turn solution u monoliththe transition monolith made possible stephen mathieson rick branson achille roussel tom holmes many morespecial thanks rick branson helping review edit post every stagesee segment sign free workspace catch demo
205,Lobsters,scaling,Scaling and architecture,MicroServices - Check Size Before Ordering,http://blog.jenkster.com/2018/07/microservices-check-size.html,microservices check size ordering,,recent year seen several startup go microservicesfirst approach two cent distilled watching project unfold netflix amazon one big problem coordinating different team project sooner later hit situation need coordinate release date five different project get something major launched day hosed one subproject always slip five team sit nearidle month burning time money fast forward year two wake sixmonth dropdead release cycle monthfive codefreeze monthfour crunch window horrible never improves coordinating group people hard citation theresa may situation microservices brilliant allow turn humancoordination problem systemcoordination problem system coordination still hard supporting multiple version dealing concurrency issue building distributed faulttolerant system properly computer science hard tractable computer concurrency problem pain solve people concurrency problem nightmare scenario second herdofcats concurrency problem microservices turn people coordination problem software coordination problem microservices way quagmire problem twentyodd developer coordinate talking project coordination problem boil andy catherine ready choosing microservices created api versioning problem created concurrency problem created distributed faulttolerant problem benefit absence relevant people problem microservices create software problem microservices way quagmire
206,Lobsters,scaling,Scaling and architecture,Supercharging Kafka  Enable Realtime Web Streaming by Adding Pushpin,https://hackernoon.com/supercharging-kafka-enable-realtime-web-streaming-by-adding-pushpin-fd62a9809d94,supercharging kafka enable realtime web streaming adding pushpin,supercharging kafka enable realtime web streaming adding pushpin exposing kafka message via public http streaming api matt butler apache kafka source kafka strength realtime data pipeline highthroughput faulttolerant low latency scalability kafka limit pushpin kafka pushpin open source github repo benefit resourceoriented api authentication api management web tier scaleability expose kafka message via http streaming api building kafka serversent event example project work viewspy relaypy first need setup virtualenv install dependency create suitable env kafka pushpin setting run django server run pushpin run relay command repo github,supercharging kafka enable realtime web streaming adding pushpinexposing kafka message via public http streaming apimatt butlerapache kafka new hotness come adding realtime messaging capability system core open source distributed messaging system us publishsubscribe system building realtime data pipeline broadly speaking distributed horizontally scaleable commit login kafka cluster topic producer consumer broker topic categorization group messagesproducers push message kafka topicconsumers pull message kafka topickafka broker kafka nodekafka collection kafka brokerstake deep dive kafka hereoverall kafka provides fast highly scalable redundant messaging publishsubscribe modela pubsub model messaging pattern publisher categorize published message topic without knowledge subscriber would receive message likewise subscriber express interest one topic receive message interest without knowing anything publisher source kafka strengthsas messaging system kafka transformative strength catalyzed rising popularityrealtime data pipeline handle realtime messaging throughput high currencyhighthroughput ability support highvelocity highvolume data per second faulttolerant due distributed nature relatively resistant node failure within clusterlow latency millisecond handle thousand messagesscalability kafka distributed nature allows add additional node without downtime facilitating partitioning replicationkafka limitsdue intrinsic architecture kafka optimized provide api consumer friendly access realtime data many orgs hesitant expose kafka endpoint publiclyin word difficult expose kafka across public api boundary want use traditional protocol like websockets http overcome limit integrate pushpin kafka ecosystem handle traditional protocol expose public api accessible standardized waypushpin kafkaserversent event sse technology browser receives automatic update server via http connection standardized standard kafka natively support protocol need add additional service make happenpushpin primary value prop open source solution enables realtime push requisite evented apis github repo core reverse proxy server make easy implement websocket http streaming http longpolling service structurally pushpin communicates backend web application using regular shortlived http requestsintegrating pushpin kafka provides notable benefit resourceoriented api provides logical resourceoriented api consumer fit existing rest api word expose data standardized moresecure protocolsauthentication reuses existing authentication token data formatsapi management harness existing api management system load balancersweb tier scaleability number web consumer grows substantially may economical performant scale web tier rather kafka clusterin next example expose kafka message via http streaming apibuilding kafka serversent eventsthis example project read message kafka service expose data streaming api using serversent event sse protocol http written using python django relies pushpin managing streaming connectionshow worksin demo drop pushpin instance top kafka broker pushpin act kafka consumer subscribes topic republishes received message connected client client listen event via pushpinmore granularly use viewspy set sse endpoint relaypy handle messaging input outputfirst need setup virtualenv install dependency virtualenv venv venvbinactivatepip install r create suitable env kafka pushpin setting kafkaconsumerconfig bootstrapservers groupid mygroup gripurlhttp run django server python managepy run pushpin pushpin route run relay command relay command set kafka consumer according kafkaconsumerconfig subscribes topic republishes received message pushpin wrapped sse formatclients listen event making request pushpin event topic output stream might look like okcontenttype texteventstreamtransferencoding chunkedconnection transferencodingevent messagedata helloevent messagedata worldrepo githubsubscribe get daily roundup top tech story
207,Lobsters,scaling,Scaling and architecture,Envoy vs NGINX vs HAProxy: Why the open source Ambassador API Gateway chose Envoy,https://blog.getambassador.io/envoy-vs-nginx-vs-haproxy-why-the-open-source-ambassador-api-gateway-chose-envoy-23826aed79ef,envoy v nginx v haproxy open source ambassador api gateway chose envoy,envoy v nginx v haproxy open source ambassador api gateway chose envoy nginx haproxy envoy open source ambassador api gateway world microservices observing proxy battle build kubernetes evaluating proxy haproxy baker street airbnb smartstack fully addressed end joey yelp nginx nginx deployed haproxy nginx together similar tension enterprise edition envoy proxy lyft hot restart eventual consistency exposing dynamic apis configuration would start envoy platform company cloud native computing foundation year later ambassador envoy grpc rate limiting shadowing canary routing authentication envoy update excited year ambassador,envoy v nginx v haproxy open source ambassador api gateway chose envoynginx haproxy envoy battletested proxy end choosing envoy core proxy developed open source ambassador api gateway application deployed kubernetes worldin today cloudcentric world business logic commonly distributed ephemeral microservices service need communicate network core network protocol used service socalled layer protocol eg http grpc kafka mongodb forth protocol build top typical transport layer protocol tcp managing observing crucial cloud application since large part application semantics resiliency dependent trafficthe proxy battleambassador designed get go servicesoriented world u deciding early build kubernetes knew wanted avoid writing proxy considered haproxy nginx envoy possibility level three proxy highly reliable proven proxy envoy newest kid blockevaluating proxieswe started evaluating different feature set three proxy soon realized proxy many way commodity infrastructure proxy outstanding job routing traffic reliably efficiently minimum fuss feature parity felt could implement critical missing feature proxy open source took step back reconsidered evaluation criterion given rough functional parity solution refocused effort evaluating project qualitative lens specifically looked project community velocity philosophy focused community wanted vibrant community could contribute easily related community wanted see project good forward velocity would show project would quickly evolve customer need evolved finally wanted project would align closely possible view microservices worldhaproxyseveral year ago u worked baker street haproxybased clientside load balancer inspired airbnb smartstack haproxy reliable fast proven proxy happy haproxy longerterms concern around haproxy haproxy initially released internet operated differently today velocity haproxy community seem high example added ssl four year experienced challenge hitless reloads able reload configuration without restarting proxy fully addressed end despite epic hack folk like joey yelp haproxy team started catch minimum set feature needed microservices ship november highperformance web server support hitless reloads nginx designed initially web server time evolved support traditional proxy use case nginx two variant nginx plus commercial offering nginx open source per nginx nginx plus extend nginx role frontend load balancer application delivery controller sound perfect unfortunately though since wanted make ambassador open source nginx plus option usnginx open source number limitation including limited observability health check circumvent limitation nginx open source friend yelp actually deployed haproxy nginx togethermore generally nginx forward velocity haproxy concerned many desirable feature would locked away nginx plus nginx business model creates inherent tension open source plus product sure dynamic would play contributed upstream note haproxy similar tension enterprise edition seems le divergence feature set ee ce haproxy envoy proxyenvoy newest proxy list deployed production lyft apple salesforce google others many way release envoy proxy september triggered round furious innovation competition proxy spaceenvoy designed ground microservices feature hitless reloads called hot restart observability resilience advanced load balancing envoy also embraced distributed architecture adopting eventual consistency core design principle exposing dynamic apis configuration traditionally proxy configured using static configuration file envoy supporting static configuration model also allows configuration via grpcprotobuf apis simplifies management scale also allows envoy work better environment ephemeral serviceswe loved feature set envoy forwardthinking vision product also discovered community around envoy unique relative haproxy nginx unlike two proxy envoy owned single commercial entity envoy originally created lyft need lyft make money directly envoy matt klein creator envoy explicitly decided would start envoy platform company commercial pressure proprietary envoy plus envoy enterprise edition community focus right feature best code without commercial consideration finally lyft donated envoy project cloud native computing foundation cncf provides independent home envoy insuring focus building best possible proxy remain unchangeda year later happier decision build ambassador envoy rich feature set allowed u quickly add support grpc rate limiting shadowing canary routing observability name case envoy feature set met requirement eg authentication able work envoy community implement necessary featureswith hundred developer working envoy envoy code base moving forward unbelievable pace excited continue taking advantage envoy ambassador wrote envoy update excited year blog stay tuned continue iterating ambassador article last updated august reflect update envoy community
208,Lobsters,scaling,Scaling and architecture,Extending Magic Pocket Innovation with the first petabyte scale SMR drive deployment,http://blogs.dropbox.com/tech/2018/06/extending-magic-pocket-innovation-with-the-first-petabyte-scale-smr-drive-deployment/,extending magic pocket innovation first petabyte scale smr drive deployment,magic pocket smr using dropbox storage architecture dropbox magic pocket architecture libzbc challenge using smr used rust optimize storage dropbox,magic pocket exabyte scale custom infrastructure built drive efficiency performance dropbox product ongoing platform innovation continually look opportunity increase storage density reduce latency improve reliability lower cost next step evolution new deployment specially configured server filled capacity highdensity smr shingled magnetic recording drive dropbox first major tech company adopt smr technology currently adding hundred petabyte new capacity highdensity server significant cost saving conventional pmr perpendicular magnetic recording drive shelf smr drive reputation slower write conventional drive challenge benefit cost saving denser drive without sacrificing performance new product support active collaboration small team way largest enterprise customer lot data write experience fast initial magic pocket launch attacked problem inventive software server architecture ensure solution matched standard annual data durability availability ambition use expertise software large distributed system enable u take advantage ongoing development drive technology competitor believe future storage innovation including solidstatedrives ssds benefit architectural approach developing smrs investment pay multiple post describe adoption smr hdd technology dropbox storage platform magic pocket discus chosen use smr hardware tradeoff consideration challenge encountered along way smr using conventional perpendicular magnetic recording pmr hdds allow random writes across entire disk shingled magnetic recording smr hdds offer increased density sacrificing random writes forced sequential writes squeezing track smr disk together cause head erase next track small conventional area outside diameter allows caching random writes well using ssd smr hdds offer greater bit density better cost structure gb decreasing total cost ownership denser hardware goal build highest density storage server smr currently provides highest capacity ahead traditional storage alternative pmr three type smr hdds consider drivedevice managed host aware host managed smr disk originally evaluated host aware host managed smr disk finally settled host managed disk fleet drive device managed smr disk allow host treat like conventional drive nonsequential writes buffered small conventional area disk later transcribed sequential zone involves reading data sequential zone writing original data merged new data back sequential zone host aware drive allow host understands smr disk control writing sequential zone host open close zone monitor write pointer partially write sequential zone avoid conventional area caching performance bottleneck caused rewriting zone host aware drive offer control drive managed smr defining priority host managed smr drive require host manage sequential zone drive copying new data sequential zone caching data conventional area host must explicitly open fill close sequential zone host managed smr offer control way data stored drive consistent built thing dropbox storage architecture magic pocket mp store user data block max size block organized extent mp platform operates accordingly since extent written appendonly fashion immutable sequential writes smr ideal mp workload check blog post indepth overview dropbox magic pocket architecture data center implementing smr disk number hardware tradeoff consider worked optimize performance data transfer speed also needed consider hardware reliability total cost ownership required u look every element hardware stack chassis density latest design fit approximately lff large form factor disk single chassis make densest storage system production lff disk per chassis design constraint physical limit rack space standard datacenter rack requirement stay depth limit keep u needing design custom datacenter rack keep rack limited housing fully configured chassis avoid deviating standard datacenter flooring specification memory cpu one thing came testing decision increase memory per host keep inmemory index block offsetslength disk smr drive significantly increased capacity individual chassis machine store time block chassis previous architecture mean block index need proportional increase memory resulting per machine also slightly upgrade cpu moving core thread per chassis additional processing power necessary keep total chassis io performance writes read sa controller order improve reliability reduce complexity moved raid controller host bus adapter hba initial benefit using raid leverage cache reduce write latency proved costly endeavor lot overhead creating individual raid managing associated firmware bug raid controller expose single block device also worked enable direct io reduce cpu usage doublebuffering added benefit removing overhead creating many raid device cutting overall provisioning time storage system hour total quick minute allows u focus realizing technology le time setting adding hba simplified architecture expense initial cache device understood emerging technology exploration unknown order u reduce amount exposure focused removing complexity success criterion cache removal raid controller discovered needed compensate loss write cache solution add caching layer maintain performance requirement ssd drive could compensate decision remove raid controller previous generation storage system data network directly written drive writing large smr drive timeconsuming needed ensure network get stalled drive busy make process asynchronous nonblocking added ssd cache data lazily flushed smr disk background design work u seeing density increase saturating sata bus need use another transfer protocol discovered pushing limit sata bus become bottleneck see ssds future generation likely nvme design caching network magic pocket started lower density chassis around moved density time network speed increased compromise recovery time density lever lower tco using smr disk put new chassis per host level storage density required another increase network bandwidth assist system recovery comfortable acceptable increase failure domain long recovery time meet sla decided needed design smr based chassis nic card per chassis nonblocking clos fabric network uplink benefit gained ability quickly add data chassis upon deployment ability quickly drain chassis time repair ensuring magic pocket meet sla per magic pocket design object storage device osd daemon behaves similar key value store optimized largesized value run one daemon per disk per machine daemon access disk osd treat disk block device directly manages data layout drive using filesystem smr able optimize head movement prioritize disk io operation based type fully within software stack communicate smr disk use libzbc basis disk io smr store metadata index sequential zone got lucky two factor first capacity size evenly divide across zone logical space x divisible excess space would lost required invasive change reclaim space second ratio metadata block data fit well ratio conventional area sequential area discovered large writes much better smr averaging mb optimize writes buffered certain stage originally tried flushing ssd smr soon possible multiple small writes efficient moved model buffering le writes larger size osd redesign rpc route live put ssd caching get background writes sent directly queue disk operation data stored smr highest priority managing live traffic live traffic made incoming new block writes support user data one biggest challenge latency writes disk must sequential aligned boundary however live data come always fit neat chunk using staging area come rescue background process flushing block ssd disk take care alignment making sure large writes managing background repair require huge amount read writes le timecritical occur slowly challenge using smr chief challenge making workload smrcompatible taking random readwrite activity making sequential accomplish rewrote osd metadata frequently updated kept conventional area smr disk readwrite writes supported immutable block data kept sequential zone needed overcome dealing sequential writes smr disk accomplished key workarounds example use ssd staging area live writes flushing disk background since ssd limited write endurance leverage memory staging area background operation implementation software prioritizes live userfacing readwrites background job readwrites improved performance batching writes larger chunk avoids flushing writes often moving go rust also allowed u handle disk larger disk without increased cpu memory cost able directly control memory allocation garbage collection check presentation used rust optimize storage dropbox learn ongoing collaboration hardware partner leveraged cutting edge technology ensure entire chain component compatible configuration used expander distribute hba controller allowing hba evenly spread connection drive however expander initially incompatible smr case others like collaborated vendor codeveloped firmware needed create functional harmony within hardware chain one mechanical challenge average drive chassis limited many hardware variation make bottleneck discovered space limiter looking component fit system design present new challenge future new storage design give u ability work future iteration disk technology immediate future plan focus density design efficient way handle large traffic volume total number drive pushing physical limit form factor design take consideration potential failure much data system improving efficacy compute system committed iterating improving magic pocket dropbox infrastructure deployment one step along way exciting challenging journey introducing new storage technology reliable way journey involved thinking mechanical structure system also major software update would required without collaboration engineering team would possible infrastructure benefit twofold thanks greater density better cost structure unleashing dropbox user creative energy debugging data center project contributor chris dudte victor li preslav le jennifer basalone alexander sosa rajat goel ashley clark james turner vlad seliverstov sujay jayakar ramus aljamal
209,Lobsters,scaling,Scaling and architecture,Lessons from Building Observability Tools at Netflix,https://medium.com/netflix-techblog/lessons-from-building-observability-tools-at-netflix-7cfafed6ab17,lesson building observability tool netflix,lesson building observability tool netflix scaling log ingestion mantis distributed request tracing dapper paper grpc analysis metric atlas data persistence cassandra replica shard one replica tailoring user interface different user group conclusion reach,lesson building observability tool netflixour mission netflix deliver joy member providing highquality content presented delightful experience constantly innovating product rapid pace pursuit mission innovation span personalized title recommendation infrastructure application feature like downloading customer profile growing global member base million member choose enjoy service thousand type device also consider scale variety content maintaining quality experience member interesting challenge tackle challenge developing observability tool infrastructure measure customer experience analyze measurement derive meaningful insight higherlevel conclusion raw data observability mean analysis log trace metric post share following lesson learned point business growth learned storing raw application log scale address scalability switched streaming log filtering selected criterion transforming memory persisting neededas application migrated microservices architecture needed way gain insight complex decision microservices making distributed request tracing start sufficient fully understand application behavior reason issue augmenting request trace application context intelligent conclusion also necessarybesides analysis logging request trace observability also includes analysis metric exploring metric anomaly detection metric correlation learned define actionable alerting beyond threshold alertingour observability tool need access various persisted data type choosing kind database store given data type depends particular data type written retrieveddata presentation requirement vary widely team user critical understand user deliver view tailored user profilescaling log ingestionwe started tooling effort providing visibility device server log user go one tool instead use separate dataspecific tool logging server providing visibility log valuable log message include important contextual information especially error occurhowever point business growth storing device server log scale increasing volume log data caused storage cost balloon query time increase besides reducing storage retention time period addressed scalability implementing realtime stream processing platform called mantis instead saving log persistent storage mantis enables user stream log memory keep log match sqllike query criterion user also choice transform save matching log persistent storage query retrieves sample playback start event apple ipad shown following screenshot mantis query result sample playback start eventsonce user obtains initial set sample iteratively refine query narrow specific set sample example perhaps root cause issue found sample specific country case user submit another query retrieve sample countrythe key takeaway storing log persistent storage scale term cost acceptable query response time architecture leverage realtime event stream provides ability quickly iteratively identify relevant subset log one way address problemdistributed request tracingas application migrated microservices architecture needed insight complex decision microservices making approach would correlate decision inspired google dapper paper distributed request tracing embarked implementing request tracing way address need since interprocess communication us http grpc trend newer service use grpc benefit binary protocol implemented request interceptor http grpc call interceptor publish trace data apache kafka consuming process writes trace data persistent storagethe following screenshot show sample request trace single request result calling second tier server one call thirdtier server sample request tracethe smaller square beneath server indicate individual operation graycolored server tracing enableda distributed request trace provides basic utility term showing call graph basic latency information unique approach allow application add additional identifier trace data multiple trace grouped together across service example playback request trace request relevant given playback session grouped together using playback session identifier also implemented additional logic module called analyzer answer common troubleshooting question continuing example question playback session might given session receive video video offered high dynamic rangeour goal increase effectiveness tool providing richer relevant context started implementing machine learning analysis error log associated playback session analysis basic clustering display common log attribute netflix application version number display information along request trace example given playback session error log noticed similar device error netflix application version number display application version number user found additional contextual information helpful finding root cause playback errorin summary key learning effort tying multiple request trace logical concept playback session case providing additional context based constituent trace enables user quickly determine root cause streaming issue may involve multiple system case able take step adding logic determines root cause provides english explanation user interfaceanalysis metricsbesides analysis logging request trace observability also involves analysis metric user examine many log overwhelming extended offering publishing log error count metric monitoring system called atlas enables user quickly see macrolevel error trend using multiple dimension device type customer geographical location alerting system also allows user receive alert given metric exceeds defined threshold addition using mantis user define metric derived matching log publish atlasnext implemented statistical algorithm detect anomaly metric trend comparing current trend baseline trend also working correlating metric related microservices work anomaly detection metric correlation learned define actionable alerting beyond basic threshold alerting future blog post discus effortsdata persistencewe store data used tool cassandra elasticsearch hive chose specific database based primarily user want retrieve given data type write rate observability data always retrieved primary key time range use cassandra data need queried one field use elasticsearch since multiple field within given record easily indexed finally observed recent data last week accessed frequently older data since user troubleshoot recent issue serve use case someone want access older data also persist log hive longer time periodcassandra elasticsearch hive advantage disadvantage term cost latency queryability cassandra provides best highest perrecord write read rate restrictive read must decide use row key unique identifier given record within row use column key timestamp contrast elasticsearch hive provide flexibility read elasticsearch allows index field within record hive sqllike query language allows match field within record however since elasticsearch primarily optimized free text search indexing overhead writes demand computing node write rate increase example one observability data set initially stored data elasticsearch able easily index one field per record write rate increased indexing time became long enough either data available user queried took long data returned result migrated cassandra shorter write ingestion time shorter data retrieval time defined data retrieval three unique key serve current data retrieval use casesfor hive since record stored file read relatively much slower cassandra elasticsearch hive must scan file regarding storage computing cost hive cheapest multiple record kept single file data replicated elasticsearch likely next expensive option depending write ingestion rate elasticsearch also configured replica shard enable higher read throughput cassandra likely expensive since encourages replicating record one replica order ensure reliability fault tolerancetailoring user interface different user groupsas usage observability tool grows user continually asking new feature new feature request involve displaying data view customized specific user group device developer server developer customer service given page one tool user want see type data page offer whereas user want see subset total data set addressed requirement making page customizable via persisted user preference example given table data user want ability choose column want see meet requirement user store list visible column table another example involves log type large payload loading log customer account increase page loading time since subset user interested log type made loading log user preferenceexamining given log type may require domain expertise user may example given log netflix device understanding data log requires knowledge identifier error code string key tool try minimize specialized knowledge required effectively diagnose problem joining identifier data refer providing description error code string keysin short learning customized view helpful context provided visualization surface relevant information critical communicating insight effectively usersconclusionour observability tool empowered many team within netflix better understand experience delivering customer quickly troubleshoot issue across various facet device title geographical location client app version tool essential part operational debugging toolkit engineer netflix evolves grows want continue provide engineer ability innovate rapidly bring joy customer future blog post dive technical architecture share result ongoing effort metric analysis using machine learning log analysisif work sound exciting please reach u kevin lew sangeeta narayanan sangeetan
210,Lobsters,scaling,Scaling and architecture,Caching beyond RAM: the case for NVMe,https://memcached.org/blog/nvm-caching/,caching beyond ram case nvme,caching beyond ram case nvme dormando june post explore design ramification increasing cost ram caching system memcached page story tutorial storage system called extstore accelerate optane cache ram breakdown example cache memory used double breaking item memory important even larger item eat ram quickly extstore help workload variation okay test setup mccrusher third tag testoptane script latency throughput measurement test result million key per second million key per second extstore io thread many extstore io thread want push boundary extstore learning conclusion extstore requires ram per item disk dram cost optane ssd reducing ram reduces reliance multisocket server,caching beyond ram case nvme dormando june caching architecture every layer stack embody implicit tradeoff performance cost tradeoff however constantly shifting new inflection point emerge alongside advance storage technology change workload pattern fluctuation hardware supply demand post explore design ramification increasing cost ram caching system ram always expensive dram price risen high density ram involve multisocket numa machine bloating power overall cost concurrently alternative storage technology flash optane continue improve specialized hardware interface consistent performance high density relatively low cost increasing economic incentive explore offloading caching ram onto nvme nvm device implication performance still widely understood explore design implication context memcached distributed simple cachefocused keyvalue store quick overview see page story tutorial memcached ram backed keyvalue cache act large distributed hash table data lifetime governed lru oldest untouched data evicted make room fresh data low latency submillisecond high throughput important page may request data memcached several time cause time stack quickly memcached huge cost reduction cutting query backend system either database flashdisk drive cpubound code templating rendering memcached storage system called extstore allows keeping portion data le recently used key disk freeing ram see link full breakdown work short key stay ram value split disk recent frequently accessed key still value ram test done help accelerate optane provided hardware guidance also thanks netflix adoption extstore great feedback cache ram breakdown example database could data active given time period say hour want cache active data might need ram ram might highly utilized cache memory used ram could responsible hit cache rest ram amount however cut ram usage miss rate would least double doubling load db depending backend system performs losing ram would double backend cost breaking item memory might find vast majority case live within ram smaller number large still important item take even larger item eat ram quickly keep mind larger item significant percentage network utilization one request take bandwidth small one extstore help splitting value large le recently used item key pointing onto disk save majority lessused ram depending use case reduce overall ram cut le lot larger item increase hit ratio ram move large item disk cache longer tail small item increase hit ratio reduce backend cost reduce server count network spare handle loss broken server cut size cache fleet increase overall cache easily add hundred gigabyte cache capacity per server cache object used expensive create new pool larger object caching precomputed data big data store machine learning training data workload variation okay example cache hit evenly distributed theoretical system io limit similar high end ssd optane drive case ram relied saturate network iop byte average necessary saturate nic properly designed cluster extra headroom necessary growth usage spike failure within pool mean item size byte average might possible however assuming byte overhead per key extstore able store disk could fit ram careful capacity planning required many lost machine tolerated dead machine take percentage cache much network bandwidth necessary reducing server count make network dense many iop necessary access recent data reducing reliance disk latency guarantee necessary cache based disk lookup still lot faster backend long item live ssd tolerate certain amount writes burning everyone workload compatible external storage carefully evaluate ram used cache pool using disk cache exclusively small item short ttl high write rate ram still cheaper calculating done monitoring ssd tolerance drive writes per day device could survive year writes per hour tolerance dwpd optane high tolerance high end flash drive test setup test done intel xeon machine sporting core ram ssd optane drive one optane drive used test writing extstore work one drive configuration reflects user mccrusher used run test specifically third tag containing testoptane script mccrusher designed primarily run fast possible parse response stack many query per syscall make attempt time anything test run localhost though never used single core cpu testoptane script specifically describes configuration used test memcached configured use worker thread machine core hyperthreads balloon program mccrusher used take ram million key loaded memcached avoid extstore simply using buffer pool test run number mccrusher client varied well number extstore io thread server io thread saturate device many overload cause queue test run minute warmup latency throughput measurement since mccrusher time result two script used generate result data benchsample periodically run stats command memcached using counter determine average throughput data sampled every second inspected significant standard deviation latencysample script pretend blocking memcached client sample request time time benchsample running used avoid trap like percentile remove outlier grouping causing misleading result every test full breakdown latency sample provided sampling done maximum rate one per millisecond note event loop used avoid determine time elapsed time waiting processed stack event happen time test three general test done ascii multiget mode allows extstore use fewest packet generate response well heavily pipeline request internally lower latency device reach higher throughput easily test pipelined get many get request stacked packet extstore service request independently test extstore easily able saturate o ability serve buffered io kswapd kernel thread maxed latency graph show optane able keep latency flash drive higher client load pipelined get may look odd need research likely caused queuing internally since mccrusher aggressive optane drive able saturate system much fewer io thread crusher client production workload optane provide much consistent low latency service multiget pipelined set previous two workload readonly test set also done memcached rate roughly extstore flushing drive time read happening optane come strong result unfortunately wobbling graph due leaving little ram free test optane performance consistent o struggled keep reference pure ram multiget load test exact configuration memcached thread etc result million key per second contrived benchmark gotten server many core past million key per second extstore io thread optane drive able come much closer saturating io limit thread client optane ssd latency breakdown show ssd typically order magnitude higher wait time optane staying bucket ssd slipping many extstore io thread underlying o becomes saturated causing wobble queueing optane graph meanwhile ssd continues benefit extra thread resource needed overcome extra latency flash many workload ssd optane completely viable bulk read still come ram extstore used service long tail large object keep request response time want push boundary extstore drive like optane go long way high write tolerance go well cache workload low latency help smooth tradeoff requesting cache data disk small size currently adventageous extstore requires ram every value disk disk requires lot le ram particular machine probably dense allow safe failover avoid nic saturation test type batched multiget pipelined get batched multiget pipelined set ssd io thread optane io thread learning extstore flusher background thread combined code manages lru benchmark set consistently sent server cause starvation extstore flushing production insert rate memcached tend come wave even small millisecond across keep consistent performance thread latency sampling tough current script provides useful data much better program would pace one request every millisecond onto pool blocking thread allowing u determine server pausing request simply slow full timing every sample also saved graphed would visualize clustering response come underlying o drive buffered io limitation known ahead time workload order hundred thousand operation per second much le ram extstore focused stability time eventually direct io async io able better utilize device high load extstore bucketing could allow interesting mix optane along traditional flash internally extstore organizes data page disk space typically new item clustered particular page item short ttl clustered together item survive page compaction clustered well reduces need compaction time new item andor short ttl item could rest optane drive compacted item could sit flash drive providing even greater cost saving conclusion workload currently possible due cost possible workload containing mixed data size large pool significant cost reduction extstore requires ram per item disk chart assuming byte overhead per item key metadata visualizes ram overhead fall item size get larger dram cost optane ssd depending drive cache shift ram optane flash money spent purely ram drop reducing ram reduces reliance multisocket server get high ram density numa capable machine often necessary multiple socket multiple cpu half ram attached cpu since memcached highly efficient cut ram well cut half motherboardcpu even power cost ram reduced cost reduction specific workload reasonable high speed low latency ssd open new era database cache design demonstrate high performance number wide variety use case reduction cost expansion cache usage
211,Lobsters,scaling,Scaling and architecture,Missing the point about microservices  its about testing and deploying independently,https://erikbern.com/2018/06/04/missing-the-point-about-microservices.html,missing point microservices testing deploying independently,missing point microservices testing deploying independently iteration speed written gingerbread man strategy let talk testing deploying microservice benefit able test deploy independently course thing get harder steve yegge epic rant questionable reason consider microservices torvaldstennenbaum flame war summary tagged software startup,missing point microservices testing deploying independently ok first preface whole blog post thing really struggle term microservices ca nt put finger exactly maybe term hopelessly illdefined maybe gotten picked hype train whatever stick type terminology let roll blog post might mildly controversial throwing itchy feeling long ca nt get rid respect want disagree vehemently maybe something u learn weird story first real company spotify used serviceoriented architecture scratch also spent time google used serviceoriented architecture basically since continuously working people call microservice architecture nt even occur people might want build thing monolith guess coming different direction many either way particular nonstandard reason spotify google get back later let start talking iteration speed iteration speed sort obsessed iteration speed written past deserves post future quick summary iteration speed always going strongest competitive advantage industry ca nt really patent anything proprietary technology often much le valuable company would like admit start shipping new feature quicker learn faster user run faster competitor aka gingerbread man strategy let talk testing deploying course many way iterate faster today let focus two particular aspect testing deploying often big proponent continuous deployment also huge proponent fast test suite reading far without graphic deserve one tracing back dependency fit together look like something like head seems like could improve lot thing could test deploy thing faster course long series step get fully automated test write come back deploys automated come back deploying multiple time per day figure get come back anyway trick test deploy thing maybe splitting thing small independent way microservice benefit able test deploy independently clear splitting thing make sense regardless large x nt expect breaking sweet old monolith two service derive tremendous value rarely valuable unless test deploy part independently see many blog post people missing point need deploy two service production tandem thing wrong need run two service together order run test thing wrong end microservice ca nt tested isolation thing wrong end microservice ca nt deployed isolation thing wrong wrong thing putting ton work separating thing independent unit without reaping benefit fast testing deploying cycle course thing get harder going dwell much written including steve yegge epic rant testing thing isolation mean part need make assumption part behave mock properly deploying new version api call annoying done multiple smaller step tracing request massive pain could go day american fond saying free lunch questionable reason consider microservices mentioned value able test deploy thing independently think benefit fairly marginal best writing service different language think argument mostly invoked junior dev want implement new system clojure great news poor person waking getting paged shopping cart service forcing application independent piece nt sprawl cobweb interdependency used think super strong argument clearly huge monolithic code base great linux kernel show write highly modular code inside single process actually torvaldstennenbaum flame war still highly relevant scaling two piece software independently necessarily strong reason since also scale fat binary look facebook breaking software different performance characteristic could occasionally valid argument say nodebased webserver need something cpu heavy could many case solved something like background thread mode codebase run worker process web server process summary obsessed iteration speed could write blog post takeaway want leave automated testing awesome continuous deployment really sweet two thing first see engineer starting twiddle thumb waiting test run know time right split thing microservices awesome keep splitting longer see engineer twiddling thumb test blazingly fast lean back relax watch company outiterate competitor superior development process tagged software startup
212,Lobsters,scaling,Scaling and architecture,The Original Serverless Architecture is Still Here,http://engineering.khanacademy.org/posts/original-serverless.htm,original serverless architecture still,redirecting http blogkhanacademyorgtheoriginalserverlessarchitectureisstillhere,redirecting http blogkhanacademyorgtheoriginalserverlessarchitectureisstillhere
213,Lobsters,scaling,Scaling and architecture,"Random Slicing: Efficient and Scalable Data Placement for Large-Scale Storage Systems, Miranda et al",http://hpc.ac.upc.edu/PDFs/dir05/file004529.pdf,random slicing efficient scalable data placement largescale storage system miranda et al,,obj endobj obj filterflatedecodeid index info rlength rsize stream b qm endstream endobj startxref eof obj stream b rq e h endstream endobj obj endobj obj endobj obj endobj obj stream vx n g endstream endobj obj stream z l l
214,Lobsters,scaling,Scaling and architecture,Producing 200 OpenStreetMap extracts in 35 minutes using a scalable data workflow,https://www.interline.io/blog/scaling-openstreetmap-data-workflows/,producing openstreetmap extract minute using scalable data workflow,osm extract osm extract interline openstreetmap planetutils planetopenstreetmaporg citiesjson planetutils readme valhalla tilepacks valhalla routing engine kubernetes argo workflow manager kubernetes pod docker container next step workflow osm extract valhalla tilepacks consulting service,interline offer osm extract service enabling software developer gi professional download chunk openstreetmap data major city region around worldosm extract simple step process run describe step well run step extract parallel interline cloud infrastructure screenshot osm extract interlineevery day interline server download latest update openstreetmap update local copy planet file process data generate variety geospatial data product reliably updating planet file requires bit legwork behind scene encapsulated helper script planetutils library simplifies process step run osmplanetupdate download recent osm planet file released weekly planetopenstreetmaporg mirror apply minutely diffs update planet file current point timefetch copy citiesjson file github geojson file contains polygon defining boundary extract continuation extract definition created contributor mapzen metro extract service file open addition revision allrun osmplanetextract specifying local copy citiesjson file generate extract citiesregionsbehind scene planetutils handle call osmosis osmconvert update planet generate extract information command see planetutils readme install package using homebrew dockerthis workflow simple slow updating planet generating extract single machine take day addition extract updated planet starting point additional data product valhalla tilepacks valhalla tilepacks combine osm planet file elevation data produce downloadable data organization run instance valhalla routing engine heavy lifting requires substantial mix computing resource parallel workflow rescue osm extract valhalla tilepacks generated using argo workflow screenshot argo management dashboard simplify image extract job shownat interline great appreciation kubernetes power much infrastructure kubernetes ideal running highly available service managing complex resource wanted data workflow system leverage infrastructure introduce little additional complexity possible constraint found argo workflow manager great fit need argo workflow step kubernetes pod docker container simplifying resource request access secret eg password access key external service workflow described directed graph easy syntax specifying dependency stepsargo provides flexible method specifying output file parameter fed subsequent stepsparallel workflow well supported one step fan number parallel task see diagram argo workflow resembles simple process key adjustment update local planet using osmplanetupdate copy cloud storage bucketfetch citiesjson file divide geojson feature smaller chunk extract eachrun chunk parallel task quickly generate extractsbecause argo task defined kubernetes pod finegrained control resource allocated task kubernetes scheduler effective autoscaling cluster based quickly increasing cluster size run task parallel importantly reducing cluster size task complete additionally task cpu intensive others memory intensive define multiple node pool ensure high utilization cluster resource currently run task extract task requesting cpu node generating extract parallel take minute workflow also allows u run multiple data pipeline together sharing common output reducing amount work duplicated instance updated planet file generated first step also used input valhalla workflow next step workflowour strategy working openstreetmap data source snout tail look opportunity turn piece giant data download useful meal user working additional input output interline workflow tuning workflow also always work process week find way improve performance step look forward sharing update workflow futurein meantime like invite add next step professional workflow work openstreetmap data sign free osm extract developer previewif organization need deploy worldwide routing engine premise cloud account evaluate valhalla tilepacksif organization run custom data workflow need help consider workflow approach contact u discus interline consulting service glad share experience argo kubernetes related technology
215,Lobsters,scaling,Scaling and architecture,Command-line Tools can be 235x Faster than your Hadoop Cluster,https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html,commandline tool faster hadoop cluster,introduction tom hayden amazon elastic map reduce mrjob millionbase archive storm learn data wikipedia acquire sample data rozim build processing pipeline parallelize bottleneck mawk conclusion,introductionas browsing web catching site visit periodically found cool article tom hayden using amazon elastic map reduce emr mrjob order compute statistic winloss ratio chess game downloaded millionbase archive generally fun emr since data volume containing around million chess game skeptical using hadoop task understand goal learning fun mrjob emr since problem basically look result line file aggregate different result seems ideally suited stream processing shell command tried amount data able use laptop get result second processing speed hadoop processing took minute processing speed reporting time required process data machine cluster took minute tom remarksthis probably better would take run serially machine probably good kind clever multithreaded application locallythis absolutely correct although even serial processing may beat minute although tom project fun often people use hadoop socalled big data tool realworld processing analysis job done faster simpler tool different techniquesone especially underused approach data processing using standard shell tool command benefit approach massive since creating data pipeline shell command mean processing step done parallel basically like storm cluster local machine even concept spout bolt sink transfer shell pipe command pretty easily construct stream processing pipeline basic command extremely good performance compared many modern big data toolsan additional point batch versus streaming analysis approach tom mention beginning piece loading game analysis locally get bit short memory game data loaded ram analysis however considering problem bit easily solved streaming analysis requires basically memory resulting stream processing pipeline create time faster hadoop implementation use virtually memorylearn datathe first step pipeline get data pgn file since idea kind format checked wikipedia event f return match site belgrade serbia yugoslaviajug date round white fischer robert j black spassky boris v result move game follow interested result game real outcome case mean white case mean black case mean game draw also case meaning game ongoing scored ignore purposesacquire sample datathe first thing get lot game data proved difficult thought would looking around online found git repository github rozim plenty game used compile set data twice tom used test next step get data pipelinebuild processing pipelineif following along timing processing forget clear o page cache otherwise get valid processing timesshell command great data processing pipeline get parallelism free proof try simple example terminalsleep echo hello world intuitively may seem sleep second print hello world fact step done time basic fact offer great speedup simple noniobound processing system capable running single machinebefore starting analysis pipeline good get reference fast could simply dump data devnullin case take second go would kind upperbound quickly data could processed system due io constraintsnow start analysis pipeline first step using cat generate stream datasince result line file interesting simply scan data file pick line containing result grepcat pgn grep result give u result line file want simply use sort uniq command order get list unique item file along countscat pgn grep result sort uniq cthis straightforward analysis pipeline give u result second certainly better assuming linear scaling would taken hadoop cluster approximately minute processin order reduce speed take sort uniq step pipeline replace awk wonderful toollanguage eventbased data processingcat pgn grep result awk split re substr length re white re black re draw end print whiteblackdraw white black draw take result record split hyphen take character immediately left case win black case win white case draw note builtin variable represents entire recordthis reduces running time approximately second since processing twice much data speedup around timesso even point already speedup around naive local solution additionally memory usage effectively zero since data stored actual count incrementing integer almost free memory space term however looking htop running show grep currently bottleneck full usage single cpu coreparallelize bottlenecksthis problem unused core fixed wonderful xargs command allow u parallelize grep since xargs expects input certain way safer easier use find argument order make sure file name passed xargs nullterminated corresponding tell xargs expected nullterminated input additionally n many input give process p indicates number process run parallel also important aware parallel pipeline guarantee delivery order problem used dealing distributed processing system f grep indicates matching fixed string fancy regex offer small speedup notice testingfind type f name pgn xargs grep f result gawk split re substr length re white re black re draw end print nr white black draw result run time second additional reduction processing time parallelizing grep step pipeline get u approximately time faster hadoop implementationalthough improved performance dramatically parallelizing grep step pipeline actually remove entirely awk filter input record line case operate containing string result find type f name pgn xargs awk result split re substr length re white re black re draw end print whiteblackdraw white black draw may think would correct solution output result file individually want aggregate together resulting correct implementation conceptually similar mapreduce implementation would befind type f name pgn xargs awk result split re substr length re white re black re draw end print whiteblackdraw white black draw awk game white black draw end print game white black draw adding second awk step end obtain aggregated game information desiredthis improves speed dramatically achieving running time second time faster hadoop implementationhowever make bit faster still using mawk often dropin replacement gawk offer better performancefind type f name pgn xargs mawk result split re substr length re white re black re draw end print whiteblackdraw white black draw mawk game white black draw end print game white black draw find xargs mawk mawk pipeline get u runtime second around time faster hadoop implementationconclusionhopefully illustrated point using abusing tool like hadoop data processing task better accomplished single machine simple shell command tool huge amount data really need distributed processing tool like hadoop may required often day see hadoop used traditional relational database solution would far better term performance cost implementation ongoing maintenance
216,Lobsters,scaling,Scaling and architecture,"Prometheus: The Good, the Bad, and the Ugly",https://timber.io//blog/prometheus-the-good-the-bad-and-the-ugly,prometheus good bad ugly,http hexpmpackagesprometheusex good writeup counterargument,operational insight critical success software deployment timber exception need know service delivered consistent basis user anomaly reported team quickly multitude solution exist gaining type insight benefit tradeoff well cost considerationsat timber use product analyze log offer u rich powerful insight granular health application time recognize need higherlevel view system functioning whole type view better served quantitative data tested sla internal expectation threshold took responsibility strategy implementation view late timberwhile developing strategy would use approach visibility briefly considered hosted solution decided based evaluation infrastructure potential cost left considering selfhosted solution including influxdb statsd riemann opentsdb graphite prometheusi also looking solution would grow system nt require highdegree specialized knowledge opentsdb documentation start first need setup hbase maintaining hbase cluster operational metric alone worth psychological temporal tollanother key concern got data service core service written elixir yet many integration popular metric protocol otherwise integration poorly maintained influxdb prometheus ended major competitor offered standalone binary require management external data system allowed rich metadata metric two key difference offering prometheus pull data service influxdb need data pushed influxdb instanceinfluxdb collect every data point prometheus collect summary data pointsboth point benefit tradeoff collecting every data point influxdb support complex highresolution query cost higher network traffic larger ondisk storage pushing data influxdb mean origin system located anywhere whereas prometheus ingests data scraping metric summary plain text format http endpoint every serverreasoning difference found drawn towards prometheus better solution team prometheus rich set communitysupported library elixir http hexpmpackagesprometheusex prometheus make testing metric easy since view http endpoint scrape development need run prometheus locally mock destinationhere example summary format prometheus scrape host type erlangvmprocesscount gauge help erlangvmprocesscount number process currently existing local node erlangvmprocesscount type erlangvmprocesslimit gauge help erlangvmprocesslimit maximum number simultaneously existing process local node erlangvmprocesslimit overall fact prometheus summarizes data nt hurt u even benefited u recover easily temporary network failure issue also requires le configuration server since server nt need know prometheus instance located instead prometheus find appropriate server querying amazon apii set instance running expose metric dedicated port allows u maintain finegrained security rule access instance limit likelihood metric would exposed outside world even public facing system also run prometheus node exporter every machine collect system level metricsthe major detractor found prometheus pull system service running heroku multiple dynos heroku nt expose dynos outside world except via loadbalancer way prometheus reliably scrape dynos loadbalancer randomly assign dyno respond scrape request mean prometheus service appears single host erratic difference metric scrape future plan solve moving system aws account ecosystem alternative would use prometheus push gateway allows service push data dynos centralized host prometheus reliably scrape fromprometheus overall huge boon u allows u understand aggregate view system operational health also letting u drill health status individual component also powerful builtin alerting system integrate pagerduty allow u quickly react abnormal situation still number wart however make cumbersome use teach team memberschief among prometheus obstacle query language promql difficult understand requires comprehension prometheus data structure seem basis linear algebra understanding combine reduce data form need tricky documentation difficult approach example use following query break number http response status code sum statuscode irate httprequeststotal job collector method post requestpath frame beyond phrase sum assumption query readily comprehensible people make difficult formulate maintain query well introduce new team member systemanother difficulty working prometheus come histogram data type jack neely good writeup blog linuxczar various issue surrounding prometheus histogram implementation reliable insight written part prometheus contributor brian brazil counterargument histogram cumulative regardless operational cost brazil point though found immensely difficult forecast write code correct bucket histogram fact rebucketed histogram occasion woefully initial assumption bucketseven difficulty though prometheus given u invaluable view product allows u monitor system whole prometheus allows u understand thing go wrong go log know log understand
217,Lobsters,scaling,Scaling and architecture,Axioms of Web architecture,https://www.w3.org/DesignIssues/Principles.html,axiom web architecture,design issue principle design carpenter evolvability tolerance decentralization evolution b carpenter editor architectural principle internet follow science insecurity design issue tim bl,tim bernerslee date last change date status personal view editing status first draft design issue principle design fall back folklore principle good design sometimes need uri started collection written many place principle simplicity modularity stuff software engineering decentralization tolerance life breath internet brian carpenter enumerated principle design net carpenter third pair idea found commonly useful web mentioned keynote note evolvability largely motherhood apple pie still need home keep simple stupid simplicity easily quote often ignored strange way perhaps eye beholder language us fewer basic element achieve power simpler sometimes simplicity confused easy understand example twoline solution us recursion pretty simple even though people might find easier work though solution avoids recursion xml processing instruction thing start simple look simple extra sort thing language complicate clean design element attribute complication underlying syntax great effect specification refer xml processing figure processing instruction well element design system language feature broken relatively loosely bound group relatively closely bound feature division good thing made part design good engineering mean want change system luck future change one part require understand test part allow people independently change part time classic good software design book written corollary toii le frequently met modular design hinge simplicity abstract nature interface definition module design inside module need know modular design arbitrary partitioning bit necessary make sure system designed made modular part also necessary realize system matter big wonderful seems always designed part another larger system often much difficult modularity tolerance liberal require conservative expression principle applies pretty well life typical uu tenet commonly employed design across internet write html accept superset strict principle contentious browser lax expect system work better also encourages laxness part web page writer principle tolerance blunt need perfectly clear protocol specification draw precise distinction conformance nonconformance principle tolerance excuse product contravenes standard decentralization principle design distributed system including society point single common point involved operation trend limit way system scale produce single point complete failure centralization social system apply concept example make knowledge representation system requires anyone us concept automobile use term http wwwkrorgstdsindustryautomobile restrict set us system particular formulation automobile work semantic web must avoid conceptual bottleneck internet avoids network bottleneck someone else already invented system would work system one kind simple thought test described detail evolution design issue connectted modularity insideout designing system modular part asyet unspecified larger system critical property system try one thing well leaf thing module also avoid conceptual centralization two module claim need unique center larger system choosing computer language class program range plainly descriptive dublin core metadata content database html though logical language limited power access control list conneg content negotiation include limited propositional logic though declarative language verge turing complete postscript pdf nt told fact turing complete though one led use way xslt sql unashamedly procedural java c choice language common design choice low power end scale typically simpler design implement use high power end scale attraction openended hook anything placed door us bounded imagination programmer computer science spent lot effort making language powerful possible nowadays appreciate reason picking powerful solution least powerful reason le powerful language data stored language write simple declarative anyone write program analyze many way semantic web attempt largely map large quantity existing data onto common language data analyzed way never dreamed creator example web page weather data rdf describing data user retrieve table perhaps average plot deduce thing combination information end scale weather information portrayed cunning java applet might allow cool user interface analyzed search engine finding page idea data way find java applet mean set running front person hope good enough explanation principle million example choice chose html programming language wanted different program different thing present differently extract table content index b carpenter editor architectural principle internet internet architecture board june follow talk science insecurity meredith patterson make point principle least power important security interface may exposed attack design issue tim bl
218,Lobsters,scaling,Scaling and architecture,Evolving Away From Entities,http://www.michaelnygard.com/blog/2018/04/evolving-away-from-entities,evolving away entity,controller hat page bit code hat attached comment,let try making api map action original controller decomposing monolith microservices monolith already work know current design solves feature needed controller method index buildrequest createrequest requestsindex approverequest rejectrequest notice something interesting method none talk hat one actually care hat index get hat serve hat page show everything least interesting behavior hat controller appears almost entirely process requesting hat approving rejecting request let set bookmark called request come back later hat actually get used let look comment user start post comment reply another comment bit code check see user hat comment fragment offer user option put hat get carried comment controller hat attached comment know happens user doffs old comment still show wear hat time comment nice comment controller nt method hat although use hat data get hat data user model hasmany hat understand feature much better instead talking abstract talk part feature activated used understand lifecycle data creates response signal essential designing successful microservices try design service vaccuum find nt work together need bunch glue code service consumer hint start trying solve distributed twophase commit across microservices nt gotten concrete enough actual use case
220,Lobsters,scaling,Scaling and architecture,Structured Concurrency in High-level Languages,http://250bpm.com/blog:124,structured concurrency highlevel language,structured concurrency highlevel language introduction article blog post discussion coroutine like control construct concurrency coroutine also like variable coroutine ca nt orphaned nathaniel article coroutine orphaned coroutines naturally aggregate pool libdill trio already taken different type cancellation even type cancellation thread part bundle creates conclusion april discussion forum,structured concurrency highlevel language introduction recently stumbled upon nathaniel smith article structured concurrency nice readable explains entire problem basic idea structured concurrency go read dealing problem many year addressed several blog post however always tried keep earth low level partially playing c maybe bit assembly mixed partly feel uneasy discussing grand highlevel concept lowlevel detail sorted however nathaniel blog post subsequent discussion made realize maybe already time look bit higher stack go beyond c nt hand wo nt done approach think would structured concurrency look like natively supported highlevel language follows raw thoughs particular order coroutine like control construct forget parallelism second talking concurrency much like control construct function call coroutine nice trick hide ugly old goto launch coroutine jump code coroutine get point would block jump code different coroutine implemented properly coroutines even get close classic control structure term performance coroutine also like variable coroutine state entire stack whatnot statement sound like fundamentally different look statement control variable even declared outside construct int control variable stored stack maybe coroutines actually yes libdill allow kind thing void foo int x int int char stk int b gomem foo stk sizeof stk stack would look like remaining problem make sure foo running scope stack declared exited still running would use memory stack stack pointer stack grew new stack frame coroutine would overwrite look like solve scoping issue functional highlevel concurrency model coroutine ca nt orphaned many good argument allowing coroutines orphaned allowing child coroutine run parent coroutine exit find nathaniel article example stackbased coroutines drastic example orphaned coroutines bad design also cause memory corruption undefined behavior accept fact orphan coroutines imagine introducting concurrency programming language move dumb old linear call stack call tree see animation warned see unsee envisaged year ago since hard time thinking concurrency way box picture stack frame coroutine orphaned despite said use case orphaned coroutines specifically sometimes want coroutine part object allocated heap imagine example tcp socket object contains coroutine send keepalives socket created one coroutine easily passed different unrelated coroutine nt want keepalives stop sent original coroutine exited back coroutine like variable idea variable mostly nicely nested call stack always true every need allocate variable heap c actually provides two mechanism access variable variable life stack address life heap address via pointer p thing coroutines sure extend language example like stack coroutine canceled scope exited go foo heap coroutine canceled p freed p go foo coroutines naturally aggregate pool principle come observation often arbitrary number coroutines thing instance network server probably one coroutine per connection coroutines exactly programmer want deal unified way rather care one separately end day want single handle point bundle coroutines rather many handle pointing different coroutine int b bundle int tcpaccept listener bundlego b handletcpconnection shutdownrequested break close b tcp handler canceled libdill call concept bundle nathaniel smith trio call nursery name suck former becuase generic bundle likely nameclash unrelated concept latter fundamental concept long sylables semantic relation name thing represents quite vague maybe better edited proposed rope name already taken different type cancellation top head cancel thread bundle block thread bundle exit block least one thread bundle exit cancel thread last one completely intuitive take seriously given seems solve bunch pesky problem like timeouts ctrlc handling consider code int b bundle bundlego b foo bundlego b sleep bundlego b waitforctrlc close b foo finish cancel timer ctrlc waiter timeout expires foo canceled well ctrlc waiter ctrlc pressed foo canceled well timer exactly semantics want said different kind cancellation seem profoundly different way may require different syntax example cancel thread probably used handle error encounter error cleanup cancel thread turn mean ca nt say whether use cancel mechanism advance creating bundle nt know beforehand whether error wait wait want specify advance part business logic application single bundle terminated case case sound like sloppy programming even nt seem two fully equal option thinking even type cancellation consider accept loop example bundle worker thread semantics thread finish bundle closed imagine want shut bundle want give worker thread second finish gracefully would somehow wrap allstyle bundle worker thread anystyle bundle together new timeout coroutine one even imagine three four level nesting thread part bundle creates alternative seen example user creates bundle add thread parent continues executing normal int b bundle int tcpaccept listener bundlego b handletcpconnection shutdownrequested break close b clear could yes alternative nicely designed first thread get priviliged position automatically simply part creator function let say introduce construct treat coroutines created within context including main one belonging implicit bundle note foo bar feel different code within construct void foo void bar go foo go bar also consider would similar construct work given thread bundle finish thread canceled code within construct canceled point sleep exit go sleep bar baz canceled goto end scope quux one way solve kind problem would simply block main coroutine bundle executing void bar void foo go bar r foo get foo bar done solution kind neat however one thing nt like main coroutine decides whether foo treated bundle whereas fact distinction part foo business logic conclusion implementing structured concurrency higherlevel language entirely trivial pose hard design question opinion topic welcome april forum
221,Lobsters,scaling,Scaling and architecture,"Multi-Coring"" and Non-Blocking instead of ""Multi-Threading""",http://ithare.com/multi-coring-and-non-blocking-instead-of-multi-threading-with-a-script/,multicoring nonblocking instead multithreading,bug hare,author bug hare follow job title sarcastic architecthobbies thinking aloud arguing manager annoying hr calling spade spade keeping tongue cheek following slide script talk bristol uk apr note script planned speak slightly different transcript turned wink good afternoon everybody thanks coming hope talk able tell thing three might interest say tin going argue usual multithreading pattern especially using mutexes applevel way design multicore nonblocking system moreover going argue actorbased system better alternative really wide range realworld use case start let provide brief outline talk part talk argue multithreading really ultimate goal worth pursuing nomatterwhat rather tool implement two relatedbutdistinct concept multicoring closely related scalability nonblocking precisely guarantee response blocked longlasting io operation importantly user care exactly achieve thing open door different implementation second part talk move towards discussion personal favorite subject actor much time go deeply detail still discus thing certain aspect actor think implemented actor fare sharedmemory architecture achieve goal specified armed concept one single actor able move architecture use nothing actor applevel among thing see example architecture aaa firstperson shooter clientside serverside mention realworld system handle billionsmessagesperday write tensofbillionsdbtransactionsperyear purely accidentally happen make billionsofdollars process wink last certainly least briefly discus applicability limit messagepassing system general actor particular giving away spoiler say according experience architecting serious system heavilyloaded distributed interactive system benefit implemented actorfest mind come inevitable conclusion waiting architect next system actorfest really start talk like make important announcement confess really presentation rather talk prepared dadum guy also see tshirt mean anything good talk attributed anything bad fault btw think accent bad grateful speaking accent much worse mine preliminary aside proceed substance talk first part talk want take deep breath ask multithreading qualify almighty business requirement merely puny implementation detail qualifies requirement happen big fan following criterion laid one former manager many customer losegain implement feature guy currently worth well billion something right one consequence many customer gain approach certain thing exist customer space business requirement plain simple btw consistent wellknown requirementforrequirements good requirement implementationfree indeed specifying much requirement restricts ability build optimal system example serious security saying use tlsovertcp restricting ability improve latency using udpwithdtls want app run inbrowser write use javascript programming language preventing using c via emscripten c via forth separation goodrequirements badrequirements mind ask whether use multithreading qualifies good requirement answer going contrary intuitive feeling lot developer big fat rule thumb use multithreading qualify good requirement simple reason multithreading directly observable enduser space word unless product o librarytobeusedbyotherdevelopers get customer due writing app multithreaded otoh still business requirement satisfied using multithreading thinking bit observe businesslevel requirement provide response within certain time care multithreading indeed use multithreading thing correctly come play provide reply faster stand use case multithreading game responsiveness measured term millisecond hpc day qualify responsive enough especially compared wait next century specifically multithreading usually summoned solve one two relatedbutstillseparate problem first one thing faster using multiple cpu core one fairly obvious case one single cpu core sufficient whateverweneedtodo within allotted time get job done find way use multiple core close cousin requirement scalability sense seen ability scale job asmanycoresaswemightneed second problem often trying solve multithreading keeping app blocked long external operation indeed scenario desktop app hang waiting dns server reply internet happens bad thing tm argument though whether multithreading good way satisfy requirement completely different story two simple thing cover vast majority use case multithreading summarise part talk really want multithreading instead two separate distinct business requirement first one multicoring related scalability second one nonblocking note still point saying multithreading bad though certainly come later wink time multithreading firm business requirement open u door looking alternative implementation
222,Lobsters,scaling,Scaling and architecture,Dynamics of Scaling an Organisation,https://blog.gojekengineering.com/the-dynamics-of-scaling-an-organisation-cb96dbe8aecd,dynamic scaling organisation,concept scaling problem scaling leverage organisation graph maslow underlying communication structure organisation chiefly determine product processproduct paradox edge scaling big notation spend next ten minute understanding word scale chinese whisper think would feel going work band brother sister versus large homogeneous amorphous group note concurrency concurrency parallelism asynchronous synchronous edge quality bandwidth course entropy scarcity game theoretic information dissemination metaorganisation data structure conclusion human touch commitment truth reality finally enable human maximally fulfil humanity acknowledgement http http wwwdticmilgettrdocpdf http enwikipediaorgwikiokr kpi http enwikipediaorgwikikpi gojekjobs,gojek privilege scaling data science team member one location people across three countriesthis past experience helped come certain preliminary conclusion team best function version general theory organisation post try discus relatively general idea opposed detail apply specifically data science team also definitely work progress happy proven wrong certain point refine view meant comprehensive theory hopefully contains nugget wisdom highlight certain concept think serve crucial mental checkpoint frameworkmy favourite characterisation organisation bunch people working together create product service information processing system word one multiple goal achieve beyond single individual could output group formed simply put organisation solution scaling problem let start talking scale optimally best address scaling challengesscalingi like think scaling depicted diagram output typically related product service organisation represented different level relate different objective lowstaff turnover moralei would argue organisation encounter diminishing return scale bad line incremental team member contribute le le marginal productivity complexity communication bottleneck arise fact even scaling output linearly respect team member probably quite feat hopefully feel anecdotally intuitive ever felt difference member project versus however imagine organisation able achieve increasing return scale labeled excellent team would experience network effect grew presumably within reasonable bound company would leave competition dirt think theoretical proposition disagreei seen built team adding novel skillset homogeneous group like rocket fuel productivityand get better one presumably shift excellent level output upwards dashed line increase slope improving leverage tool process enhance individual productivity team communication bitorganisations graphit help many way view organisation graph made node edge typically node would represent individual edge would represent interfacingcommunication may make sense view multiple individual node certain circumstance example communication overhead within team genuinely negligible modelling lower level granularityremember organisation way scale output graph perspective stuff happens two place org within node edge interface node two dimension optimise let consider one timeoptimising node level bit straightforward regardless whether node individual extremely tightly knit group involves thing like keeping individual motivated range compensation holistic maslow approach tool productive eg software noncommunication process efficient information dissemination whole section later career growth guidance clarity purpose output definitely low one targeting wrong output selfdevelopment learning skill design thinking lot rest individual imperativethere seems literature addressing spend time optimising communication many believe including upon time communication structure reflection underlying product built building web application business one might expect certain divide engineering team focusing tech business team focusing corporate client one component product recommendation system one might expect separate data science team working perhaps limited communication team relative internal team communication believe alan kay argued exact opposite fact underlying communication structure organisation chiefly determine product although course feedback personally call processproduct paradox seems counterintuitive increasingly true experienceconsider startup whose product often directly reflect founding team identity early day consider previous example divide org three team one mainly engineer couple data scientist business folk likewise another team mainly business folk others argue would result product solves business problem informed possible tech perhaps even thing yet exist market indeed recommendation engine also fundamental personalisation built throughout bad paradoxso get communication pattern important let explore organise pattern reduce overhead improve qualityedge scalingif familiar big notation best advice ever give life spend next ten minute understanding seriously way think scalability field world would benefit tremendously people domain politician scientist artist understood betterconsider dense complete graph communication overhead edge increase increase number n people node complete case connect first node others next node one le connection made calculate sum idea magnitude mean individual end order remember big communication channel think unrealistic even divided team talking talking channel fancy halving graph density still talking channel point scale quadratically word scale business forget competitor committing homicide committed suicidethe good thing alternative organisational design completely possible take common solution approach instance hierarchy graphically speaking mean organising node ktree k number split level treethis could represent ceo vps report team lead report good news tree n node n edge problem case downside excessive hierarchy including team isolation employee dissatisfaction communication breakdown height tree k split number time need multiply k get number leaf node hence logarithm example individual simplicity would leaf communication depth least reasonable smaller organisation scale much fact would go far say indirect communication across node chinese whisper lead issuesanother alternative divide people extremely tightly woven multiskilled swat team multidisciplinary nature team small size eg people precisely think independent node organisation able take reasonably sized project endtoend communication still build across team one might choose limit degree node say swat team structure fact special case common structure large functional team like product data science business intelligence think problem team often get big meaning node abstraction really node degree number connection team usually skillsets tend relatively homogeneous fails node abstraction one belief team work le unit contributes constant crossteam crossskill interaction requiredthat said benefit specialised functional team lead call heterogeneous swat team structure usual swat team certain functional team individual float around required kept small perhaps figure swat team degree restrictionthe kind nerd analogy mind swatlike team superheroesdifferently skilled character gamemilitary tactical unit pick favourite make new one going quest endtoend multiskilled team sometimes encounter obstacle requires drill teamsome artillery call functional support team temporary heavy lifting carry onthere separate advantage approach individual motivation level think would feel going work band brother sister versus large homogeneous amorphous group also permanent situation plenty scope occasional rotation individual across nodesthe heterogeneous swat team structure currently best come organisation people think scale well said imagine sort alteration might work best given environment albeit underlying principle modela note concurrencyorganisations many way distributed system human being would remiss highlight absolutely crucial point concurrency well point common misconception parallelismthe usual analogy digging hole dig faster could get bigger spade corresponds making node better example skill mentorship alternative would get friend help point probably get way might make sense increase output digging identical hole time basis parallelism many similar task independently timeconcurrency sometimes confused parallelism much broader idea friend different task digging sharpening spade carrying dirt way importantly different party interact certain point lead bottleneck shovel anymore waiting someone clear dirt around stuck idle unproductive fact visualise happens interdependent group wait slowest member task different ie parallelism hence different colour simplify illustrating member syncing least common denominator rather drawing full concurrency graph two main thing avoid painful idle time firstly try make interaction asynchronous nonblocking eg text message instead synchronous eg big meeting describe section edge communication quality secondly limit crossteam dependency exactly swat team model come worth noting beyond simply wasting productivity idleness due excessive dependency leading cause demotivationedge qualitynow spoken scaling communication let look maintain high standard communication quality could translated model edge weightsthickness denoting quality alternatively cost given interaction link though cost make sense could look metric like weighted average number edge graph something minimise subject constraint couple strong view topic convey one one first kneejerk solution communication breakdown seen multiple organisation scale overcommunicate one hand good communication indispensable hand seen quantity communication simply scale like staying office hour later two hour get done factor need leveragein context communication leverage often seen bandwidth case much information exchanged per unit time mentioned information indeed lens information theory prof useful analysing communication course think openended conversation person allows flexibility discussed tradeoff take time never quite know person say next ironically despite amazing flexibility one would characterise bandwidth lownow consider extreme let say two people alice bob playing game deciding whether launch new product feature game follows alice say go go bob coincide decision made simply execute disagree toss coin head alice win tail bob win flexibility reduced would certainly hard time talking colour sky game bandwidth extremely high interaction could take secondsformally say example state space x set possibility assuming uniform distribution possibility system bigger state space higher entropy communication want low entropy communication much possible model help reason thing feel free use discard pleaseas rule maximum output minimum burden interaction always use lowest entropy medium possible web form make conversation histogram make paragraph email improving communication entropy may one lowest hanging fruit increasing organisation effectiveness bit design thinking go long waywhen inperson meeting required couple rule useful example rule could amaximum people meeting duration given topic given day unless brainstormingdesign session ruthless punctuality otherwise thing get sloppy everyone time wasted clear agenda reached end meeting regardless time allotted extend cautionone outlandish idea enforce scarcity meeting fixing maximum number meeting given individual per day party need bid slot like individual join meeting using sort currency currency could anything internal organisation token portion one bonusthe game theoretic motivation would people included meeting impact productivity assuming bonus sufficiently tightly coupled objectivesi invite someone meeting unless thought would add value amount costing fact rigorous game theoretic analysis company process also go long way perhaps topic another postinformation disseminationif organisation information processing system node communicating periodically make sense consider bidirectional channel oneway information dissemination well couple typical channel information usually spread email social text messenger eg whatsapp telegram work messaging eg slack publishsubscribe system eg twitter yammer kanban eg trello integrated system eg jira notionso many othersnow powerful concept terminology belt pretty straightforward articulate principle might useful information dissemination particular consideration whether adhoc request required selfservice possible synchronous v asynchronous structured v unstructured entropy minimum duplication complexity overhead many specialised dashboard v one placemassive amount information need traverse company daily basis digestible summary one place trump constant email barragemetaorganisationit always seemed striking organisation devote much effort building product yet building organisation afterthought common data structure representing product often thoroughly model member build product interaction take following relational database view actual product say delivery application versus counterpart organisation imagine possibility small metateam within organisation focused exclusively optimising company opposed product directlyconclusion human touchthis post getting long wrap one final note engineering principle applied organisation extremely useful end day must course remember people people couple miscellaneous thing learnt diversity cultural gender make product better people happier balance diversity shared quality list endless ubiquitous teamwork excellence focus exclusively one humility make sure every single member low ego life short strictly cultivate zero b environment go hand hand humility also involves commitment truth mean always focusing reality problem team focus always win long runfinally enable human maximally fulfil humanity need inspiration inspire practicing quality aspire yourselftake organic approach thing create atmosphere people simply much possible everybody secretly want like often afraid restricted indoctrinated soand remember together would appreciate thought around guy able crack thisacknowledgementsthanks inspiring people known andor worked year experience helped formulate view organisational theory http http wwwdticmilgettrdocpdf example okr http enwikipediaorgwikiokr kpi http enwikipediaorgwikikpi team level nb find term extremely corporate aesthetic matter hiring engineer serving million people practice philosophy lean engineering product quick decisionmaking culture people part amazing team check gojekjobs
223,Lobsters,scaling,Scaling and architecture,Step by step guide on how to setup a multi-region Kubernetes app,http://blog.fgribreau.com/2018/04/step-by-step-guide-to-setup-multi.html,step step guide setup multiregion kubernetes app,imagecharts new kubernetes cluster region gce uswest coast preemptible node feature chaos engineering preemptible vm good news observed cluster cost reduction two get static ip gce kubernetes yaml namespace deployment horizontal pod autoscaler pod disruption budget podantiaffinity request limit continuous delivery pipeline imagecharts redsmin gce container registry kubectx configuring geodns geodns geo routing,nt want spread one saas across continent finally opportunity imagecharts saas imagecharts first hosted kubernetes cluster europewest google cloud engine region kind issue considering real user screenshot courtesy cloudflare dns traffic see important part traffic come u zone good news time try multiregion kubernetes step step guide new kubernetes cluster region gce first thing first let create new kubernetes cluster uswest coast really really enable preemptible node feature get chaos engineering free continuously test application architecture configuration robustness preemptible vm mean cluster node wo nt last hour good news observed cluster cost reduction two get static ip gce next need create new ip address gce associate ip kubernetes service always stay static ip gcloud compute address create region wait gcloud compute address list name region address status reserved reserved kubernetes yaml app yaml defines multiple kubernetes object namespace appns service give access app outside myappservice app deployment object myappdeployment autoscaling horizontal pod autoscaler finally pod disruption budget app apiversion kind namespace metadata name appns apiversion kind service metadata label app myapp zone zone name myappservice namespace appns spec type nodeport port name port targetport protocol tcp selector app myapp type loadbalancer loadbalancerip ip apiversion kind deployment metadata namespace appns name myappdeployment label app myapp spec replica much revision history deployment want keep revisionhistorylimit strategy type rollingupdate rollingupdate specifies maximum number pod unavailable update maxunavailable specifies maximum number pod created desired number pod maxsurge template metadata label app myapp spec affinity podantiaffinity preferredduringschedulingignoredduringexecution weight podaffinityterm topologykey kubernetesiohostname labelselector matchexpressions key app operator value myapp restartpolicy always container image image name myapprunner resource request memory cpu limit memory cpu send traffic readinessprobe httpget path ready port initialdelayseconds timeoutseconds periodseconds failurethreshold successthreshold restart container livenessprobe httpget path healthz port initialdelayseconds timeoutseconds periodseconds started pod need sure work may take time nt want rush thing thx rolling update failurethreshold successthreshold port name containerport containerport name monitoring containerport apiversion kind horizontalpodautoscaler metadata name myapphpa namespace appns spec scaletargetref apiversion kind deployment name myappdeployment minreplicas maxreplicas targetcpuutilizationpercentage apiversion kind poddisruptionbudget metadata name myapppdb namespace appns spec minavailable selector matchlabels app myapp thing note image ip zone replaced gitlabci deploy stage see continuous delivery pipeline section kind service type loadbalancer loadbalancerip ip type loadbalancer loadbalancerip tell kubernetes creates tcp network load balancer ip must regional one always good practice split monitoring production traffic monitoring port liveness readiness probe production traffic port another good practice setup podantiaffinity tell kubernetes scheduler place app pod replica available node put app pod node good news principle easily applied kubernetes thanks request limit finally use poddisruptionbudget policy tell kubernetes scheduler react disruption occurs eg one preemptible node dy continuous delivery pipeline used gitlabci power imagecharts redsmin deployment work alternative well gitlabyaml http docsgitlabcomceciyaml image docker fetch faster reuses project workspace falling back clone nt exist variable gitstrategy fetch use overlay performance reason http docsgitlabcomcecidockerusingdockerbuildhtml usingtheoverlayfsdriver dockerdriver overlay service docker dind stage build test deploy template reuse beforescripttemplate setupgcloud image lakoonodegclouddocker latest retry variable default variable zone clustername master beforescript echo setting gcloud echo gcloudserviceaccount tmp cipipelineidjson gcloud auth activateserviceaccount keyfile tmp cipipelineidjson gcloud config set computezone zone gcloud config set project gcloudproject gcloud config set containeruseclientcertificate false gcloud container cluster getcredentials clustername gcloud auth configuredocker quiet afterscript rm f tmp cipipelineidjson build kubectl apply f echo waiting deployment grep v successfully environment variable setup gitlabci pipeline setting ui dockerregistry eg container registry use easiest way leverage gce container registry gcloudproject eg google cloud project name gcloudserviceaccount encoded service account json creating new service account gce console get json like type serviceaccount projectid privatekeyid privatekey begin private key end private key n clientemail gitlab clientid authuri http tokenuri http http http make work ci need transform order inject environment variable cat serviceaccountjson packagename eg myapp container image name yaml syntax defines template beforescripttemplate setupgcloud reuse particularly fond oneliner make gitlabci job wait deployment completion grep v successfully simple job note pipeline run commits pushed master removed environment code simplicity important part deployus magic happens job includes shared deploy job template includes setupgcloud template specify three variable zone clustername ip address expose alone sufficient make pipeline deploy multiple kubernetes cluster gce bonus store generated yaml artifact future inspection pushing code something like video let connect one kubernetes cluster tip easily switch kubectl context use kubectx cluster kubectl get servicemyappservice w namespaceimagecharts name type clusterip externalip port age myappservice loadbalancer point able access public static ip curl configuring geodns warning used cloudflare geodns part could use another provider cost get basic geodns loadbalancing region basic load balancing origin server check every second check region geo routing activated create two pool one per region using static ip created create monitor finally loadbalancer distribute load based visitor geolocation done
224,Lobsters,scaling,Scaling and architecture,Executing commands in Pods using K8s API,https://blog.openshift.com/executing-commands-in-pods-using-k8s-api/,executing command pod using api,introduction api endpoint basic information protocol deprecated http header guide issue communication protocol connection lifecycle conclusion minishift ruby,introduction part exploration kubernetes working project wanted execute command inside pod rather forcing container specific behaviour wanted utilize api mechanism exposed kubectl exec subcommand investigating found exec yet sport extensive documentation hopefully post help find similar situation api endpoint kubernetes doc mention exec endpoint openshift documentation offer basic information know endpoint life parameter need pas need issue post request path namespacepods nameexec two string need replaced path fairly obvious query string parameter described correctly table openshift documentation single exception command parameter included multiple time first let take look look like single command parameter one simply execute binbash pod commandbinbash stdintrue stderrtrue stdouttrue ttytrue multiple command parameter commandbinbash commandc commandbinbash stdintrue stderrtrue stdouttrue ttytrue give u something like binbash c binbash could logically transcribed binbash c binbash protocol kubectl oc use spdy protocol moment deprecated second option use websockets seems best way anyway one two protocol spdy websockets required communication endpoint api refuse request without upgrade header http header provide necessary information request need contain set header required api handled websockets client eg upgrade etc two need provided user first one authorization value bearer token authenticates request kubernetes follow guide openshift simply get token user oc whoami header accept value value rejected acceptable even though example shown documentation show incorrect value applicationjson issue progress make documentation accurate communication protocol information place websocket able establish connection api start communicating write websocket data passed standard input stdin receiving end websocket standard output stdout error stderr api defines simple protocol multiplex stdout stderr single connection every message passed web socket prefixed single byte defines stream message belongs codemeaning stdin stdout stderr every message received socket need get first byte decide whether stdout stderr ruby would look something like data case datashift stdout datapack c forceencoding sterr datapack c forceencoding else unknowndata data end send data api need convert byte prepend indicate message belongs stdin stream data l lan data dataunpack c socketsend dataunshift connection lifecycle one last problem may proxy roadblock way api may simply reach tcp timeout get around send empty message every keep connection busy threadnew loop socketsend sleep end end conclusion information possible write application communicate kubernetes api process running inside kubernetes cluster sample ruby excerpt tested openshift using minishift oc version openshift kubernetes example use ruby straightforward translate favourite language read go check endpoint used kubectl upstream source code http
225,Lobsters,scaling,Scaling and architecture,"Waiting time, load factor, and queueing theory  why you need to cut your systems a bit of slack",https://erikbern.com/2018/03/27/waiting-time-load-factor-and-queueing-theory.html,waiting time load factor queueing theory need cut system bit slack,waiting time load factor queueing theory need cut system bit slack queueing theory mortgage startup written past dynamic programming exponentially distributed lognormally distributed derive exact value geometric distribution one use case get done le stressed note google borg heroku routing tagged math statistic queue software,waiting time load factor queueing theory need cut system bit slack reading operation research lately including queueing theory started way understand complex mortgage process work mortgage startup turned little hammer see nail everywhere one particular relationship turn somewhat complex relationship cycle time throughput example situation might apply good cpu load database cpu load much impact latency v cpu load average time take respond email function busy relationship tech team throughput term trello card per day whatever cycle time time adding card done need loan underwritten day many underwriter need need book meeting manager hisher calendar full meeting work far going book meeting situation need book meeting people full calendar independent far user file bug developer assuming put developer full time triagingsolving incoming bug keep person x busy time bug get resolved case turn running system lower throughput yield dramatic cycle time improvement instance kind busy v super swamped anecdotally impact email response time easily written past almost mythical way nt understand math behind principle first relationship might seem nonsensical garden hose throughput hose liter water completely independent cycle time length hose database handle query per second currently query load latency higher query load reason like much else life variability database thought worker handle exactly one query time going queue time nonnegative number query number query sitting queue vary time due chance might query arrive almost time case worker process query serially work way queue turn simulate fairly easily using dynamic programming let say time query exponentially distributed time query lognormally distributed exact distribution nt super important good reason model world using one simulate whole system something like queryenqueued numpycumsum numpyrandomexponential size n querytime numpyrandomlognormal size n queryfinished numpyzeros n range n queryfinished queryenqueued querytime ca nt finish previous query finished plus time query queryfinished max queryfinished queryfinished querytime run snippet different value k plot latency function load factor get chart super interesting opinion utilization get twice latency utilization start hitting say utilization go go fast getting towards utilization asymptote vertical line want optimize higherpercentiles little bit load push latency lot basically forced run system really low load factor leeway system described called using fancy notation turn slightly simpler case instead lognormal distribition exponential distribution derive exact value latency key observation number event queue geometric distribution mean end f f load factor mom told trust equation internet let simulated safe side case ca nt see two line exactly top one use case get done le stressed talked lot technical context let switch gear bit allow put corporate philosopher hat metaphorical hat like wear wear mutter thing like speed company innovates fundamentally limited size iteration cycle reduce size cycle observation spending time super urgent stuff highest priority fairly mild assumption stuff extremely long cycle time prevent let split work thing need happen v thing wait really nt want spend time critical path company major project exactly time stuck reactive mode backlog stuff build instead make sure big chunk important imminent deadline attached strategy take urgent thing delegateautomate average load urgent thing possible output marginally lower quality upside improve latency order magnitude respond email slack message faster find time unexpected meeting etc generally information propagate faster note make sense colocate batch job lowlatency processing let latter take precedence time fact think intuition behind building google borg might seem like parallelization would help fact make problem even harder faster machine single request queue always lower latency two machine request queue fact reminds really old blog post got widespread attention heroku routing tagged math statistic queue software
226,Lobsters,scaling,Scaling and architecture,"Touring a Fast, Safe, and Complete(ish) Web Service in Rust  Brandur Leach",https://brandur.org/rust-web,touring fast safe complete ish web service rust brandur leach,techempower benchmark pgbouncer without compacting gc errorchain http testing methodology article published location hacker news sending pull request,year crisis faith interpreted language fast fun work small scale project get big attractive veneer quickly wash away big ruby javascript name program production never ending game whackamock fix one problem find new one somewhere else matter many test write welldisciplined team new development sure introduce stream bug need shored course month year central problem edge people reliably good job building testing happy path human terrible considering edge condition edge corner cause trouble year program service constraint like compiler discerning type system tool help u find think edge spectrum permissiveness across world programming language thesis right time spent development satisfying language rule lead le time spent fixing problem online possible build reliable system programming language stricter constraint language strongest constraint skewed way far end spectrum building web service rust language infamous uncompromising compiler language still new somewhat impractical slog learning rule around type ownership lifetime despite difficulty interesting learning experience throughout working run fewer forgotten edge condition runtime error way broad refactoring longer terrorinducing run novel idea feature rust core library various framework make possible built service actixweb web framework layered actix actor library rust actix similar might see language like erlang except add another degree robustness speed making heavy use rust sophisticated type concurrency system example possible actor receive message handle runtime would disallowed compiletime small chance recognize name actixweb made way top techempower benchmark program built sort benchmark often turn little contrived due optimization contrived rust code sitting right top list contrived c java code regardless feel validity benchmark program takeaway actixweb fast rust consistently ranking alongside c java techempower author actixweb actix commits prodigious amount code project six month old already featurecomplete better apis web framework seen open source language many framework bankrolled large organization huge development team nicety like websockets steaming response graceful shutdown http cookie support static file serving good testing infrastructure readily available box documentation still bit rough yet run single bug using diesel orm talk postgres comforting thing project orm written someone lot past experience building orms spent considerable time trench active record many pitfall common earlier generation orms avoided example diesel try pretend sql dialect across every major database excludes custom dsl migration raw sql used instead automagical connection management global level bake powerful postgres feature like upsert jsonb right core library provides powerful safety mechanic wherever possible database query written using diesel typesafe dsl misreference field try insert tuple wrong table even produce impossible join compiler tell typical operation case postgres batch insert conflict upsert timehelpers logtimed lognew step upsertepisodes log ok diesel insertinto schema episode table value insepisodes onconflict schema episode podcastid schema episode guid doupdate set schema episode descriptioneq excluded schema episode description schema episode expliciteq excluded schema episode explicit schema episode linkurleq excluded schema episode linkurl schema episode mediatypeeq excluded schema episode mediatype schema episode mediaurleq excluded schema episode mediaurl schema episode podcastideq excluded schema episode podcastid schema episode publishedateq excluded schema episode publishedat schema episode titleeq excluded schema episode title getresults selfconn chainerr error upserting podcast episode complex sql difficult represent using dsl luckily great alternative form rust builtin includestr macro ingests file content compilation easily hand diesel parameter binding execution diesel sqlquery includestr sqlcleanerdirectorysearchsql bind text directorysearchdeletehorizon bind bigint deletelimit getresult deleteresults conn chainerr error deleting directory search content batch query life sql file expired select id directorysearch retrievedat interval limit deletedbatch delete directorysearch id select id expired returning id select count deletedbatch lose compiletime sql checking approach gain direct access raw power sql semantics great syntax highlighting favorite editor actixweb powered tokio fast event loop library cornerstone rust concurrency story starting http server actixweb spawn number worker equal number logical core server thread tokio reactor http handler written variety way might write one return content synchronously fn index req httprequest byte block underlying tokio reactor finished appropriate situation blocking call need made example rendering static view memory responding health check also write http handler return boxed future allows u chain together series asynchronous call ensure reactor never needlessly blocked fn index req httprequest box future itemhttpresponse errorerror example might responding file reading disk blocking io albeit minimally waiting response database waiting future result underlying tokio reactor happily fulfill request example concurrency model actixweb support future rust widespread universal notably diesel support asynchronous operation operation block using directly within actixweb http handler would lock thread tokio reactor prevent worker serving request operation finished luckily actix great solution problem form synchronous actor actor expect run workload synchronously assigned dedicated oslevel thread syncarbiter abstraction provided easily start number copy one type actor sharing message queue easy send work set referenced addr start dbexecutor actor database connection thread let addr syncarbiter start dbexecutor sqliteconnection establish testdb unwrap although operation within synchronous actor blocking actor system like http worker need wait finish get future back represents message result work implementation fast workload like parsing parameter rendering view performed inside handler synchronous actor never invoked need response requires database operation message dispatched synchronous actor http worker underlying tokio reactor serf traffic waiting future resolve render http response result sends back waiting client first glance introducing synchronous actor system might seem like purely disadvantage upper bound parallelism however limit also advantage one first scaling problem likely run postgres modest limit around maximum number allowed simultaneous connection even biggest instance heroku gcp google cloud platform max connection smaller instance limit much lower small gcp database limit big application coarse connection management scheme eg rail also many others tend resort solution like pgbouncer sidestep problem specifying number synchronous actor extension also implies maximum number connection service use lead perfect control connection usage connection held synchronous actor need one written synchronous actor check individual connection connection pool starting work check back done service idle starting shutting us zero connection contrast many web framework convention open database connection soon worker start keep open long worker alive approach connection requirement graceful restarts worker phased immediately establish connection even worker phased still holding onto one synchronous operation fast purely asynchronous approach benefit ease use nice future fast getting properly composed time consuming compiler error generate make mistake truly stuff nightmare lead lot time spent debugging writing synchronous code faster easier personally fine slightly suboptimal runtime speed mean implement core domain logic quickly might sound disparaging model performance characteristic keep mind slow compared purelyasynchronous stack ie future everywhere still conceptually sound concurrent model real parallelism compared almost framework programming language still really really fast write ruby day job compared threadless model normal ruby gil constrains thread performance using forking process vm without compacting gc talking order magnitude better speed memory efficiency easily end day database going bottleneck parallelism synchronous actor model support much parallelism expect get also supporting maximum throughput action need database access like good rust program apis almost everywhere throughout return result type future plumb version result containing either successful result error using errorchain define error internal defined certain group explicit purpose user facing errorchain error user error badrequest message string description bad request display bad request message failure surfaced user make sure map one user error type params build log request maperr e errorkind badrequest etostring waiting synchronous actor attempting construct successful http response potentially handle user error render implementation turn quite elegant note future composition differs andthen handle success failure receiving result opposed andthen chain onto success let message server message new log params send message synchronous actor syncaddr send message andthen move actorresponse transform actor response http response re result httpresponse server transformusererror re renderusererror responder error intended seen user get logged actixweb surface internal server error although likely add custom renderer point transformusererror render function abstracted reuse generically api render json response web server render html pub fn transformusererror f re result httpresponse render f result httpresponse f fnonce statuscode string result httpresponse match re err e error errorkind badrequest format activates display trait show error display definition render statuscode badrequest format e r r like web framework across many language actixweb support middleware simple one initializes perrequest logger installs request extension collection request state live long request pub mod loginitializer pub struct middleware pub struct extension pub logger impl server state actixweb middleware middleware middleware fn start self req mut httprequest actixweb result started let log reqstate log clone reqextensions insert extension log ok started done fn response self req mut httprequest resp httpresponse actixweb result response ok response done resp shorthand getting usable logger request pub fn log server state req mut httprequest logger reqextensions get extension unwrap nice feature middleware state keyed type instead string like might find rack ruby example benefit type checking compiletime mistype key also give middlewares power control modularity wanted strongly encapsulate middleware could remove pub extension becomes private module tried access logger would prevented visibility check compiler like handler actixweb middleware asynchronous returning future instead result would example let u implement rate limiting middleware made call redis way block http worker mention actixweb pretty fast actixweb document recommendation http testing methodology settled series unit test use testserverbuilder compose minimal app containing single target handler execute request nice compromise despite test minimal nonetheless exercise endtoend slice http stack make fast complete test fn testhandlergraphqlget let bootstrap testbootstrap new let mut server bootstrapserverbuilderstart app appmiddleware middleware loginitializer middleware handler handlergraphqlget let req server client method get format query testhelpers urlencode b podcast id asstr finish unwrap let resp serverexecute reqsend unwrap asserteq statuscode ok respstatus let value testhelpers readbodyjson resp json macro really cool asserteq json data podcast value make heavy use serdejson standard rust json encoding decoding library json macro used last line code look closely notice inline json string json let write actual json notation right code get checked converted valid rust structure compiler far elegant approach testing http json response seen across programming language ever fair say could written equivalent service ruby tenth time took write one rust rust learning curve lot language succinct write appeasing compiler often long frustrating process said experienced passing final hurdle running program experiencing haskellesque euphoria seeing work exactly intended contrast interpreted language get running try even edge condition almost certainly still wrong rust also make big change possible unusual refactor thousand line time program run perfectly afterwards anyone seen large program interpreted language productionscale know never deploy sizable refactor important service except miniscule chunk anything else risky write next web service rust know yet getting point least consider daily dose tangentially related photography rust beam near pier san francisco articletouring fast safe complete ish web service rustpublishedmarch locationsan franciscofind twitter please post comment discussion hacker newsdid make mistake please consider sending pull request
227,Lobsters,scaling,Scaling and architecture,Computational Complexity of Air Travel Planning,http://www.ai.mit.edu/courses/6.034f/psets/ps1/airtravel.pdf,computational complexity air travel planning,,obj linearized h l e n endobj xref n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n trailer size info r root r prev id startxref eof obj type catalog page r metadata r pagelabels r endobj obj l filter flatedecode length r stream rrau fy n endstream endobj obj endobj obj type page parent r resource r content r r r r r r r r mediabox cropbox rotate endobj obj procset pdf text font r r extgstate r r colorspace r endobj obj iccbased r endobj obj endobj obj filter flatedecode length r stream n vax endstream endobj obj endobj obj filter flatedecode length r stream d k gz bm endstream endobj obj endobj obj filter flatedecode length r stream c  endstream endobj obj endobj obj filter flatedecode length r stream endstream endobj obj type fontdescriptor ascent capheight descent flag fontbbox fontname llkmelarial italicangle stemv xheight r endobj obj type font subtype truetype firstchar lastchar width encoding winansiencoding basefont llkmelarial fontdescriptor r endobj obj endobj obj filter flatedecode length r stream  b wo endstream endobj obj type font subtype truetype firstchar lastchar width encoding winansiencoding basefont llkmhfbitstreamveraserifroman fontdescriptor r endobj obj type fontdescriptor ascent capheight descent flag fontbbox fontname llkmhfbitstreamveraserifroman italicangle stemv xheight r endobj obj endobj obj filter flatedecode length r stream p r r n endstream endobj obj endobj obj filter flatedecode length r stream v n p endstream endobj obj endobj obj filter flatedecode length r stream gn endstream endobj obj type extgstate sa true sm default endobj obj filter flatedecode length stream k  q afa b r x z
228,Lobsters,scaling,Scaling and architecture,The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors,https://people.csail.mit.edu/nickolai/papers/clements-sc.pdf,scalable commutativity rule designing scalable software multicore processor,,obj length filter flatedecode stream x wiq iy xd
229,Lobsters,scaling,Scaling and architecture,"Clouds, fallacies and traps",https://kosfar.github.io/clouds-fallacies-and-traps/,cloud fallacy trap,cloud fallacy trap scaling monitoring visibility advertised feature integration deprecation eol new feature documentation support bug hack upstreams security notification epilogue,cloud fallacy trap working team utilises cloud service iaa paas saas make thing rolling product owner happy may look shopping promenade first sight however requires highlevel thinking antipattern recognition careful design along way also learn build lot integration glue unfold system architecting skill way predicted beginning advantage using cloud generally known salesman talk depending provider expertise engineer run service behind scene get stuff like upgraded kernel without rebooting instance downtime sophisticated wrapper around complex datastore operation apis readytouse metric shipper high granularity configurability notification security patch required maintenance operation serious issue infrastructure thing like upgrading mongodb cluster without even moving little finger gaining visibility precalculated statistic hidden postgresql system table becomes selfservice norm however reality perfect quality gap cloud provider huge month experience cloud service come across lot welldisguised trap frequently take advantage common fallacy cloud advocate cloud skeptic argument time boil believe everything going work well cloud course usually argument fails highlight aspect take consideration cloud service evaluation done engaging operation current cloud infrastructure executed post outlining main pain point recognised using cloud goody grouped meaningful category used reference reminder skeptic advocate scaling cloud infinite scaling autoscaling make even better well start wish never meet kind provider implemented upscaling part autoscaling yes true provider scale instance based resource metric documented evaluation window scale party cautious enough learn end month bill slide door thing cloud provider tested maximum scale able operate specific technology solr index grown fold since started using cassandra cluster lie behind log management platform handle log event volume since last year pay attention ask beforehand maximum capacity complexity cloud guy bought service handle sky limit may plan migration clustered variant slightly diffferent api specification land completely different technology take time production code refactored learning curve advanced finally pay attention instance flavor pricing provider inflexible specify flavor price may high future use case monitoring visibility lucky cloud provider provide log metric celebrate though unless first doublecheck readiops metric pop dashboard iop metric indeed exotic calculation blkio kernel subsystem make sure capacity resource metric also expressed utilisation otherwise knowing byte free memory little usage cloud provider may also provide alerting framework may may fulfill need provided integration alerting provider choice prepared build integration glue ship stuff advertised feature integration one thing told take granted operational readiness feature advertised documented service provider website integration security officer decided enable order start auditing user action may one click away check documentation check seems uptodate check enable integration though realise simply work start questioning person earth tried use least paid attention deprecation eol new feature cloud service provider sent email notification month ago saying going deprecate legacy alerting framework posted blog post saying going drop supported protocol expect going get big flashy notification ui day eol something like last email notification switchover new shiny stuff overoptimistic overoptimistic case may end alerting framework weekend client connection getting rejected speak new cloud software release may change behavior cloud machinery one day waf rule work expected new feature introduced creates artificial traffic website start blocking random ip hopefully incident rare however pay attention business critical cloud service blogsnewslettersfeeds documentation quality documentation varies lot cloud service provider take granted bit may missing may exactly accurate one way another need proofcheck procedure dryruns especially making study preparing complex maintenace task provider stand others open sourced documentation take time issue pr find inaccuracy sre colleague right next fall next time encounter issue support different type support contract buy provider general contract expensive least first impression realise exprertise people form gratis support layer varies lot time lead frustration extensive waiting authoritative answer escalation without professional enterprise support contract expect explanation missing data point monitoring provider unexpected ping failure favorite cloud load balancer may take day week side note expensive enterprise contract time cover around clock day month best response incident fast possible investigation part though went wrong case may take day even case bug hack even better famous cloud provider may bug pipeline talking pipeline using extensively one day decide something slightly different like making master database readonly promoting slave realise failover succeed readonly flag propagated slave thus alter statement slave fail nothing automation pipeline tried check condition revert least notify dryruns plan failure even trustful provider upstreams security notification expect cloud provider notify data corruption bug resolved last upstream version production database job fool guiding star let know expect make version available couple hour upstream release time pas truth told make happy one day receive notification amqp provider new security patch applied amqp instance norm though albeit epilogue let take break list exhaustive one lot said performance expectation high availability design inbuilt collaborativeness name list grow grow sky hope topic appetising challenged enough order ask indepth question next time find youself messing current cloud service started poc ing one stay calm pay bill question everything
231,Lobsters,scaling,Scaling and architecture,How to Quantify Scalability,http://www.perfdynamics.com/Manifesto/USLscalability.html,quantify scalability,,equal bang buck b cost sharing resource c diminishing return contention negative return incoherency
232,Lobsters,scaling,Scaling and architecture,Services by Lifecycle,http://www.michaelnygard.com/blog/2018/01/services-by-lifecycle,service lifecycle,prebonsai tree workshop avoiding entity service last post focus behavior instead data model like crc card dealing consistency cap c cap versus c acid divide service lifecycle business process devs library backtracking error race monolith microservices march contact,post took lot longer pull together expected hard write easy write much like prebonsai tree would grow control get pruned back meantime delivered workshop spent lovely holiday time family new year january devoid holiday high time got back business avoiding entity service last post made case entity service recap entity service set crud operation business entity person location contract order etc antipattern creates high semantic operational coupling edge service suffer commonmode failure shared dependency entity service change outage entity service large failure domain lot good advice spring eric evan hugely influential book domaindriven design written service era seems apply well expert ddd though going offer technique may may described dig bounded context idea need reread whole book comment several way avoid entity service post explores one though one particularly like future post look additional technique focus behavior instead data think service know always end back crud recommend thinking term service responsibility say responsible knowing data apply policy aggregate stream concept summary facilitate kind change calculate something know service figure need know instance service restricts content delivery based local law need know thing jurisdiction applies classifier content classifier allowed jurisdiction notice specific individual request slowlychanging data thus make sense service know told lead u deeper discussion service know data get service gui legal team maybe feed purchase data vendor maybe need apply machine learning based lawsuit penalty incurred violate local law kidding last one answer question firm design situate service ecosystem model like modeling like use technique objectoriented design crc card let lay physical token represent service play game simulate request coming edge service add information request pas along another service following tell ask principle team deal card player simulate request passing physical object around quickly reveal gap design common gap see service know send request lack knowledge service continue processing solution either statically introduce next party provide url data lead handler service receives request insufficient incoming request either lack information implicit context turned data request playing crc game ok assume service already data naturally depends slowlychanging data service us considered asynchronous process relative current request make note slowlychanging data remember build flow needed populate follow tell ask strictly activation set strict tree anywhere service call one downstream sending instruction forward parallel rather serially making query followed instruction dealing consistency matter passing request along extra data life would simple often happens trouble come side effect service pure function service call always result result parameter need service make library avoid operational overhead service make sense change something world mean state state change unavoidable concern servicebased architecture consistency immediately come issue many word already written cap good misguided pure marketing even wrote earlier post subtle difference c cap versus c acid let look one way deal consistency face changing state divide service lifecycle business process many business process entity go series milestone particular state change allowed certain attribute others subset property valid whatever mean entity transition next stage business process instead viewing single entity bunch booleans currentstate attribute implies state machine unknown consumer view state different thing example consider process peertopeer lending situation loan requestor start creating project proposal requestor provide descriptive text amount request medium asset project big vivid picture get funded faster loan request completed requestor submits approval point requestor longer allowed change amount requested analyst host company review proposal parallel background job check requestor credit score repayment history criminal record analyst review request either assigns target interest rate reject request outright indicates information needed requestor approved proposal visible funders funders commit certain amount contingent credit score stage none proposal information changed although whole proposal could withdrawn fully funded funders must transfer money within day additional funders allowed join project time go waiting list case committed funders fail supply money fund funders account get transfered shortterm holding account project information individual information tax id etc go underwriter produce legal loan document party sign moment leaving tributary flow notice moving business process cause previous information become effectively readonly original form system monolith state field loan model object wide object since everything initial proposal ultimate payment made loan microservice would exactly end entity service crud operation high coupling service shown try playing crc design find request reach loan service le evident diagram cost embedding state model entity service directly put state field loan every loan must go state machine lock u single kind business process time already knew company exploring directfunded loan banking partner would minimum two flavor process one process proliferating branch briefly considered using devs library represent state plus state machine edn data loan ultimately decided instead thought could make state service shown business process move along really sending document service example proposal project send projectstarter document contains attribute needed project analyst approves project analyst gui backend creates loanstarter sends unfundedloan service likewise funding received collection service creates loanpackage document sends underwriting service collection gatherer document collection break kneecap downstream set schedule payment receive requestor payment issue backer also keep set ledger track balance per account service facility add update information relevant service ignores anything incoming document need give u lot flexibility build overall process consider directfunding scenario need new directfunding service find suitable candidate sends document bank receives response favorable response directfunding create version loanpackage document underwriting word treating stage service connected welldefined document format allows u introduce pathway without creating state machine hell additional benefit easily monitor flow document see process healthy monitor service activity create cumulative flow diagram get lot visibility since stage triggered human eg analyst even figure staff model must scale business throughput also clear style work well event transfer instead document transfer would natural put document onto message bus overall think style offer nice degree alignment technology business downside see requires service developer understand service contributes value stream business process backtracking error race still minute window opportunity perceived inconsistency sneak example happens requestor try change proposal analyst reviewing worse change millisecond analyst click approve gui document go project service matter tell proposal service proposal longer edited without withdrawing request resubmitting new project post already getting long going answer question next time take another month since past holidayfuntimes serious winter month interested learning breaking monolith might like monolith microservices workshop hosting session march sunny florida especially dear friend colleague back home know march great time minnesota contact schedule workshop company
233,Lobsters,scaling,Scaling and architecture,Kickstarting free software: one week later,http://pythonsweetness.tumblr.com/post/171830663802/kickstarting-free-software-one-week-later,kickstarting free software one week later,mitogen extension ansible mitogen library writing distributed program require zero deployment vastly improves ansible performance initial tester bug found fixed one report via reddit performance improvement fantastical minute minute withitems many lesson real user stake real money towards something seemingly mundane free infrastructure thank,incredibly intense first week crowdfunding mitogen extension ansible involving far effort anticipated worked almost flat waking early hour ensure query answered thoroughly complain much fun change almost nothing experience already campaign reached exposure receivedas recap mitogen library writing distributed program require zero deployment prototype extension implementing architectural change vastly improves ansible performance common scenario laying framework extend advantage far beyond simple overhead reductioninitial testersa great deal work simply staying top bug report ensuring experience prototype solid report one tester assume hit bug could report itof many report received addressed almost promptly fabulous bug found fixed along one report via reddit performance improvement fantastical exceeds even contrived overheadheavy example mitogen playbook runtime went minute minute awesome work common theme anywhere withitems appears mitogen profound impact obvious reason loop module executed repeatedly one iteration guaranteed compiled ready targetso many lesson developing campaign thought exercise one idle sunday evening actually practical project taken lot work far anticipated almost every step learned something novel reuseable knowledge anyone attempting similar project future write time permitsregardless outcome campaign already proven one exciting result real user stake real money towards something seemingly mundane free infrastructure think beyond amazing world content throw million dollar junk icos almost weekly crowdfunding free software seems practice happen far oftenthank youi wish thank everyone support shown thus far encourage consider tapping ansible user know shoulder let know project working close infrastructure consulting please consider using final week corner bos regarding associating company logo sexy project promise receive many eyeball coming yearsthanks reading david
234,Lobsters,scaling,Scaling and architecture,Regional evacuation in under 10 minutes at Netflix,https://medium.com/netflix-techblog/project-nimble-region-evacuation-reimagined-d0d0568254d4,regional evacuation minute netflix,project nimble region evacuation reimagined three prior article failovers took long minute minute minute eureka archaius minute zuul minute make failing fast fast first iteration pin high right tool dark capacity bakes amis redblack edda spinnaker base ami ribbon eureka u e v c c v r atlas turned going next join u,project nimble region evacuation reimaginedwe proud present nimble evolution netflix failover architecture make region evacuation order magnitude faster netflix goal customer whenever want come watch favorite show lot work center around making system ever available averting limiting customerfacing outage one important tool toolbox route traffic away aws region unhealthy netflix continues grow quickly point even short partial outage affect many customer critical able route traffic away region quickly needed article describes reimagined region failover used take close hour le minute remaining cost neutralthe history region evacuation netflix captured three prior article traffic failovers important tool disposal time nimble take u next level optimizing way use existing capacity migrate traffic part project requirement wanted minimal change core infrastructure disruption work schedule onerous maintenance requirement dropped engineering team companyfailovers took longwhen set journey began breaking time took traffic failover minute decide whether would push failover button failover operation took time operation made enormous amount aws mutation could potentially confuse state healthy region result failover somewhat risky slow path minute provision resource aws included predicting necessary scaleup scaling destination region absorb traffic nontrivial netflix service autoscale following diurnal pattern traffic cluster overprovisioned point could absorb additional traffic would see failed traffic another region result failover needed include step computing much capacity required service minute service start boot aws instance launch service download resource required operate make backend connection register eureka apply morphing change specified archaius configuration management register aws instance tough could much coax starting faster threat receiving traffic minute proxy traffic destination region compensate dns ttl delay used zuul proxy migrate traffic via backend tunnel region approach also allowed u gauge readiness region take traffic instance generally need time reach optimal operation eg via jiting found needed move traffic increment give new instance chance absorb new traffic minute cut dns call repoint dns entry completed within second dns ttls generally meant bulk device would move within minute considered failover complete vast majority device moved using new dns entry served destination regionall step add minute considered unacceptably long remember operating scale customer together watch million hour content every day minute broken experience impact lot people needed something much fasterhow make failing fast fastwe set aggressive goal able fail traffic le minute order hit kind speed needed eliminate long pole needed service start instantly ready take traffic without warmup period could meet requirement regional failover would consist purely flipping dns record letting network move user overfirst iteration pin high service maintain homeostatic balance using autoscaling policy service run asg auto scaling group service cpubound instance may choose add instance group average cpu usage cross threshold remove instance average cpu usage drop another lower threshold given year since migration cloud mechanism netflix dev team operationally familiar wellunderstood normal crisis operationsif attempted modify group run cold precalculating needed capacity absorb failover another region would need make significant change either would need change signal team used autoscaling something centralized giving service instruction divorced normal operation would need alter every autoscaling policy kind linear worse transformation take account failover absorption need idea opening targeted consultation hundred scaling policy netflix seem like winning strategywe also considered simply abandoning autoscaling altogether pinning calculated value would hide performance regression code absorbing potentially enormous buffer intended regional evacuation absorption would need come automated way frequently calculate desired service size given incoming rps scale buffer based metric mechanism yet availablewe needed come something cleverthe right tool dark capacityhaving capacity ready take traffic seemed like right solution adding active service frontline would add operational burden want incur would keep spare instance ready without affecting production solution essentially combine benefit extra capacity without distributed burden operating itnetflix bakes amis rather configuration management prepare instance base launch realized could keep instance hidden away shadow asgs would work dark group topping capacity service shadowing would ensure total isolation group streaming metric analysis path activated need figure mechanism add running service needed failover would look like provisioned new instance spontaneously called uponour setup based relatively unknown detach attach instance mechanism aws provides asgs essentially pluck instance dark autoscaling group push ether make subsequent api call pop running service group created orchestrator detaches instance keep track attache intended destinationsit straightforward test detach attach mechanism single asg production environment incorporating many asgs would need much better mechanism track active service follow redblack push clone configuration production service leveraged netflix edda spinnaker cloud management deployment tool apis track change frontline asgs clone dark autoscaling cluster identical launch configuration automatically deploy pipeline rollback operation happen also needed predict many dark instance need service instance service service may easily get overwhelmed negative downstream upstream effect service eventually customer reason important get prediction right every one service high level calculation based time day much traffic expect service specific region see failovernearly netflix production application inherit base ami base ami provides wellknown netflix environment featuring consistent package system configuration well kernel tuning prepopulated set environment variable also autodetects asg service set number variable corresponding variable needed match parent service shadowing base ami team helped u interject early boot process making sure dark instance match system environment shadowing blissfully unaware actual locationnow mechanism create dark autoscaling group move instance production keep instance traffic path otherwise created elaborate mechanism pinning service high trying avoid netflix us two primary mechanism send traffic instance amazon elastic load balancer elbs internal ribbon system elbs attach one asgs dark instance existed different shadow group service main asg service elbs never saw dark instance communication method thus disabled order prevent ribbon traffic communicating dark capacity runtime team helped u devise library included service common platform prevent dark capacity registering eureka service discovery system gating starting state mode ribbon never noticed instance ready take traffic would come ready wait signal registering eurekafinally even munching customer traffic instance produce incredible amount metric functioning needed instance silent end enlisted help insight team help u disable atlas reporting hit starting gate final piece puzzle transition functioning status dark instance registered communicate reported metric fact gone entire startup procedure ready take traffic flip switch built switchhow turned going nextwe indeed reached goal minute failovers complete operation minute opposed minute used take rolling change software orchestrate took team two approximately six month project timeline great example small team make big difference fast netflixfor wideranging impactful project touching control plane service every aws region operate nimble simplicity allows scale long team crossregional dependency nimble work without extra effort need build integration enable special code time streamingpath service enabled nimble defaultthe lack heavy maintenance burden allows traffic team focus innovation future nimble already seen tackle regional disruption looking explore new way use nimble instance service need emergency dose capacity allow hit button engage failover capacity also investigating using nimble basis quicker autoscaling response aws start fresh instance prewarmed instance ready go answering question sound interesting join u luke kosewski amjith ramanujam niosha behnam aaron blohowiak recently katharina probst
235,Lobsters,scaling,Scaling and architecture,Designing scalable application with Elixir: from umbrella project to distributed system,https://medium.com/matic-insurance/designing-scalable-application-with-elixir-from-umbrella-project-to-distributed-system-42f28c7e62f1,designing scalable application elixir umbrella project distributed system,umbrella project pro con time start worrying encapsulation end interface module end end scale distributed system rpc erlang distribution protocol fitmodel end end end modelsinterface node end test end rpc end end datasets true model true test modelsinterface true utils true espec test end distributed task false temporary oneforone end end end test end end datasetsinterface tasksupervisor datasetsinterface node end end datasetsinterface model protocol ok end erlang erlang cowboy plug espec test end match dispatch remote ok end end http limiting concurrency poolboy local poolboy oneforone end end ok end ok ok reply end end fitmodel end poolboy infinity poolboy end end conclusion,general three level code organization elixir project service level obvious way split complex system separate elixir application datasets model utils context level break responsibility inside particular service implementing context module datasetsfetchers datasetscollections implementation level particular module define datastructures function datasetsfetchersaws datasetsfetcherskaggle umbrella project pro consas mentioned main advantage using umbrella project code one place run together development test environment may play around whole system important write integration test test component altogether important early stage project development time project already split relatively independent part ready scalingcompare approach many programming language usually start monolith project try extract part separate application starting microservice approach tremendously complicates development processbut time start worrying encapsulation may noticed idea including apps main application dependency good right elixir language enough construction proper encapsulation module function public private add another project dependency module available call public function naive implementation zillow data fitting themain application look like defmodule mainzillow def rffit datasetsfetcherszillowdata utilspreprocessingnormalizedata modelsrffitmodel endendwhere datasetsfetchers utilspreprocessing modelsrf module different application freedom thoughtless using module another application couple service turn system back monolith two side still want part project accessible development test need somehow forbid crossapplication couplingthe way creating convention function one application may used another one best way extracting public function separate interface modulesinterface modulesinterfacesthe idea move public application function function called application separate module example datasets application special interface module fetchers function defmodule datasetsinterfacesfetchers alias datasetsfetchers defdelegate zillowdata fetchers defdelegate landsatdata fetchersendin simple implementation interface module delegate function call corresponding module future decided extract run datasets application another node module main part communication logicdoing application rewrite mainzillow module def rffit datasetsinterfacesfetcherszillowdata utilsinterfacespreprocessingnormalizedata modelsinterfacesrffitmodelendgenerally speaking convention want call function another application must interface modulethis approach still allows easy development testing creates set simple rule protect code tight coupling creates basis future scaling scale distributed systeminterface applicationsimagine data processing become timeconsuming decide run model separate node need remove model inumbrella true dependency run application another nodeif run elixir console iex mix main application folder access model application module anymore iex modelsinterfacesrffitmodel data undefinedfunctionerror function undefined module modelsinterfacesrf available code model application still inside umbrella project run main application accessible model module function exist another node run application onlybut know beam vm designed distributed application many way access code run another machine rpcit easy run function remote node using erlang rpc module rpc us erlang distribution protocol communication nodesone may reproduce simple experiment run themain project sname main option one terminal tabiex sname main mixand model project another tab iex sname model mixnow run calculation iex main rpccall model modelsinterfacesrf fitmodel data struct modelsrfcoefficient b data data change need make project utilize approach idea simple need add one application project implement communication logic modelsinterfacemodelsinterface config lib modelsinterface modelsinterfaceex lmex rfex mixexthis thin layer help main access modelsinterface function couple small module duplicate function interface module defmodule modelsinterfacerf def fitmodel data modelsinterfaceremotecall modelsinterfacesrf fitmodel data endendthis module call function implementation remotecall modelsinterface module defmodule modelsinterface def remotecall module fun args env mixenv doremotecall module fun args env end def remotenode applicationgetenv modelsinterface node end defp doremotecall module fun args test apply module fun args end defp doremotecall module fun args rpccall remotenode module fun args endendthe module get node location configuration remote procedure call might see environment specific implementation doremotecall allows simplify testing process discus laterthe next quick refactoring replace modelsinterfaces modelsinterface done forget add modelsinterface application dependency main applicationdefp deps datasets inumbrella true model inumbrella true test modelsinterface inumbrella true utils inumbrella true espec test endagain left model dependency test environment allows making direct call application test environmentthat able access model via iex console iex main modelsinterfacerffitmodel data struct modelsrfcoefficient b data data let summarize change new simple interfacing application still code one place still test passed distributed tasksdirect remote procedure call useful need simple synchronous interface another application want effectively run asynchronous code remote node better choose distributed taskselixir specific tasksupervisor used dynamically supervise task supervisor start inside remote application supervise task execute code let use distributed task accessing datasets application first need add tasksupervisor child datasets application supervisor defmodule datasetsapplication moduledoc false use application import supervisorspec def start type args child supervisor tasksupervisor name datasetstasksupervisor restart temporary shutdown opts strategy oneforone name datasetssupervisor supervisorstartlink child opts endendthe datasetsinterface module separate interfacing application defmodule datasetsinterface def spawntask module fun args env mixenv dospawntask module fun args env end defp dospawntask module fun args test apply module fun args end defp dospawntask module fun args tasksupervisorasync remotesupervisor module fun args taskawait end defp remotesupervisor applicationgetenv datasetsinterface tasksupervisor applicationgetenv datasetsinterface node endendso use asyncawait pattern difference task spawned remote node supervised remote supervisor name location supervisor set configuration file config datasetsinterface tasksupervisor datasetstasksupervisor node model trick test environment protocolsrpc distributed task builtin erlangelixir abstraction allow communicate using elixir term without additional serialization deserialization need communicate application written elixir need common approach http protocolas example let implement simple http interface utils application first thing need new utilsinterface application utilsinterface module similar structure modelsinterface look like defp doremotecall module fun args ok resp httpoisonpost remoteurl serialize module fun args deserialize respbody endfor example used simple erlang termtobinary binarytoterm serialization defp serialize term erlangtermtobinary term defp deserialize data erlangbinarytoterm data utils project need http server listen external request used cowboy plug thisdefp deps cowboy plug espec test endthe plug module responsible handling request defmodule utilsinterfacesplug use plugrouter plug match plug dispatch post remote ok body conn plugconnreadbody conn module fun args deserialize body result apply module fun args sendresp conn serialize result endendit deserializes module fun args tuple function call sends result back clientand forget start plug via cowboy server utils applicationchildren plugadapterscowboychildspec http utilsinterfacesplug port please note good practice call function directly deserialized data simplify example real world need sophisticated approach limiting concurrency poolboythe last feature wan na describe post allows protect application resource overflowing imagine example model application use quite lot memory model fitting want limit number client want access model application create limited pool worker process interface level using poolboy librarypoolboy need started byapplication supervisor defmodule modelsapplication use application def start type args pooloptions name local modelsinterface workermodule modelsinterfacesworker size maxoverflow child poolboychildspec modelsinterface pooloptions opts strategy oneforone name modelssupervisor supervisorstartlink child opts endendyou may see poolboy option name supervisor worker module size pool maxoverflowthe worker module simple genserver call corresponding function defmodule modelsinterfacesworker use genserver def startlink opts genserverstartlink module ok end def init ok ok def handlecall module fun args state result apply module fun args reply result state endendand last change modelsinterfacesrf module instead function delegation spawn worker process inside pool defmodule modelsinterfacesrf def fitmodel data withpoolboy modelsrf fitmodel data end def withpoolboy args worker poolboycheckout modelsinterface result genservercall worker args infinity poolboycheckin modelsinterface worker result endendthat absolutely sure model application handle limited number requestsconclusionas conclusion wan na give recommendation start microservices beginning easy elixir umbrella projectuse context implementation module organize logic inside applicationthink carefully application interface allow direct call implementation function applicationswhen scaling distributed system place communication logic separate application use erlang distribution protocol communication beam applicationsi hope approach abstraction described article help write better code elixir hit enjoyed article hesitate contact question proposal wonderful week anton
236,Lobsters,scaling,Scaling and architecture,Quadrupling Ansible performance with Mitogen,http://pythonsweetness.tumblr.com/post/171589071872/quadrupling-ansible-performance-with-mitogen,quadrupling ansible performance mitogen,mitogen extension ansible project need support mitogen ansibleplaybook mitogen library writing distributed program require zero deployment basic playbook ssh pipelining mitogen extension ansible unmodified playbook executes time faster consuming le bandwidth half cpu time twice many target change required target machine documentation issue possible networkcapable fork steroid absolutely everything reused ansible slower equivalent ssh command speed paradigm shift adore could autumn pushing brain ether evil agent required teased last year pull mode opening volley connection delegation caching serving ansible code ram intermediary mbytes data every playbook step become importer bug fix delegation also operates recursively asynchronous connect topologyaware file synchronization synchronize rsync generalized forwarding mitogen message routing topologyaware git pull exist gitfetchpack gitlsremote gitpull final word inversion control applevel connection persistence cost slow tooling star github per invocation per year future beautiful want exist without support want crowdfund awesome tooling deliver feature built ansible never dreamed possible pledge today like find combating obsolescence beloved tool mgmt,tl dr mitogen extension ansible exists today awesome promised want push thing much value free time project need support allegedly site developer two summer ago found situation doubt familiar despite preference unrelated problem inevitably gravitate towards whoever deal following exhausting day spent watching dogslow ansible job fail repeatedly one evening dusted personal aid help relax ancient perpetually unfinished hobby project whose sole function simply remind thing always improvesomething miracle struck early hour next morning almost every outstanding issue solved disbelief code ran reliably month later first time living memory excited report delivery project one sufficient complexity warranted extreme persistence case concept implementation decadethe miracle come form mitogen tiny python library heard hope ansible user soon eternally glad discovering ansibleplaybook completes reasonable time even face deeply unreasonable operating conditionsmitogen library writing distributed program require zero deployment specifically designed fit need infrastructure software like ansible without upfront configuration support unix machine featuring installed python interpreter say almost concept hard explain even fellow engineer value easy grasp trace show two ansible run basic playbook m latency network single target host first run employ ssh pipelining ansible current optimal configuration consumes almost mbytes network bandwidth running time secsthe second us prototype mitogen extension ansible far reasonable kbytes consumed sec unmodified playbook executes time faster consuming le bandwidthless half cpu time consumed host machine meaning one metric handle least twice many target crucially change required target machine including new software nasty ondisk cache contend withwhile pure overhead measured benefit much extend realworld scenario see documentation time issue time cpu exampleshow possible mitogen perhaps easily described kind networkcapable fork steroid allows program establish lazilyloaded duplicate remote host without requiring upfront remote disk writes communicate copy exist copy turn recursively split produce child bidirectional message routing every copy handled automaticallyin context ansible unlike ssh pipelining one ssh invocation sudo invocation script compilation required every playbook step script reuploaded step mitogen one exists per target duration playbook run code cached ram step absolutely everything reused saving m every stepthe extension represents around week work replaces hundred line horrid shellrelated code ansible already point one realworld playbook ansible slower equivalent ssh command presently connection establishment singlethreaded prototype good host rest assured limitation day numberednot speed paradigm shift adoreif seems impressive improved upon prepare deep shock think extension performance improvement something surreptitious beachhead intend thoroughly assault sense realitythis performance side effect far interesting property ansible longer running host machine temporarily distributed throughout target network duration run bidirectional communication piece believe crazy functionality enableswhat told possible eliminate final turn sharply negative simultaneously reducing resource consumption surely ansible execute faster equivalent raw ssh command bet care thing could autumn read pushing brain ether evil agent requiredas teased last year ansible take name fasterthanlight communication device science fiction yet despite improvement still fundamentally bound speed information physically propagates pull agentbased tooling strongly advantageous control flow occurs point measurement necessary inform flow penalty incurred traversing networktoday reducing latency ansible mean running within target network pull mode playbook stored target alongside example secret decrypting vault hairy mechanic required keep sync executing appropriate far cry simplicity tapping ansibleplaybook liveyml laptop option last resortwhat would amazing hybrid could performance scaleability benefit pull combined stateless simplicity push without introducing dedicated host permanent cache agent running target machine amount persistent intermediate state introduce huge headache without sacrificing fabulous ability shut everything simple ctrlcthe opening volley connection delegationas first step exploiting previously impossible functionality enhance extension support delegating connection establishment machine target network avoiding cost establishing hundred ssh connection low throughput high latency network linkunlike ssh proxying huge benefit caching serving ansible code ram intermediary avoiding uploading approximatey code every playbook step ensuring cached response delivered low latency lan fabric target network target machine replaces transmission mbytes data every playbook step order kilobyte worth tiny remote procedure callsall mitogenside infrastructure exists today already used implement become support could flipped line code ansible extension importer bug fix work perfectlyfinally reminder since mitogen operates recursively delegation also operates recursively code caching connection establishment happening hop useful navigating slow link complicated firewall setup see enables exciting new scenariosasynchronous connectansible intended manage many machine simultaneously extension improvement presently work well singlemachine playbook niche application many usershaving newfound ability delegate connection establishment intermediary target network far away laptop high latency connection ability subdelegate intermediary implement divide conquer strategy forming large tree comprising final network target machine playbook run responsibility caching connection multiplexing evenly divided across tree neatly avoiding single resource bottlenecksi rewrite mitogen connection establishment asynchronous creation many downstream connection scheduled parallel ability enqueue command prior completion including recursive command would cause connection turn used intermediariesthe cost establishing connection become cost code upload latency single ssh connection per tree layer connection layer occur parallel imaginary node cluster split quarter rack node per rack connection via m network complete well secondstopologyaware file synchronizationso playbook laptop deploying django application via synchronize module ubuntu machine running datacentre m away run playbook entail groan followed long walk second rsync run invoked time via connection synchronize mbyte asset design team stop tweaking minute roundtrips buried invocation puny connection forced send total mbytes toward target networkwhat point continually resending file set machine faroff network could uploaded exactly automatically cached redistributed within target network producing exactly one upload per layer hierarchy stop delegating connection establishment module caching partial copy ansible within network nothing prevents implementing kind smart another feature cinch build bidirectional communication exists topologyaware code prototype extension already provides todaygeneralized forwardingafter brutal hour meeting involving executive hero bob senior disaster architect iii emerges bloodstained yet victorious tyrannical security team backends talk impunity entire internet aptget reach packagesdebianorg second bob daily ansible ci job requiresthat evening regaled giddy betrothed hr coordinator ii heroic story war bob catch brief yet chilling glimmer doubt transpired another way sleepily ponders succumbing cosier battle waged fatigued heavy eyelid suddenly aware bob emerges bathed mysterious utopian dreamscape ci job executed infinitely quickly war poverty exist impossible always possiblebuilding mitogen message routing forwarding kind pipe network socket becomes trivial including scheme would allow exposing transient locked http proxy bob aptget invocation long necessary line yaml playbookwhile already possible ssh forwarding handconfiguration involved messy becomes extremely hairy target forward host machine initial goal support forwarding unix tcp socket cover use case mind speaking topologyaware git pullanother common security fail seen ansible playbook call git directly target machine including granting machine access git server horrid violation even readonly access implies machine need permanent firewall rule exist scant moment pull progress granting backends access site complex githubcom may well abandon outbound firewalling enough even puniest script kiddy exfiltrate production databasewhat git could run permission local ansible user user machine served efficiently target machine duration push faster machine talking githubcom single readonly repository intended building generalized forwarding topologyaware git repeat caching singleupload trick file synchronization time implementing git protocol nodein scheme implement single roundtrip necessary gitfetchpack pull changed object laptop high latency link propagating lan speed throughout target network gitlsremote output delivered part message initiate pull result efficient normal gitpull backends longer require network access gitthe final word inversion controlremember talked making ansible run faster equivalent ssh command well today ansible requires one network roundtrip per playbook step like ssh must pay penalty every roundtrip unless something give something partial delegation control target machine itselfwith inversion control role ansibleplaybook simply becomes shipping code selective chunk data target machine machine execute make control decision without necessitating conversation master step figure execute nextansible framework enable implementing today significantly extending prototype extension existing strategy plugin teaching automatically send wait batch task rather single task timeaside improved performance semantics existing linear strategy preserved playbook need changed cope target machine task suddenly begin running concurrently order different previouslyapplevel connection persistenceas final battle latency playbook development debugging support detaching connection tree ansibleplaybook exit teach extension reuse startup reduce overhead repeat run especially many target order hundred millisecond new ssh connection module compilation code uploads requiredconnection persistence open floodgate adding sweet new tooling although sure desirable expose implementation detail like forever also extending interface provided ansible simple example could provide ansiblessh tool reuses connection tree along ansible tunnelling delegation dynamic inventory authentication configuration forward pipe remote shellthe cost slow toolingansible star github representing user github account ever thought star appears grow star per week around london going rate hire one user conservatively could expect user trotting minute run ansibleplaybook liveyml least per weekwe expect ansible running merely twice slowly necessary minute run lost productivity across user economic cost region per invocation per year reality average user running ansible far often including thousand time per minute various ci system worldwide run often last far longer minute recommend mental guesstimation left exercise reader already blind drunkthe future beautiful want bemy name david nothing jinx day quite like slow tooling poured easily hour form project decade time project reached inflection point fun part science done effect real small highly predictable set milestone remain deliver hope agree much brighter futurebefore reading doubt would believed possible provide feature described without complex infrastructure running target network hope join disproving one final impossibilitywhile everything exist time exist without support like try something crazy would allow devote delivering vastly improved daily routine thousand people like meyou may guessed already want crowdfund awesome toolingwhat value would place extra productive hour every working week uk easy question around per year risk contributing already proven component hope agree nobrainer employerto encourage success offering unique permanent placement brand github repository documentation fund returned minimum goal reached however week sufficient ensure well tested extension full attention given every bug ready save many hour right time enjoy early sunlight springtotalling much le economic damage caused single run today ansible grand plan divided incrementally related stretch goal imagine achieve full funding finale deliver feature built ansible never dreamed possiblewhat pledge today like find outcombating obsolescence beloved toolsas modern area deployment tooling exposed ebb flow software industry far typical unexpected disruption happens continuously without ongoing evolution exposure buggy unfamiliar new tooling guaranteed benefit barely justifying cost integration know well rational idea like costbenefit rarely win heart buzzwordhungry youthful infrastructure team counterargument must presented another wayas recent example growing love mgmt designed outset agentbased reactive distributed system much mitogen nudge ansible towards however unlike mgmt ansible preserve zeroinstall agentless nature laying sound framework significantly exciting feature alone win loyalty least guaranteed every migrationtriggering new feature implemented system headed minimal effort long foreseeable futuredavid
237,Lobsters,scaling,Scaling and architecture,How we built Hamiltix.net for less than $1 a month on AWS,https://blog.badsectorlabs.com/how-we-built-hamiltixnet-for-less-than-1-a-month-on-aws.html,built hamiltixnet le month aws,aws power complexity affordability warning serverless computing rise terrible maybe great jquery semanticui momentjs mutlidatespicker fairly simple cicd first post submodule support twelvefactor setup blog logging monitoring pushover,aws power complexity affordability enter amazon web service aws aws cloud service provider power may biggest name internet let see simple static site backend warning following architecture design choice made purpose cost saving well chance learn new technology make claim best choice given task many case much better technology make sense normally first step kind project start linux server serverless computing rise never dealt lambda serverless technology let give shot overall design hamiltix look like hamiltixnet aws stack see lambda star show nt heard lambda think service run function however complex trigger many list basically aws service trigger lambda lambda offer nodejs python java c net core go environment since already ranking module python stuck python course rest function well cloudwatch event rule kick lambda need run interval getting ranking ticket api gateway fire dynamic content website like advanced search actual ticket purchasing also made decision use javascript framework front end mostly incredebly complex people suggest terrible maybe great could use react static site sure also mean dealing animated route custom routing graphql redux sas le jsx already exhausted want present ticket cleanly user build next facebook jquery semanticui momentjs mutlidatespicker external javascript library used hamiltixnet without need server hosting site stored distributed cloudfront setting static site aws fairly simple ajax call site javascript sent api gateway turn call correct lambda function handle whatever task requested hamilton ticket price high set staging environment us ticket broker sandbox api test function commit master work need two separate environment api gateway corresponding alias lambda function nt forget publish change api gateway api gateway point lambda handler function alias corresponds either staging prod done stagevariable setting endpoint resource screen api gateway need allow api gateway permission access alias use aws provides nice awscli command set lambda proxy integration stage screen ensure stage appropriate stage variable staging prod apis call staging prod lambda alias respectively setting staging prod lambda alias difficult handled gitlab cicd pipeline cicd read first post know big fan gitlab built cicd hamiltix repo set lambda submodule gitlab currently support one gitlabciyml file repo gitlabciyml file lambda nearly identical purpose variable section top additional cp statement custom directory needed change lambda function strict twelvefactor follower notice build release stage combined certianlly possible break build step pas zip artifact stage fast nt done yet variable set git strategy gitstrategy clone gitsubmodulestrategy recursive key secret defined project ci setting exposed env variable awsaccesskeyid awsaccesskeyid awssecretaccesskey awssecretaccesskey awsdefaultregion name myfunction filename myfunctionpy handler myfunctionlambdahandler runtime role arn aws iam xxxxxxxxxxxxx rolexxxxxxxxxx file fileb deploy cicommitrefnamezip stage test deploy test stage test image badsectorlabscodechecking latest docker image contains lot code checking tool script cpd minimumtokens language python file pylint output good look worth breaking build pylint badcontinuation linetoolong importerror missingdocstring filename true maxlinelength must pas wrong changed deploystaging stage deploy image badsectorlabsawscompressanddeploy latest variable alias staging desc staging build commit cicommitsha script virtualenv p env source envbinactivate pip install r requirementstxt mkdir dist cp filename dist copy file needed dist copy directory module etc cp rf dist cd dist zip deploy cicommitrefnamezip cd deactivate l lart echo creating updating name capture code hash updatedcreated lambda function r needed jq strip quote aws lambda updatefunctioncode functionname name zipfile file jq r aws lambda createfunction functionname name runtime runtime role role handler handler zipfile file jq r echo publishing latest staging version aws lambda publishversion functionname name description desc jq r version echo published latest version version aws lambda updatealias functionname name name alias functionversion version aws lambda createalias functionname name name alias description staging functionversion version echo successfully updated name alias environment name masterstaging master deployprod stage deploy image badsectorlabsawscompressanddeploy latest variable alias prod desc prod build commit cicommitsha script virtualenv p env source envbinactivate pip install r requirementstxt mkdir dist cp filename dist copy file needed dist copy directory module etc cp rf dist cd dist touch prod canary tell lambda function use prod secret zip deploy cicommitrefnamezip cd deactivate l lart echo creating updating name capture code hash updatedcreated lambda function r needed jq strip quote aws lambda updatefunctioncode functionname name zipfile file jq r aws lambda createfunction functionname name runtime runtime role role handler handler zipfile file jq r echo publishing latest prod version aws lambda publishversion functionname name description desc jq r version echo published latest version version aws lambda updatealias functionname name name alias functionversion version aws lambda createalias functionname name name alias description prod functionversion version echo successfully updated name alias environment name masterprod master manual using ci setup lambda check prod ospathexists prod read env variable production environment otherwise use staging variable note staging production variable must defined lambda setting alias take snapshot lambda setting prevent setting change breaking alias already exist ci setup pushing static site asset look nearly identical setup blog logging monitoring lambda working away becomes necessary monitor default lambda log standard cloudwatch nice need go back see caused issue nt help alert issue occurs many way solve issue including many would leverage aws service already lifetime pushover account decided use instant push notification unhandled lambda error def sendpushover message title soundpushover send pushover message param message string message send param title string title message param sound string one key pushover pushover default bike bike bugle bugle cashregister cash register classical classical cosmic cosmic falling falling gamelan gamelan incoming incoming intermission intermission magic magic mechanical mechanical pianobar piano bar siren siren spacealarm space alarm tugboat tug boat alien alien alarm long climb climb long persistent persistent long echo pushover echo long updown long none none silent return none pushover import init pushoverinit install install pythonpushover pushover import client send pushover notification via api hamiltix key pushoverinit xxxxxxxxxxxxxxxxxxxxxxxxxxxxx client client xxxxxxxxxxxxxxxxxxxxxxxxxxxxx clientsendmessage message titletitle soundsound def lambdahandler event context try return main event context except exception e print fatal caught exception format e import traceback errortrace tracebackformatexc print errortrace errortitle error lambdafunctionname sendpushover errortrace errortitle soundfalling raise e make sure lambda function return getting push alert time error help u respond issue soon come sendpushover used alert thing well like time ticket purchaced cash register sound naturally
238,Lobsters,scaling,Scaling and architecture,Testing Database Changes the Right Way,https://heapanalytics.com/blog/engineering/testing-database-changes-right-way,testing database change right way,safely accurately evaluate change database approach work approach benchmarking accurately map production zfs file system performance investigation enter shadow prod staging environment datalayer change mean shadow prod machine experiencing exact workload production machine populating shadow prod machine quickly easily give u new machine exact copy existing production machine except change want testing mirroring read writes analyzing result previous post discussed tool built query plan occur control machine one get production query plan collect experimental machine change expected compared control machine query plan collect experimental machine change query expected improve actually improve query expected improve stay buffer shared seq scan number actual execution time m buffer shared index scan using numbersnidx number actual execution time m realworld example table partitioning rolled partitioning found performance improvement production matched expected shadow prod unexpected regression mmalipser apply heap,hood heap powered petabytescale cluster postgres instance dataset large customer run wide variety openended ad hoc analytical query cover large portion dataset order support ever larger customer complex query large amount data always working making query faster published blog post optimize database performance found one biggest problem ran scaled cluster safely accurately evaluate change database post look system designed answer question approach work initial method testing database change largely based around synthetic isolated benchmark testing new schema would copy small amount production data onto isolated machine run example query would also write benchmark would take subset data measure long would take insert database everything looked good would start rolling change production unsurprisingly approach benchmarking accurately map production sometimes would encounter customer whose data different benchmark data critical way change would cause regression customer time change would work well isolated benchmarking behave completely differently deployed full workload specific example one change enabling compression across cluster replacing file system zfs file system tested compression isolation everything looked great rolled production discovered longer able ingest incoming data real time eventually able improve ingestion performance keep data compressed performance investigation still expensive mistake significant amount engineering work go fixing problem temporarily provided degraded experience customer enter shadow prod order safely accurately test change roll production built system call shadow prod think shadow prod staging environment datalayer change machine shadow prod mirror machine production different configuration want evaluate read writes go production machine mirrored shadow prod machine mean shadow prod machine experiencing exact workload production machine importantly shadow prod designed something happen shadow prod machine would minimal impact production shadow prod give u place test database change environment exactly like production without risk associated making customervisible change rest post detail shadow prod work make sure result get accurate populating shadow prod machine spin new shadow prod machine lean postgres backuprestore functionality database change require schema change eg testing different hardware configuration setting spin new machine desired configuration restore postgres backup onto quickly easily give u new machine exact copy existing production machine except change want testing since restoring backup impact production system additional benefit setting machine way test backup every time spin new shadow prod machine schemalevel change bit work need go getting data new schema idea still applies able restore backup existing production machine transform data new schema since restoring machine backup transform machine impact production whenever testing change spin two copy machine designate one control machine experimental machine control machine configuration production setting experiment way baseline compare query experimental machine protects u possibility whatever populate machine affected resulting performance mirroring read writes mirroring read writes conceptually simple whenever read write sent production also send query relevant machine shadow prod practice tricky developed technique make easier writing data might need handle different schema experimental node rest database instead inserting data directly database table use set postgres userdefined function insert data underlying table way service insert data database need aware underlying schema instead inserting data service use fixed query call necessary userdefined function function handle inserting data database correct schema enables u many different schema shadow prod still interface insert data ingestion code treat shadow prod database like prod database need know table live order mirror production read shadow prod whenever user run query heap generate sql query run main database cluster machine shadow prod return result production query shadow prod outage affect production read experiment require custom logic generate correct sql writing custom logic much additional work need logic anyway roll experiment production schema change possible completely paper difference schema using postgres feature view table inheritance analyzing result order trust result shadow prod experiment need make sure performance change control experimental machine due change made due unrelated factor previous post discussed tool built automatically collect data every query execute data includes postgres query plan ie description query executes number row generated part query execution even number block read disk portion query execution include unique identifier every query run enables u correlate query run shadow prod database corresponding query control machine production along performance tool shadow prod allows u perform direct comparison experimental condition every query run using data built extremely helpful workflow validating result experiment specify code exact difference expect see query run control machine query run experimental machine specifying expected difference code allows u programmatically run sanity check performance data following query plan occur control machine one get production mean control machine different production important way need resolve continuing query plan collect experimental machine change expected compared control machine mean optimization working intended query plan collect experimental machine unexpected change change query plan beyond expect mean either change thing expect flaw design experiment query expected improve actually improve query expected improve stay like check help u make sure change performance attributable change made external factor simple example give understanding tooling work let say table million random number create table number n select trunc random bigint generateseries g let say user commonly run query count many time number occurs table select count number n believe creating index number table make query faster create index number n test impact index setup shadow prod experiment setup control machine like production index table experimental machine create new index table run query experimental machine tooling automatically collect information query executed emit following information aggregate actual buffer shared seq scan number actual filter n row removed filter buffer shared planning time m execution time m practice tool emits information json information json make easy programmatic analysis data information format display purpose key piece information bolded postgres took m execute query practically time spent performing sequential scan number table run query experimental machine get following output aggregate actual buffer shared index scan using numbersnidx number actual index cond n buffer shared planning time m execution time m case see query took m postgres us index scan instead sequential scan read much fewer block disk wanted sanity check experiment make sure shadow prod working expected would write code check following condition sequential scan number table become index scan index scan number table significantly faster sequential scan index scan number table read significantly fewer block sequential scan besides change change query execution performance part query remains code checking condition automatically surface query fail one check allow u either verify index impact expected discover specific query realworld example table partitioning recently used shadow prod test table partitioning database technique table split several smaller table contain disjoint subset data case splitting single event table three smaller table session pageviews otherevents expected improve performance query touch lot session lot pageviews due improved disk locality information benefit table partitioning read official postgres doc table partitioning tested table partitioning shadow prod ran following sanity check scan event table control machine corresponded scan exactly one session pageviews otherevents scan event table becoming scan one child table change query plan scan either session table pageviews table experimental machine read significantly fewer block run significantly faster corresponding scan control machine besides scan event table portion query plan comparable performance experimental machine control machine running check tooling automatically display diff expected change query execution actual change allows u quickly isolate eliminate confounding factor experiment check also enable u detect edge case optimization making fix roll anything production tooling exposed serious flaw experiment would corrupted result one unintentional configuration difference machine server local ssds use read cache slower eb disk done feature zfs read cache happened disabled control machine due machine restart made experimental machine look better relative term discovered problem tool flagged query experimental machine faster query control machine even though plan another issue able detect particular type query optimized partitioned schema query wound inefficiently reading data three session pageviews otherevents table thanks shadow prod able detect fix problem rolled anything production rolled partitioning found performance improvement production matched expected shadow prod unexpected regression mean shadow prod actually work way accurately measure impact change shadow prod environment invaluable tool testing database change able run test productionlike environment without risk affecting production allows u confident change make finally put work roll production mean go idea customervisible performance improvement matter week month much le risk question feel free reach twitter mmalipser building kind system sound interesting definitely apply heap
239,Lobsters,scaling,Scaling and architecture,"OSDDI: Jonathan Balkind, OpenPiton",https://abopen.com/news/osddi-jonathan-balkind-openpiton/,osddi jonathan balkind openpiton,openpiton,back sun microsystems made bold move open sourcing rtl code ultrasparc processor via opensparc project targeted data centre application full configuration featuring core hardware thread processor used sun fujitsu system typically supporting workload web java application server openpiton processor build opensparc designed enable scalable architecture research prototype million core latest episode open source digital design insight get hear jonathan balkind provides introduction openpiton group behind commercial application relevance data centre future
241,Lobsters,scaling,Scaling and architecture,"Scaling Kubernetes to 2,500 Nodes",https://blog.openai.com/scaling-kubernetes-to-2500-nodes/,scaling kubernetes node,running kubernetes etcd kubeapiserver gke etcd datadog prometheus fluentd issue autoscaler kube master kubecontrollermanager kubescheduler high availability autoscaler pod kubedns antiaffinity rule docker image pull dota pending kubelet quota networking public benchmark warning arp cache dnsmasq dmesg hiring,running kubernetes deep learning research two year largestscale workload manage bare cloud vms directly kubernetes provides fast iteration cycle reasonable scalability lack boilerplate make ideal experiment operate several kubernetes cluster cloud physical hardware largest pushed node cluster run azure combination vms path scale many system component caused breakage including etcd kube master docker image pull network kubedns even machine arp cache felt helpful share specific issue ran solved etcd passing node cluster researcher started reporting regular timeouts kubectl command line tool tried adding kube master vms running kubeapiserver seemed solve problem temporarily passed replica knew treating symptom cause comparison gke us single vm node made u strongly suspect etcd cluster central store state kube master looking datadog saw write latency spiking hundred millisecond machine running etcd replica despite machine using ssd capable iop latency spike blocking whole cluster benchmarking performance fio saw etcd able use available iop write latency etcd sequential io making latencybound moved etcd directory node local temp disk ssd connected directly instance rather networkattached one switching local disk brought write latency etcd became healthy cluster ran well passed node point saw high commit latency etcd time noticed kubeapiservers reading etcd set prometheus monitor apiservers also set auditlogpath auditlogmaxbackup flag enabled logging apiserver surfaced number slow query excessive call list api event root cause default setting fluentd datadog monitoring process query apiservers every node cluster example issue fixed simply changed process le aggressive polling load apiservers became stable etcd egress dropped almost nothing negative image represents egress another helpful tweak storing kubernetes event separate etcd cluster spike event creation affect performance main etcd instance set etcdserversoverrides flag something like etcdserversoverridesevents http http http another node failure hit etcd hard storage limit default cause stop accepting writes triggered cascading failure kube node failed health check autoscaler decided thus needed terminate worker increased max etcd size quotabackendbytes flag autoscaler sanity check take action would terminate cluster kube master colocate kubeapiserver kubecontrollermanager kubescheduler process machine high availability always least master set apiservercount flag number apiservers running otherwise prometheus monitoring get confused instance use kubernetes mainly batch scheduling system rely autoscaler dynamically scale cluster let u significantly reduce cost idle node still providing low latency iterating rapidly default kubescheduler policy spread load evenly among node want opposite unused node terminated also large pod scheduled quickly switched following policy kind policy apiversion predicate name generalpredicates name matchinterpodaffinity name nodiskconflict name novolumezoneconflict name podtoleratesnodetaints priority name mostrequestedpriority weight name interpodaffinitypriority weight use kubedns extensively service discovery soon rolling new scheduling policy started reliability issue found failure happening certain pod kubedns new scheduling policy machine ended running copy kubedns creating hotspot exceeded allowed azure vm external domain lookup fixed adding antiaffinity rule kubedns pod affinity podantiaffinity requiredduringschedulingignoredduringexecution weight labelselector matchexpressions key operator value kubedns topologykey kubernetesiohostname docker image pull dota project started kubernetes scaled noticed fresh kubernetes node often pod sitting pending long time game image around would often take minute pull fresh cluster node understood dota container would pending true container well digging found kubelet serializeimagepulls flag default true meaning dota image pull blocked image changing false required switching docker rather aufs speed pull also moved docker root instanceattached ssd like etcd machine even optimizing pull speed saw pod failing start cryptic error message rpc error code desc nethttp request canceled kubelet docker log also contained message indicating image pull canceled due lack progress tracked root large image taking long pullextract time long backlog image pull address set kubelet imagepullprogressdeadline flag minute set docker daemon maxconcurrentdownloads option second option speed extraction large image allowed queue image pull parallel last docker pull issue due google container registry default kubelet pull special image gcrio controlled podinfracontainerimage flag used starting new container pull fails reason like exceeding quota node able launch container node go nat reach gcrio rather public ip quite likely hit perip quota limit fix simply preloaded docker image machine image kubernetes worker using docker image save optpreloadeddockerimagestar docker image load optpreloadeddockerimagestar improve performance whitelist common openaiinternal image like dota image networking experiment grow larger also become increasingly complex distributed system rely heavily network operation first started running distributed experiment became immediately obvious networking configured well directly machine got throughput kube pod using flannel maxing machine zone public benchmark show similar number meaning issue nt likely bad config instead something inherent environment contrast flannel add overhead physical machine work around user add two different setting disable flannel pod hostnetwork true dnspolicy clusterfirstwithhostnet though read warning kubernetes documentation arp cache despite dns tuning still saw intermittent issue dns resolution one day engineer reported nc v redis server taking second print connection established tracked issue kernel arp stack initial investigation redis pod host showed something seriously wrong network communication port hanging multiple second dns name could resolved via local dnsmasq daemon dig printing cryptic failure message internalsend invalid argument dmesg log informative neighbor table overflow meant arp cache run space arp used mapping network address address physical address mac address fortunately easy fix setting option etcsysctlconf common tune setting hpc cluster particularly relevant kubernetes cluster since every pod ip address consumes space arp cache kubernetes cluster incidentfree month planning scale even larger cluster recently upgraded version excited see officially support interested building large scale compute cluster hiring
242,Lobsters,scaling,Scaling and architecture,Scaling SQLite to 4M QPS on a Single Server (EC2 vs Bare Metal),https://blog.expensify.com/2018/01/08/scaling-sqlite-to-4m-qps-on-a-single-server/,scaling sqlite qps single server v bare metal,wwwbedrockdbcom job queue replicated caching layer explosive growth trying path million query per second sqlite consortium result bare metal v github observation conclusion epilogue help u share post like,expensify unusual technology stack many way example use dns internally configurationmanaged etchosts file work great similarly make limited use aws instead hosting hardware web database layer work great surprising use mysql postgres instead using none sqlite work great granted sqlite wrapped custom distributed transaction layer named bedrock open sourced available wwwbedrockdbcom workhorse behind core database also power missioncritical job queue provides locallyhosted replicated caching layer important function depends bedrock accordingly given explosive growth upgrading server power bedrock top priority along upgrading bedrock help incredible sqlite team sqlite take advantage hardware end roll entirely new generation selfhosted hardware following basic spec ram nvme ssd storage physical core hyperthreading monster server nearly biggest possible buy run stock linux kernel without getting supercomputer territory getting bunch hardware alone enough clear spec would pointless database almost nothing scale handle kind hardware well almost nobody try trying traditional wisdom always build meaning better large number small server relatively small number large server company shard data separate server put customer server route customer right server sign design work super well many application mean never need learn manage big database lot small database expensify bit different sharding clear faultline every single user share expense every single user regardless work critical feature enables u offer good support large accounting firm manage hundred even thousand client company take firm want give firm access company data without needing migrate company onto server accounting firm already said another way accountant expensify inbox show everything need aggregated across client simultaneously without needing switch one client next see anything need done sure way even shard lack sharding make bunch similar kind multiclient processing super easy path million query per second obviously sqlite never designed anything like change fact incredibly well albeit modification created sqlite year request enthusiastic sponsor sqlite consortium tremendous help modification open source available anyone ask sure hook largely scope post consist remember disable posix advisory lock prevents accessing database external process eg command line tool database running acceptable use make random deterministic reason nondeterministic default disable malloc global lock remember apparently needed case extend maximum size memory map minor tweak like word core design sqlite pretty much point think people realize incredible tool rather effort optimizing performance system came optimizing bios kernel power setting eliminate extraneous memory access eg disable prefetching prevent cpu selfthrottling conserve power result bare metal v general conception faster cheaper easier hosting hardware maybe old school never quite subscribed notion best price possibly get server prepay year year commitment price still pay day one equal cost hardware think amazon going take chance going buy hardware unless get paid front confident get paid back many time amount cash hardware except rather getting five year solid use need pay amount money time unlike phone server hardware turn pumpkin year come astonishing premium perhaps actual hardware cost fine value convenience much le known also come enormous performance penalty time writing largest instance possibly buy vcpus ram cost per hour meaning single server commit year prepay get year server compare host fraction front ongoing cost vertical axis show total number query per second horizontal axis show many thread used get datapoint test run thread count configuration eg thread thread etc fastest individual second used result important take cpu cache warm filter artificially slow sample full performance test found github query sum range row randomly located inside row two column database random integer one indexed mean test fully cached ram database precached prior test writes done test read thread numa aware meaning local memory access done inside local memory node database split numa node though contrary expectation numa awareness actually matter much test everything done stock ubuntu install orange line show total aggregate performance box capping around query per second blue line show test bare metal machine get upwards query per second keep climbing duration test care thread purpose test worry going higher red line show test row table larger physical ram bare metal machine observation first wow ton horsepower query per second single server nothing sneeze even recognizing pretty artificial query testing test select get query per second even artificial second sqlite scale amazingly almost box test achieves almost perfect linear scalability add physical cpu hyperthreading work much better expected eeking extra capacity third wtf wrong really assumed vcpu roughly equal physical cpu man wrong really highlighted basic chart time showing average performance thread performance change thread count blue line show pretty much expect perthread performance stay stable remarkably new physical core activated start drop hit hyperthreading territory fact scaling far linearly expected test designed way ensure cpu accessing remote ram time mean pretty much worstcase scenario term numa access seem degrade performance test much look like intel something right purley new memory architecture pretty solid additionally red line show even read go disk negligible impact total performance attribute crazy fast access nvme ssd drive much better expected orange line show new thread box come substantial penalty others first chart show pretty obviously virtual cpu actually mean physical cpu hyperthreading pretty disappointing misleading really assumed every vcpu implied equal performance importantly however curve orange line easier see test rerun replacing row database singlerow database remove memory access equation please excuse jagged data run shorter period per sample thus bit noisy get point across blue line show qps per thread bare machine new thread added would expect get perfect linear scalability physical core linearly drop hyperthreading come play red line box performance degrades even hitting physical core drop precipitously even worse curve line signal memory access slowing thing power throttling get core start hyperthreading power consumed cpu go clearly configured thing kind balanced performance profile mean clock speed cpu reduced total power consumption get high thereby limiting power consumption expense performance result curved performanceperthreadcount profile level linear downward slope thread point cpu use cpu used thread compete slot adding thread consume power linear new thread take time away existing thread turn major hidden advantage hosting hardware mean configure bios suit need particular case change everything conclusion summary crazy powerful server surprisingly cheap buy long hosting hardware sqlite scale almost perfectly parallel read performance little work numa bottleneck seem nearly big problem theorized virtual cpu none remotely powerful actual cpu host epilogue testing done meltdown spectre sure performance test impacted running test anybody interested write share result early indication suggest reduction performance work remains done additionally though test focused read performance mention sqlite fantastic write performance well default sqlite us databaselevel locking minimal concurrency box option enable wal mode get fantastic read concurrency shown test lesser known branch sqlite page locking enables fantastic concurrent write performance reach sqlite folk sure tell help u share post like like loading related
243,Lobsters,scaling,Scaling and architecture,"Real-Time Applications Metrics Available on Clever Cloud, a European PaaS",https://www.clever-cloud.com/blog/features/2018/01/16/realtime-metrics/,realtime application metric available clever cloud european paas,current state enter clever cloud metric geospatial timeseries database quantum metricsgraphicsjs work metric hardware metric additional metric php league advanced usage documentation come resource get started term condition laurent doguin,glorious day bringing realtime application metric user already beta tester new metric solution big thank order thanks amazing team officialy rolling metric user today current state right want get idea metric application limited choice use third party tool manage several user happily new relic ssh application instance use local tool like htop satisfying situation knew needed fix internal monitoring system know thing allows application restart automatically among thing also slowly coming limit find new solution enter clever cloud metric original name original use case clever cloud metric name new monitoring solution based geospatial timeseries database based hadoop kafka use global scale monitor infrastructure application store hardware metric could get vms wondering made choice based mostly two thing scale issue known solution without calling new one try reimplement clustering logic clustering managed zookeeper remember hadoop kafka proven solution production year granted easy manage good news job chose u appealing reason u timeseries specialization perfect monitoring use regular datastore regular queryability hard proper time series analysis put simply sql cut warpscript warpscript extensible stack oriented programming language offer function several high level framework ease speed data analysis simply create script containing data analysis code submit platform execute close data resides get result analysis json object integrate application happens exactly need made super easy use thanks quantum query interface based polymer mean also dead easy integrate existing webapps able use something similar clever cloud console query metric application interested know console dashboard made metricsgraphicsjs work new tab console called metric show real time graph scaler hardware resource nt see anything probably enabled metric application need add environment variable enablemetricstrue exactly access hardware metric default configuration give access variety hardware metric like cpu ram disk io network io course available scaler application additional metric depending language runtime using also get access specific metric instance running top jvm activated jmx classic protocol everything metric jvm see metric exposed jmx also work haskell ekg favorite missing please let u know add support example php application give default number idle active worker well number request per second metric collected statsd mean also send metric particularly useful measure business event simple example counter incremented time specific api called statsd new leaguestatsdclient statsd configure array host port statsd increment myapipageview send statsd data need statsd client configured host port using one nicely integrated lumen php league every metric record accessible statsd prefix see simply click advanced topright corner screen presented dropdown advanced usage sometimes want go extra step select custom view drop redirected following screen click quantum perma link redirected quantum instance ready execute custom query pretty much whatever want trying anything familiar stackbased language warpscript please visit documentation come plan give metric course wether come reverse proxy give insight request go runtime framework like jmx ekg welcome suggestion anything think part default monitoring dashboard language framework please let u know plan support metric used prometheus idea monitoring regularly poll endpoint send back json containing metric already access metric api future able directly write metric well please keep mind nothing written stone roadmap thing might change really hope like new feature would love get feedback please tell u think resource get started plan add sample project write local statsd intance many others available github free signup feel coding productivity brand new project scale clicking get started agree clever cloud term condition laurent doguin head developer relation clevercloud devoxx champion also nerdy metal head living paris
244,Lobsters,scaling,Scaling and architecture,The Lambda and the Kappa Architectures,https://muratbuffalo.blogspot.com/2018/01/the-lambda-and-kappa-architectures.html,lambda kappa architecture,article jimmy lin look lambda kappa architecture lambda kappa spark apache beam written earlier mike franklin talk compared spark kappa architecture naiad tensorflow timely dataflow differential dataflow cake eat mad question diamond age book neal stephenson posted summary google tfx tensorflow extended platform presentation paper,article jimmy lin look lambda kappa architecture considers larger question one size fit answer concludes depends year ask pendulum swing apex one tool rule apex multiple tool maximum efficiency apex drawback one tool leaf efficiency table multiple tool spawn integration problem rdbms world already saw play one size rdbms fitted could nt anymore stonebraker declared one size fit seen split dedicated oltp olap database connected extracttransformload etl pipeline last couple year seeing lot one size fit hybrid transactionalanalytical processing htap solution introduced lambda kappa ok back telling story lambda kappa architecture perspective lambda kappa architecture anyway lambda nathan marz multitool solution batch computing layer top fast serving layer batch layer provides stale truth contrast realtime result fast approximate transient twitter case batch layer mapreduce framework storm serving layer top enabled fast response serving layer introduced integration hell lambda meant everything must written twice batch platform realtime platformthe two platform need indefinitely maintained parallel kept sync respect interact component integrates feature kappa jay kreps one tool fit solution kafka log streaming platform considers everything stream batch processing simply streaming historic data table merely cache latest value key log log record update table kafka stream add table abstraction firstclass citizen implemented compacted topic course already familiarknown database people incremental view maintenance kappa give one tool fit solution drawback ca nt efficient batch solution general need prioritize lowlatency response individual event highthroughput response batch event spark apache beam spark considers everything batch online stream processing considered microbatch processing spark still one tool solution written earlier mike franklin talk compared spark kappa architecture apache beam provides abstractionsapis big data processing implementation google dataflow framework explained millwheel paper differentiates event time processing time us watermark capture relation two using watermark provides information completeness observed data respect event time complete minute mark late arriving message trigger makeup procedure amend previous result course close kappa solution treat everything even batch stream would say naiad tensorflow timely dataflow differential dataflow one tool fit solution using similar dataflow concept apache beam cake eat mad question important claim paper right integration bigger pain point pendulum onetool solution side later efficiency becomes bigger pain point pendulum swing back multitool solution pendulum keep swinging back forth best world solution paper emphasizes free lunch think argument one tool solution efficient batch solution need prioritize lowlatency response individual event rather prioritizing highthroughput response batch event ca nt one tool solution refined made efficient ca nt finelytunableconfigurable tool crude hammer nanotechnology transformable tool one diamond age book neal stephenson highly parallel io computation flow would help achieve best world solution paper mention using api abstraction compromise solution quickly caution also able achieve best world abstraction leak summingbird twitter example api based solution reduced expressiveness dag computation traded achieving simplicity need maintain separate batch realtime implementation summingbird domain specific language dsl allows query automatically translated mapreduce job storm topology analogous pendulum problem day posted summary google tfx tensorflow extended platform one tool fit solution approach like ml approach today think reason integration easeofdevelopment biggest pain point day efficiency training addressed parallel training backend training already accepted batch solution integrationdevelopment problem alleviated start seeing lowlatency training demand machine learning workload may expect see pendulum swing multitoolspecialization solution space another example pendulum thinking decentralized versus centralized coordination problem take centralized coordination simple efficient strong attraction go decentralized coordination solution big pain point centralized solution geographic separation induced latency even hierarchical solution federation get close best world presentation paper respect jimmy lin take subject trench twitter time also academic evaluate intrinsic strength idea abstracted away technology really enjoyed reading paper format big data bite article written relaxed format manages teach lot page however worried read first two paragraph gave bad signal first paragraph referred one tool solution hammer associated crude rough next paragraph said high level message simple free lunch safe position may even vacuous concerned jimmy lin refraining take position well turn indeed final take thing considered took strong position article first rant yes really sidebar called rant lambda architecture strong word
245,Lobsters,scaling,Scaling and architecture,To 30 Billion and Beyond,https://www.rdegges.com/2018/to-30-billion-and-beyond/,billion beyond,ipify ipify bootstrap enter heroku fan heroku book check ugh investigation cluster module ab tool ipify httprouter popularity google alert adept ipify hit billion request http awsamazoncomlambdapricing ipify future http wwwwhoisxmlapicom ipify shoot email twitter github r,several year ago created free web service ipify ipify freely available highly scalable ip address lookup service query rest api return publicfacing ip address created ipify time building complex infrastructure management software needed dynamically discover public ip address cloud instance without using management apis searched online freely available reverse ip lookup service find suitable solution website could attempt scrape ip bad form would likely result complaint host apis charged money yuck apis allowed limited number lookup per day scared managing lot instance time apis appeared wanted upon using error go randomly otherwise high quality inspected dig record particular provider noticed entire service running single server record terminating request directly scalablehighly available service world api service looked somewhat ok trying raise money via donation stay alive integrating api service brink death make feel terribly comfortable reason figured throw small service together solve problem many people possible writing software return single string terribly hard assumed worst case spend buck per month treat public service ipify first iteration ipify quite simple wrote extremely tiny loc api service node playing around node lot time since entire premise ipify service returning string figured perfect use case node technology handling lot request minimal cpu usage api service built node threw together simple static site power frontend deployed bucket amazon configured cloudfront origin amazon cdn service sit front bucket cache page ultrafast load time designer stretch little bootstrap love thing turned halfdecent getting everything working quick testing thing seemed working ok moved onto next phase deployment enter heroku big fan heroku even written book using many year consider one underrated service developer world never used go check decided wanted run ipify scalable highly available cheap way heroku simplest best option went deployed ipify heroku minute two ran single dyno web server limited testing thing seemed working well feeling pretty happy familiar heroku let explain ipify infrastructure working heroku ran ipify web service small dyno web server ram limited cpu process crashed critical issue heroku would automatically restart heroku ran load balancer accepted incoming request app forwarded dyno web server process request nice setup everything highly available heroku load balancer dyno everything requires maintenance configuration management sort deployment code automated cheap paid run single web server fast heroku run top amazon web service aws infrastructure running one popular cloud hosted destination world aws useast virginia mean geographically running east coast u across water europe terribly far rest continental u mean user part world make enormous hop reach service point feeling pretty good took day worth work build setup test move production integrated ipify infrastructure management code solve problem initially needed resolve thing going great month started noticing ugh never marketed ipify ended ranking really high ip address api search phrase google guess year working copy editing seo paid within month ipify ranking near top google search result bringing thousand new user increased visibility service started seeing issue heroku load balancer firing warning node server servicing incoming request quickly enough ended happening many user would make api request ipify node server would start responding request slowly latency would rise heroku load balancer would notice start buffering request sending onto node server node server servicing request quickly enough load balancer would return user request would die pretty picture simple added another heroku dyno way twice capacity thing would run smoothly downside running two production dynos heroku spiked cost gone past one dyno pay normal rate figured service likely capped popularity ok paying thing would go back normal thing quite work way even week passed already getting alert heroku telling issue looked stats could see traffic doubled appeared like ipify simply getting much usage added another dyno brining monthly cost decided investigate frugal guy idea losing seemed unpleasant investigation started investigating going first thing check see many request per second rps ipify actually getting pretty surprised number low memory serf correctly ipify getting around rps time saw small number figured something bad must happening code service rps across two small web server must something horribly wrong first thing noticed running single node process easy fix started using node cluster module bam immediately running one process cpu core effectively doubled throughput heroku dynos rps still seemed like tiny number digging instead load testing heroku load testing locally laptop laptop far stronger small ram heroku dyno figured see much better throughput testing using ab tool surprised see even laptop unable surpass threshold rps node process run linux laptop ab work effectively basic profiling found node spending lot time performing basic string manipulation operation extract ip address xforwardedfor header clean matter experimented unable boost throughput much limit point production ipify service able serve roughly rps across two dynos ipify total serving million requestsmo impressive decided rewrite service go started using month performance purpose see whether could get throughput equivalent go server ipify rewriting ipify go short fun experiment gave opportunity mess around many different go routing stack gorillamux martini httprouter benchmarking playing around three routing tool ended using httprouter performed significantly better two popular option laptop able achieve requestssecond go server massive improvement memory footprint also much lower hovered around new found love go immediately took action deployed new gobased ipify service heroku result fantastic able get rps throughput single dyno brought hosting back total throughput billion requestsmo several day later talking experienced go developer ended rewriting string handling functionality netted extra rps throughput point able sustain billion requestsmo per dyno give take bit thrilled say least popularity although able cut back hosting cost reasonable range short period time within roughly two month ipify started experiencing issue growth continued rise astounding rate around time set google alert ipify know people mentioned started getting notification people started using ipify personal work project began receiving email company asking embed product including large smart tv provider numerous medium agency iot vendor etc knew ipify servicing around billion requestsmo back spending hosting cost last long noticed rather quickly ipify traffic continued rapidly increase period several month would add additional billion request month also started issue burst traffic ipify would receive massive amount burst traffic short period time would die quickly presumed traffic part bootstrapping script cron job similar timed operation later discovered user notification antivirus vendor started blocking ipify started getting used root kit virus nasty piece software attacker would use ipify get victim public ip address firing central location used malicious purpose suppose sort usage responsible large amount burst traffic fan helping vx author spending money assist made decision keep ipify running neutrally serve anyone want use never fan developer service pick choose people use like keep thing simple left deal burst traffic problem dealing burst traffic tricky primarily two choice run additional dynos cost always prepared burst traffic use auto scaling tool like adept automatically create destroy dynos based traffic pattern behalf eventually decided manually involve go option simply want spend additional fund adept service even though used fantastic around time spending ipify servicing billion requestsmo brings u recent past ipify hit billion request past month ipify hit new record surpassed billion requestsmo several occasion exciting milestone surpass something fun watch today ipify routinely serf rps almost never consistent traffic always variable usage routinely high completely given trying detect traffic pattern meaningful way average response time range depending traffic pattern today service run depending burst traffic factor factor calculator assuming spend look ipify able service request total cost astoundingly low compare expected cost running service something like lambda ipify would rack total bill compute request quick note backofanapkin math plugged ipify number lambda pricing example aws website http awsamazoncomlambdapricing extremely satisfied ipify cost frugal service serf simple purpose ipify future lead future ipify continues grow service gotten request lot different thing support something heroku support better web design metadata ip address etc incredibly busy project nowadays ended passing ownership ipify along good friend http wwwwhoisxmlapicom jonathan team good people working build portfolio valuable interesting developer api service still help thing timetotime jonathan team currently implementing new feature ipify working hard roll pretty cool change excited including improved ui data endpoint look forward seeing ipify continues grow coming year question would like get touch please shoot email r p read far might want follow twitter github subscribe via r email email new article publish
246,Lobsters,scaling,Scaling and architecture,Introduction to modern network load balancing and proxying,https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236,introduction modern network load balancing proxying,introduction modern network load balancing proxying load balancing proxy server google search load balancing network load balancing proxying defines figure service discovery health checking load balancing naming abstraction fault tolerance cost performance benefit load balancer v proxy connectionsession load balancing osi model figure tcp application load balancing grpc figure load balancing osi model load balancer feature service discovery health checking active passive load balancing power least request load balancing sticky session tl termination sni observability security do mitigation tarpitting configuration control plane post service mesh data plane v control plane whole lot type load balancer topology middle proxy figure alb nlb cloud load balancer haproxy nginx envoy edge proxy figure embedded client library figure finagle eurekaribbonhystrix grpc sidecar proxy figure envoy nginx haproxy linkerd blog post introducing envoy post service mesh data plane v control plane summary proscons different load balancer topology current state art load balancing load balancer still relevant tcpudp termination load balancer figure tcpudp passthrough load balancer figure nat connection tracking nat performance resource usage allows backends perform customized congestion control tcp congestion control bbr form baseline direct server return dsr clustered load balancing direct server return dsr figure gre fault tolerance via high availability pair figure vip bgp fault tolerance scaling via cluster distributed consistent hashing figure anycast ecmp consistent hashing maglev network load balancer nlb current state art load balancing protocol support dynamic configuration istio post service mesh data plane v control plane advanced load balancing observability extensibility lua fault tolerance architecture overview global load balancing centralized control plane figure envoy universal data plane api service mesh data plane v control plane evolution hardware software ipv dpdk fdio conclusion future load balancing,introduction modern network load balancing proxyingit brought attention recently dearth introductory educational material available modern network load balancing proxying thought load balancing one core concept required building reliable distributed system surely must quality information available searched found picking indeed slim wikipedia article load balancing proxy server contain overview concept fluid treatment subject especially pertains modern microservice architecture google search load balancing primarily turn vendor page heavy buzzword light detailsin post attempt rectify lack information providing gentle introduction modern network load balancing proxying frankly massive topic could subject entire book interest keeping article somewhat blog length try distill set complex topic simple overview depending interest feedback consider detailed follow post individual topic later pointwith bit background wrote way go network load balancing proxying wikipedia defines load balancing computing load balancing improves distribution workload across multiple computing resource computer computer cluster network link central processing unit disk drive load balancing aim optimize resource use maximize throughput minimize response time avoid overload single resource using multiple component load balancing instead single component may increase reliability availability redundancy load balancing usually involves dedicated software hardware multilayer switch domain name system server processthe definition applies aspect computing network operating system use load balancing schedule task across physical processor container orchestrator kubernetes use load balancing schedule task across compute cluster network load balancer use load balancing schedule network task across available backends remainder post cover network load balancing onlyfigure network load balancing overviewfigure show high level overview network load balancing number client requesting resource number backends load balancer sits client backends high level performs several critical task service discovery backends available system address ie load balancer talk health checking backends currently healthy available accept request load balancing algorithm used balance individual request across healthy backends proper use load balancing distributed system provides several benefit naming abstraction instead every client needing know every backend service discovery client address load balancer via predefined mechanism act name resolution delegated load balancer predefined mechanism include builtin library well known dnsipport location discussed detail belowfault tolerance via health checking various algorithmic technique load balancer effectively route around bad overloaded backend mean operator typically fix bad backend leisure v emergencycost performance benefit distributed system network rarely homogenous system likely span multiple network zone region within zone network often built relatively undersubscribed way zone oversubscription becomes norm context overundersubscription refers amount bandwidth consumable via nics percentage bandwidth available router intelligent load balancing keep request traffic within zone much possible increase performance le latency reduces overall system cost le bandwidth fiber required zone load balancer v proxywhen talking network load balancer term load balancer proxy used roughly interchangeably within industry post also treat term generally equivalent pedantically proxy load balancer vast majority proxy perform load balancing primary function might additionally argue load balancing done part embedded client library load balancer really proxy however would argue distinction add needle complexity already confusing topic type load balancer topology discussed detail post treat embedded load balancer topology special case proxying application proxying embedded library offer abstraction load balancer outside application connectionsession load balancingwhen discussing load balancing across industry today solution often bucketed two category category refer layer layer osi model reason become obvious discus load balancing think unfortunate term use osi model poor approximation complexity load balancing solution include traditional layer protocol tcp udp often end including bit piece protocol variety different osi layer ie tcp load balancer also support tl termination load balancer figure tcp termination load balancingfigure show traditional tcp load balancer case client make tcp connection load balancer load balancer terminates connection ie responds directly syn selects backend make new tcp connection backend ie sends new syn detail diagram important discussed detail section dedicated load balancingthe key takeaway section load balancer typically operates level tcpudp connectionsession thus load balancer roughly shuffle byte back forth make sure byte session wind backend load balancer unaware application detail byte shuffling byte could http redis mongodb application application load load balancing simple still see wide use shortcoming load balancing warrant investment application load balancing take following specific case example two client want talk backend connect load balancerthe load balancer make single outgoing tcp connection incoming tcp connection resulting two incoming two outgoing connectionshowever client sends request per minute rpm connection client b sends request per second rps connectionin previous scenario backend selected handle client handling approximately le load backend selected handle client b large problem generally defeat purpose load balancing first place note also problem happens multiplexing keptalive protocol multiplexing mean sending concurrent application request single connection keptalive mean closing connection active request modern protocol evolving multiplexing keptalive efficiency reason generally expensive create connection especially connection encrypted using tl load balancer impedance mismatch becoming pronounced time problem fixed load balancerfigure termination load balancingfigure show load balancer case client make single tcp connection load balancer load balancer proceeds make two backend connection client sends two stream load balancer stream sent backend stream sent backend thus even multiplexing client vastly different request load balanced efficiently across backends load balancing important modern protocol load balancing yield tremendous amount additional benefit due ability inspect application traffic covered detail load balancing osi modelas said section load balancing using osi model describing load balancing feature problematic reason least described osi model encompasses multiple discrete layer load balancing abstraction eg http traffic consider following sublayers optional transport layer security tl note networking people argue osi layer tl fall sake discussion consider tl http protocol logical http protocol header body data trailer messaging protocol grpc rest etc sophisticated load balancer may offer feature related sublayers another load balancer might small subset feature place category short load balancer landscape vastly complicated feature comparison perspective category course section touched http redis kafka mongodb etc example application protocol benefit load balancing load balancer featuresin section briefly summarize high level feature load balancer provide load balancer provide featuresservice discoveryservice discovery process load balancer determines set available backends method quite varied example include health checkinghealth checking process load balancer determines backend available serve traffic health checking generally fall two category active load balancer sends ping regular interval eg http request healthcheck endpoint backend us gauge healthpassive load balancer detects health status primary data flow eg load balancer might decide backend unhealthy three connection error row load balancer might decide backend unhealthy three http response code rowload balancingyes load balancer actually balance load given set healthy backends backend selected serve connection request load balancing algorithm active area research range simplistic one random selection round robin complicated algorithm take account variable latency backend load one popular load balancing algorithm given performance simplicity known power least request load balancingsticky sessionsin certain application important request session reach backend might caching temporary complex constructed state etc definition session varies might include http cooky property client connection attribute many load balancer support sticky session aside note session stickiness inherently fragile backend hosting session die caution advised designing system relies themtls terminationthe topic tl role edge serving securing servicetoservice communication worthy post said many load balancer large amount tl processing includes termination certificate verification pinning certificate serving using sni etcobservabilityas like say talk observability observability observability network inherently unreliable load balancer often responsible exporting stats trace log help operator figure wrong remediate problem load balancer vary widely observability output advanced load balancer offer copious output includes numeric stats distributed tracing customizable logging point enhanced observability free load balancer extra work produce however benefit data greatly outweigh relatively minor performance implicationssecurity do mitigationespecially edge deployment topology see load balancer often implement various security feature including rate limiting authentication do mitigation eg ip address tagging identification tarpitting etc configuration control planeload balancer need configured large deployment become substantial undertaking general system configures load balancer known control plane varies widely implementation information topic please see post service mesh data plane v control planeand whole lot morethis section scratched surface type functionality load balancer provide additional discussion found section load balancer belowtypes load balancer topologiesnow covered high level overview load balancer difference load balancer summary load balancer feature move various distributed system topology load balancer deployed following topology applicable load balancer middle proxyfigure middle proxy load balancing topologythe middle proxy topology shown figure likely familiar way obtain load balancing reader category encompasses hardware device cisco juniper etc cloud software solution amazon alb nlb google cloud load balancer pure software selfhosted solution haproxy nginx envoy pro middle proxy solution user simplicity general user connect load balancer via dns need worry anything else con middle proxy solution fact proxy even clustered single point failure well scaling bottleneck middle proxy also often black box make operation difficult observed problem client physical network middle proxy backend hard telledge proxyfigure edge proxy load balancing topologythe edge proxy topology shown figure really variant middle proxy topology load balancer accessible via internet scenario load balancer typically must provide additional api gateway feature tl termination rate limiting authentication sophisticated traffic routing pro con edge proxy middle proxy caveat typically unavoidable deploy dedicated edge proxy large internetfacing distributed system client typically need access system dns using arbitrary network library service owner control making embedded client library sidecar proxy topology described following section impractical run directly client additionally security reason desirable single gateway internetfacing traffic ingress systemembedded client libraryfigure load balancing via embedded client libraryto avoid single point failure scaling issue inherent middle proxy topology sophisticated infrastructure moved towards embedding load balancer directly service via library shown figure library vary greatly supported feature well known featurerich category finagle eurekaribbonhystrix grpc loosely based internal google system called stubby main pro library based solution fully distributes functionality load balancer client thus removing single point failure scaling issue previously described primary con librarybased solution fact library must implemented every language organization us distributed architecture becoming increasingly polyglot multilingual environment cost reimplementing extremely sophisticated networking library many different language become prohibitive finally deploying library upgrade across large service architecture extremely painful making highly likely many different version library running concurrently production increasing operational cognitive loadwith said library mentioned successful company able limit programming language proliferation overcome library upgrade painssidecar proxyfigure load balancing via sidecar proxya variant embedded client library load balancer topology sidecar proxy topology shown figure recent year topology popularized service mesh idea behind sidecar proxy cost slight latency penalty via hopping different process benefit embedded library approach obtained without programming language lockin popular sidecar proxy load balancer writing envoy nginx haproxy linkerd detailed treatment sidecar proxy approach please see blog post introducing envoy well post service mesh data plane v control planesummary proscons different load balancer topologiesthe middle proxy topology typically easiest load balancing topology consume fall short due single point failure scaling limitation black box operationthe edge proxy topology similar middle proxy typically avoidedthe embedded client library topology offer best performance scalability suffers need implement library every language well need upgrade library across servicesthe sidecar proxy topology perform well embedded client library topology suffer limitationsoverall think sidecar proxy topology service mesh gradually going replace topology servicetoservice communication edge proxy topology always required prior traffic entering service meshcurrent state art load balancingare load balancer still relevant post already discussed great load balancer modern protocol move load balancer feature detail mean load balancer longer relevant although opinion load balancer ultimately completely replace load balancer servicetoservice communication load balancer still extremely relevant edge almost modern large distributed architecture use twotiered load balancing architecture internet traffic benefit placing dedicated load balancer load balancer edge deployment load balancer perform substantially sophisticated analysis transformation routing application traffic handle relatively small fraction raw traffic load measured packet per second byte per second optimized load balancer fact generally make load balancer better location handle certain type do attack eg syn flood generic packet flood attack etc load balancer tend actively developed deployed often bug load balancer load balancer front health checking draining load balancer deploys substantially easier deployment mechanism used modern load balancer typically use bgp ecmp finally load balancer likely bug purely due complexity functionality load balancer route around failure anomaly lead stable overall systemin following section describe several different design middleedge proxy load balancer following design generally applicable client library sidecar proxy topologiestcpudp termination load balancersfigure termination load balancerthe first type load balancer still use termination load balancer shown figure load balancer saw introduction load balancing type load balancer two discrete tcp connection used one client load balancer one load balancer termination load balancer still used two reason relatively simple implementconnection termination close proximity low latency client substantial performance implication specifically terminating load balancer placed close client using lossy network eg cellular retransmits likely happen faster prior data moved reliable fiber transit enroute ultimate location said another way type load balancer might used point presence pop scenario raw tcp connection terminationtcpudp passthrough load balancersfigure passthrough load balancerthe second type load balancer passthrough load balancer shown figure type load balancer tcp connection terminated load balancer instead packet connection forwarded selected backend connection tracking network address translation nat take place first let define connection tracking nat connection tracking process keeping track state active tcp connection includes data whether handshake completed whether fin received long connection idle backend selected connection etcnat nat process using connection tracking data alter ipport information packet traverse load balancerusing connection tracking nat load balancer passthrough mostly raw tcp traffic client backend example let say client talking selected backend located client tcp packet arrive load balancer load balancer swap destination ip port packet also swap source ip packet ip address load balancer thus backend responds tcp connection packet go back load balancer connection tracking take place nat happen reverse directionwhy would type load balancer used place termination load balancer described previous section given complicated reason performance resource usage passthrough load balancer terminating tcp connection need buffer tcp connection window amount state stored per connection quite small generally accessed via efficient hash table lookup passthrough load balancer typically handle substantially larger number active connection packet per second pps terminating load balancerallows backends perform customized congestion control tcp congestion control mechanism endpoint internet throttle sending data overwhelm available bandwidth buffer since passthrough load balancer terminating tcp connection participate congestion control fact allows backends use different congestion control algorithm depending application use case also allows easier experimentation congestion control change eg recent bbr rollout form baseline direct server return dsr clustered load balancing passthrough load balancing required advanced load balancing technique dsr clustering distributed consistent hashing discussed following section direct server return dsr figure direct server return dsr direct server return dsr load balancer shown figure dsr build passthrough load balancer described previous section dsr optimization ingressrequest packet traverse load balancer egressresponse packet travel around load balancer directly back client primary reason interesting perform dsr many workload response traffic dwarf request traffic eg typical http requestresponse pattern assuming traffic request traffic traffic response traffic dsr used load balancer capacity meet need system since historically load balancer extremely expensive type optimization substantial implication system cost reliability le always better dsr load balancer extend concept passthrough load balancer following load balancer still typically performs partial connection tracking since response packet traverse load balancer load balancer aware complete tcp connection state however load balancer strongly infer state looking client packet using various type idle timeoutsinstead nat load balancer typically use generic routing encapsulation gre encapsulate ip packet sent load balancer backend thus backend receives encapsulated packet decapsulate know original ip address tcp port client allows backend respond directly client without response packet flowing load balanceran important part dsr load balancer backend participates load balancing backend need properly configured gre tunnel depending low level detail network setup may need connection tracking nat etcnote passthrough load balancer dsr load balancer design large variety way connection tracking nat gre etc setup across load balancer backend unfortunately topic beyond scope articlefault tolerance via high availability pairsfigure fault tolerance via ha pair connection trackingup considering design load balancer isolation passthrough dsr load balancer require amount connection tracking state load balancer load balancer dy single instance load balancer dy connection traversing load balancer severed depending application may substantial impact application performancehistorically load balancer hardware device purchased typical vendor cisco juniper etc device extremely expensive handle large amount traffic order avoid single load balancer failure severing connection leading substantial application outage load balancer typically deployed high availability pair shown figure typical ha load balancer setup following design pair ha edge router service number virtual ip vip edge router announce vip using border gateway protocol bgp primary edge router higher bgp weight backup steady state serving traffic bgp extremely complicated protocol purpose article consider bgp mechanism network device announce available take traffic network device link weight prioritizes link traffic similarly primary load balancer announces edge router higher bgp weight backup steady state serving trafficthe primary load balancer crossconnected backup share connection tracking state thus primary dy backup take handling active connectionsthe two edge router two load balancer crossconnected mean one edge router one load balancer dy bgp announcement withdrawn reason backup take serving trafficthe setup many high traffic internet application still served today however substantial downside approach vip must correctly sharded across ha load balancer pair taking account capacity usage single vip grows beyond capacity single ha pair vip need split multiple vipsthe resource usage system poor capacity sits idle steady state given historically hardware load balancer extremely expensive lead substantial amount idle capitalmodern distributed system design prefers greater fault tolerance activebackup provides eg optimally system able suffer multiple simultaneous failure keep running ha load balancer pair susceptible total failure active backup load balancer die timeproprietary large hardware device vendor extremely expensive lead vendor lockin generally desirable replace hardware device horizontally scalable software solution built using commodity compute serversfault tolerance scaling via cluster distributed consistent hashingfigure fault tolerance scaling via clustered load balancer consistent hashingthe previous section introduced load balancer fault tolerance via ha pair well problem inherent design starting early mid large internet infrastructure started design deploy new massively parallel load balancing system shown figure goal system mitigate downside ha pair design described previous sectionmove away proprietary hardware load balancer vendor commodity software solution built using standard compute server nicsthis load balancer design best referred fault tolerance scaling via clustering distributed consistent hashing work follows n edge router announce anycast vip identical bgp weight equalcost multipath routing ecmp used ensure general packet single flow arrive edge router flow typically source ipport destination ipport short ecmp way distributing packet set identically weighted network link using consistent hashing although edge router particularly care packet arrive general preferred packet flow traverse set link avoid order packet degrade performancen load balancer machine announce vip identical bgp weight edge router using ecmp edge router generally select load balancer machine floweach load balancer machine typically perform partial connection tracking use consistent hashing select backend flow gre used encapsulate packet sent load balancer backenddsr used send packet directly backend client via edge routersthe actual consistent hashing algorithm used load balancer area active research tradeoff primarily around equalizing load minimizing latency minimizing disruption backend change minimizing memory overhead complete discussion topic outside scope articlelet see design mitigates downside ha pair approach new edge router load balancer machine added needed consistent hashing used every layer decrease number affected flow much possible new machine addedthe resource usage system run high desired maintaining sufficient burst margin fault toleranceboth edge router load balancer built using commodity hardware tiny fraction cost traditional hardware load balancer one question typically asked design edge router talk directly backends via ecmp need load balancer reason primarily around do mitigation backend operational ease without load balancer backend would participate bgp would substantially harder time performing rolling deploysall modern load balancing system moving towards design variant two prominent publicly known example maglev google network load balancer nlb amazon currently os load balancer implement design however company know planning release one os excited release modern load balancer crucial piece missing os networking spacecurrent state art load balancingyes indeed last several year seen resurgence load balancerproxy development track well continued push towards microservice architecture distributed system fundamentally inherently faulty network becomes much difficult operate efficiently used frequently furthermore rise autoscaling container scheduler etc mean day provisioning static ip static file long gone system utilizing network becoming substantially dynamic requiring new functionality load balancer section briefly summarize area seeing development modern load balancersprotocol supportmodern load balancer adding explicit support many different protocol knowledge load balancer application traffic sophisticated thing regard observability output advanced load balancing routing etc example writing envoy explicitly support protocol parsing routing grpc redis mongodb dynamodb protocol likely get added future including mysql kafkadynamic configurationas described increasingly dynamic nature distributed system requiring parallel investment creating dynamic reactive control system istio one example system please see post service mesh data plane v control plane information topicadvanced load load balancer commonly builtin support advanced load balancing feature timeouts retries rate limiting circuit breaking shadowing buffering content based routing etcobservabilityas described section general load balancer feature increasingly dynamic system deployed becoming increasingly hard debug robust protocol specific observability output possibly important feature modern load balancer provide outputting numeric stats distributed trace customizable logging virtually required load balancing solutionextensibilityusers modern load balancer often want easily extend add custom functionality done via writing pluggable filter loaded load balancer many load balancer also support scripting typically via luafault tolerancei wrote quite bit load balancer fault tolerance load balancer fault tolerance general treat load balancer expendable stateless using commodity software allows load balancer easily horizontally scaled furthermore processing state tracking load balancer perform substantially complicated attempting build ha pairing load balancer technically possible would major undertakingoverall load balancing domain industry moving away ha pairing towards horizontally scalable system converge via consistent hashingand load balancer evolving staggering pace example envoy provides please see envoy architecture overviewglobal load balancing centralized control planefigure global load balancingthe future load balancing increasingly treat individual load balancer commodity device opinion real innovation commercial opportunity lie within control plane figure show example global load balancing system example different thing happening sidecar proxy communicating backends three different zone b c illustrated traffic sent zone c traffic sent zone bthe sidecar proxy backends reporting periodic state global load balancer allows global load balancer make decision take account latency cost load current failure etcthe global load balancer periodically configures sidecar proxy current routing informationthe global load balancer increasingly able sophisticated thing individual load balancer example automatically detect route around zonal failureapply global security routing policiesdetect mitigate traffic anomaly including ddos attack using machine learning neural networksprovide centralized ui visualization allow engineer understand operate entire distributed system aggregatein order make global load balancing possible load balancer used data plane must sophisticated dynamic configuration capability please see post envoy universal data plane api well service mesh data plane v control plane information topicthe evolution hardware softwareso far post briefly mentioned hardware v software primarily context historical load balancer ha pair industry trend area previous tweet humorous exaggeration still sum pretty well trend historically router load balancer provided extremely expensive proprietary hardwareincreasingly proprietary networking device replaced commodity server hardware commodity nics specialized software solution built top framework ipv dpdk fdio modern data center machine cost le easily saturate nic small packet using linux custom userspace application written using dpdk meanwhile cheap basic routerswitch asics ecmp routing astounding aggregate bandwidth packet rate packaged commodity routerssophisticated software load balancer nginx haproxy envoy also rapidly iterating encroaching previously domain vendor like thus load balancer also aggressively moving towards commodity software solutionsat time move towards iaa caas faa industry whole facilitated major cloud provider mean increasingly tiny portion engineer need understand physical network work black magic something longer need know squat section conclusion future load balancingto summarize key takeaway post load balancer key component modern distributed systemsthere two general class load balancer load balancer relevant modern load balancer moving towards horizontally scalable distributed consistent hashing load balancer heavily invested recently due proliferation dynamic microservice architecturesglobal load balancing split control plane data plane future load balancing majority future innovation commercial opportunity foundthe industry aggressively moving towards commodity os hardware software networking solution believe traditional load balancing vendor like displaced first os software cloud vendor traditional routerswitch vendor aristacumulusetc think larger runway onpremise deployment ultimately also displaced public cloud vendor homegrown physical networksoverall think fascinating time computer networking move towards os software system increasing pace iteration order magnitude furthermore distributed system continue march dynamism via serverless paradigm sophistication underlying network load balancing system need commensurately increased
247,Lobsters,scaling,Scaling and architecture,Netflix: What Happens When You Press Play?,http://highscalability.com/blog/2017/12/11/netflix-what-happens-when-you-press-play.html,netflix happens press play,explain cloud like netflix happens press play learned going deep cultural value netflix operates two cloud aws open connect three part netflix client backend content delivery network cdn netflix started moving aws netflix began running datacenters service outage caused netflix move aws netflix reliable aws netflix save money aws happens aws press play scalable computing scalable storage scalable distributed database big data processing analytics netflix personalizes artwork recommendation transcoding source medium watch source source medium validating video medium pipeline result pile file many file talking three different strategy streaming video build cdn first cdn small second cdns big open connect right open connect appliance netflix put open connect appliance oca using isps build cdn using ixps build cdn video proactively cached oca every day cache netflix cache video predicting want watch hosting oca isps open connect reliable resilient netflix control client finally happens press play related article,article chapter new book explain cloud like first release written specifically cloud newbie made update added happens press play cloud computing level couple tick past beginner think even fairly experienced people might get something also created somewhat expanded version article standalone kindle ebook find ebook netflix happens press play looking good introduction cloud know someone please take look think like pretty proud turned pulled chapter together dozen source time somewhat contradictory fact ground change time depend telling story audience addressing tried create coherent narrative could error happy fix keep mind article technical deep dive big picture type article example nt mention word microservice even netflix seems simple press play video magically appears easy right much given discussion cloud computing chapter might expect netflix serve video using aws press play netflix application video stored would streamed internet directly device completely sensible much smaller service netflix work far complicated interesting might imagine see let look impressive netflix statistic netflix million subscriber netflix operates country netflix nearly billion revenue per quarter netflix add million new subscriber per quarter netflix play billion hour video week comparison youtube stream billion hour video every day facebook stream million hour video every day netflix played million hour video single day netflix account peak internet traffic united state netflix plan spend billion new content learned netflix huge global lot member play lot video lot money another relevant factoid netflix subscription based member pay netflix monthly cancel time press play chill netflix better work unhappy member unsubscribe going deep netflix terrific example idea talked chapter go lot detail cloud service covered one big reason diving deeper netflix make much information available company netflix hold communication central cultural value netflix life standard fact like thank netflix open architecture year netflix given hundred talk written hundred article innerworkings operate whole industry better another reason going much detail netflix netflix plain fascinating u used netflix one time another love peeking behind curtain see make netflix tick netflix operates two cloud aws open connect netflix keep member happy cloud course actually netflix us two different cloud aws open connect cloud must work together seamlessly deliver endless hour customerpleasing video three part netflix client backend content delivery network cdn think netflix divided three part client backend content delivery network cdn client user interface device used browse play netflix video could app iphone website desktop computer even app smart tv netflix control every client every device everything happens hit play happens backend run aws includes thing like preparing new incoming video handling request apps website tv device everything happens hit play handled open connect open connect netflix custom global content delivery network cdn open connect store netflix video different location throughout world press play video stream open connect device displayed client worry talk cdn little later interestingly netflix actually say hit play video say clicking start title every industry lingo controlling three backend netflix achieved complete vertical integration netflix control video viewing experience beginning end work click play anywhere world reliably get content want watch want watch let see netflix make happen netflix started moving aws netflix launched first rented dvd u postal service netflix saw future ondemand streaming video netflix introduced streaming videoondemand service allowed subscriber stream television series film via netflix website personal computer netflix software variety supported platform including smartphones tablet digital medium player video game console smart tv personal note streaming videoondemand future might seem obvious worked couple startup tried make videoondemand product failed netflix succeeded netflix certainly executed well late game helped internet fast enough cheap enough support streaming video service never case addition fast lowcost mobile bandwidth introduction powerful mobile device like smart phone tablet made easier cheaper anyone stream video time anywhere timing everything netflix began running datacenters getting started time netflix streaming service started way netflix could launched using netflix built two datacenters located right next experienced problem talked earlier chapter building datacenter lot work ordering equipment take long time installing getting equipment working take long time soon got everything working would run capacity whole process start long lead time equipment forced netflix adopt known vertical scaling strategy netflix made big program ran big computer approach called building monolith one program everything problem growing really fast like netflix hard make monolith reliable service outage caused netflix move aws three day august netflix could ship dvd corruption database unacceptable netflix something experience building datacenters taught netflix important good building datacenters netflix good delivering video member netflix would rather concentrate getting better delivering video rather getting better building datacenters building datacenters competitive advantage netflix delivering video time netflix decided move aws aws getting established selecting aws bold move netflix moved aws wanted reliable infrastructure netflix wanted remove single point failure system aws offered highly reliable database storage redundant datacenters netflix wanted cloud computing build big unreliable monolith anymore netflix wanted become global service without building datacenters none capability available old datacenters never would reason netflix gave choosing aws want undifferentiated heavy lifting undifferentiated heavy lifting thing done provide advantage core business providing quality video watching experience aws undifferentiated heavy lifting netflix let netflixians focus providing business value took eight year netflix complete process moving datacenters aws period netflix grew number streaming customer eightfold netflix run several hundred thousand instance netflix reliable aws like netflix never experienced time aws whole service much reliable see complaint like often anymore netflix reliable taken extraordinary step make service reliable netflix operates three aws region one north virginia one portland oregon one dublin ireland within region netflix operates three different availability zone netflix said plan operate region expensive complicated add new region company operate one region let alone two three advantage three region one region fail region step handle member failed region region fails netflix call evacuating region let use example let say watching new house card episode london england closest london chance netflix device connected dublin region happens entire dublin region fails mean netflix stop working course netflix detecting failure redirects virginia device would talk virginia region instead dublin might even notice failure often aws region fail month well region actually fail every month netflix run monthly test every month netflix cause region fail purpose make sure system handle region level failure region evacuated six minute netflix call global service model customer served region amazing happen automatically aws magic sauce handling region failure serving customer multiple region netflix done work netflix pioneer figuring create reliable system using multiple region aware company go length make service reliable another advantage three region give netflix worldwide coverage netflix ran test found use netflix application anywhere world get fast service one three region netflix save money aws may surprise lot people aws cheaper netflix cloud cost per streaming view ended fraction cost old datacenters elasticity cloud netflix could add server needed return rather lot extra computer hanging around nothing handle peak load netflix pay needed needed stuff talked cloud computing chapter happens aws press play anything involve serving video handled aws includes scalable computing scalable storage business logic scalable distributed database big data processing analytics recommendation transcoding hundred function worry need understand thing since may find interesting explain briefly scalable computing scalable storage scalable computing scalable storage nothing new u netflix tv xbox android phone tablet netflix service running view list potential video watch netflix device contacting computer get list ask detail video netflix device contacting computer get detail like cloud service talked book scalable distributed database netflix us dynamodb cassandra distributed database name mean anything highquality database product database database store data profile information billing information movie ever watched kind information stored database distributed distributed mean database run one big computer run many computer data copied multiple computer one even two computer holding data fail data safe fact data copied three region way region fails data new region ready start using scalable scalable mean database handle much data ever want put one major advantage distributed database computer added necessary handle data big data processing analytics big data simply mean lot data netflix collect lot information netflix know everyone watched watched watched netflix know video member looked decided watch netflix know many time video lot putting data standard format called processing making sense data called analytics data analyzed answer specific question netflix personalizes artwork great example netflix entices watch video using data analytics capability browsing around looking something watch netflix noticed always image displayed video called header image header image meant intrigue draw selecting video idea compelling header image likely watch video video watch le likely unsubscribe netflix example different header image stranger thing might surprised learn image shown video selected specifically everyone see image everyone used see header image worked member shown random one picture group option like picture stranger thing collage netflix counted every time video watched recording picture displayed video selected stranger thing example let say group picture center shown stranger thing watched time picture watched since group picture best getting member watch netflix would make header image stranger thing forever called datadriven netflix known datadriven company data case number view associated used make best decision case header image select clever imagine better yes using data theme problem learning data likely different people think motivated kind header image probably different taste different preference netflix know netflix personalizes image show netflix try select artwork highlighting relevant aspect video remember netflix record count everything site know kind movie like best actor like let say one recommendation movie good hunting netflix must choose header image show goal show image let know movie probably interested image netflix show like comedy netflix show image featuring robin williams prefer romantic movie netflix show image matt damon minnie driver poised kiss showing robin williams netflix letting know likely humor movie netflix know like comedy video good match matt damon minnie driver image conveys completely different message comedy fan saw image might skip right selecting right header image important sends strong personalized signal indicating movie another example pulp fiction watched lot movie starring uma thurman likely see header image featuring uma watched lot movie starring john travolta likely see header image featuring john see choosing best possible personalized artwork might make likely watch particular video netflix appeal interest selecting artwork yet netflix want lie either want show clickbait image get watch video may like incentive netflix paid per video watched netflix try minimize regret netflix want happy video watch pick best header image one small example data analysis used netflix netflix us kind strategy everywhere recommendation usually netflix show video option yet many thousand video available netflix decide using machine learning part big data processing analytics talked netflix look data predicts like fact everything see see netflix screen chosen specifically using machine learning transcoding source medium watch start transitioning video handled netflix watch video favorite device choice netflix must convert video format work best device process called transcoding encoding transcoding process convert video file one format another make video viewable across different platform device netflix encodes video aws many cpu one time larger super computer source source medium sends video netflix production house studio netflix call video source medium new video given content operation team processing video come high definition format many terabyte size terabyte big imagine stack paper tall eiffel tower terabyte view video netflix put rigorous multistep process validating video first thing netflix spend lot time validating video look digital artifact color change missing frame may caused previous transcoding attempt data transmission problem video rejected problem found medium pipeline video validated fed netflix call medium pipeline pipeline simply series step data put make ready use much like assembly line factory different piece software hand creating every video practical process single multiterabyte sized file first step pipeline break video lot smaller chunk video chunk put pipeline encoded parallel parallel simply mean chunk processed time let illustrate parallelism example let say one hundred dirty dog need washing would faster one person washing dog one another would faster hire one hundred dog washer wash time obviously faster one hundred dog washer working time parallelism netflix us many server need lot server process huge video file parallel work netflix say source medium file encoded pushed cdn little minute chunk encoded validated make sure new problem introduced chunk assembled back file validated result pile file encoding process creates lot file end goal netflix support every internetconnected device netflix started streaming video microsoft window time device lg samsung bluray apple mac xbox lg dtv sony nintendo wii apple ipad apple iphone apple tv android kindle fire comcast netflix support different device device video format look best particular device watching netflix iphone see video give best viewing experience iphone netflix call different format video encoding profile netflix also creates file optimized different network speed watching fast network see higher quality video would watching slow network also file different audio format audio encoded different level quality different language also file included subtitle video may subtitle number different language lot different viewing option every video see depends device network quality netflix plan language choice many file talking crown netflix store around file stranger thing season even file shot nine episode source video file many many terabyte data took cpu hour encode one season result different video audio text file let see netflix play video three different strategy streaming video netflix tried three different video streaming strategy small cdn thirdparty cdns open connect let start defining cdn cdn content distribution network content video file discussed previous section distribution mean video file copied central location network stored computer located world netflix central location video stored build cdn idea behind cdn simple put video close possible user spreading computer throughout world user want watch video find nearest computer video stream device biggest benefit cdn speed reliability imagine watching video london video streamed portland oregon video stream must pas lot network including undersea cable connection slow unreliable moving video content close possible people watching viewing experience fast reliable possible location computer storing video content called pop point presence pop physical location provides access internet house server router telecommunication equipment talk pop later first cdn small netflix debuted new streaming service million member country watching billion hour video month streaming multiple terabit content per second support streaming service netflix built simple cdn five different location within united state netflix video catalog small enough time location contained content second cdns big netflix decided use cdns around time pricing cdns coming using cdns made perfect sense netflix spend time effort building cdn instantly reach globe using existing cdn service netflix contracted company like akamai limelight level provide cdn service nothing wrong using thirdparty cdns fact pretty much every company example nfl used akamai stream live football game building cdn netflix time work higher priority project netflix put lot time effort developing smarter client netflix created algorithm adapt changing network condition even face error overloaded network overloaded server netflix want member always viewing best picture possible one technique netflix developed switching different video another cdn different get better result time netflix also devoting lot effort aws service talked earlier netflix call service aws control plane control plane telecommunication term identifying part system control everything else body brain control plane control everything else netflix thought could better developing cdn open connect right netflix realized scale needed dedicated cdn solution maximize network efficiency video distribution core competency netflix could huge competitive advantage netflix started developing open connect purposebuilt cdn open connect launched open connect lot advantage netflix le expensive cdns expensive would save lot money better quality controlling entire video cdn client reasoned could deliver superior video viewing experience scalable netflix goal providing service everywhere world quickly supporting people providing quality video viewing experience required building system cdns must support user accessing kind content anywhere world netflix much simpler job netflix know exactly user must subscribe netflix netflix know exactly video need serve knowing serve large video stream allows netflix make lot smart optimization choice cdns make netflix also know lot member company know video like watch like watch kind knowledge netflix built really highperforming cdn let go detail open connect work open connect appliance remember said cdn computer distributed world netflix developed computer system video storage netflix call open connect appliance oca early oca installation site looked like many oca picture oca grouped cluster multiple server oca fast server highly optimized delivering large file lot lot hard disk flash drive storing video one oca server look like several different kind oca different purpose large oca store netflix entire video catalog smaller oca store portion netflix video catalog smaller oca filled video every day offpeak hour using process netflix call proactive caching talk proactive caching work later hardware perspective nothing special oca based commodity pc component assembled custom case various supplier could buy computer wanted notice netflix computer red netflix computer specially made match logo color software perspective oca use freebsd operating system nginx web server yes every oca web server video stream using nginx none name make sense worry including completeness number oca site depends reliable netflix want site amount netflix traffic bandwidth delivered site percentage traffic site allows streamed press play watching video streaming specific oca like one location near best possible video viewing experience netflix would really like cache video house practical yet next best thing put mininetflix close house netflix put open connect appliance oca netflix delivers huge amount video traffic thousand server location around world take look map video serving location video service like youtube amazon deliver video backbone network company literally built global network delivering video user complicated expensive netflix took completely different approach building cdn netflix operate network operate datacenters anymore either instead internet service provider isps agree put oca datacenters oca offered free isps embed network netflix also put oca close internet exchange location ixps using strategy netflix need operate datacenters yet get benefit regular datacenter someone else datacenter genius last two paragraph pretty dense let break using isps build cdn isp internet provider get internet service might verizon comcast thousand service main point isps located around world close customer placing oca isp datacenters netflix also world close customer using ixps build cdn internet exchange location datacenter isps cdns exchange internet traffic network like going party exchange christmas present friend easier exchange present everyone one place easier exchange network traffic everyone one place ixps located world telegeography internet exchange map london internet exchange look like london internet exchange linx drill yellow fiber optic cable see something like amsix internet exchange point amsterdam netherlands wikimedia common wire picture connects one network another network different network exchange traffic ixp like highway interchange using wire wikimedia common netflix another win ixps world putting oca ixps netflix run datacenters video proactively cached oca every day netflix video sitting video serving computer spread throughout world one thing missing video netflix us process call proactive caching efficiently copy video oca cache cache hiding place especially one ground ammunition food treasure know squirrel bury nut winter location bury nut cache winter squirrel find nut cache chow arctic explorer sent small team ahead cache food fuel supply along route taking larger team following behind would stop every cache location resupply squirrel arctic explorer proactive something ahead time prepare later oca video cache likely want watch netflix cache video predicting want watch everywhere world netflix know high degree accuracy member like watch like watch remember said netflix datadriven company netflix us popularity data predict video member probably want watch tomorrow location location mean cluster oca housed within isp ixp netflix copy predicted video one oca location called prepositioning video placed oca anyone even asks give great service member video want watch already close ready available streaming netflix operates called tiered caching system smaller oca talked earlier placed isps ixps small contain entire netflix catalog video location oca containing netflix video catalog still location big oca containing entire netflix catalog get video every night oca wake asks service aws video service aws sends oca list video supposed based prediction talked earlier oca charge making sure video list oca location one video supposed copy video local oca otherwise nearby oca video found copied since netflix forecast popular tomorrow always one day lead time video required oca mean video copied quiet offpeak hour substantially reducing bandwidth usage isps never cache miss open connect cache miss would asking specific video oca oca saying cache miss happen time cdns afford copy content everywhere since netflix know video must cache know exactly video time smaller oca video one larger oca always guaranteed netflix copy video every oca world video catalog way large store everything location video catalog netflix petabyte idea large today assume significantly larger netflix developed method choosing video store oca using data predict member want watch let take example house card popular show oca copied probably every location member worldwide want watch house card video popular house card netflix decides location copied order best serve nearby member request within location popular video like house card copied many different oca popular video server copied one copy popular video streaming video member would overwhelm server say many hand make light work video considered live copied one oca netflix want able play content time everywhere world sufficient number oca enough copy video serve appropriately video considered live ready member watch daredevil season example first time netflix released episode show device country time hosting oca isps would isp agree put oca cluster inside network first blush seems generous happy know rooted firmly selfinterest understand need talk network work know throughout book said cloud service accessed internet case netflix least watching video using netflix app talk aws internet internet interconnect network isp provides internet service get internet service comcast mean house connects comcast network using fiber optic cable comcast network network internet internet something else let say want google search type query browser hit enter request google first flow comcast network google comcast network point request go google network internet internet connects comcast network google network thing called routing protocol act like traffic cop directing network traffic go google query routed onto internet comcast network anymore google network called internet backbone internet woven together many privately owned network choose interoperate ixps looked earlier one way network connect united state map long haul fiber network intertubes study u longhaul fiberoptic infrastructure netflix done open connect placed oca cluster inside isps network mean watch netflix video talking oca comcast network video traffic comcast network never hit internet key scaling video delivery close user possible using internet backbone request satisfied local part network good thing recall said netflix already consumes internet traffic united state isps cooperate netflix would use even internet internet handle video traffic isps would add lot network capacity expensive build right netflix content served within isp network reduces cost relieving internet congestion isps time netflix member experience highquality viewing experience network performance improves everyone winwin open connect reliable resilient earlier discussed netflix increased reliability system running three different aws region architecture open connect accomplished goal may immediately obvious oca independent oca act selfsufficient videoserving archipelago member streaming one oca affected oca fail happens oca fails netflix client using immediately switch another oca resume streaming happens many people one location use oca netflix client find lightly loaded oca use happens network member using stream video becomes overloaded sort thing netflix client find another oca better performing network open connect reliable resilient system netflix control client netflix handle failure gracefully control client every device running netflix netflix develops android io apps might expect control even platform like smart tv netflix build client netflix still control control software development kit sdk sdk set software development tool allows creation application every netflix app make request aws play video using sdk controlling sdk netflix adapt consistently transparently slow network failed oca problem might arise finally happens press play long road getting learned lot learned far netflix divided three part backend client cdn request netflix client handled aws video streamed nearby open connect appliance oca open connect cdn netflix operates three aws region usually handle failure region without member even noticing new video content transformed netflix many different format best format selected viewing based device type network quality geographic location member subscription plan every day open connect netflix distributes video throughout world based predict member location want watch picture netflix describes play process let complete picture select video watch using client running device client sends play request indicating video want play netflix playback apps service running aws talked big part happens hit play licensing every location world license view every video netflix must determine valid license view particular video talk really keep mind always happening one reason netflix started developing content avoid licensing issue netflix want release show everyone world time creating content easiest way netflix avoid worrying licensing problem taking account relevant information playback apps service return url ten different oca server sort url use time web browser netflix us ip address information isps identify oca cluster best use client intelligently selects oca use testing quality network connection oca connect fastest reliable oca first client keep running test throughout video streaming process client probe figure best way receive content oca client connects oca start streaming video device noticed watching video picture quality varies sometimes look pixelated awhile picture snap back hd quality client adapting quality network network quality decline client lower video quality match client switch another oca quality decline much happens press play netflix would ever thought simple thing watching video complex related article
248,Lobsters,scaling,Scaling and architecture,Application Layer Transport Security,https://cloud.google.com/security/encryption-in-transit/application-layer-transport-security/,application layer transport security,download pdf version ciolevel summary mutually authenticated tl entity tradeoff introduction audience prerequisite cluster management google applicationlevel security alt tl protocol buffer alt design transparency stateoftheart cryptography identity model key distribution scalability session resumption longlived connection simplicity alt trust model alt credential protocol buffer master certificate handshake certificate resumption key session resumption certificate issuance human certificate machine certificate workload certificate session resumption alt policy enforcement certificate revocation alt protocol handshake protocol session resumption record protocol resumption secret authenticator secret client record protocol handshake protocol framing length type payload payload session resumption handshake protocol server side session resumption client side session resumption tradeoff key compromise impersonation attack alt handshake protocol privacy handshake message perfect forward secrecy zeroroundtrip resumption reference,cesar ghali adam stubblefield ed knapp jiangtao li benedikt schmidt julien boeuf content contained herein correct december whitepaper represents status quo time written google cloud security policy system might change going forward continually improve protection customer download pdf version ciolevel summary google application layer transport security alt mutual authentication transport encryption system developed google typically used securing remote procedure call rpc communication within google infrastructure alt similar concept mutually authenticated tl designed optimized meet need google datacenter environment alt trust model tailored cloudlike containerized application identity bound entity instead specific server name host trust model facilitates seamless microservice replication load balancing rescheduling across host alt relies two protocol handshake protocol session resumption record protocol protocol govern session established authenticated encrypted resumed alt custom transport layer security solution use google tailored alt production environment tradeoff alt industry standard tl detail found tradeoff section introduction production system google consist constellation collectively issue remote procedure call rpcs per second google engineer schedule production rpcs issued received workload protected alt default automatic zeroconfiguration protection provided google application layer transport security alt addition automatic protection conferred rpc alt also facilitates easy service replication load balancing rescheduling across production machine paper describes alt explores deployment google production infrastructure audience document aimed infrastructure security professional curious authentication transport security performed scale google prerequisite addition introduction assume basic understanding cluster management google applicationlevel security alt many application web browser vpns rely secure communication protocol tl transport layer security ipsec protect data google use alt mutual authentication transport encryption system run application layer protect rpc communication using applicationlevel security allows application authenticated remote peer identity used implement finegrained authorization policy tl may seem unusual google use custom security solution alt majority internet traffic today encrypted using tl alt began development google time tl bundled support many legacy protocol satisfy minimum security standard could designed security solution adopting tl component needed implementing one wanted however advantage building googlesuited system scratch outweighed benefit patching existing system addition alt appropriate need historically secure older tl listed key difference tl alt significant difference trust tl http semantics alt former server identity bound specific name corresponding naming scheme alt identity used multiple naming scheme level indirection provides flexibility greatly simplifies process microservice replication load balancing rescheduling host compared tl alt simpler design implementation result easier monitor bug security vulnerability using manual inspection source code extensive fuzzing alt us protocol buffer serialize certificate protocol message tl us certificate encoded majority production service use protocol buffer communication sometimes storage making alt better fit google environment alt design alt designed highly reliable trusted system allows servicetoservice authentication security minimal user involvement achieve property listed part alt design transparency alt configuration transparent application layer default service rpcs secured using alt allows application developer focus functional logic service without worry credential management security configuration servicetoservice connection establishment alt provides application authenticated remote peer identity used finegrained authorization check auditing stateoftheart cryptography cryptographic primitive protocol used alt uptodate current known attack alt run googlecontrolled machine meaning supported cryptographic protocol easily upgraded quickly deployed identity model alt performs authentication primarily identity rather host name google every network entity eg corporate user physical machine production service workload associated identity communication service mutually authenticated key distribution alt relies workload identity expressed set credential credential deployed workload initialization without user involvement parallel root trust trust chain credential established machine workload system allows automatic certificate rotation revocation without application developer involvement scalability alt designed scalable order support massive scale google infrastructure requirement resulted development efficient session resumption longlived connection authenticated key exchange cryptographic operation computationally expensive accommodate scale google infrastructure initial alt handshake connection persisted longer time improve overall system performance simplicity tl default come support legacy protocol version backwards compatibility alt considerably simpler google control client server designed natively support alt alt trust model alt performs authentication primarily identity rather host google every network entity eg corporate user physical machine production service associated identity identity embedded alt certificate used peer authentication secure connection establishment model pursue production service run production entity managed site reliability engineer sres development version production service run test entity managed sres developer example let assume product two service servicefrontend servicebackend sres launch production version service servicefrontendprod servicebackendprod developer build launch development version service servicefrontenddev servicebackenddev testing purpose authorization configuration production service configured trust development version service alt credential three type alt credential expressed protocol buffer message format master certificate signed remote signing service used verify handshake certificate master certificate contains public key associated master private key eg rsa keypair private key used sign handshake certificate certificate exercised combination alt policy discussed essentially constrained intermediate certificate authority ca certificate master certificate typically issued production machine scheduler containerized workload handshake certificate created signed locally master private key certificate contains parameter used alt handshake secure connection establishment example static diffiehellman dh parameter handshake cipher also handshake certificate contains master certificate derived ie one associated master private key sign handshake certificate resumption key secret used encrypt resumption ticket key identified resumption identifier idr unique shared among production workload running identity datacenter cell detail session resumption alt see session resumption figure show alt certificate chain consists signing service verification key master certificate handshake certificate signing service verification key root trust alt installed google machine production corporate network figure alt certificate chain alt signing service certifies master certificate turn certify handshake certificate handshake certificate created often master certificate architecture reduces load signing service certificate rotation happens frequently google especially handshake frequent rotation compensates static key exchange pair carried handshake certificate issuance order participate alt secure handshake entity network need provisioned handshake certificate first issuer obtains master certificate signed signing service optionally pass entity handshake certificate created signed associated master private key typically issuer internal certificate authority ca issuing certificate machine human borgmaster issuing certificate workload however entity eg restricted borgmaster test datacenter cell figure show signing service used create master certificate process consists following step figure certificate issuance certificate issuer sends certificate signing request csr signing service request asks signing service create certificate identity identity example corporate user identity google production service signing service set issuer certificate included csr requester certificate issuer case sign recall corresponding signing service public verifying key installed google machine signing service sends signed certificate back handshake certificate created identity signed master certificate associated private key shown process alt issuer signer certificate two different logical entity case issuer certificate issuer entity signer signing service three common category certificate alt namely human machine workload following section outline certificate created used alt human certificate google use alt secure rpcs issued human user production service issue rpc user must provide valid handshake certificate example alice want use application issue altssecure rpc authenticate internal ca alice authenticates ca using username password twofactor authentication operation result alice getting handshake certificate valid hour machine certificate every production machine google datacenters machine master certificate certificate used create handshake certificate core application machine eg machine management daemon primary identity embedded machine certificate refers typical purpose machine example machine used run different kind production development workload different identity master certificate usable machine running verified software stack case trust rooted custom security production machine master certificate issued ca rotated every month also handshake certificate rotated every hour workload certificate key advantage alt operates idea workload identity facilitates easy service replication load balancing rescheduling across machine production network use system called cluster management machine resource allocation scale way borg issue certificate part alt machineindependent workload identity implementation workload production network run borg cell cell contains logically centralized controller called borgmaster several agent process called borglets run machine cell workload initialized associated workload handshake certificate issued borgmaster figure show process workload certification alt borg figure handshake certificate creation google production network borgmaster ready schedule workload need use alt step happen client schedule workload run borg given identity borgmaster come preinstalled machine master certificate associated private key shown diagram generates borgmaster handshake certificate sign using machine master private key handshake certificate allows borgmaster issue altssecure rpcs borgmaster creates base workload master certificate corresponding private key borgmaster initiate request altssecured rpc get base workload master certificate signed signing service result signing service list borgmaster issuer certificate borgmaster verifies client authorized run workload identity specified workload configuration borgmaster schedule borg workload borglet issue workload handshake certificate corresponding private key certificate chained base workload master certificate workload handshake certificate private key securely delivered borglet mutually authenticated alt protected channel borgmaster borglet borgmaster rotates base workload master certificate reissue handshake certificate running workload approximately every two day addition workload running user cell receives resumption key identifier idr provisioned borgmaster workload need make altssecure rpc us workload handshake certificate handshake protocol idr also used part handshake initiate session resumption information session resumption alt see session resumption alt policy enforcement alt policy document list issuer authorized issue certain category certificate identity distributed every machine production network example alt policy allows ca issue certificate machine human also allows borgmaster issue certificate workload found policy enforcement certificate verification opposed certificate issuance flexible approach allows different policy enforced different type deployment example may want policy test cluster permissive one production cluster alt handshake certificate validation includes check alt policy policy ensures issuer listed certificate validated authorized issue certificate case certificate rejected handshake process fails figure illustrates policy enforcement work alt following scenario figure assume mallory corporate user want escalate privilege want issue master certificate network admin powerful identity reconfigure network go without saying mallory authorized alt policy perform operation figure certificate issuance usage mallory issue master certificate network admin identity get signed signing service similar first three step figure mallory creates sign handshake certificate locally network admin using master private key associated created master certificate mallory try impersonate network admin identity using created handshake certificate alt policy enforcer peer mallory try communicate block operation certificate revocation google certificate invalidated expires included certificate revocation list crl section describes design google internal certificate revocation mechanism time writing paper still undergoing deployment testing certificate issued human corporate user daily expiration timestamp force user reauthenticate daily many certificate issued production machine use expiration timestamps avoid relying timestamps expire production certificate lead outage caused clock synchronization issue instead use crl source truth rotation incidentresponse handling certificate figure show crl operates figure master certificate creation revocationid instance ca contact crl service asks revocation id range revocation id long id two component certificate category eg human machine certificate certificate identifier crl service chooses range id return ca ca receives request master certificate creates certificate embeds revocation id pick range parallel ca map new certificate revocation id sends information crl service ca issue master certificate revocation id assigned handshake certificate depend certificate used example handshake certificate issued human corporate user inherit revocation id user master certificate handshake certificate issued borg workload revocation id assigned borgmaster range revocation id id range assigned borgmaster crl service process similar shown figure whenever peer involved alt handshake check local copy crl file ensure remote peer certificate revoked crl service compiles revocation id single file pushed google machine use alt crl database several hundred megabyte generated crl file megabyte due variety compression technique alt protocol alt relies two protocol handshake protocol session resumption record protocol section provides high level overview protocol overview interpreted detailed specification protocol handshake protocol alt handshake protocol diffiehellmanbased authenticated key exchange protocol support perfect forward secrecy pfs session resumption alt infrastructure ensures client server certificate respective identity elliptic curve diffiehellman ecdh key chain trusted signing service verification key alt pfs enabled default static ecdh key frequently updated renew forward secrecy even pfs used handshake handshake client server securely negotiate shared transit encryption key record protocol encryption key used protect example client server might agree key used protect rpc session using aesgcm handshake consists four serialized protocol buffer message overview seen figure figure alt handshake protocol message client initiate handshake sending clientinit message message contains client handshake certificate list handshakerelated cipher record protocol client support client attempting resume terminated session include resumption identifier encrypted server resumption ticket receipt clientinit message server verifies client certificate valid server chooses handshake cipher record protocol list provided client server us combination information contained clientinit message local information compute dh exchange result result used input key derivation along transcript protocol generate following session secret record protocol secret key used encrypt authenticate payload message resumption secret r used resumption ticket future session authenticator secret server sends serverinit message containing certificate chosen handshake cipher record protocol optional encrypted resumption ticket server sends serverfinished message containing handshake value authenticator calculated using hashbased message authentication code hmac computed predefined bit string authenticator secret client receives serverinit verifies server certificate computes dh exchange result similar server derives r secret client us derived verify authenticator value received serverfinished message point handshake process client start using encrypt message client capable sending encrypted message say alt one rtt handshake protocol end handshake client sends clientfinished message similar authenticator value see step computed different predefined bit string needed client include encrypted resumption ticket future session message received verified server alt handshake protocol concluded server start using encrypt authenticate payload message handshake protocol reviewed thai duong google internal security analysis team formally verified using tool bruno blanchet assistance martin abadi record protocol handshake protocol section described use handshake protocol negotiate record protocol secret protocol secret used encrypt authenticate network traffic layer stack performs operation called alt record protocol altsrp altsrp contains suite encryption scheme varying key size security feature handshake client sends list preferred scheme sorted preference server chooses first protocol client list match server local configuration method scheme selection allows client server different encryption preference allows u phase remove encryption scheme framing frame smallest data unit alt depending size altsrp message consist one frame frame contains following field length unsigned value indicating length frame byte length field included part total frame length type value specifying frame type eg data frame payload actual authenticated optionally encrypted data sent maximum length frame plus length byte current rpc protocol limit frame length shorter frame require le memory buffering larger frame could also exploited potential attacker denial service do attack attempt starve server well limiting frame length also restrict number frame encrypted using record protocol secret limit varies depending encryption scheme used encrypt decrypt frame payload limit reached connection must closed payload alt frame contains payload integrity protected optionally publication paper alt support following mode aesgcm aesvcm mode respectively key mode protect confidentiality integrity payload using gcm vcm respectively mode support integrityonly protection using gmac vmac respectively tag computation payload transferred plaintext cryptographic tag protects integrity google use different mode protection depending threat model performance requirement communicating entity within physical boundary controlled behalf google integrityonly protection used entity still choose upgrade authenticated encryption based sensitivity data communicating entity different physical boundary controlled behalf google communication pas wide area network automatically upgrade security connection authenticated encryption regardless chosen mode google applies different protection data transit transmitted outside physical boundary controlled behalf google since rigorous security measure applied frame separately integrity protected optionally encrypted peer maintain request response counter synchronize normal operation server receives request order repeated cryptographic integrity verification fails dropping request similarly client drop repeated misordered response furthermore peer maintain counter opposed including value frame header save additional byte wire session resumption alt allows user resume previous session without need perform heavy asymmetric cryptographic operation session resumption feature built alt handshake protocol alt handshake allows client server securely exchange cache resumption ticket used resume future cached resumption ticket indexed resumption identifier idr unique workload running identity datacenter cell ticket encrypted using symmetric key associated corresponding identifier alt support two type session resumption server side session resumption client creates encrypts resumption ticket containing server identity derived resumption secret r resumption ticket sent server end handshake clientfinished message future session server choose resume session sending ticket back client serverinit message receipt ticket client recover resumption secret r server identity client use information resume session idr always associated identity specific connection alt multiple client use identity datacenter allows client resume session server may communicated eg load balancer sends client different server running application client side session resumption end handshake server sends encrypted resumption ticket client serverfinished message ticket includes resumption secret r client identity client use ticket resume connection server sharing idr session resumed resumption secret r used derive new session secret used encrypt authenticate payload message used authenticate serverfinished clientfinished message encapsulated new resumption ticket note resumption secret r never used tradeoff key compromise impersonation attack design alt handshake protocol susceptible key compromise impersonation kci attack adversary compromise dh private key resumption key workload use key impersonate workload explicitly resumption threat model want resumption ticket issued one instance identity usable instance identity variant alt handshake protocol protects kci attack would worth using environment resumption desired privacy handshake message alt designed disguise internal identity communicating encrypt handshake message hide identity peer perfect forward secrecy perfect forward secrecy pfs supported enabled default alt instead use frequent certificate rotation establish forward secrecy application tl prior version session resumption protected pfs pfs enabled alt pfs also enabled resumed session zeroroundtrip resumption tl provides session resumption requires zero roundtrips however weaker security decided include option alt rpc connection google generally longlived consequently reducing channel setup latency good tradeoff additional complexity andor reduced security handshake require reference
249,Lobsters,scaling,Scaling and architecture,The Evolution of Reddit.com's Architecture,https://www.infoq.com/presentations/reddit-architecture-evolution,evolution redditcom architecture,see presentation transcript,transcript reddit get started used reddit probably seen time hoping see le le definitely seen talk hopefully help understand used reddit quick explanation reddit frontpage internet community hub place people talk everything interested reddit number importantly topic reddit really big website currently largest u according alexa serve million user every month sort stuff like posting million time day casting million vote kind add major component let dig site look like highlevel overview architecture reddit focused part site involved core experience site leaving really interesting stuff like data analysis ad stack kind stuff core reddit experience thing know diagram much work progress made diagram like year ago looked nothing like also tell whole lot engineering organization much tell tech use monolith actually really interesting middle giant blob original monolithic application reddit reddit since big python blob talk bit detail nodejs frontend application frontend engineer reddit got tired pretty outdated stuff building modern frontend application node share code server client act api client talk apis provided api gateway act like mobile phone whatever api client new backend service also starting split various backend service highlighted core thing kind focus individual team imagine api team listing team think team explain little bit mean later written python helped u splitting stuff existing python monolith built common library allows u reinvent wheel every time come monitoring tracing kind stuff built backend use thrift thrift give u nice strong schema allow http frontend api gateway still talk outside cdn finally cdn front fastly saw talk earlier pretty cool stuff one thing use able lot decision logic outside edge figure stack going end request based domain coming path site cooky user including perhaps experiment bucketing multiple stack starting split still one redditcom deep dive since big blob complicated old dig detail giant monolith complicated beast weird diagram run code every one server monolithic server might run different part code stuff deployed everywhere load balancer front use http proxy point taken request user split various pool application server isolate different kind request path say comment page going slow today something going nt affect front page people useful u gating weird issue happen also lot expensive operation user thing like vote submit link etc defer asynchronous job queue via rabbit mq put message queue processor handle later usually pretty quickly memcache postgress section core data model talk called thing would consider gut reddit account link subreddits comment stored data model called thing based postgress memcache front finally use cassandra heavily stack seven year used lot new feature ever since came board nice ability stay one node going kind thing cool big structure site let talk part site work starting listing listing listing kind foundation reddit list ordered list link could navely think selecting link database sort see front page see subreddits etc cached result way actually running select database instead initially would happen select would happen would cached list id memcache way fetch list id easily look link primary key easy well nice system worked great vote queue thing listing needed invalidated whenever change something happens submit something frequently happens vote something vote queue something really update listing frequently also stuff vote processor anticheat processing mutate place turn running select query even occasionally invalidate expensive something like voting information need able go update cache listing nt really need rerun query instead store id id paired sort information related thing something like process vote fetch current cached listing modify example see vote link move list change score list write back kind interesting readmutatewrite operation potential raised condition lock around cache notice actually running query anymore really cache anymore actually firstclass thing storing persisted index really denormalized index point started stored originally thing nowadays cassandra vote queue pileup going talk little bit something went wrong mentioned queue usually process pretty quickly always back middle started seeing vote queue would start getting really backed middle day particularly peak traffic everybody around would delay processing vote visible user site submission would get score properly would sitting front page score going slowly much higher get many hour later queue processed observability add scale processor made worse dig nt great observability time could nt figure going saw whole processing time vote longer beyond know lock contention started adding bunch timer narrowed realized logged mentioned causing problem popular subreddits site getting lot vote make sense popular bunch vote happening time trying update listing bunch vote time waiting lock adding added people waiting lock actually nt help partitioning fix partitioned vote queue really dead simple took subreddit id link voted used like modular put one different queue look like voting link reddit going vote queue one vote queue total number processor end divided different partition fewer vying single look time worked really well smooth sailing forever really slow month respite saw vote queue slowing okay lock contention processing time looked okay average looked percentile timer saw vote going really poorly interesting dig start putting print statement see going taking amount time pretty dumb worked outlier found domain listing listing site link submitted given domain sorry danger touch screen domain listing point contention thing partitioned vying lock subreddits thing vying domain listing great causing lot issue split processing long story short much later comprehensively fixed splitting query together instead processing one vote entire single job processer little bit upfront stuff make bunch message deal different part job work right partition vying across partition learning interesting stuff really need timer code nice granular timer also give crosssection get lot info tracing way getting info case really important well figuring going weird case also kind obvious lock really bad news throughput use partitioning right thing lockless cached query going forward got new data model trying storing cache query lockless way pretty interesting promising far nt nt committed fully yet future listing importantly starting split listing altogether listing service goal team working make relevant listing user relevant listing come sort source includes data analysis pipeline machine learning normal old listing like rest site kind future extracted service nt even need know coming anymore cool thing listing thing thing said earlier thing postgress cache oldest data model reddit pretty interesting data model getting smile thing know designed takeaway certain pain point also make act something like expensive join vaguely schemaless key value table one thing per noun site like subreddit thing represented pair table postgress thing table look like abbreviated idea one row thing object exists set fixed column covered everything original day reddit needed basic selects query run site stuff would sort filter make listing back day data table however many row per thing object key value make kind bag property thing pretty neat reddit term ability make change make new addition site without go alter table production cool way lot performance issue interesting thing postgresql thing postgress done set table live single database cluster primary database cluster handle writes number readonly replica replicate asynchronously connects database would prefer use replica read operation scale better time would also thing look determines query failed would guess server try use future thing memcached thing also work memcache reduces readonly replica object serialized popped memcache read first hit miss write memcache making change rather deleting allowing repopulated next read incident waking lot error would wake suddenly alert saying replication one secondary crashed meant database getting date going fix immediately thing take replication usage site start rebuilding go back bed started seeing next day woke cached listing referring item nt exist postgress little terrifying see cached listing thing table great thing see caused page needed crash looking data nt died built lot tooling time clean listing clean bad data nt remove obviously really painful looking lot thing going whole lot u either five people time issue started primary saturating disk running iop something going slow got beefier hardware pretty good right problem solved really clue month later everything nice quiet pretty routine maintenance accidentally bumped offline primary suddenly see replication log alert firing looking log application time lightbulb went mentioned try remove dead database connection pool code look like pseudocody configuration list database consider first list primary going decide database use query take list database filter one alive take first one list primary secondary rest choose based query type server use go bug happens think primary would instead take primary list using secondary first item list try writing secondary well nt proper permission set worked write secondary oops wrote secondary created thing write cached listing good take secondary rebuild data gone good learning yeah use permission really useful annoy people helpful denormalize cached listing really important tooling healing discovery going forward change making new service using service discovery system pretty standard across stack find database nt implement logic help reducing complexity making battletested component thing service also finally starting move whole thing model service initial reason new service coming online wanted know core data reddit service start able read data starting take writes well take huge upside code lot legacy lot weird twisted way used pulling exercise going separate clean something completely rethink necessary comment tree cool another major thing said reddit place people talk stuff comment tree important thing know comment reddit threaded mean nest reply see structure conversation also linked deep within structure make bit complicated render tree pretty expensive go say okay comment thread need look comment find parent etc store hey another denormalized listing parent relationship whole tree one place figure ahead time okay subset comment going show right look comment also kind expensive defer offline job processing one advantage comment tree stuff batch message mutate tree one big batch allows efficient operation important thing note tree structure sensitive ordering insert comment parent nt exist weird tree structure right need watched thing happen system stuff try heal situation recompute tree try fix tree fastlane processing also issue sometimes end megathread site news event happening superbowl happening whatever people like comment comment one thread thread going pretty slowly affecting rest site developed thing allows u manually mark thread say thread get dedicated processing go queue always called fast lane well caused issue incident major news event happening pretty sad stuff lot people talking thread making processing comment pretty slow site fastlaned everything died basically happened immediately fast lane queue started filling message quickly rapidly filled memory message broker meant could nt add new message anymore great end thing could restart message broker lose message get back steadystate meant action site deferred queue also messed self healing turned cause stuff dealing missing parent fastlaned thread earlier comment thread happened fastlaneing queue yet processed fastlaned queue skipped bunch new message site recognized inconsistent every page thread putting new message queue please recompute broken good start restarted reddit everything went back normal use queue quota nice resource limit allow prove one thing hogging resource important use thing like quota turned quota time fast lane queue would started dropping message meant thread would inconsistent comment would nt show little rest site would kept working autoscaler cool right little meta bunch server power stuff need scale kind traffic reddit look like course week definitely seasonal see half night day couple weird hump different time zone pretty consistent overall want auto scaler save money peak deal situation need scale something crazy going pretty good job watch utilization metric reported local balancer automatically increase decrease number server requesting aws offloaded actual logic terminating aws autoscaling group work pretty well way auto scaler know going host demon register existence zookeeper cluster rudimentary health check note also using system memcache really auto scaled ever going server died would replaced automatically incident something went pretty wrong auto scaler taught u lot midst making final migration classic vpc huge number benefit u networking security stuff final component move zookeeper cluster zookeeper cluster used auto scaler pretty important plan plan migration launch new cluster vpc stop auto scaler service nt mess anything migration repoint auto scaler agent server fleet point new zookeeper cluster repoint auto scaler service restart cluster act like nothing ever happened reality actually happened little different unfortunately launched new zookeeper cluster working great stopped auto scaler service cool start repointing thing got way suddenly server get terminated took u moment realize face palmed rather heavily happened happened puppet running auto scaler server half hourly run decided restart auto scaler demon still pointing old cluster saw server migrated new cluster unhealthy terminated helpful thanks auto scaler recovery well wait minute auto scaler bring bunch whole new server relatively easy except cache server also done system state new one come nt state meant suddenly production traffic happening hammering postgress replica could handle used many cache gone time yeah pretty reasonable learning learned thing destructive action like terminating server sanity check terminating large percentage server needed improve process migration peerreviewed check list made sure extra layer defense would really helpful important thing stateful service pretty different stateless service treat differently using autoscaler technology probably best idea autoscaler nextgen autoscaler building lot stuff tooling recognize affecting larger number server really stop us service discovery system determine health instead agent box actually get little finegrain detail like actual service okay host yeah refuse take action large number server cool remember human summary observability key people make mistake use multiple layer safeguard like turned auto scaler put exit command top script nothing would happened puppet brought back little extra thing one failure cause new issue really important make sure system simple easy understand cool thank beginning u building ton stuff hiring also thursday entire infraops team many right ama would like ask question live captioning lindsay stokerlindsay white coat captioning whitecoatcapx see presentation transcript
250,Lobsters,scaling,Scaling and architecture,From bare metal to Serverless,http://loige.co/from-bare-metal-to-serverless/,bare metal serverless,invention web sir tim berners lee invention web hosting geocities grid computing globus alliance metacomputing software service saas server virtualization infrastructure service iaa platform service paas database service backend service baa released containerization container scale function service faa serverless serverless application require provision manage server focus core product business logic amazon web service commitstripcom serverless fullstackpythoncom pillar serverless server management flexible scaling high availability never pay idle right approach future probably yes reference frontconf slide deck,lately tried understand modern cloud computing brought u idea growing adoption serverless article illustrate result small research history cloud computing age bare metal serverless know past understand present carl sagan end article also illustrate definition serverless main characteristic invention web story start sir tim berners lee guy officially invented world wide web beginning web simple publishing platform researcher share information publish paper first website actually cern website published web quickly evolved grew research space become one mainstream ubiquitous product last decade first year web publishing website easy talking period go roughly time literally buy server machine configure connect stable electricity line internet connection run web server take care every single aspect making server always available imagine effort required needed scale service adopt many server invention web hosting idea web hosting popularised platform like geocities web hosting allowed everybody publish web page directly shared server low cost sometimes even free removing publisher struggle cost needed bare metal approach although web hosting cheap easy time limited possible publish static file html image etc ftp later web hosting improved supporting dynamic language like perl php database like mysql grid computing around thanks astonishing work university chicago argonne national laboratory together founded globus alliance idea grid computing formalized paper titled globus metacomputing infrastructure toolkit ian foster picture carl kesselman new idea computing defined metacomputing formalized finally made famous metacomputing networked virtual supercomputer constructed dynamically geographically distributed resource linked highspeed network metacomputing foundation grid computing generally recognized collection computer resource multiple location reach common goal grid thought distributed system noninteractive workload involve large number file task grid computing important history cloud shifted perception power distributed computing demonstration world need single supercomputer order solve complex problem wellknitted network ordinary computer orchestrated achieve complex goal sustain sophisticated service software service saas web starting evolve something complex amazoncom already thing already possible build complex interaction user browser marc benioff salesforce one first public figure strongly state desktop software needed anymore could replaced software written work directly web web browser principle lead following year definition software service saas probably reason web started play bigger role publishing platform complete runtime execute sort application game server virtualization vmware release esxi server virtualization becomes thing server virtualization possible divide one physical server multiple isolated virtual environment way provider allocate number physical machine advance server farm create virtual infrastructure new virtual machine variable characteristic initialized provided service matter minute approach way convenient flexible bare metal one interesting thing virtualization thing way back history computing example research project back ran ibm released product called vm eventually turned zvm long line product infrastructure service iaa amazon web service company spawn opportunity amazon saw renting part massive compute power undergo different official launch event launch aws found right proposition developer started build significant traction successful launch defined term infrastructure service time aws offer constituted virtual machine service scalable storage service sqs message queuing system year come aws competing iaa platform would associated idea cloud computing platform service paas heroku developed james lindenbaum adam wiggins picture orion henry heroku originally born attempt create online editor ruby rail quickly found productmarket fit complete platform deploy scale ruby web application support later increasingly extended main language framework market heroku defined idea platform service paas successful salesforce acquired company hired yukihiro matsumoto inventor ruby chief architect year later database service firebase evolved envolve prior startup founded james tamplin picture andrew lee main idea firebase provide realtime database service dedicated sdk could easily integrated website mobile application one first successful instance database could used payperuse model firebase successful year later acquired google evolved complete platform supporting development mobile apps little curiosity reason nobody far aware tried come short name dbaas rtdbaas backend service baa parse founded tikhon bernstam ilya sukhar picture james yu kevin lacker firm produced series backend tool mobile developer store data cloud manage identity logins handle push notification run custom code cloud kind product known time backend service baa company later acquired facebook kept running shut product consequently released open source license installed onpremise containerization docker launched pycon solomon hykes provides additional layer abstraction automation operatingsystemlevel virtualization window linux docker used way package application easily executed various server without worry underlying infrastructure funny story docker originally created way abstract underlying infrastructure dotcloud competitor heroku year later bankrupted docker became huge success today adopted supported almost every cloud provider container scale launch docker blast potential immense following year lot company including google hashicorp started invest significant amount money energy create solution leveraged docker run container large scale like kubernetes swarm nomad coreos idea use container abstraction run process application cluster virtual machine function service faa aws launched lambda service allowed run code form function directly cloud lambda computation model event based mean function executed predefined event occurred product popularised new class service called function service faa advent faa started hear word serverless first time serverless trying define serverless actually mean always bit tricky prefer give two definition two acclaimed industry leader field first lengthy definition come straight amazon web service serverless often refers serverless application serverless application one require provision manage server focus core product business logic instead responsibility like operating system o access control o patching provisioning rightsizing scaling availability building application serverless platform platform manages responsibility amazon web service much concise one prefer come essence serverless trend absence server concept software development hope got point serverless mean server course server somewhere developer get worry focus much possible business logic application working picture commitstripcom serverless history explored far telling industry always looking next higher level abstraction developer need fewer concern possible able quickly release feature deliver value customer picture fullstackpythoncom picture matt makai fullstackpython illustrates exactly level abstraction platform created keep removing complexity leave time actual business value app provide serverless today highest level abstraction perfect strong tradeoff developer embrace order adopt definitely remove ton concern infrastructure layer pillar serverless characteristic come serverless make valuable option definitely abstraction absence server management know many configured flexible scaling need resource allocated high availability redundancy fault tolerance built never pay idle unused resource cost right approach future personal view probably yes like abstraction serverless come strong tradeoff vendor lockin steep learning curve cold start problem softhard limit etc allows u developer focus building releasing value fast possible live time competition web highest practically decent idea somebody else already attempting implement already successfully implemented critical able market quickly able iterate improve product efficiently serverless give needed agility compete market case willing learn embrace tradeoff probably ultimate solution believe abstraction layer new platform future agility focus business logic definitely thing always want future developer reference article possible thanks amazing resource also pleasure discus topic recent frontconf munich presentation titled future serverless feel free check slide deck give precious feedback
251,Lobsters,scaling,Scaling and architecture,Reducing Microservice Complexity with Kafka and Reactive Streams,https://www.youtube.com/watch?v=k_Y5ieFHGbs,reducing microservice complexity kafka reactive stream,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature reducing microservice complexity kafka reactive stream jim riecken youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature reducing microservice complexity kafka reactive stream jim riecken youtube
252,Lobsters,scaling,Scaling and architecture,"Anatomy of a Haskell-based Application, Revisited",https://tech-blog.capital-match.com/posts/3-anatomy-of-haskell-web-app.html,anatomy haskellbased application revisited,widely read article code liability infoworld interview fundamental design choice haskell work make right make bake event sourcing general architecture model service blog post june michael snoyman web physical organization shown tidbit persistence storage migration query blog post user interface om inaugural post rewriting large chunk code worthwhile blog post circleci mean entirely different thing concurrency conclusion,nearly two year since former cto arnaud bailly posted widely read article capital architecture november since written architecture several time intended audience investor business people necessarily mean high concentration buzzword fancy graphic impressivelooking diagram flow chart dearth actual technical detail today however write capital architecture technical audience largely follow structure original post help read post first read article several time recently large part remained valid despite tremendous growth capital match business perspective total amount funded loan around million arnaud wrote article almost twentyfold growth two year backend amount data capital match handling also growing exponentially figure capital data size growing exponentially note logarithmic axis perhaps surprisingly grown twentyfold size source code determine many line added deleted ran following command main repo git log nomerges numstat prettytformat awk add del end print add del command mean producing numerical statistic line addeddeleted every nonmerge commit since november add together two first column result added line code deleted line code capital code base smaller compared two year ago despite adding ton new functionality improving security performance many area slightly surprised result ran following determine addition deletion happen git log norenames nomerges numstat prettytformat awk add del end f add printf insertion sn add f f f del printf deletion sn del f f sort rn major deletion involve removing certain thirdparty file like reactjs repo making actual dependency major deletion involve removing badly written ui code produced contractor yes outsourcing work like mistake deletion typical workflow capital match vigorously delete unused code simplify existing code eric lee microsoft argued code liability similar sentiment traced back early alan cooper father visual basic also said infoworld interview archived original everyone team agrees le code better code besides deleting code also lot refactoring explains added half million line code also deleted half million line code resulting relatively high believe face constant change ongoing continuous refactoring way codebase remains sane fundamental design choice haskell speaking refactoring happy chose haskell wonderful language enables large refactoring ease perhaps even turning could unthinkable architectural change language pure tedium business side company demand feature tight deadline sometimes write pretty bad code get thing working later feature released take time rethink rewrite code good software engineering practice naturally following work make right make paradigm meet minimum requirement business call project success make work add bell whistle make program le prone error feature rich make right find eliminate waste process assumtions start incorrect remove unecessary business logic included step improve code better performance make fast time however easy succumb temptation make thing right first time result case well could delivered feature time instead perfectionist within u took feature delivered late famous book javascript good part douglas crockford started first chapter remark young journeyman programmer would learn every feature language using would attempt use feature wrote suppose way showing suppose worked guy went wanted know use particular feature perhaps rite passage programmer realize every feature language choice useful daytoday programming haskell feature mistake others designed pl researcher experiment usefulness industrial programming yet proven capital match slow process gradually getting rid code us language feature advanced pattern turn worthwhile like evolution haskell programmer fritz ruehr pointed anticlimatic moment powerful pattern used compute factorial fibonacci number difference interesting intellectual excursion simple maintainable practical mostly valuelevel code advanced typelevel feature sometimes eerily remind stereotypical enterprisey oop code care building superstructure class hierarchy getting thing done sometimes easy forget joyful write valuelevel code concise clear lucid said embrace haskell unconditional like haskell delusional give better tool library written another language two year given bake favor offtheshelf ci server written java reliability issue end seem worthwhile u spend effort maintain code ci system could used ci server discoverable gui someone else maintain event sourcing use event sourcing perhaps controversial use haskell even haskell world plenty developer continue use conventional rdbmss cto bias rdbmss day use rdbmss way capital match us event sourcing mostly unchanged flat event file store event individual businesslevel action affect state system however greatly improved implementation subsystem better way load persist event better query talk improvement detail later blog post general architecture general architecture also largely remained intact quoting arnaud main interface system restlike api providing various resource action resource exchange outside world done using json representation resource csv user interface merely client api morally letter single page application also commandline client offer access complete api used administrative purpose three main layer involved capital application model service web model model layer full pure code carry requisite business logic app contains several businessmodels event command compared original exposition part almost entirely unchanged save minor detail like changing data family injective type family changing argument order class businessmodel type event model type event event event type command model type command command command initial value model init default init default init def execute command model return event representing outcome command act command event apply event model resulting potentially new model apply event conceptually command action outside request want perform whereas event permanent immutable record something happened example registeruser command signaling desire register particular user response may result userregistered recording fact said user registered successfully another event signaling error result act method great place validation borrowing previous example act check whether another user email already registered business rule like whether password meet minimum lengthcomplexity requirement one thing considering add ability single command emit list event list event zero one even possibility command produce zero event useful used whenever command really applicable given state often find writing noevent one constructor event type capture possibility occasionally also useful single command result multiple event handled atomically usually workaround issue creating new constructor event complicated operation even know end result want simply sequential application two existing event arnaud lamented fact original rule strictly separating business model broken quite time ensuing year longer cared strictly separating business model lesson learned goal strictly separate business model eye towards able deploy independent service exchange message perhaps fact build multiple executables actually exchanging message rule written natural language easiest kind rule break little enforcement beyond code review given goal using eventpassing manner coordination across business model becomes obvious use sharedmemory concurrency aka stm service layer service service layer conceptually layer carry action model fact two sublayers first one instm layer io allowed mutation data allowed restricted atomic access multiple different business model safely thanks stm newtype instm instm uninstm readert tvar stm deriving functor applicative monad instm monad allows several modification single tvar made atomically even across several business model well ephemeral data part business model eg certain kind feature flag also useful complicated data access across business model must atomic even though stm strictly required case useful gain full functionality io servicet monad transformer look like newtype servicet g l servicet unservicet readert tvar g l deriving functor applicative monad essentially webstatem type blog post time effort improve sneaking exceptt within turned worthwhile g type variable refers global state called capitalmatchstate store data provides pluggable function configuration value concurrent channel tqueue endowed worker thread perform action thing written queue l type variable capture local state practice mean state associated particular request including certain piece data request header information computed request header authentication info principle quite versatile bit like python magical g object make limited use really additional state additional complexity also additional instance two type monadstate code written work also instance like monadbasecontrol servicet occasionally useful one also note monad heart readert monad transformer blog post june michael snoyman readert feel familiar u using pattern year blog post fully agree michael snoyman excellent way structure haskell program said practice find developer tendency overuse servicet specifically type synonym servicet call service includes monadio constraint service default monad reach lot time even want perform pure computation developer frequently use service full power io web web layer conceptually simple layer translates http request service function translates result back http response handle thing like middleware request routing parsing data json representation serializing result back json using mix scotty servant neither opinion perfect servant considerably better conceptually terrible error message quite bit boilerplate whereas general design especially error handling liberal inappropriate use lazy text leaf something desired physical organization term concrete file mainly module like capitalxmodel capitalxservice capitalxweb x name business model usually also capitalxtypes andor capitalxtypesy data type belonging model also clientspecific type reside capitalxwebtypesz z clientspecific type arnaud blog post mentioned splitting code meant splitting multiple package sadly misinformed since merged package really independent single one containing hundred module reason inexplicable reason ghc still default compiling parallel neither stack invoke ghc way parallelizes build effectively yes stack tool j option parallelizes internal invocation cabal build process word us packagelevel parallelism modulelevel parallelism needed use stack ghcoptionsj really surprising often mentioned people complain ghc build time obvious remedy course turn modulelevel parallelization practice also several option passed ghc mostly rts option control garbage collection ghc shown improve compile time merging package back single one demonstrated improvement build time tidbit persistence storage although architecture event storage subsystem changed changed greatly actual implementation almost every line code deal event storage rewritten higher performance thanks cheap easytouse spark event loading code run parallel lowlevel system call also used directly enable higher performance mmaped file better reliability haskell side event readingwriting code also considerably simplified original labyrinth several tqeueues tvarss tmvars replaced single mvar unlike tmvar fairness guarantee higher level however still using tvars tmvars bus noted tqueues endowed worker thread perform action thing written queue although designed generic useful many context form messagepassing concurrency heavily used higherlevel event handling seen great improvement especially face exception migration versioning migration front logically changed main architecture implementation extremely different longer use event version select function deserialize old data old code function version deserialize data terrible builtin assumption shape old data look like arguably fair assumption us partial function everywhere bad practice without modern technique like lens migration code us lot highly specific data structure traversal code verbose convoluted fact function deal historical data convert directly data application currently requires mean historical migration code sometimes modified writing new migration therefore order write migration necessary understand every single previous migration since beginning time last point especially insidious beginning code quick easy write gradually became nightmare asking human remember entire history data type look like asking trouble onboarding new developer wrote incorrect migration code incorrect test said migration code silently reinterpret old data wrong way finally embarked major refactoring somewhat scarily discovered along refactoring process older migration code written early never correct first place test accompany never caught issue check semantic interpretation event exception thrown migration using partial function extensively gone day extremely brittle migration code difficult read difficult write require frequent change use snazzy lensbased migration system lens library make traversal deeply nested data structure extremely easy capital match wholeheartedly embrace sharing technical detail system future blog post query even original blog post arnaud mentioned sql great writing query indeed true mostly completed transition ixsets simply set thing together index map set query work within narrow ixsets fact plenty extremely fast blog post unscientifically compare performance acid ixset acidstate find extremely fast generally experience well code us ixsets never really disappointed performance query complicated predicate think clause sql also booleanquery data type data booleanquery booleanquery booleanquery booleanquery prop booleantrue booleanfalse deriving show functor foldable traversable also several function simplify boolean query various option simple simplification like transforming booleantrue p p also propositionaware simplifiers look proposition question performs simplification following simplification rewriting function turn query actual ixset query function dependent specific ixset question ie polymorphic function turn booleanquery endo ixset ix b others also ixsetspecific one example query operates index type use operator instead union course type also support nonixset query code turn booleanquery bool monad usually reader monad well complicated value like f g bool join story admittedly satisfactory container package introduced mapmerging using place would like see something similar ixsets really hard implement eventually way joining ixsets linearithmic complexity turn performance bottleneck worth resolving likely fork add support user interface little changed regard still using om clojurescript say however satisfied rather perhaps late change echoing sentiment inaugural post lack type system make frontend development rather unsatisfactory still choice made arnaud back wellknown rewriting large chunk code worthwhile besides lack type system gradually discovered om design issue first issue also explained blog post circleci om assumes shape ui dom essentially tree fit shape data real world seems frequently case leading either component accessing entire state avoiding global state cursor using local state exclusively peppered antipatterns like one component accessing local state another component manually calling omrefresh second issue really distinction global state local state different function handle apply function global state need use omtransact set value function omupdate yet apply function local state function omupdatestate setting value omsetstate besides naming inconsistency different method accessing global local state mean important basic component available two form one version work global state another work local state contrast reagent global state atom toplevel defined defonce local state another atom defined within let binding made available closure render function om next next iteration om radically improved way writing uis unfortunately almost reached vaporware status development since still actual release reached beta status year still lot incomplete documentation critically documented way gradually migrate om om next furthermore also seems difficult understand exacerbated repurposing standard terminology like mean entirely different thing also watching space haskelly solution purescript ghcjs made great stride since two year ago inconceivable starting scratch would choose purescript ghcjs concurrency noted arnaud concurrency mostly handled level request scenario fully moved away forkio favor higherlevel abstraction like async withasync among two former sparingly used favor latter automatic killing child thread function return throw exception practice usually use contt r io monad ease handling continuationbased nature withasync especially app startup code need launch many thread coordination concurrent thread mix messaging passing shared memory used conclusion long post brief overview system pointer mentioned like elaborate future blog post well covering changed part system like infrastructure cicd testing containerization development environment flow project management coordination team four country across three timezones would similar team thing hope unique insightful
253,Lobsters,scaling,Scaling and architecture,Scaling Slacks Job Queue,https://slack.engineering/scaling-slacks-job-queue-687222e9d100,scaling slack job queue,initial job queue system architecture redis task queue life job architectural problem incremental change full rewrite kafka front redis enqueuing job kafka sarama bias towards availability consistency simple client semantics minimum latency availability zone relaying job kafka redis data encoding replaced equivalent unicode entity character escaped default selfconfiguration consul handling failure rate limiting kafka cluster setup load testing failure testing data migration production rollout double writes guaranteeing system correctness heartbeat canary final rollout conclusion,slack us job queue system business logic timeconsuming run context web request system critical component architecture used every slack message post push notification url unfurl calendar reminder billing calculation busiest day system process billion job peak rate per second job execution time range millisecond case several minute previous job queue implementation date back slack earliest day seen u growth measured order magnitude adopted wide range us across company time continued scale system ran capacity limit cpu memory network resource original architecture remained mostly intact however year ago slack experienced significant production outage due job queue resource contention database layer led slowdown execution job caused redis reach maximum configured memory limit point redis free memory could longer enqueue new job meant slack operation depend job queue failing made even worse system actually required bit free redis memory order dequeue job even underlying database contention resolved job queue remained locked required extensive manual intervention recover incident led reevaluation job queue whole follows story made significant change core system design minimal disruption dependent system stop world changeover oneway migration room future improvement initial job queue system architecture around time last year job queue architecture could sketched follows roughly familiar people created used redis task queue life job enqueuing job web app first creates identifier based job type argument enqueue handler selects one configured redis host based hash identifier logical queue given job using data structure stored redis host handler performs limited deduplication job identical id already queue request discarded otherwise job added queue pool worker machine poll redis cluster looking new work worker find job one queue monitor move job pending queue list inflight job spawn asynchronous task handle task completes worker remove job list inflight job job failed worker move special queue retried configured number time eventually succeeds move onto list permanently failed job manually inspected repaired architectural problem postmortem outage led u conclude scaling current system untenable fundamental work would required constraint identified redis little operational headroom particularly respect memory enqueued faster dequeued sustained period would run memory unable dequeue job dequeuing also requires enough memory move job processing list redis connection formed complete bipartite graph every job queue client must connect therefore correct current information every redis instance job worker scale independently redis adding worker resulted extra polling load redis property caused complex feedback situation attempting increase execution capacity could overwhelm already overloaded redis instance slowing halting progress previous decision redis data structure use meant dequeuing job requires work proportional length queue queue become longer became difficult empty another unfortunate feedback loop semantics qualityofservice guarantee provided application platform engineer unclear hard define asynchronous processing job queue fundamental system architecture practice engineer reluctant use change existing feature limited deduplication also extremely highrisk many job rely function correctly problem suggests variety solution investing work scaling existing system complete groundup rewrite identified three aspect architecture felt would address pressing need replacing redis inmemory store durable storage kafka provide buffer memory exhaustion job loss developing new scheduler job improve qualityofservice guarantee provide desirable feature like ratelimiting prioritization decoupling job execution redis allowing u scale job execution required rather engaging difficult operationally costly balancing act incremental change full rewrite knew implementing potential architectural enhancement would require significant change web app job queue worker team wanted focus critical problem gain production experience new system component rather attempt everything series incremental change felt like efficient way make progress towards productionizing revised system first problem decided address guarantee write availability queue buildup worker dequeue job rate slower enqueue rate redis cluster would eventually run memory slack scale could happen quickly point redis cluster would unavailable accept writes enqueue additional job thought replacing redis kafka altogether quickly realized route would require significant change application logic around scheduling executing deduping job spirit pursuing minimum viable change decided add kafka front redis rather replacing redis kafka outright would alleviate critical bottleneck system leaving existing application enqueue dequeue interface place sketch incremental change job queue architecture kafka supporting component front redis enqueuing job kafka first challenge faced efficiently get job phphacklang web app kafka although explored existing solution purpose none wellsuited need developed kafkagate new stateless service written go enqueue job kafka kafkagate expose simple http post interface whereby request contains kafka topic partition content using sarama golang driver kafka simply relay incoming http request kafka return successfailure operation design kafkagate maintains persistent connection various broker follow leadership change offering low latency simple interface phphack web application kafkagate designed bias towards availability consistency writing job kafka wait leader acknowledge request replication job additional broker choice provides lowest latency possible create small risk lost job event broker host dy unexpectedly replicating right tradeoff slack application semantics though also considering adding option kafkagate allow critical job application wait stronger consistency guarantee certain operation simple client semantics kafkagate us synchronous write kafka allows u positively acknowledge job make queue notwithstanding risk lost writes described return error case failure timeouts tightens existing semantics without dramatic change allowing engineer use confidence still giving u ability modify job queue design future minimum latency order reduce amount time spent enqueue job made number optimization performance one example relates deploy route kafkagates slack deployed aws provides several availability zone az independent region az within region lowlatency link provide degree isolation failure impact az connection az typically higher latency connection stay within single az also incur transfer cost job queue preferentially route request kafkagate instance az host enqueueing job still allowing failover az improves latency cost still allowing fault tolerance future considering optimization kafkagate service running locally web app host avoid extra network hop writing kafka relaying job kafka redis next new component architecture address need relay job kafka redis execution jqrelay stateless service written go relay job kafka topic corresponding redis cluster designing service think following data encoding earlier system web app written php hack would json encode representation job storing redis subsequently job queue worker also written php would decode job payload execution new system relied jqrelay written go decode json encoded job examine reencode json write appropriate redis cluster sound simple enough right turn golang php json encoders unexpected quirk related escaping character caused u heartache specifically go character replaced equivalent unicode entity default php character escaped default behavior resulted issue json representation data structure would differ two runtimes situation exist original phponly system selfconfiguration jqrelay instance start attempt acquire consul lock keyvalue entry corresponding kafka topic get lock start relaying job partition topic loses lock release resource restarts different instance pick topic run jqrelay autoscaling group failed machine automatically replaced service go lock flow combined consul lock strategy ensure kafka topic used job queue exactly one relay process assigned failure automatically heal handling failure jqrelay relies kafka commit offset track job topic partition partition consumer advance offset job successfully written redis event redis issue retries indefinitely redis come back redis service replaced job specific error handled reenqueuing job kafka instead silently dropping job way prevent jobspecific error blocking progress given queue keep job around diagnose fix error without losing job altogether rate limiting jqrelay respect rate limit configured consul writing redis relies consul watch api react rate limit change kafka cluster setup cluster run version kafka broker run machine every topic partition replication factor retention period day use rackaware replication rack corresponds aws availability zone fault tolerance unclean leader election enabled load testing set load test environment stress kafka cluster rolling production part load testing enqueued job various kafka topic expected production rate load testing allowed u properly size production kafka cluster sufficient headroom handle individual broker going cluster leadership change administrative action give u headroom future growth slack service failure testing important understand different kafka cluster failure scenario would manifest application eg connect failure job enqueue failure missing job duplicate job tested cluster following failure scenario hard kill gracefully kill broker hard kill gracefully kill two broker single az hard kill three broker force kafka pick unclean leader restart cluster scenario system functioned expected hit availability goal data migration used load test setup identify optimal throttle rate safe data migration across broker addition experimented using lower retention period migration since need retain job successfully executed billion job flowing every day would prefer selectively migrate partition instead topic across broker planned part future work production rollout rolling new system included following step double writes started double writing job current new system job enqueued redis kakfa jqrelay however operated shadow mode dropped job reading kafka setup let u safely test new enqueue path web app jqrelay real production traffic guaranteeing system correctness ensure correctness new system tracked compared number job passing part system web app kafkagate kafkagate kafka finally kafka redis heartbeat canary ensure new system worked endtoend redis cluster kafka partition topic partition enqueued heartbeat canary every kafka partition every minute monitored alerted endtoend flow timing heartbeat canary final rollout sure system correctness enabled internally slack week showed problem rolled one one various job type customer conclusion adding kafka job queue great success term protecting infrastructure exhaustion redis memory let walk scenario queue build old system web app sustained higher enqueue rate job queue dequeue rate redis cluster would eventually run memory cause outage new system web app sustain high enqueue rate job written durable storage kafka instead adjust rate limit jqrelay match dequeue rate pause enqueues redis altogether broader picture work also improved operability job queue configurable rate limit durable storage job enqueues outstrip execution capacity finergrained tool disposal clearer client semantics help application platform team make confident use job queue infrastructure team foundation continued improvement job queue ranging tying jqrelay rate limiting redis memory capacity larger goal improving scheduling execution aspect system even new system like place always looking way make slack reliable interested helping get touch
254,Lobsters,scaling,Scaling and architecture,GoCardless Incident review: API and Dashboard outage on 10 October 2017,https://gocardless.com/blog/incident-review-api-and-dashboard-outage-on-10th-october/,gocardless incident review api dashboard outage october,summary database setup incident timeline following week reproducing failure red herring invalid wal file moving new database cluster wrapping incident next elephant room closing thought,post represents collective work core infrastructure team investigation api dashboard outage october payment company take reliability seriously hope transparency technical writeups like reflects thatwe included highlevel summary incident detailed technical breakdown happened investigation change made sincesummaryon afternoon october experienced outage api dashboard lasting hour minute request made time failed returned errorthe cause incident hardware failure primary database node combined unusual circumstance prevented database cluster automation promoting one replica database node act new primarythis failure promote new primary database node extended outage would normally last minute one lasted almost hoursour database setupbefore start helpful highlevel view store data gocardlessall critical stored postgreswe run postgres cluster node primary synchronous replica asynchronous replica mean always least copy every piece data time respond successfully api requestto manage promotion new primary node event machine failure run piece software called pacemaker node cluster client ruby rail application connect primary using virtual ip address vip also managed pacemakerput together look little like primary node fails cluster noticesit promotes synchronous replica guaranteed copy every write eg new payment primary accepted also set old asynchronous replica new synchronous replicaonce vip moved across application carry worka site reliability engineer sre add new replica back clusterincident timelineso go wrong time section british summer time monitoring detects total outage api dashboard engineer begin see evidence disk array failure primary unsure cluster nt already failed synchronous power broken primary postgres node done nt machine cluster online broken disk array believe synchronous asynchronous node online cluster software promote new primary quickly becomes clear clear error count crm resource cleanup pacemaker prompt rediscover state postgres instance effect cluster promote synchronous replicawe spend next hour trying variety approach promote new primaryour last attempt centre around editing configuration synchronous replica try promote put pacemaker cluster maintenance mode crm configure property maintenancemodetrue remove configuration flag tell postgres replica recoveryconf bring cluster maintenance mode every time brings replica back original recoveryconf left decide attempt approach run long need try something else set cluster maintenance mode one last time configure synchronous replica primary start postgres ourselvessince cluster also manages vip set maintenance mode reconfigure backend application connect actual ip address new manually promoted primary postgres node working configuration change rolled monitoring system confirm api dashboard back uphaving brought system back online next priority restore database cluster usual level redundancy meant bootstrapping third node asynchronous replicasince pacemaker still maintenance mode would automatic failover machine fail believed likely running manuallymanaged setup event primary failing wanted able promote synchronous replica quickly possiblewe decided introduce another vip managed manually infrastructure team event primary failing would promote synchronous replica move vip ourselvesas incident triggered disk array failure caution spent time verifying integrity data running every test could think found evidence data corruptiononce done felt safe gocardless service running started planning next stepsthe following weeksthe day incident whole team sat discus two issue pacemaker cluster fail elect new primary database node move back pacemaker managing cluster manuallymanaged state discussion decided taken much manual intervention existing database cluster confident bringing back pacemaker automation decided provision new cluster replicate data switch traffic overwe split two subteams one trying reproduce failure working move new database cluster minimal disruptionreproducing failureto confident new cluster needed understand existing one nt promote new primary make change fix issuefor part analysing log component involved failure analysis pulled several factor looked like could relevant reproducing issue raid controller logged simultaneous loss disk array subsequent read write operation failedthe linux kernel set filesystem backed controller readonly mode given state array even read nt possiblethe pacemaker cluster correctly observed postgres unhealthy primary node repeatedly attempted promote new primary time could nt decide primary runon synchronous replica one become new primary one postgres subprocesses crashed around time disk array failure primary happens postgres terminates rest subprocesses restartsafter restart synchronous replica kept trying restore writeahead log wal file restorecommand attempt failed message stating file invalid later lot unpick right given complexity involved clear get answer reasonable amount time could repeatedly break cluster slightly different way see could get break way production cluster octoberfortunately part unrelated work done recently version cluster could run inside docker container used help u build script mimicked failure saw production able rapidly turn cluster let u iterate script quickly found combination event broke cluster right waya red herring invalid wal fileone log entry stood real cause concern synchronous replica failing restore wal file restorecommanda quick bit background familiar postgres writeahead log postgres record everything ask write eg insert update delete query log provides strong guarantee writes lost postgres crash also used keep replica sync primarythere two way wal used streaming replication archivecommand restorecommandin streaming replication replica establish ongoing connection primary sends wal generates specify replication synchronous wait replica confirm received wal returning query generated itwith archivecommand postgres let specify shell command executed every time chunk wal generated primary make file name available command specify choose file similarly restorecommand run replica pass name next wal file database expects replay copy wherever archived toit common use streaming replication archivecommand combination streaming replication keep replica sync recent change archivecommandrestorecommand used bootstrap node behind pulling older wal file archive external cluster eg want bootstrap new node restoring last full backup replaying wal happened incident turned final act server raid controller issue archived invalid wal file backup server postgres subprocess crash caused restart synchronous replica postgres instance came back ran restorecommand pulling invalid walpostgres internal validation check saw wal file invalid discarded term data nt matter node already good copy writes streaming synchronous replication primary time failurewe matched log line validation failure postgres source code spent lot time reproducing exact type invalid wal local container setup keep mind binary format content fairly dynamic easiest thing break repeatable way end figured played part pacemaker cluster inability promote new primary least learned little internals postgres process elimination able remove step script left three condition necessary cluster break pacemaker setting defaultresourcestickinessby default pacemaker nt assign penalty moving resource postgres database process vip different machine service like postgres moving resource eg vip client connected cause disruption nt behaviour want combat set defaultresourcestickiness parameter nonzero value pacemaker consider option moving resource already runningpacemaker resource backup vipas part another piece work reduce load primary node added another vip cluster idea vip would never located primary backup process would always connect replica freeing capacity read operation primary set constraint vip would never run server postgres primary pacemaker term set colocation rule inf negative infinity preference locate backup vip postgres primary serverat time incident backup vip running synchronous replica node pacemaker promoted primaryfailure condition two process crashing onceeven configuration crashing postgres process primary nt enough reproduce production incident way get cluster state would never elect new primary crash one postgres subprocesses synchronous replica saw production log incidentall three condition necessary reproduce failure removing defaultresourcestickiness backup vip led cluster successfully promoting new primary even two process crashing almost simultaneously similarly crashing postgres process primary led cluster successfully promoting synchronous replicawe spent time testing different change pacemaker configuration ran surprising fix somehow inf colocation rule backup vip postgres primary interfering promotion process even though another node asynchronous replica backup vip could runit turned specifying colocation rule backup vip opposite way round worked fine instead specifying rule infpreference backup vip postgres primary could specify inf preference backup vip replica specified way round cluster promotes synchronous replica fine failure conditionsmoving new database clusterwhilst investigation going half team figuring migrate manually managed cluster new cluster managed pacemakerfortunately u prior work could turn weve previously spoken approach performing zerodowntime failover within cluster script coordinate publicly available github repositorythe talk go detail relevant part nt hard describeas well postgres pacemaker also run copy pgbouncer node database cluster introduce second vip layer indirection client eg ruby rail application connect new pgbouncer vip pgbouncer turn connects original postgres vipit possible pause incoming query pgbouncer put queuewe promote new primary move postgres vip note vip client connecting pgbouncer vip nt need move client experience disruptiononce cluster finished promoting new node tell pgbouncer resume traffic sends queued query new primarywe needed adapt procedure little automation performs designed migrate different node cluster two separate clustersthe subteam responsible getting u new cluster spent next couple week making adjustment performing practice run totally comfortable put together plan productionwrapping incidentwith plan place confidence understood fixed issue stopped cluster failing october ready go even testing done announced maintenance window precautionfortunately everything went planned night migrated new database cluster without hitchwe decommissioned old cluster closed incidentwhat next getting away size incidentwe feel immense duty everyone trust gocardless payment provider took time think learned incident focus could improve reliability future key item came seemingly simple pacemaker configuration lead extremely unusual behaviouron surface defining rule say two resource must run together seems like would opposite defining rule say two resource must run together reality cause system behave entirely different way certain failure condition take knowledge future work pacemakerunrelatedly conveniently move away using vip direct traffic specific postgres instance instead running proxy application server direct traffic right node based state cluster drastically reduce number resource managed pacemaker turn reducing potential weird behaviour clustersome bug surfaced fault injectiona misconfiguration surface two process crash almost time nt one going find basic test daytoday operation done fault injection part game day exercise always area harsher test postgres cluster automation like chaos monkeythat continually injects failure idea keen pursueautomation erodes knowledgeit turn automation successfully handle failure two year skill manually controlling infrastructure atrophy one size fit easy say write runbook multiple year go next need almost guaranteed outofdatethere definitely way combat one possibility thinking adding arbitrary restriction game day exercise eg cluster automation failed ca nt diagnose problem need bring service back another way elephant roomwe sure asking even run postgres instance hosted option could nt end writeup without talking littlewe periodically consider option managed postgres service recently somewhat lacking area care without turning article provider comparison recent development provider offer zerodowntime patch upgrade postgres something whilethe thing made u rule managed postgres service far infrastructure baremetal hosting provider added latency provider datacentres hosted postgres service would cause fairly drastic rework application developer currently assume latency postgres millisecond lowerof course nothing set stone hosting situation change time offering various hosting provider keep eye perhaps one day wave goodbye running postgres cluster ourselvesclosing thoughtswe like apologise one last time incident know much trust people put payment provider strive run reliable service reinforces trustat time strongly believe learning failure happen encouraging see blameless postmortem becoming increasingly common operation discipline whether happen call devops sre something else hope found one interesting usefulyou find u gocardlesseng twitter got comment question try answer best record payment merchant customer detail etc postgres standard replication setup primary node accept writes point nt comfortable cluster automation knew would spend time digging went wrong team recognised going detailed investigation lot moving part could take multiple day potentially week two essential comfortable safe running gocardless service
255,Lobsters,scaling,Scaling and architecture,The Entity Service Antipattern,http://www.michaelnygard.com/blog/2017/12/the-entity-service-antipattern,entity service antipattern,last post net microservices architecture ebook tutorial microservice reference architecture jhipster fear cycle monolith microservices march contact,last post talked need keep thing separated decoupled let look one way break entity service pattern solution problem context antipattern antipattern commonlyrediscovered solution problem context inadvertently creates resulting context like le original context word pattern make thing worse according value system contend entity service antipattern make case need establish entity service commonlyrediscovered solution problem resulting context worse starting context monolith let start commonlyrediscovered part entity service microsoft net microservices architecture ebook spring tutorial spring may give u absolute easiest way create entity service class annotated json mapping persistence mapping redhat microservice reference architecture productservice salesservice microservicefocused framework jhipster start crud data entity order make case resulting context worse starting context need assume starting context actually sake generality assume largish legacy application moreorless monolith may call integration point get work done feature pretty much local inprocess multiple instance process running different host basically like following diagram feature reside code application instance many author enumerated sin monolith wo nt belabor though feel compelled make brief aside say somehow deliver quite lot working valuable feature ran monolith might describe initial context clear code build feature go test code release cadence dictated slowestdelivering subteam little inherent enforcement boundary thus coupling tends increase time performance problem found profiling single application cause availability problem typically found one place building feature rely multiple entity straightforward though may come cost inappropriate coupling code grows large organization risk entering fear cycle feature availability may compromised inappropriate coupling via common mode application eg thread pool connection pool feature availability improved redundancy whole application reduced however application vulnerable surrounding environment case selfdenial attack memory leak race condition supposing move microservice architecture entity service might end something like example spring tutorial version assume service box comprises multiple instance service obviously moving part involved immediately mean harder maintain availability challenge performance analysis debugging well documented wo nt belabor resulting context feature get created direct interaction online shopping service individual entity service example creating account definitely online shopping account feature however require one entity use aggregate intersection entity example suppose need calculate total price cartful item involves cart product individual price account find applicable sale tax vat predict implemented online shopping service making bunch call entity service get data depict activation set term made show service activated processing single request type picture focus service elide infrastructure price cart activate four five service architecture activation represents operational coupling affect availability performance capacity also represents semantic coupling change entity service potential ripple online shopping service particularly bad case online shopping service may find brokering data format translating version user data produced account version format cart expects common corollary entity service idea stateless business process service think meme date back last century original introduction java ee entity bean session bean came back soa microservices happens picture introduce process service handle pricing cart much improvement bear mind activation set one request type consider different request type overlay activation set find entity service activated majority request make problem availability performance also mean wo nt allowed change fast like service high fanin need stable let look resulting context moving microservices entity service performance analysis debugging difficult tracing tool zipkin necessary additional overhead marshalling parsing request reply consumes precious latency budget individual unit code smaller team deploy cadence semantic coupling requires crossteam negotiation feature mainly accrue nexus api aggregator ui server entity service invoked nearly every request become heavily loaded overall availability coupled many different service even though expect individual service deployed frequently deployment look exactly like outage caller summary say criterion met label entity service antipattern stay tuned future post look instead entity service interested learning breaking monolith might like monolith microservices workshop session open public march contact schedule workshop company
256,Lobsters,scaling,Scaling and architecture,Microservice architecture best practices from tech leaders,http://codingsans.com/blog/microservice-architecture-best-practices,microservice architecture best practice tech leader,microservice architecture best practice organizational challenge post want post really long use link jump specific part switch microservice architecture steven mccord founder cto icx medium david dawson steven mccord avi cavale david dawson system architect recommended method daniel benzvi vp r similarweb avi cavale micro case study avi cavale cofounder ceo shippable microservices v monolithic architecture advantage microservice architecture scalability steven mccord founder cto icx medium easier maintainability deploying configuring without much distraction problem isolation easier hiring responsibility clearly defined deep knowledge wide variety programming language easier oversee understand easier open component disadvantage microservice architecture deployment interoperability many programing language making component work togethe harder integration test architecture wellthought beginning requires effort communication difficult monitor whole system take time learn complexity avi cavale cofounder ceo shippable daniel benzvi vp r similarweb logging one place challenging sonu kumar challenge switch system challenge splitting system challenge organizational buyin challenge team challenge switch system brujo benavides excto inaka viktor tusa devops engineer logmein possible solution brujo benavides challenge splitting system robert aistleitner jose alvarez senior developer stylesage david papp chief architect recart possible solution andras fincza vp engineering emarsys jose alvarez incrementally module module best way split monolithic system want everything certainly fail tool tip monitoring challenge organizational buyin steven mccord founder cto icx medium possible solution challenge team avi cavale cofounder ceo shippable avi cavale possible solution avi cavale andras fincza vp engineering emarsys robert aistleitner vp engineering usersnap daniel benzvi vp r similarweb viktor tusa devops engineer logmein andras fincza vp engineering emarsys daniel benzvi varun villait csaba kassai lead developer doctusoft runtime process team cultural programming theory building data david dawson matter technology programing language use andras fincza vp engineering emarsys viktor tusa devops engineer logmein andras fincza vp engineering emarsys viktor tusa devops engineer logmein steven mccord founder cto icx medium iring decision robert aistleitner vp engineering usersnap steven mccord founder cto icx medium technology suggestion csaba kassai lead developer doctusoft brujo benavides recommend technology technology selection case study cloudbased apps csaba kassai lead de veloper doctusoft varun villait ceo industry greg neiheisel cofounder cto astronomer process look emarsys emarsys andras fincza vp engineering emarsys similarweb david dawson suggestion selecting technology key takeaway like post please help u sharing andras fincza emarsys daniel benzvi similarweb david dawson,microservice architecture best practice switching microservice architecture seems easy tech leader tend underestimate complexity project make disastrous mistake transforming monolithic system microservices starting one scratch need carefully consider technological organizational challenge arise post want switch monolithic system microservice architecture gather insight experienced tech leader know disadvantage advantage microservices avoid disastrous mistake make better technological decision regarding microservices conducted interview tech leader different country israel usa compressing knowledge actionable post post really long use link jump specific part biggest mistake make switch microservice architecture without clear goal need understand real reason want people go blindly lot thing oh docker cool microservice great might fit every piece build need understand want steven mccord founder cto icx medium working system work fine driving force change microservice architecture hyped mean need jump bandwagon might best technological choice software defining reason crucial david dawson steven mccord avi cavale emphasized first question ask client ask help implement microservices answer looking want change system faster want take advantage cloud tech make aware microservices done right expensive harder building monolithic system interesting thing data introduces network data model arbitrarily partition time data lost expose full fury distributed computing david dawson system architect recommended method necessarily go microservices would go mediumsized service monolith hundred service instead bigger service align engineering team business vertical daniel benzvi vp r similarweb find right balance service microservice function either underfragment application see benefit microservice architecture overfragment application mean weight managing microservice destroy value microservice architecture provide avi cavale crucial clear philosophy microservice characteristic look company micro case study determined microservice looking piece code changed ended creating exponential test case started taking goal reduce amount testing every single change make goal define microservice different somebody say want billing microservice avi cavale cofounder ceo shippable recommended reading microservices v monolithic architecture advantage microservice architecture scalability small piece analyse see requirement piece enables scale different part application separately big benefit scale container outside vm put container kind configuration want complete portability application steven mccord founder cto icx medium easier maintainability let different team work different component moreorless independent manner deploying configuring without much distraction deploy configure tiny piece system without affecting service multiple team deliver multiple result production without interfering stepping toe problem isolation much easier isolate detect problem easier hiring looking developer thirdparty provider need train small part system responsibility clearly defined one team responsible given microservice deep knowledge team working know insideout wide variety programming language use different programming language depending best serf microservice purpose easier oversee understand split huge code base smaller project approach allows team understand project code better easier open component way easier open component existing functionality new business unit external entity boundary interface clearly defined disadvantage microservice architecture deployment interoperability drawback deployment interoperability become chief concern many programing language limit code reusability well maintainability could make hiring complicated making component work together always need ensure service composed way work together think changing single endpoint would break depending service older version harder integration test whole system compared monolithic system everything one place architecture wellthought beginning much cohesion among service lose advantage requires effort communication associated cost related investment make term communication service lot failure happen communication service difficult monitor whole system lot piece altogether could nightmare monitor take time learn using microservice architecture requires learning take time complexity microservices make whole system complex harder oversee whole operation piece lying around good engineering process end whole bunch thing lying around may never used avi cavale cofounder ceo shippable debugging production issue microservicesbased platform completely different opera without proper monitoring logging tracing facility complexity system grows significantly like going maze engineering practice standardization become critical daniel benzvi vp r similarweb logging one place challenging third party log aggregation service like loggly splunk heroku good solution come hefty price experience telemetry specially centralized logging biggest pain think verbosity level service nt might end paying cost logging infra sonu kumar site reliability engineer microsoft come switching microservice architecture top challenge tech leader developer team could face challenge switch system oncechallenge splitting systemchallenge organizational buyinchallenge team challenge switch system switching monolithic architecture microservice architecture something monolithic server probably repository deployment task monitoring many thing tightly set around changing together easy brujo benavides excto inaka company never experience microservices even green field project would harder think viktor tusa devops engineer logmein possible solution back keep monolithic server place new addition developed microservice eventually thing drained original server ended oldest biggest microservices brujo benavides challenge splitting system pretty challenging isolate component service glued together since beginning project robert aistleitner need define interaction process piece nt define good way system generate problem jose alvarez senior developer stylesage pattern many different rule splitting system microservices one tell case application two identical microservices david papp chief architect recart possible solution way split monolithic system microservices inspect monolithic system first see hurt part system taken transformed microservice andras fincza vp engineering emarsys nt monitor appropriately see system work monitor piece working monitor system detect solve problem easily jose alvarez incrementally module module best way split monolithic system want everything certainly fail tool tip monitoring new relicdatadoginfluxdbgrafana challenge organizational buyin getting organizational buyin probably hardest part steven mccord founder cto icx medium technological decision need clearly state benefit microservice architecture persuade company reallocate resource long tedious process change like accepted organization larger organization longer decision take possible solution best way convince organization switch microservice architecture transform one noncritical part system microservice way demonstrate advantage using real working microservice challenge team biggest challenge happens team requires different thinking avi cavale cofounder ceo shippable developer spend lot time understanding endtoend scenario need familiar technology might require switching mindset take time uncomfortable people working world endtoend test suddenly breaking small piece cultural change avi cavale possible solution start something small really benefit select something critical part application get small team transform part app microservice prove actually better scale organization step step avi cavale avoid switching entire system microservices andras fincza vp engineering emarsys guess biggest mistake make nt created overview implication change microservice architecture lot moving part include actually starting implement new approach robert aistleitner vp engineering usersnap monolith easy change internal interface refactor code end end run test microservices api must gold relied upon necessarily aware client moving without api future proofing going create lot headache future also make sure distributed tracing system place daniel benzvi vp r similarweb avoid trying switch microservices without figuring platform dependency also believing microservices good every microservice written different language bad practice viktor tusa devops engineer logmein handling data crucial pretty easy screw data really hard restore data migration happen step andras fincza vp engineering emarsys sharing data microservices big nono two service manipulating data start experiencing consistency issue disambiguate ownership daniel benzvi varun villait breaking application many small piece forcing transform system microservices microservice hype csaba kassai lead developer doctusoft creating isolation microservices enables changed fast need generally requires isolation several level runtime process obvious one commonly adopted quickly one process many primary cost adopting form distributed computing hard right may lead adopting containerisation event architecture various http management approach service mesh circuit breaker team cultural separating team give autonomy mean partition humantohuman communication tends lead knowledge silo duplication work working optionality v resource efficiency choice recommended reading programming theory building peter naur data largest impact adopting distributed computing approach like microservices way affect data partitioned data form need reintegrate system level give impression system give interesting potential benefit regard scaling also need much thought simple monolithic approach data architecture david dawson different opinion begin collide one side people argue matter technology programing language use almost every problem solved technology others spend much time finding right technology iterative way time think see action way bad decision mitigated andras fincza vp engineering emarsys big modern language python java c nodejavascript equally fast scalable perspective language matter every language pro con time language selection based personal preference instead technical argument viktor tusa devops engineer logmein spending lot time selecting best technology worth since difference minor importance selecting technology overvalued running cost important acceptable matter much u andras fincza vp engineering emarsys greenfield project use language programmer know greenfield project use language best coverage client side business entity system viktor tusa devops engineer logmein good thing microservice encapsulated microservice long give external microservice interface talk thing really care long interface steven mccord founder cto icx medium selecting appropriate technology technological question also hiring decision choose microservice architecture different programing language need make sure team able handle would nt recommend mixing many programming language hiring people get difficult also context switch programmer would slow development robert aistleitner vp engineering usersnap make conscious choice type development team want build want use many different programming language need build dynamic team able use learn different programming language steven mccord founder cto icx medium technology suggestion highly recommend use managed service appengine google cloud platform take lot burden shoulder also selecting languagetechnologyframework always important select appropriate one specific microservice use case force something familiar csaba kassai lead developer doctusoft brujo benavides hand tech leader happy recommend technology could good fit microservice serf specific role selecting technology microservice recommended consider maintainability faulttolerance scalability cost architecture ease deployment example framework technology brujo team us microservices scrapy web crawlingcelery rabbitmq communicate microservicesnltk tensorflow others machine learning partaws service recommended reading technology selection case study cloudbased apps selecting programing languagetechnology microservice many thing need consider one important thing see competency developer big support tool community behind languagetechnology according experience company tend select programming language according competency developer csaba kassai lead developer doctusoft use technology lot support resource active community behind would recommend ruby javascript get lot support lot people could help anything go wrong think long make sure lot people using undertaking language problem case rely external resource team posse knowledge varun villait ceo industry another factor may library exist language could used speed project ideal choice language may library certain thing may invent could another time drain obviously thing like faulttolerance scalability big factor going rewrite something scratch month initial choice scale might better biting bullet earlier think come specific team situation investment willing make greg neiheisel cofounder cto astronomer process look emarsys emarsys want apply new programming language developer need provide real logical reason consult lead developer team gather together discus pro con technology always create spike solution different technology let experiment boundary given technology see applied given microservice perfect uncovering limitation technology recommended use language team already familiar way work comfortably progress faster andras fincza vp engineering emarsys similarweb big data analytics company deal largescale challenge increase risk impact choosing wrong technology single threaded framework nodejs great network bound service scale dealing realtime intensive data processing engineer determine technology use balancing tactical strategic need looking technical organizational constraint rapid prototyping phase service deal large amount data want add new technology stack believe ecosystem use existing technology already mastered want experiment find engineer passionate willing commit technology long term ecosystem technology major factor want engage open source community rather use contribute existing framework reinvent wheel general want spread thin otherwise gain expertise defining clear guideline even checklist help facilitate healthy decisionmaking process narrow possible technological option select one probably best fit team product david dawson suggestion selecting technology data architecture point view need something provide data easily synchronise consistently usable state across network service variety approach actually looking microservice deployment observe various framework technology implementing kind pattern tech kafka spring data flow akka friend sit decided pattern approach mesh resource available decided data flow approach lot reactive programming already java devs make sense pick spring spring cloud data flow kafka probably deploy onto form cloud foundry get need lot heavier data transforms bring spark kafka stream help javascript developer would make sense instead would look adopt functional language j runtime clojurescript etc using similar reactive integration tech kafka certainly making wave space taking key takeaway stress selecting perfect technology take iterative experimental approach instead every microservice architecture unique selected technology aligned system need keep mind many different technology make hiring complicated like post please help u sharing come switching microservice architecture many challenge start transforming system microservices make sure real reason want going advantage disadvantage could big help instead following latest hype need consider unique feature system first change part system hurt starting microservice architecture scratch recommended since clearly defining boundary microservice beginning difficult decide switch microservice architecture take incremental approach take small noncritical part system see work also serf good way get organizational buyin creating microservices one best way select perfect technology microservice every technologyrelated decision influenced team current knowledge also company future hiring plan case selecting technology microservice hiring decision kind developer team want build future narrowing possible technology andras fincza emarsys daniel benzvi similarweb david dawson provided short process easily apply
257,Lobsters,scaling,Scaling and architecture,Analyzing the Performance of Millions of SQL Queries When Each One is a Special Snowflake,https://heap.engineering/analyzing-performance-millions-sql-queries-one-special-snowflake/,analyzing performance million sql query one special snowflake,everything becomes complicated analysis term dynamic schema fully known read time fascinating topic addition complex query typically unique explain analyze engineer would able go small number slow query caused query slow may reproducible fact autoexplain absolute gold mine information analyzing query slow run analysis get sense cause query slow aggregate percentage query ten second spent ten second performing index scan returned row percentage query ten second spent ten second performing ondisk sort percentage query ten second spent ten second reading pageviews index session index also used information come fairly exhaustive list root cause slow query able come explanation slow query slow table partitioning apply mmalisper,making heap fast unique particularly difficult adventure performance engineering customer run hundred thousand query per week one unique product designed rich ad hoc analysis resulting sql unboundedly complex background heap tool analyzing customer interaction gather data web io android growing list thirdparty tool stripe salesforce provide simple ui customer use query data core concept product idea event definition mapping humanlevel event like signup checkout event mean term raw click layer indirection concept analyzed underlying data make product powerful easy use add new event definition run analysis historical data tracking signups since first installed product come cost everything becomes complicated analysis term dynamic schema fully known read time handling indirection fascinating topic execute query take query expressed ui compile sql inline event definition run resulting sql query across cluster postgresql instance process easily result incredibly hairy sql query example consider simple query graphing daily unique user view blog post broken initial referrer analysis simple get yet produce hairy sql query query become complicated make use powerful feature support query include filter number time user done given event arbitrary time range well break result multiple different field simultaneously example might want filter user never seen blog post user later viewed pricing page break query specific blog post viewed addition initial referrer user viewing blog post even getting query type funnel measuring conversion rate series event retention calculating table showing user event x percentage user event week week etc additional part query make sql complicated product designed ad hoc interactive analysis opposed precanned dashboard addition complex query typically unique well furthermore since shard data customer query run customer touch completely disjoint dataset query run customer b make comparing query two customer fool errand since datasets two customer completely different making kind product fast incredibly difficult challenge even supposed determine focusing attention best improve query performance company small engineer go sample slow query rerun explain analyze explain analyze postgres feature show postgres executed query much time spent executing part query well additional execution information engineer would write summary explaining query slow could improved two main issue approach first engineer would able go small number slow query analyzing query one time meant engineer could go dozen query afternoon extremely small sample query run week likely miss issue unless common second caused query slow may reproducible fact happen variety reason query could slow due intermittent high load plan postgres us execute query change due updated table statistic number reason quite often would come across query slow unable come explanation realizing ineffective whackamole performance engineering approach tried design better way really wanted way could collect enough information every query initially ran always determine query slow turn manage find way postgres postgres provides extension called autoexplain enabled autoexplain automatically log query plan execution information every query run exactly like explain analyze automatically record information every query happens depending autoexplain configured obtain following information query executed long took execute part query many row returned part query many page read disk cache part query much time spent performing io part query absolute gold mine information analyzing query slow importantly information gathered query run run issue slow query reproducible since longer need reproduce slowness order determine query slow even better autoexplain option obtain output json make easy programmatically analyze query plan gather information wrote short job tail postgres log watch postgres plan job see query plan sends plan information separate postgres instance information gathered one place easily ask sort question since query plan every single query ever executed heap longer issue trying debug slow query longer slow top run analysis get sense cause query slow aggregate question able answer data percentage query ten second spent ten second performing index scan returned row looking query read lot data disk due event occurred percentage query ten second spent ten second performing ondisk sort ondisk sort occurs postgres process enough memory allocated sort data memory sort data disk significantly slower percentage query ten second spent ten second reading pageviews index session index session pageview event two commonly queried event well two frequent raw event make common cause slow query also used information come fairly exhaustive list root cause slow query picking random slow query figuring slow determine exactly many query slow reason filtered cause slow query analyzed remaining unexplained slow query repeatedly able come explanation slow query slow track rate change time prioritize work accordingly lot cause require multimonth engineering effort resolve critical know upfront much overall impact query performance example knew via anecdote timebased user segment feature commonly cause ondisk sort kind analysis able determine percentage slow query came feature slow due ondisk sort turned le common expected deprioritized work invest making kind query avoid ondisk sort favor another query performance project found autoexplain amazing tool determining cause slow query allows u automatically collect detailed performance information every single query executes use information determine frequent different cause slow query prioritize change working combing data decided make use table partitioning improve query performance partitioning session pageview event dramatically improve performance query require reading large amount session pageviews according data gathered autoexplain directly address slow query sound interesting heap hiring please apply question feel free reach twitter mmalisper row single shard since data heavily sharded row lot data sound
258,Lobsters,scaling,Scaling and architecture,Introduction to Apache Kafka by James Ward,https://www.youtube.com/watch?v=UEg40Te8pnE,introduction apache kafka james ward,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature introduction apache kafka james ward youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature introduction apache kafka james ward youtube
259,Lobsters,scaling,Scaling and architecture,How Netflix works: the (hugely simplified) complex stuff that happens every time you hit Play,https://medium.com/refraction-tech-everything/how-netflix-works-the-hugely-simplified-complex-stuff-that-happens-every-time-you-hit-play-3a40c9be254b,netflix work hugely simplified complex stuff happens every time hit play,reel screen long journey racing buffer time open connect interview,scalescalescalescalecomfrom reel screen long journeywhat good would tvmovie service without course tv showsmovies watch netflix getting film producer customer long arduous process showmovie netflix produce ie netflix original negotiate broadcast right company tasked distributing film tv show mean paying large sum money get legal right broadcast movie tv show customer various region around world often might distribution company even netflix might signed exclusive deal video service tv channel region mean netflix might able provide show customer later date example led house card season premiere middle east horribly delayed june full month later compared country got may even got underwood chief staff explain humorous english video netflix middle east north africa twittercomnetflixmenastore original digital copy show movie aws server original copy usually highquality cinema standard netflix process anybody watch itnetflix work thousand device play different format video sound file another set aws server take original film file convert hundred file meant play entire show film particular type device particular screen size video quality one file work exclusively ipad one full hd android phone one sony tv play video dolby sound one window computer even file made varying video quality easier load poor network connection process known transcoding special piece code also added file lock called digital right management drm technological measure prevents piracy filmsthe netflix app website determines particular device using watch fetch exact file show meant specially play particular device particular video quality based fast internet momentthe last part fetching one crucial netflix internet network delivers video netflix aws server customer device poorly managed ignored mean really slow unusable netflix virtually end company internet umbilical cord connects netflix customer take lot deliver content user want shortest time possible really crowded network million service compete spaceracing buffer timecdn nutshell cdn reviewscdnreviewscom entire gamut operation build netflix ecosystem software content technology rendered useless end user internet connection poor handle video quality basically everything internet work something requires net access request sent internet service provider isp isp forward dedicated server handle website server provide response relayed back computer form result netflix toptier site million hour video content relayed across internet server user much larger network server needed maintain performance building something called content delivery network cdn cdns basically take original website medium content contains copy across hundred server spread world say log budapest instead connecting main netflix server united state load ditto copy cdn server closest budapest greatly reduces latency time taken request response everything load really fast cdns reason website huge number user like google facebook youtube manage load really fast irrespective internet speed likenetflix open connect box supplied internet provider ndtv netflix earlier used variety cdn network operated giant akamai level limelight network deliver content growing user base mean must deliver higher number content location lowering cost led build cdn called open connecthere instead relying aws server install around world one purpose store content smartly deliver user netflix strike deal internet service provider provides red box saw cost isps install along server open connect box download netflix library region main server u multiple rather store content popular netflix user region prioritise speed rarely watched film might take time load stranger thing episode connect netflix closest open connect box deliver content need thus video load faster netflix app tried load main server usthink hard drive around world storing video closer faster get load video lot trickery go behind scene interview explains whenever hit play show netflix locate closest open connect box show loaded netflix appsite try detect one closest work fastest internet connection load video video start blurry suddenly sharpen netflix switching server till connects one give highest quality video
260,Lobsters,scaling,Scaling and architecture,GraphQL at massive scale: GraphQL as the glue in a microservice architecture,https://about.sourcegraph.com/graphql/graphql-at-massive-scale-graphql-as-the-glue-in-a-microservice-architecture/,graphql massive scale graphql glue microservice architecture,challenge centralize data decentralize control gramps middleware step import middleware step import data source step combine data source step use new schema context challenge improve error handling challenge make local development easy http githubcomgrampsgraphqldatasourcebase challenge build global scale gramps graphql apollo microservice pattern server npm package http codelengstorfcompresentationsgraphqlmicroservicesslides,jason lengstorf jlengstorf ibm talk ibm us graphql lingua franca microservices architecture jason senior developer frontend architect ibm cloud webrelated development year obsessed process efficiency graphql one exciting thing worked long time got excited potential graphql quickly received pushback others initially story shepherded adoption graphql ibm point today deployed massive scale dive let take quick detour ibm cloud built key fact node microservice architecture microservice team microservice plugin separate codebase team control workflow dev process great lot way downside thing change way given time frontends need pull data potentially many microservices serve user request internal documentation architecture often inconsistent code convention standard different different microservices fray graphql provides number key point value help bring semblance order change centralized graphql microservice data access happens single endpoint single source truth documentation enforces consistency graphql api boundary enforces clear separation underlying data source presentation layer said graphql also come cost team owns graphql microservice responsible maintaining code standard enforcing convention team continue make independent change service without go central clearinghouse easily become development bottleneck introduce single point failure one bad commit take whole system deal tracing given extra api layer graphql introduces graphql scale ibm production need jason team confronted issue boiled thing central question could centralize data layer let team keep control design approach improves error handling rather maintaining bad status quo make even worse make easy team want switch build service handle ibm scale half dozen data center around world million unique visitor challenge centralize data decentralize control ideal solution issue team maintain graphql schema aggregate schema central microservice accomplish needed standard format sharing schema called data source added model layer connector layer top graphql schema resolvers abstract away graphql syntax model expose typical crud interface wrap common export turn plugin data source data source independent github repository mean team commits deploys code independently bottleneck team owns data source loss control team code individual test suite accidental borking different individual data source combined via gramps middleware library gramps aggregate different data source single service combine via schema stitching serf single source truth data query think get data lawn seems challenging actually implementation easy fit line let walk example converting apollo graphql server use middleware let say server code look like import express fromexpress import bodyparser frombodyparser import graphqlexpress fromapolloserverexpress import myschema fromschema import mydataaccess fromdata const app new express appuse bodyparserjson appuse graphql graphqlexpress schema myschema context mydataaccess step import middleware import express express import bodyparser bodyparser import graphqlexpress apolloserverexpress import grampsexpress grampsgrampsexpress import myschema schema import mydataaccess data const app new express appuse bodyparserjson appuse graphql graphqlexpress schema myschema context mydataaccess step import data source import express express import bodyparser bodyparser import graphqlexpress apolloserverexpress import grampsexpress grampsgrampsexpress import myschema schema import mydataaccess data import schemaone grampsdatasourceone import schematwo grampsdatasourcetwo const app new express appuse bodyparserjson appuse graphql graphqlexpress schema myschema context mydataaccess step combine data source import express express import bodyparser bodyparser import graphqlexpress apolloserverexpress import grampsexpress grampsgrampsexpress import schemaone grampsdatasourceone import schematwo grampsdatasourcetwo const app new express appuse bodyparserjson appuse grampsexpress datasources schemaone schematwo appuse graphql graphqlexpress schema myschema context mydataaccess step use new schema context import express express import bodyparser bodyparser import graphqlexpress apolloserverexpress import grampsexpress grampsgrampsexpress import schemaone grampsdatasourceone import schematwo grampsdatasourcetwo const app new express appuse bodyparserjson appuse grampsexpress datasources schemaone schematwo appuse graphql graphqlexpress req schema reqgrampsschema context reqgrampscontext challenge improve error handling make error helpful clear description went wrong clarity error occurred graphql cause issue root cause underlying data fetcher information help tracing bug unique id shared client server side development print lot useful metadata clientside error message production however ca nt show data example doc link might behind firewall target endpoint might public result prod client error message tend lot le useful however clientside error corresponding serverside error share common guid display guid client log use look corresponding error server log serverside error contain additional context ca nt displayed clientside powerfully mean support ticket directly reference detail log error clear come documentation source given error immediately clear gramps library easy opt nt like use completely different error formatting library import express express import bodyparser bodyparser import graphqlexpress apolloserverexpress import grampsexpress grampsgrampsexpress import schemaone grampsdatasourceone import schematwo grampsdatasourcetwo const app new express appuse bodyparserjson appuse grampsexpress datasources schemaone schematwo appuse graphql graphqlexpress req schema reqgrampsschema context reqgrampscontext formaterror reqgrampsformaterror challenge make local development easy jason needed make right thing easy thing devs nt like change especially change imposed upon jason team make gramps graphql general successful inside ibm needed dead simple team start using created data source starter kit strong starting point new data source stepbystep tutorial building new data source unit test coverage start preconfigured travis ci code climate check http githubcomgrampsgraphqldatasourcebase also built cli local development worked well snag could run local instance graphql microservice data source developing already installed wo nt collide solution add override local data source gramps datasourcedir allows dev add local override data source obviously might mislead developer tried directly deploying still locally dependent service added multiple warning indicate local data source used challenge build global scale actually apollo express server worked scale nt much development deployment graphql microservice extremely quick started development may production july main reason nt ask permission built highly recommends approach far better meeting team realization everyone wrote data source using format dev community could share graphql data source easily share npm package decided release mit license gramps graphql apollo microservice pattern server npm package check check full slide jason talk http codelengstorfcompresentationsgraphqlmicroservicesslides
262,Lobsters,scaling,Scaling and architecture,Scaling the GitLab database,https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/,scaling gitlab database,connection pooling maxconnections database load balancing hot standby sharding citus case sharding connection pooling gitlab omnibus gitlab postgresql high availability database load balancing gitlab makara appear threadsafe octopus sticky connection background processing connection error hot standby conflict release post documentation crunchy data crunchy data database specialist gitlabcominfrastructure second database specialist combining connection pooling database load balancing public grafana dashboard service discovery job opening database specialist handbook entry,long time gitlabcom used single postgresql database server single replica disaster recovery purpose worked reasonably well first year gitlabcom existence time began seeing problem setup article take look help solve problem gitlabcom selfhosted gitlab instance example database constant pressure cpu utilization hovering around percent almost time used available resource best way possible bombarding server many badly optimized query realized needed better setup would allow u balance load make gitlabcom resilient problem may occur primary database server tackling problem using postgresql essentially four technique apply optimize application code query efficient ideally use fewer resource use connection pooler reduce number database connection associated resource necessary balance load across multiple database server shard database optimizing application code something working actively past two year final solution even improve performance traffic also increase may still need apply two technique sake article skip particular subject instead focus technique connection pooling postgresql connection handled starting o process turn need number resource connection thus process resource database use postgresql also enforces maximum number connection defined maxconnections setting hit limit postgresql reject new connection setup illustrated using following diagram client connect directly postgresql thus requiring one connection per client pooling connection multiple clientside connection reuse postgresql connection example without pooling need postgresql connection handle client connection connection pooling may need postgresql connection depending configuration mean connection diagram instead look something like following show example four client connect pgbouncer instead using four postgresql connection need two postgresql two connection pooler commonly used pgpool bit special much connection pooling builtin query caching mechanism balance load across multiple database manage replication hand pgbouncer much simpler connection pooling database load balancing load balancing database level typically done making use postgresql hot standby feature hotstandby postgresql replica allows run readonly sql query contrary regular standby allow sql query executed balance load set one hotstandby server somehow balance readonly query across host sending operation primary scaling setup fairly easy simply add hotstandby server necessary readonly traffic increase another benefit approach resilient database cluster web request use secondary continue operate even primary server experiencing issue though course may still run error request end using primary approach however quite difficult implement example explicit transaction must executed primary since may contain writes furthermore write want continue using primary little change may yet available hotstandby server using asynchronous replication sharding sharding act horizontally partitioning data mean data resides specific server retrieved using shard key example may partition data per project use project id shard key sharding database interesting high write load easy way balancing writes perhaps multimaster setup lot data longer store conventional manner eg simply ca nt fit single disk unfortunately process setting sharded database massive undertaking even using software citus need set infrastructure varies complexity depending whether run use hosted solution also need adjust large portion application support sharding case sharding gitlabcom write load typically low database query readonly query exceptional case may spike tuple writes per second time barely make past tuple writes per second hand easily read million tuples per second given secondary storagewise also nt use much data gb large portion data data migrated background migration done expect database shrink size quite bit amount work required adjust application query use right shard key quite query usually include project id could use shard key also many query nt case sharding would also affect process contributing change gitlab every contributor would make sure shard key present query finally infrastructure necessary make work server set monitoring added engineer trained familiar new setup list go hosted solution may remove need managing server nt solve problem engineer still trained likely expensive bill paid gitlab also highly prefer ship tool need community make use mean going shard database ship least part omnibus package way make sure something ship work running meaning would nt able use hosted solution ultimately decided sharding database felt expensive timeconsuming complex solution problem connection pooling gitlab connection pooling two main requirement work well obviously easy ship omnibus package user also take advantage connection pooler reviewing two solution pgpool pgbouncer done two step perform various technical test work easy configure etc find experience user solution problem ran dealt etc pgpool first solution looked mostly seemed quite attractive based feature offered data test found comment ultimately decided using pgpool based number factor example pgpool support sticky connection problematic performing write trying display result right away imagine creating issue redirected page run http error server used readonly query yet data one way work around would use synchronous replication brings many problem table problem prefer avoid another problem pgpool load balancing logic decoupled application operates parsing sql query sending right server happens outside application little control query run may actually beneficial nt need additional application logic also prevents adjusting routing logic necessary configuring pgpool also proved quite difficult due sheer number configuration option perhaps final nail coffin feedback got pgpool used past feedback received regarding pgpool usually negative though detailed case complaint appeared related earlier version pgpool still made u doubt using right choice feedback combined issue described ultimately led u deciding using pgpool using pgbouncer instead performed similar set test pgbouncer satisfied fairly easy configure nt much need configuring first place relatively easy ship focus connection pooling really well little noticeable overhead perhaps complaint would pgbouncer website little bit hard navigate using pgbouncer able drop number active postgresql connection hundred using transaction pooling opted using transaction pooling since rail database connection persistent setup using session pooling would prevent u able reduce number postgresql connection thus brining benefit using transaction pooling able drop postgresql maxconnections setting reason particular value never really clear pgbouncer configured way even peak capacity need connection giving u room additional connection psql console maintenance task side effect using transaction pooling use prepared statement prepare execute command may end running different connection producing error result fortunately measure increase response timing disabling prepared statement measure reduction roughly gb memory usage database server ensure web request background job connection available set two separate pool one pool connection background processing pool connection web request web request rarely need connection background processing easily spike connection simply due large number background process running gitlabcom today ship pgbouncer part gitlab ee high availability package information refer omnibus gitlab postgresql high availability database load balancing gitlab pgpool load balancing feature picture needed something else spread load across multiple hotstandby server limited rail application library called makara implement load balancing logic includes default implementation activerecord makara however problem dealbreaker u example support sticky connection limited perform write connection stick primary using cookie fixed ttl mean replication lag greater ttl may still end running query host nt data need makara also requires configure quite lot database host role service discovery mechanism current solution yet support either though planned near future makara also appear threadsafe problematic since sidekiq background processing system use multithreaded finally wanted control load balancing logic much possible besides makara also octopus load balancing mechanism built octopus however geared towards database sharding balancing readonly query result consider using octopus ultimately led u building solution directly gitlab ee merge request adding initial implementation found though change improvement fix applied later solution essentially work replacing activerecord baseconnection proxy object handle routing query ensures load balance many query possible even query nt originate directly code proxy object turn determines host query sent based method called removing need parsing sql query sticky connection sticky connection supported storing pointer current postgresql wal position moment write performed pointer stored redis short duration end request user given key action one user wo nt lead user affected next request get pointer compare secondary secondary wal pointer exceeds pointer know sync safely use secondary readonly query one secondary yet sync continue using primary sync write performed second secondary still sync revert using secondary order prevent somebody ending running query primary forever checking secondary caught quite simple implemented gitlab database loadbalancing host caughtup follows def caughtup location string connectionquote location query select pgisinrecovery pgxloglocationdiff pglastxlogreplaylocation string result row connectionselectall query first row row result ensure releaseconnection end code standard rail code run raw query grab result interesting part query follows select pgisinrecovery pgxloglocationdiff pglastxlogreplaylocation walpointer result walpointer wal pointer returned postgresql function pgcurrentxloginsertlocation executed primary code snippet pointer passed argument quotedescaped passed query using function pglastxlogreplaylocation get wal pointer secondary compare primary pointer using pgxloglocationdiff result greater know secondary sync check pgisinrecovery added ensure query wo nt fail secondary checking promoted primary gitlab process yet aware case simply return true since primary always sync background processing background processing code always us primary since work performed background consists writes furthermore ca nt reliably use hotstandby way knowing whether job use primary many job directly tied user connection error deal connection error load balancer use secondary deemed offline plus connection error host including primary result load balancer retrying operation time ensures nt immediately display error page event hiccup database failover also deal hot standby conflict load balancer level ended enabling hotstandbyfeedback secondary solved hotstandby conflict without negative impact table bloat procedure use quite simple secondary retry time delay primary retry operation time using exponential backoff information refer source code gitlab ee database load balancing first introduced gitlab support postgresql information found release post documentation crunchy data parallel working implementing connection pooling load balancing working crunchy data recently database specialist meant lot work plate furthermore knowledge postgresql internals wide range setting limited least time meaning much could hired crunchy help u identifying problem investigating slow query proposing schema optimisation optimising postgresql setting much duration cooperation work performed confidential issue could share private data log file cooperation coming end removed sensitive information issue opened public primary issue gitlabcominfrastructure turn led many separate issue created resolved benefit cooperation immense helped u identify solve many problem something would taken month identify solve fortunately recently managed hire second database specialist hope grow team coming month combining connection pooling database load balancing combining connection pooling database load balancing allowed u drastically reduce number resource necessary run database cluster well spread load across hotstandby server example instead primary near constant cpu utilisation percent today usually hovers percent percent two hotstandby server hover around percent time primary two host secondary loadrelated factor load average disk usage memory usage also drastically improved example instead primary load average around barely go average busiest hour secondary serve around transaction per second roughly per minute primary serf around transaction per second roughly per minute unfortunately nt data transaction rate prior deploying pgbouncer database load balancer uptodate overview postgresql statistic found public grafana dashboard setting set pgbouncer follows setting value defaultpoolsize reservepoolsize reservepooltimeout maxclientconn poolmode transaction serveridletimeout said still work left done implementing service discovery improving check secondary available ignoring secondary far behind primary worth mentioning currently plan turning load balancing solution standalone library use outside gitlab instead focus providing solid load balancing solution gitlab ee gotten interested enjoy working database improving application performance adding databaserelated feature gitlab service discovery definitely check job opening database specialist handbook entry information
264,Lobsters,scaling,Scaling and architecture,Scalable and serverless media processing using BuckleScript/OCaml and AWS Lambda/API Gateway,https://medium.com/@romain.beauxis/scalable-and-serverless-media-processing-using-bucklescript-ocaml-and-aws-lambda-api-gateway-4efe39331f33,scalable serverless medium processing using bucklescriptocaml aws lambdaapi gateway,buckling bloomberg ocaml toolbox inria financial trading facebook uber opam real world ocaml lambda handler type serverless asynchronous monad promise category theory polymorphic variant phantom type stream eventemitter api,buckling downwhen working startup environment time production quite crucial putting together code prove idea feasible releasing minimal viable product demonstration purpose oftentimes developer choose quickest path write code nodejs great tool fastpaced work used lot coincidentally also one language available runtime lambda platformhowever flexible javascript also quite difficult maintain long run inherent lack structure make easy quickly patch together small demonstration project later creates potential hassle updating refactoring part code asking new developer work code never seen beforein order make javascript suitable largescale industrial application bucklescript framework recently emerged originally supported bloomberg bucklescript interface javascript native code ocaml start defining binding javascript ocaml write ocaml code using binding finally bucklescript compiler creates javascript code ocaml code deploy native javascript benefit loop transpiling get use fabulous tool ocaml language provides javascript applicationocaml toolboxdeveloped initially france inria research center ocaml functional language statically inferred type system efficient garbage collector pragmatic approach ml language paradigm similar haskell provides usual tool functional language also amenable imperative primitive whenever appropriate instance io programming recently enjoyed interesting surge various industrial application financial trading facebook recently uber ecosystem development tool greatly improved past year particular addition solid package manager opam allinall even though still niche language constitutes pretty successful transfer academic software industryone important feature ocaml static type system ocaml every variable type determined compiletime code tricky time explicitly write type variable mean code compiles guaranteed function always called argument specific fixed typein javascript world mean longer deal variable null receiving string expected int furthermore refactoring code change function expected type trickle via compiler every call function code compile fixed call function making refactoring safe truly enjoyable experience something important dealing large scale application developer teamsin following showing ocaml feature useful writing code bucklescriptjavascript familiar language might want start taking peek real world ocaml reference book ocaml programminglambda handler typein lambda function essentially callback handler receives three argument json object describing function parameter context object callback executed function finished execution ocamlbucklescript described following type type error exn jsnullablettype callback error unittype contexttype b lambdafunction context b callback unita callback function take two argument error either null exception result universal type return nothing usual callback paradigm asynchronous javascript code error argument null function assumed executed successfully possibly returned value second argumentfinally type lambda function take input parameter universal type context variable callback return value universal type b function return nothing prefer returned value ignorednow anticipating see later turn interfaced aws api gateway serverless returned value api handler javascript object form statuscode int body string case amend type declaration add type apiresponse statuscode int body string jsttype apihandler apiresponse lambdafunctionnow time declare lambda function type apihandler guaranteed function return appropriate type javascript objectthe asynchronous monadanother typical hassle javascriptnodejs world fact one constantly deal callbackbased computation usually look like function asynchronousprocessing callback dosomeasynchronouswork function err result err return callback err domoreasynchronouswork result function err newresult err return callback err etc etc repeat pattern constantly prone making mistake order make le painful apis promise introduced however none really solve problem repeat pattern come notion monad monad quite hip term deep connection category theory really fancy way describe computing abstraction essentially monad describes certain type computation combine together let look example applied case asynchronous computation take callback executes finished either error result type type error exn jsnullablettype callback error unittype callback unit computation return result type val return computation return error val fail exn combine two computation val b b tnow let look implementation monad let return result fun callback callback jsnullablenull resultlet fail error fun callback callback jsnullablereturn error objmagic jsnullablenull let current next fun callback current fun err result match jstooption err exn fail exn callback none next ret callback return function take callback executes null error parameter given result fail function take callback executes given error null result use little trickery objmagic pas resultfinally implement usual pattern combine asynchronous computation given callback current computation next one executes current computation new callback passed nonnull error return immediately executing original callback error else pass result next computation along original callbacknow let look code rewrite using asynchronous monad define asynchronous processing pipeline asynchronouscomputation dosomeasynchronouswork fun result domoreasynchronouswork result fun etc etc execute asynchronouscomputation callbackno repeated pattern potential error polymorphic variant phantom typesfinally let look tricky fancier application ocaml type system using phantom type polymorphic variantsphantom type parametric type used constrain subclass generic type various api restriction purpose powerful tool instance make possible force api user follow predefined flow call calling init followed config finally run going use annotate different event user listen nodejs readable writable streamsin order use polymorphic variant another power feature language quite handy sometimes requires bit work understand use properly polymorphic variant collection different label sub surclassed let look code fix idea might want also check official nodejs stream api heretype ttype writeevents close unit unit drain unit unit finish unit unit error exn unit pipe readable unit unpipe readable unit readevents close unit unit end unit unit readable unit unit data string unit error exn unit writable writeevents readable readevents ttype event writeevents readevents val event unitin api define list event readable writable stream receive event associated handler process type data received event occursthen subclass generic stream type annotating event associated type stream readable writable finally function typed accept event annotated stream type making sure instance readable stream used listen data event otherwise compiler complain message streamon writable data fun error expression type data string unit expression expected type streamwriteevents second variant type allow tag datanow let peek hood see implemented external string b unit unit bssend let stream function close fn stream close fn drain fn stream drain fn finish fn stream finish fn error fn stream error fn pipe fn stream pipe fn unpipe fn stream unpipe fn end fn stream end fn readable fn stream readable fn data fn stream data fnpretty simple first declare external method stream object receive type stream stringtyped event name callback receiving value yet unknown type lowlevel binding nodejs eventemitter apithen decorate function redefining function receives polymorphic event type handler explicitly unwraps expected parameter nodejs side voila finish noticing explicit type restriction function defined type interface without nothing tell compiler accept event handler part stream type annotation
265,Lobsters,scaling,Scaling and architecture,Efficient pagination of a table with 100M records,http://allyouneedisbackend.com/blog/2017/09/24/the-sql-i-love-part-1-scanning-large-table/,efficient pagination table record,rdbms nosql golden hammer sql love keyset pagination background scanning large table problem obvious wrong solution solution solution final one today wow significantly faster previous approach time explaining performance join type possible key key row filtered extra join type official documentation rdbms summary method scanning large table remember purpose using keyset pagination like solution,huge fan database even wanted make dbms university work rdbms nosql solution enthusiastic know golden hammer problem solution alternatively subset solution series blog post sql love walk thru problem solved sql found particularly interesting solution tested using table million record example use mysql idea apply relational data store like postgresql oracle sql server chapter focused efficient scanning large table using pagination offset primary key also known keyset pagination background chapter use following database structure example canonical example user fit domain create table user userid int unsigned null autoincrement externalid varchar null name varchar collate null metadata text collate datecreated timestamp null default currenttimestamp primary key userid unique key ufuniqexternalid externalid unique key ufuniqname name key datecreated datecreated engineinnodb default comment structure externalid column store reference user system uuid format name represents firstname lastname metadata column contains json blob kind unstructured data table relatively large contains around record let start learning journey scanning large table problem need walk thru table extract record transform inside application code insert another place focus first stage post scanning table obvious wrong solution select userid externalid name metadata datecreated user case record query never finished dbms kill probably led attempt load whole table ram returning data client another assumption took much time preload data sending query timed anyway attempt get record time failed need find solution solution try get data page since record guaranteed ordered table physical logical level need sort dbms side order clause select userid externalid name metadata datecreated user order userid asc limit row set sec sweet worked asked first page record took sec return however would work page select userid externalid name metadata datecreated user order userid asc limit page page size row set sec indeed slow let see much time needed get data latest page select userid externalid name metadata datecreated user order userid asc limit page page size row set min sec insane however ok solution run background one hidden problem approach revealed try delete record table middle scanning say finished page record already visited going scan record record deleted next select execution case following query return unexpected result select userid externalid name metadata datecreated user order userid asc limit n id see query skipped record id processed application code approach two delete operation appear first record therefore method unreliable dataset mutable solution final one today approach similar previous one still us paging instead relying number scanned record use userid latest visited record offset simplified algorithm get pagesize number record table starting offset value use max returned value userid batch offset next page get next batch record userid value higher current offset query action page page contains data user select userid externalid name metadata datecreated user userid value userid record order userid asc limit row set sec wow significantly faster previous approach time note value userid sequential gap like right solution also work record future page deleted even case query skip record sweet right explaining performance learning recommend investigating result explain extended version query get next record solution time type key row filtered extra obvious never null null paging using number record offset sec index null primary null keyset pagination using userid offset sec range primary primary using let focus key difference execution plan solution since one practically useful large table join type index v range first one mean whole index tree scanned find record range type tell u index used find matching row within specified range range type faster index possible key null v primary column show key used mysql btw looking key column see eventually primary key used query row v value display number record analyzed returning result query value depends deep scroll example try get next record page record examined opposite query constant value matter load data page last one always half size table filtered v column indicates estimated percentage table filtered processing higher value better value mean query look thru whole table query value constant depends page number ask page value filtered column would last page would extra null v using provides additional information mysql resolve query usage primary key make query execution faster suspect join type parameter query made largest contribution performance make query faster another important thing query extremely dependent number page scroll deep pagination slower case guidance understaing output explain command found official documentation rdbms summary main topic blog post related scanning large table record using offset primary key keyset pagination overall different approach reviewed tested corresponding dataset recommend one need scan mutable large table also revised usage explain extended command analyze execution plan mysql query sure rdbms analog functionality next chapter pay attention data aggregation storage optimization stay tuned method scanning large table remember purpose using keyset pagination like solution
266,Lobsters,scaling,Scaling and architecture,No Bitcoin-based protocol can handle more than 20M users per month,https://runeksvendsen.github.io/blog/posts/2017-10-08-no-bitcoin-based-protocol-can-handle-more-than-20m-users-per-month.html,bitcoinbased protocol handle user per month,bitcoinbased protocol handle user per month,bitcoinbased protocol handle user per month posted october rune k svendsen solve bitcoin scalability challenge many socalled layer protocol proposed protocol operate relatively simple principle user deposit bitcoins via bitcoin blockchain layer system stuff happens within layer system without touching bitcoin blockchain assigns arbitrarily small part deposited bitcoins user recipient system recipient withdraw received bitcoins bitcoin blockchain increase scalability transferring bitcoins user user since step nothing touch blockchain fact since nothing touch blockchain transaction speed measured transaction per second theoretically unlimited practice limited latency bandwidth node problem however step user receives money human usually receive monthly wagessalaries need deposit layer system order available within current block size limit mb maximum number deposit transaction per month assuming simplest bitcoin transaction singlesignature size byte used maximum since singlesignature bitcoin transaction interesting context layer protocol essentially constitutes sending bitcoins trusted third party thus complex eg multisignature deposit transaction number monthly user le million furthermore figure include withdrawal transaction recipient withdraw fund layer system private bitcoin address decrease maximum number supported user even conclusion bitcoin scale world population layer protocol operating principle described sufficient bitcoin blockchain need increase capacity conjunction layer protocol sufficient capacity support million user per month byte per byte per transaction txblock txblock block per hour hour per day txday txday day per month million txmonth
268,Lobsters,scaling,Scaling and architecture,@scale 2017 videos,https://atscaleconference.com/videos-articles/,scale video,cooky policy,help personalize content tailor measure ad provide safer experience use cooky clicking navigating site agree allow collection information facebook cooky learn including available control cooky policy agree
269,Lobsters,scaling,Scaling and architecture,The DevOps Trap,https://medium.com/initialcommit/the-devops-trap-97cadba60620,devops trap,devops hype cycle move fast break thing balance power itil yes devops terraform puppet docker treat server cattle pet resume enhancement engineering principle future cultural difference neglected kubernetes manager sitting room enforcing process moving fast breaking thing,devops hype cycle bad usersi never server engineer run team server engineer always development side fence worked alongside server engineer liaised infrastructure manager many yearsthere key difference two discipline dev team introducing new feature fast possible move fast break thing whereas infrastructure team structure process designed ensure thing keep running smoothly risk mitigated rightly soit may seem expedient let developer loose server wrote system best people deploy look view like letting police lock people without need pesky trial defence lawyer thing go wrong little unfair blame police left charge whole process police developer culture training teach behave precisely wrong way outcome wanti great believer balance power team dev team push change infrastructure keep u checkin experience getting code production often involves submitting change request part approval process proposed change including explanation change neededtechnical implementation instruction followeda risk assessment change thing could go wrong would mean businessinstructions test whether change succeededinstructions rollback change succeededcommunication plan regarding change telling howa board committee sits consider change agree schedule applied objection raised change approach discussed agreed based itil something industry standard thing yes devops devops relatively new set practice automates process software development infrastructure team tool like terraform puppet docker created give engineer way automatically deploy server code enables devops engineer treat server cattle pet two big glaring problem change first developer think career make prone marketing fad choosing technology resume enhancement rather good old fashioned engineering principle devops future cry evangelist dutifully developer land start trying persuade management factbut really need treat server cattle pet suburban garden chicken need mile barbed wire tractor industrial cattle feederyour todo list app need microservices architecture line devops code would built hand user sooner created single rail app manually deployed herokusecond developer sometimes think order work devops learn tooling cultural difference neglectedjust learnt build autoscaling kubernetes cluster serve todo list app mean old lesson running production environment irrelevantintroducing change production still mean risk risk need controlled managed way still manager sitting room enforcing process without going spend next year relearning process lesson traditional infrastructure team learnt last year user suffer unreliable insecure appsdone well devops indispensable approach managing large number server snapchat buzzfeed skyscanner absolutely need devops well mean bringing old lesson running infrastructure team finding way keep developer culture moving fast breaking thing check
271,Lobsters,scaling,Scaling and architecture,Scaling Event Sourcing for Netflix Downloads,https://medium.com/netflix-techblog/scaling-event-sourcing-for-netflix-downloads-episode-2-ce1b54d46eec,scaling event sourcing netflix downloads,scaling event sourcing netflix downloads episode first episode flexible dilemma document oriented nosql database enter event sourcing martin fowler pattern command event aggregate rest service aggregate service command handler event handler repository service event store netflix downloads use case netflix aggregate license acquisition use case acquire license endpoint license service license service license aggregate create license command license command handler license command handler create license command license aggregate license created event license command handler license aggregate license created event event handler license aggregate event repository license created event event store license repository license aggregate license service license renewal use case license aggregate license command handler renew license command license command handler license event handler license renewed event license aggregate license aggregate event handler download limit rejection use case end beyond please velocity new york,scaling event sourcing netflix downloads episode karen casella phillipa avery robert reta joseph breuerin first episode series post introduced netflix downloads project use case led u consider solution based event sourcing pattern post provide overview general event sourcing pattern applied key use casesa flexible dilemmawhen first started design downloads licensing service content licensing restriction yet defined knew coming needed able adapt start design implement service requirement yet decided service going live matter month million member single day global press release make system flexible change easy right anyone familiar relational database know phrase flexible easy change overly true regard underlying table schema way change schema easily accessible require deep knowledge sql direct interaction database addition data mutated lose valuable context cause change previous state wasdocument oriented nosql database known providing flexibility change quickly moved direction mean provide flexible scalable solution document model provides u flexibility need data model provide u traceability determine caused data mutation given longevity data wanted ability look back time debug change stateenter event sourcingevent sourcing architectural pattern resurfacing lately valuable component modern distributed microservices ecosystem martin fowler describes basic pattern follows fundamental idea event sourcing ensuring every change state application captured event object event object stored sequence applied lifetime application state many excellent overview event sourcing pattern two favorite include short event sourcing architectural pattern maintains complete transaction history data model instead persisting data model persist event lead change data event played order building aggregate view complete data domain ability replay event point time also excellent debugging tool enables u easily explain member account particular state allows u easily test system variationsthe patternthe following diagram provides highlevel view applied event sourcing pattern netflix system responsible enforcing downloads business rule generic explanation component followingthe event sourcing pattern depends upon three different service layer command event aggregatesa command represents client request change state aggregate command used command handler determine create list event needed satisfy commandan event immutable representation change state aggregate ie action taken change state event always represented past tensean aggregate aggregated representation current state domain model aggregate take stream event determines represent aggregated data requested business logic purposeas shown number actor involved implementing patternthe rest service application layer accepts request client pass aggregate servicethe aggregate service handle client request aggregate service query existing aggregate one exist create empty aggregate aggregate service generates command associated request pass command along aggregate command handlerthe command handler take aggregate command ass based state transition validity check whether command applied aggregate current state state transition valid command handler creates event pass event aggregate event handlerthe event handler applies event aggregate resulting new aggregate state pass list event repository servicethe repository service manages state applying newly created event aggregate event saved event store resulting new state aggregate available systemthe event store abstracted interaction event readwrite functionality backing databasenetflix downloads use casewhen member selects title download license lifecycle begin netflix client application first request license license acquired netflix client downloads content member play newly downloaded content dependent member action state license change throughout lifecycle member may start pause resume stop viewing content member may remove downloaded content action potentially result state change license license created potentially renewed several time finally released deleted either explicitly member implicitly based business rulesthere large amount business logic involved lifecycle maintaining license state job eventsourcing based license accounting service track complete transaction history license member downloaded content device data model allows event played back order building aggregate view complete data objectsnetflix aggregatesthe netflix client application make several different type request translated command event aggregate support business requirement license enforcement three interrelated aggregate license downloaded title device service handler repository following description concept introduced apply key use case downloads featurelicense acquisition use casefollowing simple use case member obtaining license first time particular piece contenton initial license request client sends request acquire license endpoint member identity along title content requested download license service license service determines requested action allowed querying existing aggregate data applying appropriate business rule since first request member content assuming device studio business rule satisfied license service creates new empty license aggregate create license command pas license command handler license command handler applies create license command license aggregate generates license created event license command handler pass empty license aggregate license created event event handler creates new license aggregate event repository persists license created event event store finally license repository return new license aggregate license service package aggregate information response back clientlicense renewal use caseprior expiration license device may request extension existing license known license renewal renewing license similar original license acquisition flow major difference current license aggregate passed license command handler along renew license command license command handler generates appropriate event license event handler applies license renewed event license aggregate shown note new license aggregate expiration date day beyond current date day renewal represents business rule license renewal currently force change would make simple configuration change event handlerdownload limit rejection use caseeach time device request new renewed license license service downloaded service retrieves current aggregate member evaluates business rule validation example one validation requires content downloaded twice year device make license request license service check content previously downloaded year possible derive information retrieving license aggregate year filtering content id lot processing decided instead denormalize data create separate aggregate content download indexed member content id requires new download event service aggregate repositorywhen receive subsequent event content check previous downloads content per sequence diagram download service see member exceeded downloads content reject requestthe end beyondfor use case found event sourcing useful pattern implementing flexible robust system sunshine rose however definitely made mistake area improve upon subsequent post go detail overall flexibility architecture given u mean rapidly innovate react changing requirement debug issue changing data state time get brand new service serving million user around world relatively short timemore please next post series provide deep dive implementation detail including use data versioning snapshotting provide flexibility scale following share experience implementing event sourcing lesson learned mistake made around testing scalability optimization introduce thought future improvement extension planning going forwardthe team recently presented topic qcon new york download slide watch video join u velocity new york october even technical deep dive implementation lesson learnedthe author member netflix playback access team pride expert distributed system development operation hiring senior software engineer email kcasella netflixcom connect linkedin interested
272,Lobsters,scaling,Scaling and architecture,The Tale of Creating a Distributed Web Crawler,https://benbernardblog.com/the-tale-of-creating-a-distributed-web-crawler/,tale creating distributed web crawler,might early requirement initial design crawler dispatcher crawler supervisor database server crawl rate initial implementation perfect cloud hosting aws digitalocean http library request etl pipeline etl pipeline gravatar xpath jsonpath messaging rabbitmq zeromq fifodiskqueue roundrobin algorithm storage mongodb logging monitoring logging rotatingfilehandler mongochef managing already crawled url sqlite url filtering alexa million top site list security iptables ssl authentication memory politeness article wrong updated requirement useragent crawldelay distributed updated design crawldelay updated implementation handling robotstxt reppy urllibrobotparser google implementation robotstxt caching last crawl date dealing bug problem memory management bottleneck crawl depth dynamically generated content google chrome chrome driver python binding selenium xvfb edge case regular crawler web crawler performance number crawl rate future improvement messaging rabbitmq redis zeromq monitoringlogging new relic loggly design prefix matching final thought reddit python weekly pycoders weekly programming digest,around million record field dataset wanted analyze data analysis project mine big problem record many missing field lot field either inconsistently formatted outdated word dataset pretty dirty hope amateur data scientist least concerning missing outdated field record contained least one hyperlink external website might find information needed looked like perfect use case web crawler post find built scaled distributed web crawler especially dealt technical challenge ensued early requirement idea building web crawler exciting know crawler cool right quickly realized requirement much complex thought given seed url crawler needed autodiscover value missing field particular record web page nt contain information looking crawler needed follow outbound link information found needed kind crawlerscraper hybrid simultaneously follow outbound link extract specific information web page whole thing needed distributed potentially hundred million url visit scraped data needed stored somewhere likely database crawler needed work running laptop nt option nt want cost much cloud needed coded python language choice okay built worked many crawler previous job nothing scale needed entirely new territory initial design design initially came follows main component crawler dispatcher responsible dispatching url crawled crawler supervisor collecting result field crawler supervisor responsible supervising n child process child process would perform actual crawling refer crawler convenience database server responsible storing initial seed url well extracted field would end n crawler total thus distributing load across many node example supervisor process would equal crawler additionally interprocess communication across server would happen thanks queue theory would easily scalable could add supervisor crawl rate performance metric would increase accordingly initial implementation promising design needed choose technology would use nt get wrong goal nt come perfect technology stack instead saw mainly learning opportunity challenge willing come homebaked solution needed cloud hosting could chosen aws familiar digitalocean happens cheaper used several month vms droplet http library request library nobrainer performing http request python etl pipeline sure needed extract hyperlink every visited web page also needed scrape specific data page built etl pipeline able extract data transform format needed data analysis customizable via configuration file follows name gravatar urlpatterns type regex pattern http wwww gravatarcom avatarsupportsiteconnect w urlparsers description url find online section processor type xpath parameter expression contains text find online followingsibling ul classlistdetails href description url website section processor type xpath parameter expression ul classlistsites href field name name processor type xpath parameter expression div classprofiledescription classfn atext type trim parameter name location processor type xpath parameter expression div classprofiledescription p classlocation text type trim parameter see mapping gravatar user profile page tell crawler data extracted page urlpatterns defines pattern tentatively matched current page url match current page indeed gravatar user profile urlparsers defines parser capable extracting specific url page like pointing user personal website social medium profile field defines data want retrieve page gravatar user profile wanted extract user full name location urlparsers field contain series processor executed web page html data perform transformation xpath jsonpath find replace etc get exact data need desired format data thus normalized stored somewhere else especially useful website different represent data differently manually creating mapping took lot time list relevant website long hundred messaging initially wondered whether rabbitmq would good fit reasoned nt want separate server manage queue wanted everything lightningfast selfcontained went zeromq pushpull queue coupled queuelib fifodiskqueue persist data disk case crash occurred additionally using pushpull queue assured url would dispatched supervisor using roundrobin algorithm figuring zeromq work understanding several edge case took learning implement messaging really fun worth end especially performancewise storage good ol relational database could done job needed store objectlike result field went mongodb bonus point mongodb rather easy use administer logging monitoring used python logging module coupled rotatingfilehandler generate one log file per process especially useful manage log file various crawler process managed supervisor also helpful debugging monitor various node nt use fancy tool framework connected mongodb server every hour using mongochef checked average number resolved record going according calculation thing slowed likely meant something bad going like process crash something else yes blood sweat tear managing already crawled url web crawler likely come upon url generally nt want recrawl probably nt changed avoid problem used local sqlite database crawler dispatcher store every crawled url along timestamp corresponding crawl date time new url coming dispatcher searched sqlite database url see whether already crawled crawled otherwise ignored chose sqlite lightning fast easy use timestamp accompanying every crawled url useful debugging reference purpose case someone would filed complaint crawler url filtering goal nt crawl whole web instead wanted autodiscover url interested filter useless url interested whitelisted thanks etl configuration introduced previously filter url nt want used top site alexa million top site list concept quite simple site appearing among top site high probability useless like youtubecom amazoncom however based analysis site beyond top site much higher chance relevant analysis like personal website blog security nt want anyone tamper digitalocean vms blocked port every vm default using iptables also selectively unblocked port absolutely needed etc enabled ssl authentication mongodb user proper certificate could log used encrypted disk vms enabled every vm block repeatedly failed login request configured ssh keybased authentication vms enabled ssl authentication zeromq okay maybe went bit overboard security purpose excellent learning opportunity also effective way secure data memory month digitalocean vm come memory fairly limited many testing run determined node memory created swap file every vm politeness impressed fast able come working implementation initial design thing going smoothly early testing run showed impressive performance number crawl rate crawler pretty excited say least came upon article jim mischel completely changed mind turned crawler nt polite crawling web page nonstop restriction whatsoever sure fast crawl rate could banned webmaster reason politeness imply web crawler must identify via proper user agent string must respect rule robotstxt must send consecutive request website rapidly fairly simple implement right wrong quickly realized distributed nature crawler complicated thing quite bit updated requirement addition requirement already implemented also needed create page described crawler pas useragent header every http request made crawler include link explanation page created download robotstxt periodically every domain check whether url allowed crawled based inclusionexclusion rule crawldelay directive case absent subsequent request domain needed spaced conservative number second eg second ensure crawler nt cause unexpected load website bullet bit problematic though fact could distributed web crawler keep single uptodate robotstxt cache share worker crawler avoid downloading robotstxt frequently domain track last time every domain crawled order respect crawldelay directive implied serious change crawler updated design updated design came main difference previous design robotstxt would downloaded every domain robotstxt file would cached database every hour file would separately invalidated redownloaded per domain needed ensure crawler would respect change made robotstxt file last crawl date would cached database every domain well would used reference respect crawldelay directive contained robotstxt point feared change would slow crawler almost certainly would actually nt choice else crawler would overburden website updated implementation everything chosen far stayed except key difference handling robotstxt chose reppy library urllibrobotparser support crawldelay directive automatically handle download expired robotstxt file support directory inclusion rule ie allow directive per google implementation robotstxt rule pretty common robotstxt file found web nobrainer caching robotstxt last crawl date added second mongodb server specifically dedicated caching thing server created distinct database avoid potential databaselevel lock database held collection containing last crawl date every domain database held collection containing copy robotstxt every domain additionally modify reppy library little bit cached robotstxt file mongodb instead memory dealing bug problem spent awful lot time debugging profiling optimizing crawler development much time expected actually beyond memory various bug encountered broad range unexpected issue memory management memory nt infinite resource especially month digitalocean vm fact limit many python object held memory example dispatcher pushing url supervisor extremely fast much faster latter could ever crawl time supervisor typically crawler process disposal process needed constantly fed new url crawl set threshold many url could dequeued held memory supervisor allowed strike balance memory usage performance bottleneck quickly realized could nt let web crawler loose else would crawled whole web nt goal limited crawl depth meaning seed url immediate child url would crawled allowed crawler autodiscover web page specifically looking dynamically generated content found lot website dynamically generated using javascript mean download arbitrary web page using crawler might full content unless capable interpreting executing script generate page content need javascript engine probably many way tackle problem settled simple solution chose dedicate supervisor crawling dynamically generated web page installed google chrome chrome driver installed python binding selenium installed xvfb simulate presence monitor chrome gui centos none default node capable crawling dynamically generated web page edge case already knew building regular crawler meant dealing sort weird api edge case web crawler well picture web api certainly huge crazy vastly inconsistent one page nt built way page often contain invalid character ie incompatible encoding page server often return kind http error etc including custom one anyone server often unreachable cause timeouts domainwebsite might exist anymore might dns problem might heavy load server might incorrectly configured get idea web page huge like ten megabyte mean download one shot load memory might well run memory server sometimes return incorrectly formed html markup non html content like json xml others know web page often contain invalid incorrectly formed url url nt want crawl like large binary file eg pdfs video etc subset many problem web crawler need deal performance number web crawler normally interested crawl rate crawler number web page downloaded per second example supervisor using process estimated crawler pagessecond much interested many record original dataset properly resolved per hour mentioned previously original purpose crawler fill gap dataset either fetching missing field refreshing outdated one configuration able resolve around record per hour disappointing number sure still good enough considering web page useless filtered future improvement thing differently start messaging probably choose rabbitmq redis zeromq mainly convenience ease use even slower monitoringlogging probably use tool like new relic loggly monitor resource vms centralize log generated node design probably decentralize robotstxt last crawl date cache improve overall crawl rate mean replacing mongodb server cache stored locally supervisor every crawler process possible architecture summarize supervisor node crawler process would robotstxt last crawl date cache would replace centralized cache mongodb server due dispatcher would need dispatch url specific supervisor node upon receiving new url crawl supervisor node would need dispatch specific crawler process otherwise multiple process across different supervisor might crawl exact website time crawler might banned would nt honoring rule robotstxt fortunately zeromq support prefix matching could route url specific supervisor node based domain name already coded persistent cache mainly based sqlite could definitely reuse prevent individual cache using much memory final thought post seen built distributed web crawler fill gap dirty dataset initially nt expecting project become huge complex probably happens software project really paid end learned tremendous amount thing distributed architecture scaling politeness security debugging tool multiprocessing python robotstxt etc one question left nt answer post dataset would justify work exactly reason behind find future blog post p feel free leave question comment comment section update post featured reddit also featured python weekly pycoders weekly programming digest newsletter get chance subscribe wo nt disappointed thanks everyone support great feedback
273,Lobsters,scaling,Scaling and architecture,Use cases for persistent logs with NATS Streaming,https://dev.to/byronruth/use-cases-for-persistent-logs-with-nats-streaming,use case persistent log nats streaming,persistent log nats streaming nats streaming nats apache kafka us case publishsubscribe pattern setup nats streaming download release nats streaming github official docker image called go client downloads page boilerplate code want receive message stream want pick left case disconnect data replication stream cqrs architecture want processing want read entire history stream approach exactly many database work want share work processing message epilogue always use currently done implicit acks start message exactly example simulation example,persistent log context log ordered sequence message append go back change existing message persistent bit simply mean remembered potentially durable disk beyond server restarts nats streaming nats streaming lightweight streaming platform built top nats provides api persistent log feature include lightweight written go single binary zero runtime dependency ordered logbased persistence atleastonce delivery model automatic subscriber offset tracking support replay message stream property similar apache kafka offer term ordered logbased persistence stream certainly difference two system wo nt discussing opinion best feature nats streaming nats simplicity operating client api want learn leave comment us case going assume basic knowledge publishsubscribe pattern core api provided nats streaming even nt trouble following along list use case solved using specific variant publishsubscribe pattern show solved using subscription api nats streaming provides discus semantics guarantee provided vantage point subscriber want receive message stream want pick left case disconnect new want read entire history stream want exactly processing want share work processing message setup nats streaming want code along try pattern download release nats streaming github also official docker image called natsstreaming assuming downloaded single binary unpack run following option natsstreamingserver store file dir data maxmsgs maxbytes default nats streaming us inmemory store store option used change filebased survive restarts maxmsgs maxbytes set zero make message retained channel otherwise server default million message mb size case channel pruned message go whichever limit reached thus deleting history running shell starting writing code code example using go client several official client communitybuilt one downloads page boilerplate code first need establish connection package main import log stan githubcomnatsiogonatsstreaming convenience function log error deferred close func logcloser c iocloser err cclose err nil logprintf close error err func main specify cluster one node client id connection conn err stanconnect testcluster testclient err nil logprint err return defer logcloser conn pattern want receive message stream use case solved using subscription default configuration handle func msg stanmsg print message data string stdout fmtprintf string msgdata sub err connsubscribe streamname handle err nil logprint err return defer logcloser sub really simple nats streaming guarantee message received processed order one caveat addressed later example issue acknowledging acking message processed server disconnect timeout message redelivered later earlier message processed likewise error processing default way send ack solved adding subscription option stansetmanualackmode handle func msg stanmsg msg handled successfully manually send ack server importantly processing fails choose send ack receive message later fail connection server gone awry err msgack err nil logprinf failed ack msg msgsequence connsubscribe streamname handle stansetmanualackmode may thinking would want get message redelivered failed first time depends kind failure temporary failure second third time may work bug code never succeed want pick left case disconnect default option subscription tracked online client resubscribes later receives new message wo nt receive message published offline certain use case may desirable pick left work queue data replication stream cqrs architecture making subscriber resumable simple adding another subscription option handle func msg stanmsg connsubscribe streamname handle standurablename iwillremember standurablename option take name provide particular subscription bound stream reuse durable name different stream offset stream tracked separately end previous section asked happens bug handler code durable subscription freedom bring subscriber offline fix bug bring back online resuming left know whether handler failing logging error also immediately disconnect first nonretryable error occurs declare handler reference var sub stansubscription handle func msg stanmsg err process msg close subscription error err nil logcloser sub sub connsubscribe streamname handle standurablename iwillremember since message processed ordered closing subscription first error prevent subsequent message processed reconnect message failed redelivered followed new message approach also guarantee fully ordered processing matter message beyond failure wo nt processed thus redelivery ca nt interleaved new message guaranteed ordering also achieved using maxinflight option along manual acking handle func msg stanmsg err process msg err nil msgack connsubscribe streamname handle standurablename iwillremember stanmaxinflight stansetmanualackmode even without closing subscription guarantee message processed retried order since one message flight time past example restriction thus multiple message would queued processed want exactly processing small oversight two example happens processing succeeds ack fails nats streaming atleastonce delivery model mean continue redeliver message server nt get acknowledgment satisfy case client take responsibility retaining last message processed successfully var lastprocessed handle func msg stanmsg process message greater last one processed seen skip acknowledge server msgsequence lastprocessed err process msg err nil log error andor close subscription return processing successful set lastprocessed value lastprocessed msgsequence ack server msgack connsubscribe streamname handle standurablename iwillremember stanmaxinflight stansetmanualackmode server maintains last message id acknowledged client ensure exactly processing client also maintain view world true restarts client would need persist lastprocessed value someone load start could simple local file contains id message last processed want read entire history stream use case applicable consumer want build internal state based stream fact approach exactly many database work maintaining internal index support query change written log first durability internal process applies change inmemory index support fast lookup unless building oneoff index general want use durable subscription restart small set change need processed starting beginning another option handle func msg stanmsg connsubscribe streamname handle standurablename iwillremember standeliverallavailable nice pattern use want deploy new version internal state requires processing old message discovered bug applying feature etc done offline take long need built deployed alongside old version traffic routed new version want share work processing message far use case needed single subscriber work since ordering implied important case maybe exception first however ordering important message processing done parallel maybe reconciled later take advantage queue subscriber queue subscriber enables multiple client subscribe stream queue name message distributed member queue group handle func msg stanmsg connqueuesubscribe streamname queuename handle option durable manual ack etc option mentioned still apply including durability add durablename option durable queue subscription epilogue always use setmanualackmode provides control acking even though add couple extra line handle function nothing else ack failure logged currently done implicit acks start message assuming everything need durable important think type message handled specifically time sensitive way either subscriber need durable handle function need aware skip expired message likewise think total ordering actually required basically message stream dependent processed result previous one total ordering required maxinflight used subscription autoclosed error exactly queuesubscribe example exactly work single subscriber queue subscription lastprocessed id would need centrally atomically accessible member queue subscription desirable simplest approach would use shared keyvalue store support atomic operation setting value example simulation put together example highlight scenario discussed runnable example provided well output readme illustrate behavior
274,Lobsters,scaling,Scaling and architecture,Rapid release at massive scale (Facebook),https://code.facebook.com/posts/270314900139291/rapid-release-at-massive-scale/,rapid release massive scale facebook,rate code delivery scaled facebookcom continuous delivery scale eliminates need hotfixes allows better support global engineering team provides forcing function develop next generation tool automation process necessary allow company scale make user experience better faster bringing continuous delivery mobile nuclide buck phabricator react native infer android beta tester data show,time software industry come several way deliver code faster safer better quality many effort center idea continuous integration continuous delivery agile development devops testdriven development methodology one common goal enable developer get code quickly correctly people use safe small incremental step development deployment process facebook grown organically encompass many part rapid iteration technique without rigidly adhering one particular flexible pragmatic approach allowed u release web mobile product successfully rapid schedule many year pushed facebook front end three time day using simple master release branch strategy engineer would request cherrypicks change code passed series automated test pull master branch one daily push release branch general saw cherrypicks per day week cut new release branch picked change cherrypicked week system scaled well starting handful engineer thousand today good news added engineer got done rate code delivery scaled size team took certain amount human effort form release engineer addition tool automated system place drive daily weekly push door understood batching larger larger chunk code delivery would continue scale team kept growing saw branchcherrypick model reaching limit ingesting diffs day master branch weekly push sometimes many diffs amount manual effort needed coordinate deliver large release every week sustainable decided move facebookcom quasicontinuous push master system april next year gradually rolled first percent employee percent percent percent production web traffic progression allowed u test ability tool process handle increased push frequency get realworld signal main goal make sure new system made people experience better least make worse almost exactly year planning development course three day april enabled percent production web server run code deployed directly master continuous delivery scale true continuous push system would deliver every individual change production soon landed code velocity facebook required u develop system push ten hundred diffs every hour change get made quasicontinuous delivery mode generally small incremental visible effect actual user experience release rolled percent production tiered fashion hour stop push find problem first diffs passed series automated internal test land master pushed facebook employee stage get pushblocking alert introduced regression emergency stop button let u keep release going everything ok push change percent production collect signal monitor alert especially edge case testing employee dogfooding may picked finally roll percent production flytrap tool aggregate user report alert u anomaly many change initially kept behind gatekeeper system allows u roll mobile web code release independently new feature helping lower risk particular update causing problem find problem simply switch gatekeeper rather revert back previous version fix forward quasicontinuous release cycle come several advantage eliminates need hotfixes threepushaday system critical change get one scheduled push time someone call hotfix outofband push disruptive usually needed human action could bump next scheduled push new system vast majority thing would required hotfix simply committed master pushed next release allows better support global engineering team tried schedule three daily push accommodate engineering office around world even effort weekly push required engineer pay attention specific date time always convenient time zone new quasicontinuous system mean engineer everywhere world develop deliver code make sense provides forcing function develop next generation tool automation process necessary allow company scale take project like work pressure test across many team system made improvement push tool diff review tool testing infrastructure capacity management system traffic routing system many area team came together wanted see main project faster push cycle succeed improvement made help ensure company ready future growth make user experience better faster take day week see code behave engineer may already moved something new continuous delivery engineer wait week longer get feedback change made learn quickly work deliver small enhancement soon ready instead waiting next big release infrastructure perspective new system put u much better position react rare event might impact people ultimately brings engineer closer user improves product development product reliability bringing continuous delivery mobile evolving quasicontinuous system web possible part entire stack could build improve tool needed make reality shipping mobile platform present challenge many current development deployment tool available mobile make rapid iteration difficult facebook worked make better building opensourcing wide set tool focus specifically rapid mobile development including nuclide buck phabricator variety io library react native infer together build test stack give u ability produce quality code ready rapid deployment mobile platform continuous integration stack broken three layer build static analysis testing whenever code committed developer branch mobile master branch first built across product code could affect mobile mean building facebook messenger page manager instagram apps every commit also build several flavor product ensure covered chip architecture simulator product support build going run linters static analysis tool infer help catch null pointer exception resource memory leak unused variable risky system call flag facebook coding guideline issue third concurrent system mobile automated testing includes thousand unit test integration test endtoend test driven tool like robolectric xctest junit webdriver build test stack run every commit also run multiple time life cycle code change android alone build day applying traditional continuous delivery technique mobile stack gone fourweek release twoweek release oneweek release today use kind branchcherrypick model mobile previously used web although push production week still important test code early realworld setting engineer get quick feedback make mobile release candidate available every day canary user including million android beta tester time increased release frequency mobile engineering team grown factor code delivery velocity increased considerably despite data show engineer productivity remained constant android io whether measured line code pushed number push similarly number critical issue arising mobile release almost constant regardless number deployment indicating code quality suffer continue scale many advance available tool methodology exciting time working area release engineering proud team facebook worked together give u think one advanced web mobile deployment system scale part made possible strong central release engineering team firstclass citizen infrastructure engineering space release team facebook continue drive initiative improve release process developer customer continue share experience tool best practice
275,Lobsters,scaling,Scaling and architecture,Envoy threading model,https://medium.com/@mattklein123/envoy-threading-model-a8d44b922310,envoy threading model,envoy let know threading overview main xds api dns health checking cluster management runtime hot restart worker concurrency option file flusher connection handling nonblocking mean thread local storage rcu locking concept cluster update threading subsystem make use tl runtime feature flag override lookup route table swapping http date header caching known performance pitfall conclusion mentioned twitter dpdk link code,low level technical documentation envoy codebase currently fairly sparse rectify planning series blog post various subsystem first post please let know think topic would like see coveredone common technical question get envoy request low level description threading model us post cover envoy map connection thread well description thread local storage tl system used internally make code extremely parallel high performingthreading overviewfigure threading overviewenvoy us three different type thread illustrated figure onemain thread owns server startup shutdown xds api handling including dns health checking general cluster management runtime stat flushing admin general process management signal hot restart etc everything happens thread asynchronous nonblocking general main thread coordinate critical process functionality require large amount cpu accomplish allows majority management code written single threadedworker default envoy spawn worker thread every hardware thread system controllable via concurrency option worker thread run nonblocking event loop responsible listening every listener currently listener sharding accepting new connection instantiating filter stack connection processing io lifetime connection allows majority connection handling code written single threadedfile flusher every file envoy writes primarily access log currently independent blocking flush thread writing filesystem cached file even using ononblock sometimes block sigh worker thread need write file data actually moved inmemory buffer eventually flushed via file flush thread one area code technically worker block lock trying fill memory buffer others discussed belowconnection handlingas discussed briefly worker thread listen listener without sharding thus kernel used intelligently dispatch accepted socket worker thread modern kernel general good employ feature io priority boosting attempt fill thread work starting employ thread also listening socket well using single spinlock processing acceptonce connection accepted worker never leaf worker handling connection entirely processed within worker thread including forwarding behavior important implication connection pool envoy per worker thread though connection pool make single connection upstream host time four worker four connection per upstream host steady statethe reason envoy work way keeping everything within single worker thread almost code written without lock single threaded design make code easier write scale incredibly well almost unlimited number workersone major takeaway however memory connection pool efficiency standpoint actually quite important tune concurrency option worker needed waste memory create idle connection lead lower connection pool hit rate lyft sidecar envoy run low concurrency performance roughly match service sitting next run edge envoy max concurrencywhat nonblocking meansthe term nonblocking used several time far discussing main worker thread operate code written assuming nothing ever block however entirely true anything ever entirely true envoy employ process wide lock already discussed access log written worker acquire lock filling inmemory access log buffer lock hold time low possible lock become contended high concurrency high throughputenvoy employ sophisticated system handling stats thread local subject separate post however briefly mention part thread local stat handling sometimes required acquire lock central stat store lock ever highly contendedthe main thread periodically need coordinate worker thread done posting main thread worker thread sometimes worker thread back main thread posting requires taking lock posted message put queue later delivery lock never highly contended still technically blockwhen envoy log standard error acquires processwide lock general envoy local logging assumed terrible performance much thought given improving thisthere random lock none performance critical path never contendedthread local storagebecause way envoy separate main thread responsibility worker thread responsibility requirement complex processing done main thread made available worker thread highly concurrent way section describes envoy thread local storage tl system high level next section describe used handling cluster managementfigure thread local storage tl systemas already described main thread handle essentially managementcontrol plane functionality within envoy process control plane bit overloaded considered within envoy process compared forwarding worker seems appropriate common pattern main thread process work need update worker thread result work without worker thread needing acquire lock every accessenvoy tl system work follows code running main thread allocate processwide tl slot though abstracted practice index vector allowing accessthe main thread set arbitrary data slot done data posted worker normal event loop eventworker thread read tl slot retrieve whatever thread local data available therealthough simple incredibly powerful paradigm similar rcu locking concept essentially worker thread never see change data tl slot work change happens quiescent period work event envoy us two different way storing different data worker accessed without lockby storing shared pointer readonly global data worker thus worker reference count data decremented work worker quiesced loaded new shared data old data destroyed identical rcucluster update threadingin section describe tl used cluster management cluster management includes xds api handling andor dns well health checkingfigure cluster manager threadingfigure three show overall flow involves following component step cluster manger component inside envoy manages known upstream cluster cd api sdseds apis dns active outofband health checking responsible creating eventually consistent view every upstream cluster includes discovered host well health statusthe health checker performs active health checking report health state change back cluster managercdssdsedsdns performed determine cluster membership state change reported back cluster managerevery worker thread continuously running event loopwhen cluster manager determines state changed cluster creates new readonly snapshot cluster state post every worker threadduring next quiescent period worker thread update snapshot allocated tl slotduring io event need determine host load balance load balancer query tl slot host information lock acquired note also tl also fire event update load balancer component recompute cache data structure etc beyond scope post used various place code using previously described procedure envoy able process every request without taking lock previously described beyond complexity tl code code need understand threading work written single threaded make code easier write addition yielding excellent performanceother subsystem make use tlstls rcu used extensively within envoy example include runtime feature flag override lookup current feature flag override map computed main thread readonly snapshot provided worker using rcu semanticsroute table swapping route table provided rds route table instantiated main thread readonly snapshot provided worker using rcu semantics make route table swap effectively atomichttp date header caching turn computing http date header every request rps per core quite expensive envoy centrally computes date header every half second provides worker via tl rcuthere case previous example provide good taste kind thing tl used forknown performance pitfallsalthough overall envoy performs quite well known area need attention used high concurrency throughput already described post currently worker acquire lock writing access log inmemory buffer high concurrency high throughput required perworker batching access log cost order delivery written final file alternatively access log become per worker threadalthough stats heavily optimized high concurrency throughput likely atomic contention individual stats solution perworker counter periodic flushing central counter discussed followup postthe existing architecture work well envoy deployed scenario connection require substantial resource handle guarantee connection evenly spread worker solved implementing worker connection balancing worker capable forwarding connection another worker handlingconclusionenvoy threading model designed favor simplicity programming massive parallelism expense potentially wasteful memory connection use tuned correctly model allows perform well high worker count throughputas briefly mentioned twitter design also amenable running top full user mode networking stack like dpdk could lead commodity server processing million request per second full processing interesting see get built next yearsone last quick comment asked many time chose c envoy reason remains still widely deployed production grade language possible build architecture described post c certainly right even many project certain use case still tool get job donelinks codesome link interface implementation header discussed post
276,Lobsters,scaling,Scaling and architecture,Serverless: A lesson learned. The hard way.,https://sourcebox.be/blog/2017/08/07/serverless-a-lesson-learned-the-hard-way/,serverless lesson learned hard way,first experience serverless architecture update follow,writing article first experience serverless architecture felt pretty good learned achieved however happened next morning left sour taste mouthafter week holiday got morning got ready work checked mail message took shower ate breakfast know daily routine received budget notification amazon point already guess happenedaws budget notification mailat first glance nt seem like much forecasted expected actual total le quickly logged amazon console went straight billing greeted whopping forecasted cost actual cost left light headed feeling lot money could probably tell budgeted value mailnot wasting time quickly dug log found mistake made bug found writing blog post yesterday forgot change code deploy bucket set new file added trigger lambda function determines cache duration based file type store meta data object overwritten trigger event seen new object turn trigger lambda function check handle behavior present checked meta data already would terminate script simple return statement exactly part went wrong refactor forgot return statement continued overwriting file thus creating infinite loopi suspect running since little published blog post log full iti thought day could nt get worse hour later guess cost update periodically updated cost went againactual forecasted cost aws billing pagethe actual cost forecasted make think twice using payperuse service future one little mistake cost lot money budget notification late little could itright continuously refreshing billing page amazon hoping wont go higher probably wont sleeping much tonighti saying serverless bad careful keep eye log test everything set budget alarm even though still ended cost could much much worse without onesome stats people interested clearly tell lambda activeaws cost per servicethis probably stupid thing ever one missing return ended costing wrote follow sure check
277,Lobsters,scaling,Scaling and architecture,A Million WebSockets and Go,https://medium.freecodecamp.org/million-websockets-and-go-cc58418460bb,million websockets go,million websockets go introduction idiomatic way channel struct depending operating system gb memory io goroutines gb gb http server receives gb gb optimization netpoll connread implementation netnetfdread call eagain error polldescwaitread call dig deeper epoll kqueue issue getting rid goroutines netpoll implementation gb control resource gb goroutine pool zerocopy upgrade stored field samename header type websocket implementation zerocopy upgrade benchmark zerocopy upgrade gb summary solution solution solution solution conclusion reference,sergey kamardina million websockets gohi everyone name sergey kamardin developer mailruthis article developed highload websocket server goif familiar websockets know little go hope still find article interesting term idea technique performance introductionto define context story word said need servermailru lot stateful system user email storage one several way keep track state change within system event mostly either periodic system polling system notification state changesboth way pro con come mail faster user receives new mail bettermail polling involves http query per second return status meaning change mailboxtherefore order reduce load server speed mail delivery user decision made reinvent wheel writing publishersubscriber server also known bus message broker eventchannel would receive notification state change one hand subscription notification otherpreviously first scheme show like browser periodically polled api asked storage mailbox service changesthe second scheme describes new architecture browser establishes websocket connection notification api client bus server upon receipt new email storage sends notification bus bus subscriber api determines connection send received notification sends user browser today going talk api websocket server looking ahead tell server million online idiomatic waylet see would implement certain part server using plain go feature without optimizationsbefore proceed nethttp let talk send receive data data stand websocket protocol eg json object hereinafter referred packetslet begin implementing channel structure contain logic sending receiving packet websocket channel struct packet represents application level data type packet struct channel wrap user connection type channel struct conn netconn websocket connection send chan packet outgoing packet queue func newchannel conn netconn channel c channel conn conn send make chan packet n go creader go cwriter return c websocket channel implementationi like draw attention launch two reading writing goroutines goroutine requires memory stack may initial size kb depending operating system go versionregarding mentioned number million online connection need gb memory stack kb connection without memory allocated channel structure outgoing packet chsend internal io goroutineslet look implementation reader func c channel reader make buffered read reduce read syscalls buf bufionewreader cconn pkt readpacket buf chandle pkt channel reading goroutinehere use bufioreader reduce number read syscalls read many allowed buf buffer size within infinite loop expect new data come please remember word expect new data come return laterwe leave aside parsing processing incoming packet important optimization talk however buf worth attention default kb mean another gb memory connection similar situation writer func c channel writer make buffered write reduce write syscalls buf bufionewwriter cconn pkt range csend writepacket buf pkt bufflush channel writing goroutinewe iterate across outgoing packet channel csend write buffer attentive reader already guess another kb gb memory million httpwe already simple channel implementation need get websocket connection work still idiomatic way heading let corresponding waynote know websocket work mentioned client switch websocket protocol mean special http mechanism called upgrade successful processing upgrade request server client use tcp connection exchange binary websocket frame description frame structure inside connectionimport nethttp somewebsocket httphandlefunc func w httpresponsewriter r httprequest conn websocketupgrade r w ch newchannel conn idiomatic way upgrading websocketplease note httpresponsewriter make memory allocation bufioreader bufiowriter kb buffer httprequest initialization response writingregardless websocket library used successful response upgrade request server receives io buffer together tcp connection responsewriterhijack callhint case go linkname used return buffer syncpool inside nethttp call nethttpputbufio reader writer thus need another gb memory million connectionsso total gb memory application nothing yet optimizationslet review talked introduction part remember user connection behaves switching websocket client sends packet relevant event word subscribes event taking account technical message pingpong client may send nothing else whole connection lifetimethe connection lifetime may last several second several daysso time channelreader channelwriter waiting handling data receiving sending along waiting io buffer kb eachnow clear certain thing could done better netpolldo remember channelreader implementation expected new data come getting locked connread call inside bufioreaderread data connection go runtime woke goroutine allowed read next packet goroutine got locked expecting new data let see go runtime understands goroutine must woken look connread implementation see netnetfdread call inside netfdunixgo func fd netfd read p byte n int err error n err syscallread fdsysfd p err nil n err syscalleagain err fdpdwaitread err nil continue break go internals nonblocking readsgo us socket nonblocking mode eagain say data socket get locked reading empty socket o return control uswe see read syscall connection file descriptor read return eagain error runtime make polldescwaitread call netfdpollruntimego func pd polldesc waitread error return pdwait r func pd polldesc wait mode int error re runtimepollwait pdruntimectx mode go internals netpoll usageif dig deeper see netpoll implemented using epoll linux kqueue bsd use approach connection could allocate read buffer start reading goroutine really necessary really readable data socketon githubcomgolanggo issue exporting netpoll getting rid goroutinessuppose netpoll implementation go avoid starting channelreader goroutine inside buffer subscribe event readable data connection ch newchannel conn make conn observed netpoll instance pollerstart conn netpolleventread func spawn goroutine prevent poller wait loop become locked receiving packet ch go receive ch receive read packet conn handle somehow func ch channel receive buf bufionewreader chconn pkt readpacket buf chandle pkt using netpollit easier channelwriter run goroutine allocate buffer going send packet func ch channel send p packet cnowriteryet go chwriter chsend p starting writer goroutine needednote handle case operating system return eagain write system call lean go runtime case cause actually rare kind server nevertheless could handled way neededafter reading outgoing packet chsend one several writer finish operation free goroutine stack send bufferperfect saved gb getting rid stack io buffer inside two continuously running control resourcesa great number connection involves high memory consumption developing server experienced repeated race condition deadlock often followed socalled selfddos situation application client rampantly tried connect server thus breaking even morefor example reason suddenly could handle pingpong message handler idle connection continued close connection supposing connection broken therefore provided data client appeared lose connection every n second tried connect instead waiting eventsit would great locked overloaded server stopped accepting new connection balancer example nginx passed request next server instancemoreover regardless server load client suddenly want send u packet reason presumably cause bug previously saved gb use actually get back initial state goroutine buffer per connectiongoroutine poolwe restrict number packet handled simultaneously using goroutine pool naive implementation pool look like package gopool func new size int pool return pool work make chan func sem make chan struct size func p pool schedule task func error select case pwork task case psem struct go pworker task func p pool worker task func defer func psem task task pwork naive implementation goroutine poolnow code netpoll look follows pool gopoolnew pollerstart conn netpolleventread func block poller wait loop pool worker busy poolschedule func receive ch handling poller event within goroutine poolso read packet upon readable data appearance socket also upon first opportunity take free goroutine poolsimilarly change send pool gopoolnew func ch channel send p packet cnowriteryet poolschedule chwriter chsend p reusing writing goroutineinstead go chwriter want write one reused goroutines thus pool n goroutines guarantee n request handled simultaneously arrived n allocate n buffer reading goroutine pool also allows u limit accept upgrade new connection avoid situation zerocopy upgradelet deviate little websocket protocol already mentioned client switch websocket protocol using http upgrade request look like get w host mailru connection upgrade secwebsocketkey secwebsocketversion upgrade websocket switching protocol connection upgrade secwebsocketaccept upgrade websockethttp upgrade examplethat case need http request header switch websocket protocol knowledge stored inside httprequest suggests sake optimization could probably refuse unnecessary allocation copying processing http request abandon standard nethttp serverfor example httprequest contains field samename header type unconditionally filled request header copying data connection value string imagine much extra data could kept inside field example largesize cookie headerbut take return websocket implementationunfortunately library existing time server optimization allowed u upgrade standard nethttp server moreover neither two library made possible use read write optimization optimization work must rather lowlevel api working websocket reuse buffer need procotol function look like func readframe ioreader frame error func writeframe iowriter frame errorif library api could read packet connection follows packet writing would look getreadbuf putreadbuf intended reuse bufioreader syncpool example func getreadbuf ioreader bufioreader func putreadbuf bufioreader readpacket must called data could read conn func readpacket conn ioreader error buf getreadbuf defer putreadbuf buf bufreset conn frame readframe buf parsepacket framepayload expected websocket implementation apiin short time make libraryideologically w library written impose protocol operation logic user reading writing method accept standard ioreader iowriter interface make possible use use buffering io wrappersbesides upgrade request standard nethttp w support zerocopy upgrade handling upgrade request switching websocket without memory allocation copying wsupgrade accepts ioreadwriter netconn implement interface word could use standard netlisten transfer received connection lnaccept immediately wsupgrade library make possible copy request data future use application example cookie verify session benchmark upgrade request processing standard nethttp server versus netlisten zerocopy upgrade benchmarkupgradehttp nsop bop allocsop benchmarkupgradetcp nsop bop allocsopswitching w zerocopy upgrade saved u another gb space allocated io buffer upon request processing nethttp summarylet structure optimization told abouta read goroutine buffer inside expensive solution netpoll epoll kqueue reuse buffersa write goroutine buffer inside expensive solution start goroutine necessary reuse bufferswith storm connection netpoll work solution reuse goroutines limit numbernethttp fastest way handle upgrade websocket solution use zerocopy upgrade bare tcp connectionthat server code could look like import net githubcomgobwasws ln netlisten tcp try accept incoming connection inside free pool worker free worker accept anything try later help u prevent many selfddos resource limit case err poolscheduletimeout timemillisecond func conn lnaccept wsupgrade conn wrap websocket connection channel struct help u handlesend app packet ch newchannel conn wait incoming byte connection pollerstart conn netpolleventread func cross resource limit poolschedule func read handle incoming packet chrecevie err nil timesleep timemillisecond example websocket server netpoll goroutine pool zerocopy conclusionpremature optimization root evil least programming donald knuthof course optimization relevant case example ratio free resource memory cpu number online connection rather high probably sense optimizing however benefit lot knowing improvethank attention reference
279,Lobsters,scaling,Scaling and architecture,Experimenting with scaling and full parallelism in PostgreSQL,http://www.cybertec.at/experimenting-scaling-full-parallelism-postgresql/,experimenting scaling full parallelism postgresql,scaling postgresql billion row second scaling parallel query postgresql parallel query postgresql server farm implementing scalability postgresfdw jit compilation speedup improving postgresql scalability even twitter postgressupport,lot said written scaling postgresql many core subsequently many machine running postgresql server farm something possible many year however definitely changed lot development gone scaling postgresql running single query many cpu postgresql first release allowed run query many node development moved thing constantly improving make postgresql even betterscaling postgresql billion row second currently project going cybertec require postgresql scale well beyond single server idea scale postgresql infinitely course new still exciting push limit beloved open source database every dayscaling parallel queriestraditionally postgresql used single cpu core query course used severe limitation fortunately removed many thing inside postgresql already done parallel project move parallelism available many query benefit multicore systemsto show simple query benefit parallelism compiled simple data structure node tdemo table publictdemo column type collation nullable default id integer null nextval tdemoidseq regclass grp integer data real index idxid btree id query used test pretty simply count number row group select grp count data tdemo group running parallel mode best plan produced optimizer possible version following node explain select grp partialcount data publictdemo group grp query plan finalize groupaggregate group key grp sort sort key grp gather worker planned partial hashaggregate group key grp parallel seq scan tdemo postgresql process large table using worker process case system contains least cpu core performance basically increase linear way worker process added worker aggregate data partial aggregate added linear trend important necessary precondition use hundred thousand cpu timeif normal application usually sufficient single box aggregate million row little time single database node however data grows even scaling many node might necessaryassuming data node contains cpu core google cloud box million row performance improve depending number process used first important observation line go straight core also interesting see still gain little bit speed using process job benefit see related intel hyperthreading expect boost around given type query single database node vm reach around million row per second simple aggregationpostgresql parallel query postgresql server farmhowever goal reach billion row second way get add serversto reach goal following architecture used idea partitioned table distributed data reside actual nodesin first step second server added see indeed process twice amount data within timeframehere execution plan explain analyze select grp count data tdemo group query plan finalize hashaggregate actual group key tdemogrp append actual foreign scan actual foreign scan actual partial hashaggregate never executed group key tdemogrp seq scan tdemo never executed planning time m execution time m example million row deployed database server beauty execution time stay samelet u try query x million row node explain analyze select grp count data tdemo group query plan finalize hashaggregate actual group key tdemogrp append actual foreign scan actual foreign scan actual foreign scan actual foreign scan actual foreign scan actual foreign scan actual partial hashaggregate never executed group key tdemogrp seq scan tdemo never executed planning time m execution time m wow need le second billion row result look like node select grp count data tdemo group grp count row together billion row shardsthe important observation kind query shard added demand performance needed amount data grows postgresql scale nicely every node people addimplementing scalabilityso actually needed achieve result first work vanilla postgresql first thing needed functionality postgresql postgresfdw need ability push aggregate remote host easy part trickier part teach postgresql shard work parallel fortunately patch allowed making append node fetch data concurrently parallel append important precondition code workhowever many year postgresql could aggregate data joined basically restriction stopped many performance optimization thanks incredible work done kyotaro horiguchi done wonderful job removing restriction possible u build top aggregate much data actually reach billion row per second given complexity task necessary explicitly list kyotaro work without achievement would close impossiblebut making work solution heavily based postgresfdw able fetch large amount data postgresfdw us cursor remote side point postgresql postgresql cursor fully parallel yet therefore lift restriction ensure cpu core remote host used timefinally need couple point handwritten aggregate mapreduce style aggregation achieving simple easily done simple extensionjit compilation speedupswhile billion row per second certainly nice achievement cool stuff postgresql future jit compilation various optimization start make way postgresql tuple deforming column store etc see similar result using fewer fewer cpu able use fewer smaller server achieve similar performanceour test done without optimization know definitely lot room improvement important thing managed show postgresql made scale hundred maybe thousand cpu cooperate cluster work queryimproving postgresql scalability even moreat point single master server couple shard used case architecture sufficient however keep mind also possible organize server tree make sense even complex calculationalso keep mind postgresql support custom aggregate idea create aggregation function even complex stuff postgresqlcontact follow u twitter postgressupport stay date cool stuff way
280,Lobsters,scaling,Scaling and architecture,Pattern: Service Mesh,http://philcalcado.com/2017/08/03/pattern_service_mesh.html,pattern service mesh,happened first started networking computer happened first started microservices fallacy distributed computing discussed detail release martin fowler summary pattern twitter finagle facebook proxygen next logical step airbnb wrote synapse nerve netflix introduced prana netflixoss ecosystem infrastructure built jvm microservices netflix eureka service registry first widely known system space linkerd engineering team lyft announced envoy service mesh buoyant ceo william morgan network early william wrote definition platform called istio project acknowledgement monica farrell rodrigo kumpera etel sverdlov dave worth mauricio linhares daniel bryant fabio kung carlos villela revision history,since first introduction many decade ago learnt distributed system enable use case even think also introduce sort new issue system rare simple engineer dealt added complexity minimising number remote interaction safest way handle distribution avoid much possible even meant duplicated logic data across various system need industry pushed u even larger central computer hundred thousand small service new world start taking head sand tackling new challenge open question first adhoc solution done casebycase manner subsequently something sophisticated find problem domain design better solution start crystallising common need pattern library eventually platform happened first started networking computer since people first thought getting two computer talk envisioned something like service talk another accomplish goal enduser obviously oversimplified view many layer translate byte code manipulates electric signal sent received wire missing abstraction sufficient discussion though let add bit detail showing networking stack distinct component variation model use since beginning computer rare expensive link two node carefully crafted maintained computer became le expensive popular number connection amount data going increased drastically people relying networked system engineer needed make sure software built quality service required user many question needed answered get desired quality level people needed find way machine find handle multiple simultaneous connection wire allow machine talk connected directly route packet across network encrypt traffic etc amongst something called flow control use example flow control mechanism prevents one server sending packet downstream server process necessary networked system least two distinct independent computer know much computer sends byte given rate computer b guarantee b process received byte consistent fastenough speed example b might busy running task parallel packet may arrive outoforder b blocked waiting packet arrived first mean expected performance b could also making thing worse might overload b queue incoming packet processing expected people building networked service application would deal challenge presented code wrote flow control example meant application contain logic make sure overload service packet networkingheavy logic sat side side business logic abstract diagram would something like fortunately technology quickly evolved soon enough standard like tcpip incorporated solution flow control many problem network stack mean piece code still exists extracted application underlying networking layer provided operating system model wildly successful organisation use tcpip stack come commodity operating system drive business even highperformance reliability required happened first started microservices year computer became even cheaper omnipresent networking stack described proven defacto toolset reliably connect system node stable connection industry played various flavour networked system finegrained distributed agent object serviceoriented architecture composed larger still heavily distributed component extreme distribution brought lot interesting higherlevel use case benefit also surfaced several challenge challenge completely new others higherlevel version one discussed talking raw network peter deutsch fellow engineer sun microsystems compiled fallacy distributed computing list assumption people tend make working distributed system peter point might true primitive networking architecture theoretical model hold true modern world network reliable latency zero bandwidth infinite network secure topology change one administrator transport cost zero network homogeneous denouncing list fallacy mean engineer ignore issue explicitly deal complicate matter moving even distributed often call microservices introduced new need operability side discussed detail quick list one deal rapid provisioning compute resource basic monitoring rapid deployment easy provision storage easy access edge authenticationauthorisation standardised rpc tcpip stack general networking model developed many decade ago still powerful tool making computer talk sophisticated architecture introduced another layer requirement fulfilled engineer working architecture example consider service discovery circuit breaker two technique used tackle several resiliency distribution challenge listed history tends repeat first organisation building system based microservices followed strategy similar first generation networked computer mean responsibility dealing requirement listed left engineer writing service service discovery process automatically finding instance service fulfil given query eg service called team need find instance service called player attribute environment set production invoke service discovery process return list suitable server monolithic architecture simple task usually implemented using dns load balancer convention port number eg service bind http server port distributed environment task start get complex service previously could blindly trust dns lookup find dependency deal thing like clientside loadbalancing multiple different environment eg staging v production geographically distributed server etc needed single line code resolve hostnames service need many line boilerplate deal various corner case introduced higher distribution circuit breaker pattern catalogued michael nygard book release like martin fowler summary pattern basic idea behind circuit breaker simple wrap protected function call circuit breaker object monitor failure failure reach certain threshold circuit breaker trip call circuit breaker return error without protected call made usually also want kind monitor alert circuit breaker trip great simple device add reliability interaction service nevertheless like everything else tend get much complicated level distribution increase likelihood something going wrong system raise exponentially distribution even simple thing like kind monitor alert circuit breaker trip necessarily straightforward anymore one failure one component create cascade effect across many client client client triggering thousand circuit trip time used line code requires load boilerplate handle situation exist new world fact two example listed hard implement correctly large sophisticated library like twitter finagle facebook proxygen became popular mean avoid rewriting logic every service model depicted followed majority organisation pioneered microservices architecture like netflix twitter soundcloud number service system grew also stumbled upon various drawback approach probably expensive challenge even using library like finagle organisation still need invest time engineering team building glue link library rest ecosystem based experience soundcloud digitalocean would estimate following strategy engineer organisation one would need dedicate staff building tooling sometimes cost explicit engineer assigned team dedicated building tooling often price tag invisible manifest time taken away working product second issue setup limit tool runtimes language use microservices library microservices often written specific platform programming language runtime like jvm organisation us platform one supported library often need port code new platform steal scarce engineering time instead working core business product engineer build tool infrastructure mediumsized organisation like soundcloud digitalocean decided support one platform internal go respectively one last problem model worth discussing governance library model might abstract implementation feature required tackle need microservices architecture still component need maintained making sure thousand instance service using least compatible version library trivial every update mean integrating testing redeploying service suffer change next logical step similarly saw networking stack would highly desirable extract feature required massively distributed service underlying platform people write sophisticated application service using higher level protocol like http without even thinking tcp control packet network situation need microservices engineer working service focus business logic avoid wasting time writing service infrastructure code managing library framework across whole fleet incorporating idea diagram could end something like following unfortunately changing networking stack add layer feasible task solution found many practitioner implement set proxy idea service connect directly downstream dependency instead traffic go small piece software transparently add desired feature first documented development space used concept sidecar sidecar auxiliary process run aside application provides extra feature airbnb wrote synapse nerve opensource implementation sidecar one year later netflix introduced prana sidecar dedicated allowing nonjvm application benefit netflixoss ecosystem soundcloud built sidecar enabled ruby legacy use infrastructure built jvm microservices several opensource proxy implementation tend designed work specific infrastructure component example come service discovery airbnb nerve synapse assume service registered zookeeper prana one use netflix eureka service registry increasing popularity microservices architecture recently seen new wave proxy flexible enough adapt different infrastructure component preference first widely known system space linkerd created buoyant based engineer prior work twitter microservices platform soon enough engineering team lyft announced envoy follows similar principle service mesh model service companion proxy sidecar given service communicate sidecar proxy end deployment similar diagram buoyant ceo william morgan made observation interconnection proxy form mesh network early william wrote definition platform called service mesh service mesh dedicated infrastructure layer handling servicetoservice communication responsible reliable delivery request complex topology service comprise modern cloud native application practice service mesh typically implemented array lightweight network proxy deployed alongside application code without application needing aware probably powerful aspect definition move away thinking proxy isolated component acknowledges network form something valuable organisation move microservices deployment sophisticated runtimes like kubernetes mesos people organisation started using tool made available platform implement idea mesh network properly moving away set independent proxy working isolation proper somewhat centralised control plane looking bird eye view diagram see actual service traffic still flow proxy proxy directly control plane know proxy instance control plane enables proxy implement thing like access control metric collection requires cooperation recently announced istio project prominent example system still early fully understand impact service mesh larger scale system two benefit approach already evident first write custom software deal ultimately commodity code microservices architecture allow many smaller organisation enjoy feature previously available large enterprise creating sort interesting use case second one architecture might allow u finally realise dream using best toollanguage job without worrying availability library pattern every single platform acknowledgement monica farrell rodrigo kumpera etel sverdlov dave worth mauricio linhares daniel bryant fabio kung carlos villela gave feedback draft article revision history first published incorporated feedback
281,Lobsters,scaling,Scaling and architecture,Solving Imaginary Scaling Issues (at scale),https://dev.to/ben/solving-imaginary-scaling-issues-at-scale,solving imaginary scaling issue scale,devto,founder webmaster devto platform programmer write article like one discussion make friend lot fun growing application growing business technical side tried keep thing straightforward app grows complexity purposefully built pretty simple solution encountered serious scaling issue along way challenge regularly deal revolve around delivering right user experience keeping communication point generally picking right challenge take daytoday team five devs one person focus mostly business side though chip wherever addition real issue growing platform communication structure really like play around solution potential future issue usually weekend nt justify use time anyone heavy handed begin right exploring idea sharding database serving core application multiple region across world cut latency database read request hit origin server done research chosen library would likely use make possible generally thought might go implementing solution solution problem nt wo nt implementing knew heading exercise personal one eventually probably need start thinking along line team excited keep trying build simple solution problem use imagination come new feature optimization please user moment pretty nice monolithic application make good use cdn user appreciate effort put performance optimization
282,Lobsters,scaling,Scaling and architecture,Behind the scenes of a major infrastructure company,http://blog.online.net/2017/07/18/behind-the-scene-of-a-major-infrastructure-company/,behind scene major infrastructure company,pro wopr laval amsterdam first facility outside france amsterdam five optical fibre destroyed belgium paris uptime institute major issue one component supplier reserve odoo conclusion month sustain growth massively open new position paris coming month already send u resume job online dot net,online specific unsung job behind incredible ease speed feel requesting physical virtual server clics nothing virtual month customer disappointed trying acquire new baremetal dedibox server especially mid highend server family pro wopr none available unfortunately stock offer capacity planning impacted series unexpected event error totally disrupted production failed mission delivering demand infrastructure customer totally aware responsibility failure second time face situation twelve year blog post try transparent explain challenge faced sustain growth reason behind embarrassing situation little bit history twelve year ago defined settled principle master technology behind job goal control whole infrastructure avoid compromise precious data started industrial hosting provider activity decided control production pipeline took decision two main reason provide highquality service controlled start end offer customer best possible reactivity since team design build operate data center use european opticfibre network interconnect data center large part product designed internally online lab manufactured laval factory remaining part come wellknown hardware provider dell quantum computing hp supermicro operate large scale industrial infrastructure critical quality constraint daily unit hectare megawatt exabyte kilonewtons tb one largest computer assembler europe biggest infrastructure provider online people working everyday different job represented refrigeration engineer electronics engineer support specialist low level developer incredible thing everyday let build manage infrastructure second dedibox server smashing success surprising growth several year seeing large mutation major hosting provider offering longstanding player focus cloud product high margin abandon baremetal market provides le margin requires larger financial investment manufacturer new product le attractive cpu price rising dram nand market facing unprecedented crisis accelerates transformation market today provider world offering recent server configuration high volume le per month selling price always extremely competitive offer best ratio pricequalityperformance market consequently facing incredible growth four time higher forecast family product especially mid highend server one sale also mean datacenter space amsterdam june announced first facility outside france amsterdam demand amazing stock within week since offered many server could fast enough satisfy demand reached full network power capacity increase capacity amsterdam facility performing following operation purchased new link core network paris amsterdam satisfy growth next month meantime link provider upgraded network equipment impacted multiple issue european optical fibre last issue occurred july five optical fibre destroyed belgium incident delayed deployment additional capacity upgrade backbone two cisco offer high availability zone increase density uplink paperwork issue adding extra delay deployment amsterdam backbone aug today situation amsterdam starting stabilise add power capacity additional network capacity summer handle demand region paris time paris reached capacity data center delayed multiple time due administrative authorization missing hope authorization approved end nuclear fallout shelter opened production since july currently dedicated cold storage platform nuclear fallout shelter deployed capacity extension september expected extension increased power cooling space capacity factor two supposed support two year growth five week opening everything filled due huge demand mainly highend service full since optimizing facility retrofit density increase first generation data hall meet constantly increasing demand acquired new building hyperscale facility provides extraordinary characteristic one biggest investment ever made facility offer three time capacity decisive project sustain growth seven next year finished design december construction ongoing one biggest data center europe deliver net power january target pue last week opened first room capacity first room provision capacity partner facility paris launch biggest challenge data center industry lead time requires twelve eighteen month design build data center always rejected idea delivering cheap data center sustain growth reducing cost spite customer never option decision recognized certification delivered uptime institute forecast surpassed year filled rack square meter data center space plan keep one step ahead sustain growth midterm period hardware failure back february alerted one supplier erratum concerning component used entrylevel server dedibox sc dedibox xc dedibox xc scaleway erratum impact component reducing lifetime accelerated rate seeing occurrence erratum data center since supplier totally stopped production component waiting delivery upgraded fixed component continue production currently causing stock issue server listed time online lab team worked release earlier product able accelerate design new server ready production factory france currently starting production electronic board next generation server plan deploy new server data center october dram nand market crisis impact delivery sale also mean ram ssd volatile pressure market october started facing major issue one component supplier ram ssd disk price raised week week delivery time guaranteed anymore started disrupt production pipeline currently situation worse expected outbreak price problem nt even know receive ssds ram order minimize cost improve time market use lean manufacturing strategy strategy allows u move fast upgrade hardware frequently method offer many advantage except kind scenario happens today situation still difficult adapting supply chain situation even visibility delivery lead time supplier continue receive order delivery time really unstable delayed three month way illustration partially receiving order month ago since may try secure ssd hdd disk ram stock meet demand coming month work three different manufacturer get backup solution case delivery issue effort still enough satisfy demand market supply chain team magic everyday improve situation still critical today shortage sale also mean nothing faster speed light except maybe growth speed reserve utilization probably know shortage address acquiring range complex expensive similar data center getting hard scaling point unless buy black market majority available range owned government administration dealing long legal process take month succeed brexit froze month negotiation finalizing currently address space used acquiring three new range ip keep healthy reserve coming month online scaleway hope conclude acquisition end summer supply chain transformation phoenix must burn emerge janet fitch back july decided revamp industrialize supply chain increase production speed server per month moved per site logistic platform unique centralized logistic platform site paris amsterdam logistic platform new supply chain center totally running november massively increase daily production new setup underestimated direct impact delivery server addition issue totally rethink process method scale manufacturing process running nearly full production capacity last adjustment finalized logistic platform delivered february month delay changed information system favor odoo setup new solution required many change organization long runningin period thing worked well centralized stock logistic platform previously split four facility period ton hardware transferred inventoried dispatch spare stock data center took time expected still fully operational delay impact system customer success team day day operation test bench qualify hardware assembly yet fully operational result bottleneck server delivery time supply chain team undercapacity nt anticipate correctly team sizing success challenge rapid industrialization conclusion announced dedibox real earthquake deployed sold thousand unit month reached first company achievement completely filledin datacenter huge market share everything easy scale product huge empty datacenter simple network team people first issue encountered reaching insane milestone nothing sell next month time needed build first selfowned datacenter first soldout period caused irreversible damage online long period high demand dedicated server created market picked competitor never succeed catch even year later period decided learn error changed everything needed never end situation reworked brand image technical assistance invested massively construction facility deployed independent network focused quality refused provide wobbly infrastructure product major refactoring end success know today currently grow factor two every two year delivered total growth capacity seven year without major difficulty next step develop large scale industrialization anticipated right time faced many issue cascade succeed bit enthusiast underestimated part restructuring way work everyday mistake direct impact stock sale last month team work everyday incredible thing ensure smooth experience online service good news everything back normal september plan face similar issue coming year people counting working run part internet let face easy scale hope like transparency want thanks one help u make great amazing thing feedback suggestion question story please leave u comment happy answer sustain growth massively open new position paris coming month already send u resume job online dot net netcraft hosting provider computer jul
283,Lobsters,scaling,Scaling and architecture,Cluster Schedulers,https://medium.com/@cindysridharan/schedulers-kubernetes-and-nomad-b0f2e14a896,cluster scheduler,introduction even scheduler white paper borg scheduler google cluster scheduler wild recent post pitfall need scheduler packaging problem acm queue article external developer becoming interested linux container bryan cantrill statically linked deployment problem life cycle problem problem actually packaging problem image processing company datacenter pex cryptography use mac deployment problem circus wsgi server life cycle problem make code execution fork exec straightforward kubernetes docker rkt networking incremental refactor infrastructure sandi metz katrina owen vault nomad driver immediate win minimal change required existing stack deployment code fantastic consul integration simplified graceful restarts long poll consul consultemplate flexibility democratization operation simplicity pain point acl overcommit reservation conclusion,post aim understand purpose scheduler way originally envisaged developed well translate solve problem rest come handy even running scale challenge retrofitting scheduler existing running hybrid deployment artifact imgix chose nomad problem yet new problem new tool future hold usthis embarrassingly long post medium allow create link subsection searching title interest probably best bet get even scheduler cluster scheduler need problem actually kubernetes nomad immediate pain introductioncluster scheduler initially popularized white paper borg google google famously running everything container orchestrated borg decade containerization outside large organization made possible runaway success docker turn led birth kubernetes together tool succeeded capturing zeitgeist well enjoying steady uptick adoption across boardi invariably sceptical necessity scheduler especially smaller company albeit finding scheduler fascinating learn curiosity sentiment expressed twitter past asked strange look back tweet since exactly year later rolled scheduler company work posted july fairly good idea scheduler problem solved even solved see need worksurprisingly brendan burn one creator kubernetes responded question back said response said work true originally made comment small company even smaller engineering team scheduler certainly huge investment usi see point last year sometimes truly understand power piece technology actually use iti recently started using nomad work around early june enough talk hashicorp folk last year familiar nomad aimed working directly helped gain better appreciation even scheduler required reading anyone interested learning large scale computing purpose scheduler serve white paper borg scheduler google followed book datacenter computer introduction design warehousescale machine google luiz andr barroso ur hlzlethe borg paper begin stating borg achieves high utilization combining admission control efficient taskpacking overcommitment machine sharing processlevel performance isolation support highavailability application runtime feature minimize faultrecovery time scheduling policy reduce probability correlated failure borg simplifies life user offering declarative job specification language name service integration realtime job monitoring tool analyze simulate system behaviorthe paper go claim borg provides three main hide detail resource management failure handling user focus application development operates high reliability availability support application let u run workload across ten thousand machine cluster scheduler wildall aforementioned point make perfect sense one google anywhere near google scale exception precious majority organization nowhere near even fraction scale sort constraint warrant operation system complex borga strain thought become increasingly popular recent year google way thing right google also best way rest u corroborated birth number company open source project heavily inspired google internal infrastructure tooling recent post pitfall style thinking proved resonate incredibly well many clearly laid many fallacy school thought scepticism also shared acquaintance many worked really large scale organization word one acquaintance buying machine point pretty easy run one thing per machine move life get point inefficiency problem going kill youto honest company never get pointso need scheduler company need scheduleri generally find need word best left marketeers people deploying application without container let alone scheduler much longer even google deploying constellation service container probably also need thing like test coverage continuous integration good observability tool number thing come consider basic requirement building operating software day ageone particular mistake see lot u tend make including truly certain tool technology centering conversationevaluation around stock problem aim solve feature provide opposed problem actually hand often opportunity cost get anywhere much thought devoted promised golden future part inevitable live vacuum instead influenced unholy trinity hype developer advocacy genuine fear missing outit tempting especially enamored new piece technology promise moon retrofit problem space solution space said technology however minimal intersection evaluating cluster scheduler basis problem invented solve google fool errand make sense first fully understand current shortcoming one infrastructure deciding whether scheduler tool best suited solve problemspersonally see three primary unsolved problem packaging problem deployment problem lifecycle problem want delve deeper problem exploring scheduler help tackle problemsthe packaging problemthe requirement every company data driven especially consumer facing company mobile first made polyglot architecture inevitability even company getting startedeven year exist posix like standard packaging different language framework runtimes tool come closest solving problem fact docker highly unlikely scheduler would limelight unbridled success docker fact acm queue article google state kubernetes conceived developed world external developer becoming interested linux container google developed growing business selling publiccloud infrastructurethe introduction docker ushered paradigm shift infrastructure space astonishing velocity new tool workflow built around docker make feel justified labeling current era veritable gold rush momentum behind containerization contributed toward ubiquity every major cloud provider paas offering come form support containerization well increasingly widespread adoption metamorphosis yet complete suspect time way organization operate application would street ahead done previouslyirrespective might think technical merit lack thereof docker hard deny absolutely revolutionary even primarily packaging format biggest draw docker still remains fact helped attain parity different environment thereby accelerating speed development testing internally docker leverage control group namespaces capability primitive built linux kernel purely implementation detail something doubt much popularity opposed promise build ship run app anywhere docker lends towards solving problem every bit topical unrelated core primitive upon built docker genius main reason behind preposterous popularity usability quote bryan cantrill bryan cantrill velocity primitive developed google solve problem docker ended solving rest u borg paper state vast majority borg workload run inside virtual machine vms want pay cost virtualization also system designed time considerable investment processor virtualization support hardwaregoogle invent cgroups developer build ship run app anywhere google also famously deploys static binary borg paper go far state borg program statically linked reduce dependency runtime environment structured package binary data file whose installation orchestrated borggo rust c family language compile static binary dynamic language like python ruby javascript php complicated packaging story however built docker image fact follow acm queue article go state docker image fact closer hermetic ideal even static binary many way achieve hermetic image borg program binary statically linked build time knowngood library version hosted companywide repository even borg container image quite airtight could application share socalled base image installed machine rather packaged container base image contains utility tar libc library upgrade base image affect running application occasionally significant source troublemore modern container image format docker aci harden abstraction get closer hermetic ideal eliminating implicit host o dependency requiring explicit user command share image data containersthe primitive upon docker built might invented solve problem docker solves standardization packaging something docker got rightdocker solved packaging problem way tool successfully done bearing testament fact technology built solve specific problem google translate exceedingly well solution entirely different problem outside ivory tower google problem solved far divorced original context solution developed incidental irrelevant debunking notion need google scale use technology pioneered googlethe deployment problempackaging code deployment artifact first step operationalizing application assuming artifact ready deployed across fleet server next immediate problem face deployment graceful restarts rollbacksthe deployment space extremely fractured different organization ecosystem adopting varying practice deploy code safely reliably bigger company entire engineering team dedicated working building internal tooling automate build release process softwarethis fragmentation notwithstanding fundamental way deployment done remains somewhat similar across spectrum bring bunch new process kill old one ideally without dropping connection different application handle graceful restarts differently mostly handling specific signal common approach gracefully restart application allow old process finish handling request still progress exit request drained making sure dying process accept new connection meantimei seen done panoply way set failure mode dedicated library guard rail application code process supervisor config socket activation specialized socket manager configuration management custom offtheshelf tooling like fabric capistrano proxy configs plain old shell script hefty dollop tribal knowledge name lather rinse repeat deploys also rollback every single service one run suddenly look like trivial problem anymorethe aforementioned approach unsophisticated brittle prove crippling number service scale especially microservices environment wrangle ruby ecosystem thin mongrel unicorn god monit python stack well gunicorn uwsgi modwsgi twisted supervisor horrifying proposition right exacerbated greatly jvm get thrown mix wont happen company anything big data deployment short continue massive pain point everything except trivial case packaging one standard unixlike way deploy heterogenous application workloadsthe life cycle problemfor sake argument let u assume solution packaging deployment conundrum faced responsibility ensuring healthy operation service duration lifetime even eschewing fancy terminology like slos slas slis least requirement service need meet functionalapplications fail number reason application logic centric incorrect handling unexpected input edge case application crashing due application error detected mitigated process manager restarting crashed application time application fail due network upstreams undiscoverable request upstreams ratelimited timing case one would hope application code proxy fronting application robust enough handle failuresand sometimes machine fail requiring relocation application onto host process manager help since design host centric way many historically monitored distributed health application scripting nagios check alert human operator workarounds explored post little later tool traditional operation toolkit specifically designed purposemore importantly problem distributed system partial failure component application fail application function degraded state partial failure manifest myriad form including increased timeouts response latency degraded application performance symptom several cause noisy neighbor unbounded queue lack sufficient backpressure poor cache hit ratio database lock contention poor query pattern partial outage upstreams third party service name good observability application help uncover issue observability tooling built autoheal application instead built help diagnosis debugging system via dashboarding auditing exploratory analysis worst case scenario alerting human operatorin short building distributed system day one one server container one operating distributed system yet tool use build run monitor system largely mark managing lifecycle application best semiautomated process tooling yore either hostcentric problem actually hadwe identified three problem continue plague u make lot sense look concrete example problem actually work decided switch part stack schedulerthe packaging problemi work imgix image processing company process ten thousand image transformation per second worst case would require u freshly fetch many image real time gc http backend serve back real time also need support real time user driven configuration update entire fleet machine addition load balancing request host based locality various factor well purging varying image format cache internal externalall clientfacing javascript application well sale marketing tool run heroku gcp platform usually standard disparate packaging deployment mechanism different language little anyone adhere guideline get something running various component core stack written lua c objectivec python go application run inside datacenter deployment quite straightforward git push heroku masterpython bit odd man among main language maintain fair share old python code new code meant run within datacenter authored python c objectivec lua go application compile convenient static binary python deployment story historically something albatross earliest python application deployed pex format one thankfully monolithic application actively developed maintained deployed docker containerour initiative dockerize single python application halfhearted understandably result somewhat mixed part dockerizing python code yielded result hoping viz hermetically sealed artifact capturing pure python well native dependency shared library header file compiled c module except odd issue openssl often fixed pulling specific version cryptography library deploying python form docker container worked well enoughoutside python see need docker rest application deployed binary fairly good artifact caching versioning delivery system also use mac core image rendering layer requires reimage every server recompile image processing software every time o x upgrade detail done outside scope post suffice say docker solution problem goal limit docker usage single python application write future core service pythonthe deployment problemcircus process socket manager mozilla especially good deploying python web application without requiring application fronted wsgi server initially used circus run application year ago switched descendant daemontools pretty reliable far used run core service toolkit small program allows one compose flexible process pipeline little effort without leaving behind deep process tree switch circus made owing circus general unreliability problem socket client circusctl often timing conversation circus daemon circus daemon oftentimes unresponsive long period worse pegging cpu certain box huping circus usually fixed still something needed monitoring deployment fully automated hood involved getting latest artifact right host circus restart application application ran child process circus circus managed listening socket restarting entailed circus sending old application process configurable signal upon receipt application would manually deregister consul use service discovery ensuring request routed drain connection finally exit new process brought would first register consul service name dying process start accept ing new connection worked well enough except every application codebase carried boilerplate code consul registration upon starting deregistration upon shut downas stated process manager scope limited single host almost never run single instance application unless something deployed staging environment experiment outside core image rendering pipeline application operate include everything http server internally authored router proxy asynchronous worker cpu memory intensive batch job number called oddjobs described internal wiki thusly oddjobs miscellaneous bunch singleton lacked internet access longrunning way run could described interesting imperfectone feature consul provides distributed locking consul lock n foo cmd allows one obtain n lock prefix foo consul key value store consul agent host actually run cmd child process lock lost communication disrupted owing physical node failure instance child process would terminatedif needed one instance oddjob running given time running job say two physical host consul lock would mean one would actually acquire lock given time process held lock crashed process running host could acquire lock relinquished crashed process guaranteeing exactly one process running given time unless course host crashed case far bigger problem oddjobs running work equally well job question singletonthis met requirement soon enough quite critical internal service running oddjobs including prometheus alertmanagerthe life cycle problemwhen consul everything kitchen sink tempting use solve every problem tool like zookeeper consul ilk meant building block distributed system using distributed locking directly mean reliably run job effective use toolsoon enough became evident approach foolproof liked sometimes service deregister consul correctly exited leaving almost dead entry behind per se problem index consul store service state health check status absence index meant leader thrashing raft timeouts service polled consul time child oddjob process would escape parent process get reparented init happened consul launched child process shell relay signal like sigterm child exited leading child process getting reparented initon linux child process set prsetpdeathsig via prctl ensures parent process dy reason child would get signal even parent meant consul agent acquired lock forked child oddjob process meant run child set prsetpdeathsig execing would mean signal would delivered child correctlyexcept consul written go make code execution fork exec straightforward solution wrap actual binary needed run consul lock small c program called deathsig consul would fork exec deathsig turn would set prsetpdeathsig execing run actual binary needed run consul lockthis worked never really meant final solution even oddjobs first conceived implemented fashion two year ago goal eventually move scheduler built purposeit begged question scheduler kubernetes kubernetes incredibly sophisticated opinionated framework orchestrating container flourishing fantastic community pretty convinced point kubernetes midsized large organization wellstaffed operation team would benefit however kubernetes right choice u number reasonsdockerone biggest argument kubernetes fact would required u change package existing application worked hybrid deployment artifact instead prescribing artifact docker rkt container kubernetes would definitely something might seriously considered reason stated docker usage something hope limit proliferateto personally docker perplexing piece software one hand make encoding every application standard format ridiculously easy running docker container production standard docker tooling though entirely different kettle fish conversation people working large company entire team kernel expert support docker based container infrastructure one mentioned docker still remains nightmare still run hundred kernel panic singledigit docker lockup dayeven taken grain salt state docker company entire team dedicated work container based infrastructure adopting scheduler would lock u docker feel awfully reassuring particularly sole exception one application benefit docker would offered would outweighed overhead let alone consnetworkingthe issue kubernetes networkingdismissed fanboys fud kubernetes networking something continues complex often papered workflow layer abstraction complexity matter fact claimed entirely warranted acm article go explain whyall container running borg machine share host ip address borg assigns container unique port number part scheduling process container get new port number move new machine sometimes restarted machine mean traditional networking service dns domain name system replaced homebrew version service client know port number assigned service priori told port number embedded url requiring namebased redirection mechanism tool rely simple ip address need rewritten handle ip port pairswe run ten service hundred let alone thousand application certain exception bind advertise port consul single docker application run use host network foresee changing foreseeable futurelearning experience borg decided kubernetes would allocate ip address per pod thus aligning network identity ip address application identity make much easier run offtheshelf software kubernetes application free use static wellknown port eg http traffic existing familiar tool used thing like network segmentation bandwidth throttling management popular cloud platform provide networking underlay enable ipperpod bare metal one use sdn software defined network overlay configure routing handle multiple ip per machineagain setting sdn defining new definitely something would done absolutely called see need overheadincremental refactor infrastructureintroducing scheduler significant infrastructural change never rewrite appears golden rule many developer internalized good effect many infrastructure engineer seem qualm completely throwing caution wind meet people working startup tell smugly spent month dockerizing application run kubernetes cluster hard wonder time could put better use bad code rolled back illadvised infrastructural choice usually end riskysandi metz katrina owen book bottle oop propose blueprint refactoring claim good news able see abstraction advance find iteratively applying small set simple rule rule known flocking rule select thing find smallest difference make simplest change remove differencemaking small change mean get precise error message something go wrong useful know work level granularity gain experience begin take larger step take big step encounter error revert change make smaller one following flocking change one line run test every test fail undo make better changethe book introduces concept context refactoring ruby code feel translates equally well infrastructural refactors well make simplest infrastructural change thing alike remove pain pointa good rule thumb come adopting new tooling especially operation tooling pick one integrate seamlessly existing infrastructure require one make least number change already working luxury time resource dedicated personnel greenfield infrastructure beginning clean slate might able experiment option u possibility gone path golden rewrite couple time never gone well hoped firmly committed incremental refactorswe using consul well year developed many best practice well architectural pattern around use consul run gamut coordination task across stack load balancing separate category real time traffic different physical machine separate rack driving customer facing dashboard maintenance page update freeing frontend engineer hand backend maintenance importance versatility consul overstatedhaving operated consul year well aware failure mode consistency guarantee well edge case developed sound abstraction architectural pattern workaround replacing integral component stack along hardwon operational knowledge something company size could illafford gain brought scheduler making deployment lifecycle management process bit easier standardized expense requiring u replace consul worth effortin addition consul also use vault secret management going use scheduler needed one would require u make simplest change yet integrate well existing stack gain u benefit hoped reapthat scheduler nomad different cluster scheduler vary feature provide heuristic use make scheduling decision tradeoff make constraint impose flexible one used anything simple distributed cron replacement tool capable scheduling million container across ten thousand geographically distributed node feature rich one offer finegrained control expense operational complexitynomad key advantage scheduler space ease use ease operation paramount importance system especially come operation tooling certain overlap functionality provided two kubernetes primarily platform distributed containerized application happens scheduler built whereas nomad primarily flexible scheduler used build platform comprising hybrid deployment artifact docker container run standard docker tooling lxcs java jar binary running root without isolation whatsoever recommended via different immediate winsswitching nomad yielded immediate win way used deploy run application many win specific nomad point marked asterisk benefit scheduler would provided wellminimal change required existing stackadopting nomad require u change packaging format could continue package python docker build binary rest application also required u make little change application code boilerplate code deleted require u change way service discovery integrated well tool already using deployment codethe ability describe complex deployment policy hitherto indescribable flexible manner via declarative job file something wished year ago started hard see go back deploying application wayevery application deployed nomad job file life repo application codebase edited developer responsible building application job file let specify among thing following form number instance application want application need colocated physical resource limitation one might want set application process cpu ram network bandwidth port want bind application also choose let nomad pick port application bind thathaving common syntax allows developer specify operational semantics application build put onus operation service best suited make decision given performance characteristic business requirement applicationfantastic consul integrationnomad allows express service name want register application consul well tag want associate service config job file mean application developer application responsible registering consul registering subsequent health check deregistering shutting le code write fewer bug failure mode application nomad fantastic integration consul really shine come greatly simplifies graceful restarts simplified graceful restartsnomad allows every task specify killtimeout specifies duration wait application gracefully quit forcekilling nomad sends sigint task exit configured timeout sigkill sent taskapplications start query consul upstream service store information inmemory data structure application fire background coroutinegoroutine long poll consul change upstreams update inmemory data structure whenever information changeslet assume process running want deploy new version code let also assume two downstreams x one upstream z time deployment three open connection x one connection held open z might look something like following obviously extremely contrived example real system might ten hundred upstreams downstreams thousand bidirectional rpc call progress time deployment furthermore also possible four service x z might middle deployed time let work exampleduring deploy old process deregistered consul new process b registeredx learn b consul future request routed process b process since consul cp system might take though usually longer second change propagate x would continue routing request process need make sure long enough x start routing new request process bonce x got update start routing new request process b old connection open process might still progress mean exit finished responding request progress might look something like following completed servicing request still progress exit x continue routing future request bnomad make extremely easy deregisteres consul automatically brings b also send sigint point either handle signal exit nt get sigkilled killtimeout default killtimeout configured job file much higher batch job depending whether application shut need cleanup releasing resource file descriptor lock might acquired exiting application either choose handle signal ignore itif choses handle sigint exit accord stick around sometime even x stop routing request later however long timeout configured get sigkill leaving process b behind default probably way high value could tweaked based application characteristic like response time specific endpoint agreed upon sla applicationat edge use haproxy whose config get redrawn consultemplate zero downtime haproxy reloads slightly complicated described beyond scope post flexibilityscaling application count resource constraint becomes quite easy editing job file redeploying application leaving paper trail behind bad deploy requires hotfix rollback need edit job file revert application known good version deploy democratization operationthe job file look similar application run nomad making extremely easy look job file unfamiliar application instantly able tell deployedsince deployment uniform application push come shove could even deploy service primary owner reasonable amount confidence every engineer operation engineer familiar deployment greatly democratizes operation service across stacksimplicityit would remiss include ease use ease operation nomad win switching service run nomad never allconsuming project took month dedicated effort set probably time last one month spent learning nomad experimenting till gained enough confidence use productionthe simplicity made possible nomad fairly straightforward architecture exist group nomad server central pink node form node raft cluster leader responsible raft log replication among follower node surrounding central server run nomad client blue dot nomad schedule task node task could docker container java jar binary long running batch job nomad client communicate health node well information pertaining node resource utilization nomad server use information make scheduling decision communicate back client might bird eye view cover basic pain pointsnomad still young project use auxiliary service single one core service switched nomad currently plan immediate future either still early day u result far promising feature nomad lack would greatly benefit fromaclnomad currently lack form access control support workaround lot people resort run nomad behind another proxy case haproxy allows one specify policy access controlovercommitsome scheduler allow oversubscription borg paper state rather waste allocated resource currently consumed estimate many resource task use reclaim rest work tolerate lowerquality resource batch job whole process called resource reclamation estimate called task reservation computed borgmaster every second using finegrained usage resource consumption information captured borglet initial reservation set equal resource request limit allow startup transient decay slowly towards actual usage plus safety margin reservation rapidly increased usage exceeds itnomad vein borg colocates cpu memory intensive batch job physical host latency sensitive service lack quota priority oversubscription support make resource reclamation impossible problem something back mind next time think switching critical service conclusionmost u focus scale think scheduler problem scheduler born solve large scale computing become pivot around public discourse revolves google infrastructure everyone else make sense technology adapted solve real immediate problem faced organizationsthe application develop become vastly complex building decade ago even core business logic simple need feature additional integration robust data pipeline high reliability availability quality service guarantee customer satisfaction rapid innovation continuous deployment quick feedback loop constant iteration thrown clear relief importance standard reliable toolingthese requirement even pre seriesa startup day startup ai iot realm flavor month need collect process massive amount data becoming increasingly common problem across spectrum requiring standardized sophisticated tooling infrastructurein many way word scheduler much misnomer since scheduler make scheduling decision like physical host run specific application scheduler manage entire lifecycle application birth redeployment monitoring eventual decommissioningschedulers might initially seem like scary thing way pay grade engineering organization reality however scheduler game changer enormous step traditional way managing software life cycle flexibility afford immediate benefit provide overstated enoughgoing back conversation post began year ago would disagreed tweet today disagree different reason solely job operation engineer ensure reliable deployment application responsibility share developersschedulers superior replace traditional operation tooling abstraction underpinned complex distributed system ironically dint complexity make vastly simpler end user reason operational semantics application build scheduler make possible developer think operation service form code making possible truly get one step closer devops ideal shared ownership holistic software lifecyclewhich truly right way google u think software
284,Lobsters,scaling,Scaling and architecture,"How Discord Scaled Elixir to 5,000,000 Concurrent Users",https://blog.discordapp.com/scaling-elixir-f9b8e1e7c29b,discord scaled elixir concurrent user,five million concurrent user million event per second erlang anger message fanout people started using discord large scale group roverwatch benchmarking blog post manifold http githubcomdiscordappmanifold fast access shared data consistent hashing library chris moo ets converted code pure elixir mochiglobal http githubcomdiscordappfastglobal limited concurrency cascading service outage circuit breaker semaphore http githubcomdiscordappsemaphore conclusion,beginning discord early adopter elixir erlang vm perfect candidate highly concurrent realtime system aiming build developed original prototype discord elixir became foundation infrastructure today elixir promise simple access power erlang vm much modern userfriendly language toolsetfast forward two year nearly five million concurrent user million event per second flowing system regret choice infrastructure lot research experimentation get elixir new ecosystem erlang ecosystem lack information using production although erlang anger awesome follows set lesson learned library created throughout journey making elixir work discordmessage fanoutwhile discord rich feature boil pubsub user connect websocket spin session process genserver communicates remote erlang node contain guild internal discord server process also genservers anything published guild fanned every session connected itwhen user come online connect guild guild publishes presence connected session guild lot logic behind scene simplified example fine approach originally built discord group le however fortunate enough good problem arise people started using discord large scale group eventually ended many discord server like roverwatch concurrent user peak hour began see process fail keep message queue certain point manually intervene turn feature generated message help cope load figure became fulltime jobwe began benchmarking hot path within guild process quickly stumbled onto obvious culprit sending message erlang process cheap expected reduction cost erlang unit work used process scheduling also quite high found wall clock time single call could range due erlang descheduling calling process meant peak hour publishing event large guild could take anywhere erlang process effectively single threaded way parallelize work shard would quite undertaking knew better waywe knew somehow distribute work sending message since spawning process erlang cheap first guess spawn another process handle publish however publish could scheduled different time discord client depend linearizability event solution also scale well guild service also responsible evergrowing amount workinspired blog post boosting performance message passing node manifold born manifold distributes work sending message remote node pid erlang process identifier guarantee sending process call equal number involved remote node manifold first grouping pid remote node sending manifoldpartitioner node partitioner consistently hash pid using group number core sends child worker finally worker send message actual process ensures partitioner get overloaded still provides linearizability guaranteed solution effectively dropin replacement awesome sideeffect manifold able distribute cpu cost fanning message also reduce network traffic node network reduction guild nodemanifold available github give spin http githubcomdiscordappmanifoldfast access shared datadiscord distributed system achieved consistent hashing using method requires u create ring data structure used lookup node particular entity want fast chose wonderful library chris moo via erlang c port process responsible interfacing c code worked great u discord scaled started notice issue burst user reconnecting erlang process responsible controlling ring would start get busy would fail keep request ring whole system would become overloaded solution first seemed obvious run multiple process ring data better utilize machine core answer request however noticed hot path could better let break cost hot patha user number guild average user erlang vm responsible session live session itwhen session connects lookup remote node guild interested inthe cost communicating another erlang process using requestreply session server crash restart would take second cost lookup ring even account erlang descheduling single process involved ring process work could remove cost completely first thing people elixir want speed data access introduce ets ets fast mutable dictionary implemented c tradeoff data copied move ring ets using c port control ring converted code pure elixir implemented process whose job ring constantly copy ets process could read directly ets noticeably improved performance ets read still spending second looking value ring ring data structure actually fairly large copying ets majority cost disappointed language could easily shared value safe read way erlang research found mochiglobal module exploit feature vm erlang see function always return constant data put data readonly shared heap process access without copying data mochiglobal take advantage creating erlang module one function runtime compiling since data never copied lookup cost decrease bringing total time thing free lunch though cost building module data structure large ring runtime take second good news rarely change ring penalty willing takewe decided port mochiglobal elixir add functionality avoid creating atom version called fastglobal available http githubcomdiscordappfastgloballimited concurrencyafter solving performance node lookup hot path noticed process responsible handling guildpid lookup guild node getting backed inherent back pressure slow node lookup previously protected process new problem nearly session process trying stampede ten process one guild node making path faster solve problem underlying issue call session process guild registry would timeout leave request queue guild registry would retry request backoff perpetually pile request get unrecoverable state session would block request timed receiving message service causing balloon message queue eventually oom whole erlang vm resulting cascading service outageswe needed make session process smarter ideally even try make call guild registry failure inevitable want use circuit breaker want burst timeouts result temporary state attempt made knew would solve language would solve elixir language could use atomic counter track outstanding request bail early number high effectively implementing semaphore erlang vm built around coordinating communication process knew want overload process responsible coordination research stumbled upon performs atomic conditional increment operation number inside ets key since needed high concurrency could also run ets writeconcurrency mode still read value since return result gave u fundamental piece create semaphore library extremely easy use performs really well high throughput library proved instrumental protecting elixir infrastructure similar situation aforementioned cascading outage occurred recently last week outage time presence service crashed due unrelated issue session service even budge presence service able rebuild within minute restarting live presence within presence servicecpu usage session service around time periodyou find semaphore library github http githubcomdiscordappsemaphoreconclusionchoosing use getting familiar erlang elixir proven great experience go back start would definitely choose path hope sharing experience tool prof useful elixir erlang developer hope continue sharing progress journey solving problem learning lesson along waywe hiring come join u type stuff tickle fancy
285,Lobsters,scaling,Scaling and architecture,"Code Running on Ionized Particles, or WTF is Serverless",https://www.clever-cloud.com/blog/company/2017/07/04/code-running-ionized-particles-wtf-serverless/,code running ionized particle wtf serverless,serverless tm serverless major performance tradeoff better server management already happened without serverless conclusion term condition,often say technology cycle concept think bad idea hard kill rebranded regular basis hyped marketing bullshit today talk serverless tm serverless quite interesting case serverless application server probably many server running code server trying suppress definitely hardware use power application sorry think run nodejs application live air floating wind electron running without cpu ram question thing positively want suppress serverless world server serverless major performance tradeoff serverless function service hosting model upload small piece code sometimes data service get instant opsfree ability answer url previously uploaded code response server boot installation management everything automated developer code publish code code started respond sandboxed environment lifecycle bound request life destroying context http request vast majority case bad performance stability idea get fascination requestbound lifecycle application environment first time shown great achievement myth short lived container single let explain thought vast majority application see today made accept connection internet using http mainly others protocol get data database compute serve ability reach database quickly main performance focus latency still thing tell dbaas vendor nt let run code next data connecting database involves vast quantity tcp protocol point establishing tcp connection long slow many case time consuming part request want keep database connection several web request keep tcp pool awake want destroy pool every web request database server also work better stable connection much easier monitor audit built paradigm yes also keep tcp connection open working database http based api take look rust client wrote anyway destroying context http request vast majority case bad performance stability idea better server management already happened without serverless year ago consensus around factor application made application able launch able take care lifecycle model reaction application server model really difficult scale make business logic configuration spliced many environment one place serverless actually use application server closed source application server strong locking model code produced lock provider interportability illusion something like jee application server portability except one install server even bigger price making easy deploy provide sustainability application really important help developer focus value innovation provide quick feedback loop serverless name removing hardware server server management time consuming coordination managing server work agree created clever cloud provide application sustainability ability easily deploy sure future managed update monitoring self healing scalability security whole server management commodity deliver clever cloud think compatible application new legacy impact application architecture portability automation great idea help people send code leave management software solves timing synchronization constraint people give ability innovate build adapt software market easily someone say like serverless say want hear server ops stuff work conclusion create code sure writing someone else paper hurt sooner later software industry quite young year old yet massive company emerged quickly billion dollar business bringing huge social change mean developer today job evolve last year think need picture global shift within evolution productivity image source disney free signup feel coding productivity brand new project scale clicking get started agree clever cloud term condition
286,Lobsters,scaling,Scaling and architecture,Scaling a Web Service: Load Balancing,http://blog.vivekpanyam.com/scaling-a-web-service-load-balancing/,scaling web service load balancing,load balancer note service layer load balancing layer load balancing layer load balancing dns load balancing manual load balancingrouting anycast misc information talk primer talk talk comment hacker news email bob mical,post going look one aspect site like facebook handle billion request stay highly available load balancing load balancer load balancer device distributes work across many resource usually computer generally used increase capacity reliability order talk load balancing general way make two assumption service scaled start many instance want request go instance first assumption mean service stateless shared state something like redis cluster second assumption necessary practice thing like sticky load balancing assume sake simplicity post load balancing technique going talk layer load balancing http http w layer load balancing tcp udp layer load balancing dns load balancing manually load balancing multiple subdomains anycast miscellaneous topic end latency throughput direct server return technique vaguely ordered step take site get traffic example layer load balancing would first thing much earlier anycast first three technique help throughput availability still single point failure remaining three technique remove single point failure also increasing throughput help u understand load balancing going look simple service want scale note scaling technique accompanied nontechnical analogy purchasing item store analogy simplified representation idea behind technique may entirely accurate service let pretend building service want scale might look something like system wo nt able handle lot traffic go whole application go analogy go checkout line store purchase item nt cashier ca nt make purchase layer load balancing first technique start handling traffic use layer load balancing layer application layer includes http http websockets popular battletested layer load balancer nginx let see help u scale note actually load balance across ten hundred service instance using technique image show two example analogy employee store directs specific checkout line cashier purchase item note also terminate ssl layer load balancing previous technique help u handle lot traffic need handle even traffic layer load balancing could helpful layer transport layer includes tcp udp popular layer load balancer haproxy layer load balancing well ipv let see help u scale layer load balancing layer load balancing able handle enough traffic case however still need worry availability single point failure layer load balancer fix dns load balancing section analogy separate checkout area people based membership number example membership number divisible two go checkout near electronics otherwise go one near food get right checkout area employee store directs specific checkout line purchase item layer load balancing need scale even probably add layer load balancing complicated first two technique layer three network layer includes layer load balancing could look understand work first need little background ecmp equal cost multi path routing ecmp generally used multiple equal cost path destination broadly allows router switch send packet destination different link allowing higher throughput exploit load balancing perspective every load balancer mean treat link load balancer load balancer path destination give load balancer ip address use ecmp split traffic amongst load balancer analogy two separate identical store across street one go depends dominant hand get right store separate checkout area people based membership number example membership number divisible two go checkout near electronics otherwise go one near food get right checkout area employee store directs specific checkout line purchase item usually done hardware topofrack switch tl dr unless running huge scale hardware nt need dns load balancing dns system translates name ip address example may translate examplecom also return multiple ip address shown multiple ip returned client generally use first one work however implementation look first returned ip many dns load balancing technique including geodns roundrobin geodns return different response based request let u route client server datacenter closest roundrobin return different ip request cycling available ip address multiple ip available technique change ordering ip response dns load balancing would work example different user routed different cluster either randomly based location nt single point failure assuming multiple dns server even reliable run multiple cluster different datacenters analogy check online list shopping complex store operates list put closest shopping complex first look direction one go first open one list two separate identical store across street one go depends dominant hand get right store separate checkout area people based membership number example membership number divisible two go checkout near electronics otherwise go one near food get right checkout area employee store directs specific checkout line purchase item manual load balancingrouting content sharded across many server datacenters need route specific one technique could helpful let say catjpg stored cluster london cluster similarly let say dogjpg stored nyc datacenters cluster might happen content uploaded nt replicated across datacenters yet example however user nt wait replication complete order access content mean application need temporarily direct request catjpg london request dogjpg nyc instead http cdnexamplenetcatjpg want use http similarly dogjpg need set subdomains datacenter preferably cluster machine done addition dns load balancing note application need keep track content order rewriting analogy call corporate office asking location carry cat food look direction location go first open one list two separate identical store across street one go depends dominant hand get right store separate checkout area people based membership number example membership number divisible two go checkout near electronics otherwise go one near food get right checkout area employee store directs specific checkout line purchase item anycast final technique post anycast first little background internet us unicast essentially mean computer get unique ip address another methodology called anycast anycast multiple machine ip address router send request closest one combine technique extremely reliable available system handle lot traffic anycast basically allows internet handle part load balancing u analogy tell people trying go store direct closest location two separate identical store across street one go depends dominant hand get right store separate checkout area people based membership number example membership number divisible two go checkout near electronics otherwise go one near food get right checkout area employee store directs specific checkout line purchase item misc latency throughput aside technique also work increase throughput low latency service instead trying make service handle traffic add way lowlatency highthroughput system direct server return traditional loadbalancing system request go layer load balancing response return layer well one optimization offload lot traffic load balancer direct server return mean response server nt go back load balancer response service large useful tool information facebook google netflix large internet company use technique scale great talk resource patrick shuff gave talk explaining facebook us technique handle billion user cloudflare brief primer anycast artur bergman ceo fastly gave talk scaled cdn dave temkin director global network netflix gave talk scaled netflix cdn please feel free comment hacker news email question want notified whenever publish new blog post follow twitter disclaimer although mention facebook google post none content included based nonpublic information random trivia post written february forgot publish header image bob mical
287,Lobsters,scaling,Scaling and architecture,Serving 39 Million Requests for $370/Month on AWS Lambda,https://trackchanges.postlight.com/serving-39-million-requests-for-370-month-or-how-we-reduced-our-hosting-costs-by-two-orders-of-edc30a9a88cd,serving million request aws lambda,extracting content chaos web aws lambda api gateway serverless framework released mercury web parser step go serverless simply moving serverless environment single greatest impact reducing hosting cost break step lower memory allocation decrease memorysize allocation serverless config lambda pricing calculator step cache api gateway response note scaling final cost breakdown api gateway lambda total cost bullish serverless previously required server spend hour configuring environment serverless boilerplate prettier mercury web parser wild mercury amp converter mercury reader bloomberg lens,joined postlight engineer last year first task big one rewrite readability parser api unfamiliar readability parser api powered popular readability readitlater app along many service apps across web parser api accepted link article internet returned structured representation article extracting content chaos web readability postlight different story reason rewrite threefold groundbreaking readability api grown old somewhat brittle year parsed result stored database meant database grown store massive slice internet next impossible perform slightly complex query meant little idea happening api also meant api response contain updated result original article changed service originally written maintained people longer company new engineer like lacked domain knowledge necessary update fix api nontrivial way last probably significantly free readability api costing company roughly per month rewrite goal produce functionally equivalent library would return better result original write something well tested extensible language postlight engineer could easily contribute reduce monthly cost language settled javascript choosing javascript meant new library could theory run server browser every engineer postlight least web experience choosing javascript also meant nearly every one could contribute mercury readability written python many postlight engineer write beautiful python lacked inbrowser benefit finally focused cost answer simple drastically reduce cost chose serverless architecture running aws lambda api gateway built deployed using serverless framework last october released mercury web parser result astounding cost dropped immediately today mercury web parser cost around per month operate roughly two order magnitude le cost operate readability api mileage need course vary previous cost also included database expense chose forego serverless setup opting instead shortterm caching breakdown step go serverless adopting serverless architecture dramatically decrease cost apis assuming meet need service serverless environment say simply moving serverless environment single greatest impact reducing hosting cost made switch hard work done extremely expensive operating cost immediately shrunk two order magnitude even initial serverless switch still room cut cost experience lambda pricing break like lambda free tier includes free request per month gbseconds compute time per month memory size choose lambda function determines long run free tier lambda free tier automatically expire end month aws free tier term available existing new aws customer indefinitely looking optimize cost lambda important thing note apart incredible fact function le million shortrunning invocation per month free indefinitely memory size allocated function determines cost run function cost increase decrease proportionally time take execute function note gbseconds calculation function execution time per gb memory allocated function example invoke function one time second allocated function executed compute time decreased allocation execution time remained second cut compute time half mean reduce lambda cost either speeding time take execute function lowering memory allocation since already feeling good execution speed memory obvious first step step lower memory allocation memory allocation lambda function range process lowering lambda cost simple incrementally decrease memorysize allocation serverless config using serverless directly lambda dashboard deploy keep eye function latency requires setting cloudwatch dashboard function topic another post function latency show significant change hour day week keep change enjoy decreased cost remember time halve function memory allocation roughly halving lambda cost example according official lambda pricing calculator million invocation excluding free tier running average second fast particularly slow memory would cost make translates mercury web parser currently running function well step cache api gateway response make sense service caching response api gateway significantly cut lambda invocation mercury user example often request result article pay around month api gateway cache hour ttl last month api request served cache meaning le half request required invoking lambda function save u around month lambda cost note scaling may immediately obvious apart cost saving serverless architecture also come considerable decrease maintenance configuration complexity api could serving request request using configuration cache deploy method additionally service scale handle request cost necessarily increase much remember half api request served cache far percentage increased api used final cost breakdown current cost running mercury web parser api optimization look like api gateway cost request served per million api call received cache data transfer calculated rate lambda request cost compute cost averaging memory allocation total cost bullish serverless experience serverless architecture particular using serverless framework aws incredible used lambda everything parsing web writing slack bot batch resizing hundred thousand photo parallel batch transcoding hundred thousand video use confident serverless approach solving lot problem previously required server date mercury web parser tool power best validation decision go serverless possible sidenote like give serverless try want spend hour configuring environment try serverless boilerplate prepacked modern javascript transpiling sensible linter code formatting prettier mercury reader chrome extension cleaner reading experience mercury web parser wild postlight used mercury web parser power mercury amp converter tool make web site google ampready one line code mercury reader chrome extension used million user click button remove ad distraction article leaving text image beautiful reading view site bloomberg lens chrome extension io share sheet application present related news company data person information article web also supporting thousand developer use mercury every day extract structured content article web mentioned start amount million requestsmonth cost u little le buck every million request scale effort happier number
288,Lobsters,scaling,Scaling and architecture,Instagram Makes a Smooth Move to Python 3,https://thenewstack.io/instagram-makes-smooth-move-python-3/,instagram make smooth move python,instagram python django python hui ding lisa guo instagram started python first place hui ding mike krieger issue led instagram consider new stack ding lisa guo ding point team decided move python ding guo push process look like guo report thing went really smoothly team make happen seamlessly guo ding python performed since ding uswgi last word advice engineer considering migrating python guo ding guo ding,day million photo video shared instagram unstoppable photocentric social medium platform million registered user million active every day talk operating scale instagram kill level company barely even dream even impressive though fact instagram serf incredible amount traffic reliably steadily running python little help django hood yes python easy learn jackofalltrades general purpose programming language one everybody industry dismisses yeah python great many way bad really scalable ahem four hundred million user per day instagram scaled become biggest python user world company recently moved python zero user experience interruption instagram engineer hui ding lisa guo talked new stack share python love describe python migration experience instagram started python first place hui ding joined instagram first engineer hired acquisition instagram founded bought facebook since gone six engineer beginning worked closely instagram cofounder mike krieger early day still small team lot context history chose python reason consistent instagram engineering motto simple thing first python userfriendly engineer easy get speed get product allowing team concentrate user facing feature python simple clean favor pragmatism proven technology finally really popular language make growing engineering team easier issue led instagram consider new stack ding grew became obvious python fastest language mean aws made easier throw machine needed grow point diminishing return certain point resource going performance regression user growth within three five year looking estimated one billion user joining community time came consider option first question would high enough return justify instagram user growth rising steadily fast server growth lisa guo faced specific challenge increasing network io activity server needed parallel way process user request realistically php python bestsupported ecosystem facebook going platform would require learning curve whole lot new training engineer portal typing comparison php used facebook web server ding seen order magnitude faster performance improvement would changed ultimately gain simply number compelling lot tool investment python already able get hundred million user pythondjango stack decided would continue also significant decision engineer really love python actually reason people want come work u point team decided move python ding decision time invest version language mature going anywhere language next version great growing community support made sense going stay python next ten year invest latest version language shortly decided push python announced would longer supported performance speed longer primary worry time market speed hui ding guo three major motivation behind push python first python traditionally typed language would lot development conflict started working new piece code big motivation u announcement python would start support typing devs excited news second networking increasingly bottleneck u third python fast newer version continues get faster runtime nobody working make faster latest release version community working also get contribute back push process look like guo overall took ten month different stage first team undertook massive code modification took two three month included replacing incompatible thirdparty package one supporting python working rule python new package also deleting unused package unit testing took two month undertook slow steady rollout four month early february completely running python instagram infrastructure engineering team took total month complete migration python report thing went really smoothly team make happen seamlessly guo important factor look migrating would constantly check small change master branch never merging major diffs getting bug fixed smaller audience iterating small step key approach maintaining stability still moving quickly ding pitfall given incompatibility two version key taking time upfront thoroughly scope solution respect problem started profiling getting clear understanding potential benefit tradeoff scope problem really well make sense said simple thing first mean move slow take risk handoff seamless transition python python interruption user experience python performed since ding performance gain expectation python right box pleasant surprise see percent cpu saving uswgidjango percent memory saving celery four month since rollout expect see constant percent performance improvement promising start one common mechs use thrift written python team working facebook make serialization faster last word advice engineer considering migrating python guo query get convince manager python hard sell given python reputation slow efficiency work specialty python efficiency big draw efficiency one place push back worry speed ding performance speed longer primary worry time market speed guo advice start small module show benefit get people excited changing ding people industry company still struggling migration reached asking well exactly lisa said small step approach took drive migration made go well approach made instagram world number one visual social platform
289,Lobsters,scaling,Scaling and architecture,Understanding Serverless Architectures,https://speakerdeck.com/dawny33/understanding-serverless-architectures,understanding serverless architecture,fewer faster,copyright fewer faster llc slide content description owned creator
290,Lobsters,scaling,Scaling and architecture,Calado's Microservices Prerequisites,http://philcalcado.com/2017/06/11/calcados_microservices_prerequisites.html,calado microservices prerequisite,watch video recording read slide bother lot unless medium large fleet think microservices start architecture simplest thing could possibly work prerequisite older presentation adoption microservices soundcloud martin fowler work microservices prerequisite second largescale microservices implementation time digitalocean rapid provisioning compute resource discussed term welldefined lot lot first joined digitalocean control plane team tasked fixing problem decided use container kubernetes new compute platform monitoring alerting offering basic monitoring toolbox around challenge documented something critical dealing fastpaced change mttr important mtbf hammond think prevent failure developing ability respond mean time failure mtbf mean time repair mttr system constant change control mtbf better invest great mttr prefer basic telemetry across whole microservices fleet lot detailed telemetry core service checking recent change became first step incident detection workflow rapid deployment single change single feature might require deploying many service first described soundcloud engineering blog squashfs procfile easy provisioning storage database refactorings easy access edge bff pattern authenticationauthorisation user urn finagle standardised rpc service mesh linkerd acknowledgement andy palmer dave cameron mike robert daniel bryant rafael ferreira vitor pellegrino andrew kiellor marcos tapajs douglas campos carlos villela,may gave talk craft conf budapest focusing economics microservices watch video recording read slide talk briefly discussed set proposed prerequisite microservices thing believe place considering widespread adoption architecture style since presentation list referenced work distributed system space want use post expand prerequisite bother decide adopt microservices explicitly moving away one moving piece complex system new world many moving part act unpredictable way team service created changed destroyed continuously system ability change adapt quickly provide significant benefit organisation need make sure guard rail place delivery come standstill amidst neverending change guardrail prerequisite discus possible successfully adopt new technology without place presence expected increase probability success reduce noise confusion migration process admittedly list prerequisite presented long depending organisation culture infrastructure might require massive investment upfront cost expected though microservices architecture supposed easier style need make sure ass return investment making decision lot indeed could try put mind ease tell required smaller fleet service unless medium large fleet think microservices start architecture simplest thing could possibly work need sophisticated even mature answer prerequisite stated even mature company like digitalocean soundcloud started basic implementation beginning lot exploration boatload copy paste make sure working answer item ob answer today need longterm solution learn go time technology space maturing thing becoming commodity offtheshelf another option forget microservices focus next iteration architecture around coarse grained service fewer moving part definitely reduce prerequisite substantially always keep reducing size scope service engineering organisation platform matures prerequisite first discussed older presentation adoption microservices soundcloud really appreciate martin fowler work microservices prerequisite list martin fellow thoughtworkers compiled go like soundcloud started migration towards architecture martin work available pretty much arrived conclusion moved second largescale microservices implementation time digitalocean confirmed need item time identified item missing proven crucial successful microservices adoption easy provisioning storage easy access edge authenticationauthorisation standardised rpc full list microservices prerequisite priority order follows rapid provisioning compute resource basic monitoring rapid deployment easy provisioning storage easy access edge authenticationauthorisation standardised rpc rapid provisioning compute resource martin say able fire new server matter hour naturally fit cloudcomputing also something done without full cloud service able rapid provisioning need lot automation may fully automated start serious microservices later need get way us word server day could using actual server virtual machine container function combination thing added compute resource item pretty much mean anything give cpu memory run code ten year ago used deploy service application application server large software layer multiplexed single compute unit many application service could use time deployment architecture norm many year back serving hundred request per second considered internetscale design allowed organisation maximise utilisation expensive hardware across different even providing multitenant service different company share expensive application server time cost compute resource decreased drastically onpremise offered cloud reduced need application server layer even application server provided outofthebox service application deployed thing like automatic security service discovery administrative panel etc operating ever complicated server became expensive top traffic increased moved vertical horizontal scalability something product never good supporting force led u common deployment architecture used day relationship instance service compute resource relationship directly impact microservices architecture discussed term microservices welldefined one thing absolutely sure someone say word lot small service given deployment architecture described also mean lot compute unit creates need automatic fast elastic provisioning compute unit serve demand microservices first joined digitalocean spent lot time team thinking internal system control plane cloud back composed mostly three different monolith running fixed set virtual machine defined via chef databags quickly became clear u convoluted errorprone workflow scale improve provisioning situation migrating microservices team tasked fixing problem decided use container kubernetes new compute platform spent first six month making sure new service deployed new system migrating legacy monolith step enabled u move ahead architecture change still working many new product release fact monitoring alerting offering first product developed new system acted tracer bullet shaping backlog priority platform team basic monitoring martin say many looselycoupled service collaborating production thing bound go wrong way difficult detect test environment result essential monitoring regime place detect serious problem quickly baseline detecting technical issue counting error service availability etc also worth monitoring business issue detecting drop order sudden problem appears need ensure quickly rollback mentioned microservices architecture complex system much control predict lot chaos driven state continuous change service deployed redeployed many time day turn problem exclusive microservices fact john allspaw others building toolbox around challenge almost decade working monolithic architecture flickr etsy work allspaw documented something critical dealing fastpaced change put another way mttr important mtbf type f definitely saying failure acceptable condition positing since failure happen important case important spend time energy response failure trying prevent agree hammond said think prevent failure developing ability respond mean time failure mtbf elapsed time failure system operation mean time repair mttr average time required fix problem operation simplified term mtbf tell often failure happen mttr tell quickly problem solved detected system constant change control mtbf better invest great mttr start investing reducing mttr start realising way often reducing recovery quickly arrives diminishing return timetorecovery step incident management sometimes take longest chunk time spent seen painful part incident management mean time detection mttd metric reflects time elapsed incident happening operator detecting trigger recovery process make realise need invest telemetry quickly detect problem although need exists architecture style microservices add different challenge monolithic architecture always know problem obviously monolith left find class function problem life world sophisticated tool like newrelic help go code level detect problem tool also widely used microservices architecture helpful detected service service behaving unexpected way many service collaborate fulfil request also need make sure compare service allows pinpoint outlier get distracted environmental issue prefer basic telemetry across whole microservices fleet lot detailed telemetry core service soundcloud experience monitoring microservices led u focus standardised dashboard alert made sure every single service exported common set metric granularity used build dashboard first graphite eventually prometheus allowed u compare metric across different service dashboard provided u insight needed reduce mttd quickly realised enough hundred service deployed time dozen small team need able correlate potential issue change deploys new code change infrastructure case built small service returned feed change made engineer automated tool changed deployment tooling make sure every change even scaling service adding one two instance would reported feed checking recent change became first step incident detection workflow rapid deployment martin say many service manage need able quickly deploy test environment production usually involve deploymentpipeline execute couple hour manual intervention alright early stage looking fully automate soon martin proposes item direct followup previous one state speedy recovery incident likely require deployment new code configuration deploys quick deterministic possible wholeheartedly agree another fundamental driver prerequisite directly related incident response single monolith ok cumbersome manual deployment process even costperdeploy high regarding step operator need take risk mistake end often pay deployment usually contain many change impacting various feature developed different people team microservices becomes way around single change single feature might require deploying many service perform many deployment different service important one deployment inexpensive low risk martin mentioned build pipeline tend fit bill perfectly prerequisite struggled soundcloud monolith deployed using capistrano shell script long interactive process file containing instruction deployment complicated many corner case often referred taxcodemd first described soundcloud engineering blog beginning decided service could written language runtime team felt comfortable strategy several advantage amongst disadvantage could make assumption application deployed jar file ruby script go everything minimum common denominator across code base standardised every service makefile sitting root service code directory script build target even invoke another build system like sbt rake make command finished deployment tooling would create squashfs artefact containing everything directory including code asset generated binary code also include herokustyle procfile described run process squashfs image deployed operator scale updown version process way heroku process allowed u scale dozen service number required manual step high introduces risk make worse lowerlevel primitive directly support interesting deployment technique like bluegreen deployment canary server even ab testing issue team ended building glue code top provided tooling script seen sideprojects code quality varied drastically couple big production incident caused defective script grew dozen close hundred service invested better tooling deployment biggest difference moved away deploying engineer laptop build pipeline started using jenkins eventually moved thoughtworks gocd heavy automation led u deterministic faster build exactly one need number deployment per day go one hundred easy provisioning storage company coming monolith microservices single large wellmaintained database server many year single storage data database setup usually welltuned many replica wellintegrated system like search engine data analytics tool many challenge using monolithic database though related update schema changing removing table column require manually making sure code either via programming metaprogramming relies old structure year every classic database refactorings applied monolith inhouse tool written common one nevertheless still common team adopting microservices reuse shared schema one extra tablecolumnview big deal think engineer slowly murdered million paper cut top change management overhead described bringing velocity also join away data coupling service never know internals main reason trend company migrating microservices organisation tend invest lot provisioning deployment forget offer reasonable way storage system team rely even spinning mysql server take second many item one must pay attention making isolated system anywhere close productionready replica backup security tuning telemetry several aspect matter lot often engineer zero experience setting owning database system working cloudnative architecture one many databaseasaservice offering allow outsource operational task vendor cloud provider like digitalocean infamous people computer mediumterm plan provide quickandeasy provisioning mysql database internal use first step unblock move microservices much le ambitious instead building sophisticated tooling getgo invested time cleaning documenting standardised chef cookbook related script would make possible team spinup productiongrade mysql server without much hassle easy access edge first microservice organisation usually written isolation single person small team investigating approach solution challenge experience scope service usually small common author get working version development test environment time get closer making production engineer face question expose new thing external user outside local network similarly challenge around database main issue prior moment nobody company think problem monolith exposed user likely whole public internet many year sort feature protect internal vpn malicious mistaken user ratelimiting logging featureflags security telemetry alert request reason common strategy first microservice seems exposing directly internet either different hostname special path technique relies client often mobile singlepage application combine result request multiple endpoint work well enough one service model tends break service added client code become increasingly complicated exposing service public internet simple task take customer user seriously need make sure internetfacing system deal sort incident malicious user unexpected peak traffic every single service deal increase costperservice considerably exact scenario first service soundcloud hundred million user quickly became clear needed limit service exposed internet thought building gateway bind service small engineering team lot product feature deliver needed intermediate solution case started using monolith gateway request would first hit monolith would call service background strategy worked well next service several issue first problem detected created weird coupling new service monolith changing service would often require change redeploy monolith top monolith running old version rail without good concurrency rely serialising request new service request time increasing every new service added time invested proper gateway introduced time migrated bff pattern followed similar path digitalocean three monolith instead one migrated edge gateway much earlier authenticationauthorisation another important component usually think first microservices getting close production microservice know making request kind permission nave approach problem microservice require user identifier part request made check user authorisationauthentication system database might good enough single service service add redundant expensive call making authorisation system soundcloud using monolith edge gateway already memory information user could changed http client code always pas information header http request coming monolith downstream service moved edge gateway approach decided component would make one request authentication service forward user urn also geolocation information oauth scope available request every call made downstream music industry allowed access depends country happen much sophisticated setup possible migrated internal service use internal sdk microservices based finagle standardised rpc last point le contentious nevertheless important component architecture collaborate lot unpredictable way need make sure talk mean able understand way byte sent wire also convention standard place soundcloud obvious choice initial service http json unfortunately saying use http json actually buy much format tell send authorisation information pagination tracing architectural style rpc use handle failure etc also started suffering performance issue heavily textual protocol dataintensive team moved use thrift company moving heavily distributed architecture today would suggest grpc internal rpc top every time need serialise message say post bus like kafka use protocol buffer serialisation protocol across push pull use case grpc protocol buffer offer everything need soundcloud digitalocean staff team solely focused building tooling around rpc microservices something company afford day interesting solution concept service mesh dedicated infrastructure layer making servicetoservice communication safe fast reliable longtime finagle user favourite player game linkerd several option space acknowledgement andy palmer dave cameron mike robert daniel bryant rafael ferreira vitor pellegrino andrew kiellor marcos tapajs douglas campos carlos villela gave feedback draft article
291,Lobsters,scaling,Scaling and architecture,Cloud Native Landscape Project,https://github.com/cncf/landscape,cloud native landscape project,cloud native landscape cncf redpoint venture amplify partner http githubcomcncflandscapeapp trail map interactive version landscapecncfio new entry cloud native landscapeyml logo http cloudconvertcom nt use svgs embedded text logo grpc logo filetype svg nt use svgs embedded text direction correction landscapeyml processedlandscapeyml crunchbase external data landscapeyml crunchbase data processedlandscapeyml file best practice badge http bestpracticescoreinfrastructureorg without graduate nonupdated item http landscapecncfiocategoryprovisioning runtime orchestrationmanagement appdefinitionanddevelopment paascontainerservice serverless formatcardmode groupingno licenseopensource sortlatestcommit http landscapecncfiocategoryprovisioning runtime orchestrationmanagement appdefinitionanddevelopment paascontainerservice serverless observabilityandanalysis formatcardmode groupingno licensenotopensource sortlatesttweet license crunchbase http datacrunchbasecomdocsterms landscapeyml creative common attribution license format installation install direction landscapeyml vulnerability reporting issue info cncfio adjusting landscape view,cloud native landscape cncf cloud native landscape project intended map previously uncharted terrain cloud native technology attempt categorize project product offering cloud native space many route deploying cloud native application cncf project representing particularly welltraveled path built collaboration redpoint venture amplify partner software interactive landscape extracted http githubcomcncflandscapeapp used landscape well repo includes data image specific cncf landscape trail map cloud native trail map provides overview enterprise starting cloud native journey interactive version please see landscapecncfio new entry cloud native project least github star clearly fit existing category generally included put project single category best fit generally list company product one box represent major bestknown offering occasionally make exception large company note allowed listing product project multiple box logo landscape would multiply many time many unlikely create new category productsprojects rather find best home current option generally including commercial version open source software exception showing certified kubernetes implementation closed source product need link clear description product stealth mode company project company need logo logo need include name crunchbase organization company organization control software normally owner trademark whether trademark formally filed think company project included please open pull request add landscapeyml logo either upload svg hostedlogos directory put url value fetched netlify generate staging server preview update please check logo information appear correctly add lgtm pull request confirming review requesting merge logo following rule produce readable attractive logo require svgs smaller display correctly scale work modern browser logo another vector format like ai eps please open issue convert svg often http cloudconvertcom note may need zip file attach github issue please note require pure svgs reject svgs contain embedded pngs since problem bigger scaling seamlessly also require svgs convert font outline render correctly whether font installed see nt use svgs embedded text multiple variant exist use stacked horizontal logo example use second column stacked first horizontal cncf project logo nt use reversed logo ie nonwhite nontransparent background color reversed logo create issue attached produce nonreversed version logo must include company product project name english fine also include word another language nt version logo name please open issue create one please specify font match item name english word logo acme rocket logo show rocket product name rocket logo show acme rocket product name acme rocket otherwise logo look place sort alphabetically logo include company andor product name tagline allows larger readable exception format logo ever shown includes tagline google image often best way find good version logo ensure uptodate version search grpc logo filetype svg substitute project product name grpc either upload svg hostedlogos directory put url value fetched nt use svgs embedded text direction fixing correction please open pull request edits landscapeyml file processedlandscapeyml generated never edited directly error data crunchbase open account edit data nt like project description edit github project nt showing license correctly may need paste unmodified text license license file root project github order github serve license information correctly external data canonical source data landscapeyml day download data project company following source project info github funding info crunchbase market cap data yahoo finance cii best practice badge data update server enhances source data fetched data save result processedlandscapeyml json file latter app load display data best practice badge explained http bestpracticescoreinfrastructureorg linux foundation lf core infrastructure initiative cii best practice badge way freelibre open source software floss project show follow best practice project voluntarily selfcertify cost using web application explain follow best practice cii best practice badge inspired many badge available project github consumer badge quickly ass floss project following best practice result likely produce higherquality secure software interactive landscape display status nonexistence badge opensource project also feature available filter bar see item without badge note passing badge requirement project graduate cncf nonupdated item generally remove open source project commit month note project hosted github need mirror github fetch update try work project mirror broken view project sorted last update ignoring category like kcsps certified kubernetes member http landscapecncfiocategoryprovisioning runtime orchestrationmanagement appdefinitionanddevelopment paascontainerservice serverless formatcardmode groupingno licenseopensource sortlatestcommit generally remove closed source product tweeted month nt apply chinese company without twitter account since twitter blocked view product sorted last tweet ignoring category like kcsps certified kubernetes member http landscapecncfiocategoryprovisioning runtime orchestrationmanagement appdefinitionanddevelopment paascontainerservice serverless observabilityandanalysis formatcardmode groupingno licensenotopensource sortlatesttweet item removed apply readded using regular new entry criterion license repository contains data received crunchbase data licensed pursuant apache license subject crunchbase data access term available http datacrunchbasecomdocsterms permitted used linux foundation landscape project everything else apache license version except project product logo generally copyrighted company created simply cached reliability trail map static landscape serverless landscape landscapeyml file alternatively available creative common attribution license format cncf trail map available format cncf cloud native landscape available format cncf serverless landscape available format installation install run locally install direction necessary install locally want edit landscapeyml via github web interface vulnerability reporting please open issue sensitive information email info cncfio adjusting landscape view file describes key element landscape big picture specifies put section app definition development orchesteration management runtime provisioning cloud platform observability analyzis special also specifies locate link serverless preview info qr code element top left width height property position row col specify much column row expect given horizontal vertical section see element fit section need either increase width horizontal section increase height amount row single horitzontal section adjust position section beside adjust width parent div width srccomponentsbigpicturefullscreenlandscapejs width toolsrenderlandscapejs zoom margin serverless approach file srccomponentsbigpictureserverlesscontentjs srccomponentsbigpicturefullscreenserverlessjs toolsrenderlandscapejs full width zoom margin sometimes total height changed need adjust height way adjust width experimental fitwidth property good want get rid extra space right section best way test layout ok visit landscape serverless look ok run babelnode toolsrenderlandscape see rendered png file srcimages folder
292,Lobsters,scaling,Scaling and architecture,"C++Now 2017: Juanpe Bolivar Postmodern Immutable Data Structures""",https://www.youtube.com/watch?v=ZsryQp0UAC8,cnow juanpe bolivar postmodern immutable data structure,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature cnow juanpe bolivar postmodern immutable data structure youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature cnow juanpe bolivar postmodern immutable data structure youtube
293,Lobsters,scaling,Scaling and architecture,The Evolution of Code Deploys at Reddit,http://redditblog.com/2017/06/02/the-evolution-of-code-deploys-at-reddit/,evolution code deploys reddit,neil williams saurabh sharma constantly deploying code reddit rollingpin career page rprogramming,neil williams saurabh sharma uspladug ufoklepoint constantly deploying code reddit every engineer writes code get reviewed check roll production regularly happens often time week deploy usually take fewer minute endtoend system power evolved year let take look changed time story start consistent repeatable deploys seed current system perl script called push written time different reddit history entire engineering team small enough fit one small conference room reddit yet aws site ran fixed number server additional capacity added manually comprised one large monolithic python application called one thing changed year request classified load balancer assigned specific pool otherwise identical application server example listing comment page served separate pool server given process could handle kind request individual pool isolated spike traffic pool fail independently different dependency push tool hardcoded list server built around monolith deploy process would iterate application server ssh machine run preset sequence command update copy code server via git restart application process essence heavily distilled real code build static file put static server make c homeredditreddit static rsync homeredditredditstatic public varwww iterate app server update copy code restarting done foreach h hostlist git push h homeredditreddit master ssh h make c homeredditreddit ssh h binrestartredditsh deploy sequential worked way server one one simple sound actually good thing allowed form canary deploy deployed server noticed new exception popping know introduced bug could abort ctrlc revert affecting request ease deploys easy try thing production low friction revert work also meant necessary one deploy time ensure new error deploy one easier know revert great ensuring deploys consistent repeatable ran pretty quickly thing good bunch new people hired bunch growing six whole engineer fit somewhat larger conference room started feel need better coordination around deploys particularly individual working home modified push tool announce deploys started ended via irc chatbot bot sat irc announced event process actually deploy looked system work told everyone beginning u using chat deployment workflow lot talk system managed deploys chat around time since used third party irc server able fully trust chat room production control remained oneway flow information traffic site grew infrastructure supporting occasionally launch new batch application server put service still manual process including updating list host push added capacity would usually grow pool several server time result iterating list server sequentially would touch multiple server pool quick succession rather diverse mix used uwsgi manage worker process told application restart would kill existing process spin new one new one took time get ready serve request combined incidentally targeting single pool time would impact capacity pool serve request limited rate could safely deploy server list server grew length deploys reworked deploy tool overhaul deploy tool written python confusingly also called push new version major improvement first fetched list host dns rather keeping hardcoded allowed u update list host without remember update deploy tool well rudimentary service discovery system help issue sequential restarts shuffled list host deploying would mix pool server allowed u safely roll higher speed deploy faster initial implementation shuffled randomly time made hard quickly revert code deploy first server time amended shuffle use seed could reused second deploy reverting another small important change always deploy fixed version code previous version tool would update master given host master changed middeploy someone accidentally pushed code deploying specific git revision instead branch name ensured deploy got version everywhere production finally new tool made distinction code focused list host sshing command run still heavily biased need protoapi sort allowed control deploy step made easier roll change build release flow example might run individual server exact command hidden sequence still specific workflow sudo optredditdeploypy fetch reddit sudo optredditdeploypy deploy reddit sudo optredditdeploypy fetchnames sudo optredditdeploypy restart fetchnames thing much concern autoscaler decided actually get cloud thing autoscale subject separate blog post allowed u save ton money site le busy automatically grow keep unexpected demand previous improvement automatically fetched hostlist dns made natural transition hostlist changed lot often different tool started quality life thing became integral able launch autoscaler however autoscaling bring interesting edge case free lunch happens server launched deploy ongoing make sure newly launched server checked get new code present server going away middeploy tool made smarter detect server gone legitimately rather issue deploy process noisily alerted incidentally also switched uwsgi gunicorn around time various reason really make difference far deploys concerned thing carried many server time number server needed serve peak traffic grew meant deploys took longer longer worst normal deploy took close hour good rewrote deploy tool handle host parallel new version called rollingpin lot time old tool took initiating ssh connection waiting command finish parallelizing safe amount allowed faster deploys instantly took deploy time minute reduce impact restarting multiple server deploy tool shuffle got smarter instead blindly shuffling list server would interleave pool server way maximally separated server pool much intentional reduction impact site important change new tool api deploy tool tool lived server much clearly defined decoupled need originally done eye open sourcefriendly ended useful shortly example deploy highlighted command api executed remotely many people suddenly seemed lot people working great meant deploys keeping one deploy time rule slowly became difficult individual engineer coordinate verbally order would release code fix added another feature chatbot coordinated queue deploys engineer would ask deploy lock either get get put queue helped keep order deploys let people relax bit waiting lock another important addition team grew track deploys central location modified deploy tool send metric graphite easy correlate deploys change metric two many service also suddenly second service coming online new mobile version website coming online completely different stack server build process first real test deploy tool decoupled api addition ability build step different location project held able manage service system many service course next year saw explosive growth reddit team went two service couple dozen two team fifteen majority service either built baseplate backend service framework node application similar mobile web deploy infrastructure common coming online soon rollingpin care deploying make easy spin new service tool people familiar safety net increased number server dedicated monolith deploy time grew wanted deploy high parallel count would caused many simultaneous restarts app server hence capacity unable serve incoming request overloading app server gunicorn main process used model uwsgi would restart worker new worker process booting unable serve request startup time monolith ranged second meant period would unable serve request work around replaced gunicorn master process stripe worker manager einhorn keeping gunicorn http stack wsgi container einhorn restarts worker process spawning one new worker waiting declare ready reaping old worker repeating upgraded created safety net allowed u capacity deploy new model introduced different problem mentioned earlier could take second worker replaced booted meant code bug surface right away could roll lot server prevent introduced way block deploy moving another server worker process restarted done simply polling einhorn state waiting new worker ready keep speed increased parallelism safe new mechanism allows u deploy lot machine concurrently deploy timing minute around server despite extra waiting safety retrospect deploy infrastructure product many year stepwise improvement rather single large dedicated effort shade history tradeoff taken step visible current system point past pro con evolutionary approach le effort given time may end dead end important keep eye evolving keep moving useful direction future reddit infrastructure need support team grows constantly build new thing rate growth company highest ever reddit history working bigger interesting project ever big issue facing u today twofold improving engineer autonomy maintaining system security production infrastructure evolving safety net engineer deploy quickly confidence sound interesting want deploy code bunch come join u reddit check list open position career page want discus post author head rprogramming
294,Lobsters,scaling,Scaling and architecture,View Counting at Reddit,http://redditblog.com/2017/05/24/view-counting-at-reddit/,view counting reddit,krishnan chandra ushrinkandanarch want better communicate scale reddit original space usage twitter algebird library implemented scala implementation hyperloglog located streamlib implemented java redis hll implementation chose conclusion check career page,krishnan chandra ushrinkandanarch senior software engineer want better communicate scale reddit user point vote score number comment main indicator activity given post however reddit many visitor consume content without voting commenting wanted build system could capture activity counting number view post received number shown content creator moderator provide better insight activity specific post post going talk implemented counting scale counting methodology four main requirement counting view count must real time nearreal time daily hourly aggregate user must counted within short time window displayed count must within percentage point actual tally system must able run production scale process event within second occurrence satisfying four requirement trickier sound order maintain exact count real time would need know whether specific user visited post know information would need store set user previously visited post check set every time processed new view post naive implementation solution would store unique user set hash table memory post id key approach work well le trafficked post difficult scale post becomes popular number viewer rapidly increase several popular post one million unique viewer post like becomes extremely taxing memory cpu store id frequent lookup set see someone already visited since could provide exact count looked different cardinality estimation algorithm considered two option closely matched looking accomplish linear probabilistic counting approach accurate requires linearly memory set counted get larger hyperloglog hll based counting approach hlls grow sublinearly set size provide level accuracy linear counter understanding much space hlls really save consider rpics post included top blog post received million unique user store million unique user id user id long would require megabyte memory count unique user single post contrast using hll counting would take significantly le memory amount memory varies per implementation case implementation could count million id using kilobyte space would original space usage article high scalability good overview algorithm many hll implementation use combination two approach starting linear counting small set switching hll size reach certain point former frequently referred sparse hll representation latter referred dense hll representation hybrid approach advantageous provide accurate result small set large set retaining modest memory footprint approach described detail google hyperloglog paper hll algorithm fairly standard three variant considered using implementation note inmemory hll implementation looked java scala implementation primarily use java scala data engineering team twitter algebird library implemented scala algebird good usage doc implementation detail sparse dense hll representation easily understandable implementation hyperloglog located streamlib implemented java code streamlib welldocumented somewhat difficult understand use library properly tune need redis hll implementation chose felt redis implementation hlls welldocumented easily configurable hllrelated apis provided easy integrate added benefit using redis alleviated many performance concern taking cpu memoryintensive portion counting application hll computation moving onto dedicated server reddit data pipeline primarily oriented around apache kafka user view post event get fired sent event collector server batch event persists kafka viewcounting system two component operate sequentially first part counting architecture kafka consumer called nazar read event kafka pas set rule concocted determine whether event counted gave name nazar eyeshaped amulet protecting evil nazar system eye protects u bad actor trying game system nazar us redis maintain state keep track potential reason view counted one reason may count event result repeat view user short period time nazar alter event adding boolean flag indicating whether counted sending event back kafka second part project pick second kafka consumer called abacus actual counting view make count available site client display abacus read event kafka output nazar depending nazar determination either count skip view event marked counting abacus first check hll counter already existing redis post corresponding event counter already redis abacus make pfadd request redis post counter already redis abacus make request cassandra cluster use persist hll counter raw count number make set request redis add filter usually happens people view older post whose counter evicted redis order allow maintaining count older post might evicted redis abacus periodically writes full hll filter redis along count post cassandra cluster writes cassandra batched group per post order avoid overloading cluster diagram outlining event flow high level conclusion hope view count better enable content creator understand full reach post help moderator quickly identify post receiving large amount traffic community future plan leverage realtime potential data pipeline deliver useful feedback redditors interested solving problem like scale check career page special thanks upowerlanguage ugooeyblob contribution project ukaitaan ubsimpson uspladug ukeysersosa reviewing editing post
295,Lobsters,scaling,Scaling and architecture,How Yelp Runs Millions of Tests Every Day,https://engineeringblog.yelp.com/2017/04/how-yelp-runs-millions-of-tests-every-day.html,yelp run million test every day,seagull apache mesos aws aws dynamodb docker elasticsearch jenkins kibana signalfx aws challenge yelpmain performance day finish scale seagull work bin packing problem greedy algorithm linear programming objective function main constraint pulp lpvariable mesos executor apollo apollo scale talking challenge scale aws asgs aws ondemand instance aws spot fleet spot instance auto scaling achieved aws invent seagull fleetmiser want help build distributed system tool view job back blog,fast feature development critical success strive increase developer productivity decreasing time test deploy monitor change enable developer push code safely run million test every day using inhouse distributed system called seagull seagull seagull fault tolerant resilient distributed system use parallelize test suite execution seagull built using following apache mesos manages resource seagull cluster aws provides instance make seagull jenkins cluster aws dynamodb store scheduler metadata docker provides isolation service required test elasticsearch track test run time cluster usage data jenkins build code artifact run seagull scheduler kibana signalfx provide monitoring alerting aws serf sourceoftruth test log challenge deploying new code production monolithic web application yelpmain yelp developer run entire test suite specific version yelpmain run test developer trigger seagullrun job schedule test cluster two important thing consider performance seagullrun nearly test running sequentially take approximately day finish scale seagullruns triggered typical day simultaneous run peak hour challenge execute seagullrun minute rather day still costeffective scale seagull work first developer trigger seagullrun console start jenkins job build code artifact generate test list test grouped together passed scheduler execute test seagull cluster finally test result stored elasticsearch developer trigger seagullrun specific version code based git shas branch say git branch name testbranch code artifact test list generated testbranch uploaded bin packer fetch test list along historic timing metadata create multiple bundle containing test efficient bundling bin packing problem use following two algorithm solve problem depending parameter passed seagull developer greedy algorithm test first sorted based historic test duration start filling bundle minute work test linear programming lp case test dependency test need run another test bundle use lp bundling objective function constraint lp equation defined objective function minimize total number bundle generated main constraint single work le minute test put one bundle dependent test put bundle use pulp lp solver solve equation objective function problem lpproblem minimize bundle lpminimize problem lpsum bundle range maxbundles objective minimize bundle one constraint constraint look like range maxbundles sumoftestdurations test alltests sumoftestdurations testbundle test testdurations test problem sumoftestdurations bundlemaxduration bundle bundle testbundle lpvariable maxbundles bundlemaxduration integer normally consider setup teardown duration lp constraint sake ignore scheduler process started jenkins host fetch bundle start mesos framework create new scheduler seagullrun run generates bundle grouping bundle around minute worth test bundle scheduler creates one mesos executor schedule seagull cluster whenever sufficient resource offered mesos master executor scheduled cluster following step occur inside executor executor start sandbox downloads build artifact uploaded step docker image corresponding test service dependency downloaded start docker container service container running test start executing finally test result metadata stored elasticsearch e write e use inhouse proxy service apollo living distributed system world one thing avoid host failure seagull fault tolerant towards instance failure example suppose scheduler two bundle run mesos offer resource agent scheduler assuming scheduler deems resource sufficient two bundle scheduled suppose reason go mesos let scheduler know gone task manager make decision retry bundle abort bundle retried rescheduled mesos provides next sufficient resource offer case agent case bundle aborted scheduler mark test executed seagull ui fetch result e using apollo load ui developer see result pas ready deploy scale talking around seagullruns every day per hour peak time launch million docker container day handle need around cpu core seagull cluster peak hour challenge scale maintain timeliness test suite especially peak hour need hundred instance always available seagull cluster using aws asgs aws ondemand instance fulfilling capacity expensive u reduce cost started using internal tool called fleetmiser maintain seagull cluster fleetmiser autoscaling engine built scale cluster based different signal current cluster utilization number run pipeline etc main component aws spot fleet aws spot instance consumed much lower price ondemand instance spot fleet provides easier interface using spot instance auto scaling cluster usage volatile major utilization pst developer work automatically scale fleetmiser us current historic utilization data different priority every day seagull cluster scale approximately core core auto scaling cluster capacity week ago fleermiser saved u cluster cost fleetmiser cluster completely aws ondemand instance auto scaling achieved seagull achieved test result response time improvement day minute also delivered large reduction execution cost developer able confidently push code without needing wait hour day verify change broken anything interested learning check aws invent talk seagull fleetmiser want help build distributed system tool like building sort thing yelp love building system proud proud seagull fleetmiser check software engineer distributed system position career page view job back blog
296,Lobsters,scaling,Scaling and architecture,Enough with the microservices,https://aadrake.com/posts/2017-05-20-enough-with-the-microservices.html,enough microservices,http adamdrakecomenoughwiththemicroserviceshtml,http adamdrakecomenoughwiththemicroserviceshtml
297,Lobsters,scaling,Scaling and architecture,Amazon's exabyte-scale data transfer service: a truck,https://aws.amazon.com/snowmobile/,amazon exabytescale data transfer service truck,amazon,aws snowmobile exabytescale data transfer service used move extremely large amount data aws transfer per snowmobile long ruggedized shipping container pulled semitrailer truck snowmobile make easy move massive volume data cloud including video library image repository even complete data center migration transferring data snowmobile secure fast cost effective initial assessment snowmobile transported data center aws personnel configure accessed network storage target snowmobile site aws personnel work team connect removable highspeed network switch snowmobile local network begin highspeed data transfer number source within data center snowmobile data loaded snowmobile driven back aws data imported amazon
298,Lobsters,scaling,Scaling and architecture,How Basic Performance Analysis Saved Us Millions,http://heap.engineering/basic-performance-analysis-saved-us-millions/,basic performance analysis saved u million,indexing data customer analytics problem unusually high cpu usage visualizing cpu use flame graph implementing fix source code additional resource,story applied basic performance analysis technique find small change resulted improvement cpu use postgres cluster save heap million dollar next year indexing data customer analytics heap customer analytics tool automatically capture every user interaction website app installed website heap automatically track every pageview click form submission owner website use heap perform many different kind aggregation different subset raw data order make possible get insight data heap let user define event term raw data example might login could defined form submission login page make analysis fast use unusual indexing strategy relies postgres partial indexing feature partial index like normal postgres index except contains row satisfy specified predicate think like regular index clause every event definition one customer creates create partial index customer raw event data restricted row match definition whenever new row inserted event table postgres automatically test event predicate partial index table add row necessary index event definition corresponding partial index make fast retrieve matching event index contains exactly event satisfy definition want learn use partial index read blog post index data go depth problem unusually high cpu usage first rolled indexing strategy cpu use significantly higher previous indexing strategy made sense thought largest customer thousand index order support filter based cs selector lot partial index contain regular expression filter thought since regular expression fairly expensive evaluate made sense testing thousand regexes every event inserted would cause postgres use ton cpu real evidence case became explanation everyone heap gave postgres used much cpu assumed fundamental tradeoff indexing strategy around october data volume continued increase started issue ingesting data coming peak hour day would take hour new event show heap dashboard completely unacceptable tool meant real time analytics instead going typical route throwing money problem thought would try hand optimizing heap ingestion throughput visualizing cpu use flame graph prior limited experience debugging performance issue googling bit came across one brendan gregg post flame graph flame graph type visualization brendan gregg invented way quickly identify part code taking cpu first step creating flame graph take sample stack process using linux perf tool perf record p pid process f g sleep sample stack given process time second second write data file called perfdata run following command brendan gregg flame graph library process file generate flame graph perf script stackcollapseperfpl outperffolded flamegraphpl outperffolded flamegraphsvg one first flame graph created postgres backend process due use connection pooling single backend process serve multiple query since vast majority query run insert flame graph postgres backend process would give u good idea cpu spent inserting event database running command pid postgres process got pgstatactivity obtained following flame graph click image open new tab click rectangle zoom hovering rectangle pull information rectangle uninitiated flame graph pretty difficult understand brendan gregg give following explanation interpret one xaxis show stack profile population sorted alphabetically passage time yaxis show stack depth rectangle represents stack frame wider frame often present stack top edge show oncpu beneath ancestry color usually significant picked randomly differentiate frame pretty clear flame graph cpu time spent execopenindices large yellow bar center right image looking flame graph tiny bit appears time split two different function buildindexinfo indexopen buildindexinfo call relationgetindexpredicate cpu time spent look like majority time spent relationgetindexpredicate looking source code relationgetindexpredicate appears purpose parse optimize partial index predicate make sense much time spent relationgetindexpredicate since parsing arbitrary expression much difficult evaluating already parsed expression let look rest time spent execopenindices remaining time spent indexopen look like indexopen call relationopen call relationidgetrelation documentation relationidgetrelation source code purpose lookup metadata different relation case mainly used looking partial index based time spent relationgetindexpredicate relationidgetrelation appears postgres spends lot time fetching parsing partial index predicate evaluating implementing fix looking source code different function significant amount caching going relationgetindexpredicate postgres first check already extracted predicate immediately return relationidgetrelation first us relationidcachelookup check relation metadata already calculated cached appears normal circumstance index metadata would fetched parsed read cache rest time unfortunately u caching work well writing event one time ten thousand different table postgres pool process us serve query process keep cache every insert assigned roundrobin amongst process inserting event one time sharded schema ten thousand underlying table unlikely two insert going table served process mean index metadata almost never cached process executing insert postgres need fetch parse index metadata destination table almost every event insert suggests simple change could make instead inserting event individually could batch insert many event going table using single command insert many event postgres would need fetch parse index metadata per batch thought batching insert reduce transaction count never save cpu resource assumed cpu going towards evaluating index predicate initial benchmark batched insert showed reduction cpu usage obtained result began testing batched insert production ultimately get improvement ingestion throughput using batch average size event ingestion latency different kafka partition looked like right deployed batching unit left hour latency able clear hour backlog minute deploying batching took another flame graph insert time appears large portion time going execqual red bar middle based source code function used evaluate partial index predicate mean postgres spending cpu actual work evaluating partial index predicate made discovery six month ago since needed add additional cpu cluster look like need next month either able find win using rudimentary performance analysis technique really take much find win way interested kind work hiring apply reach twitter additional resource
299,Lobsters,scaling,Scaling and architecture,Why Amazon is eating the world,https://techcrunch.com/2017/05/14/why-amazon-is-eating-the-world/,amazon eating world,stedi impending apocalypse great rant fedex great piece,zack kanter cofounder stedi post contributor cofounded software startup december month send update investor keep updated progress past month bit different industry retail going transformation instead writing internal news wrote impending apocalypse broader world retail specifically included thought amazon commanding lead going get larger amazon impressive company earth think one least understood people suggested post publicly go first company auto part manufacturer sold amazon vendor amazon issue purchase order bulk product marketplace seller amazon take cut thirdparty seller product sold amazoncom insight amazon internal operation initiative often publicly discussed followed aws amazon various offering time well amazon company become something personal obsession mine thought amazon impending retail apocalypse wanted share interested overall future retail consensus hit tipping point retail industry finally seeing major collateral damage amazon monster growth mainstreamnontech news started giving lot coverage lot discussion whether amazon advantage sustainable whether retailer namely walmart able mitigate amazon dominance start replicate amazon model analysis read focus amazon broad advantage call bulletpoint moat evaluate whether program replicable competing retailer like walmart program wellknown tech world example include amazon prime delivery amazon marketplace thirdparty seller sell item alongside amazon listing amazon go store amazon physical cashierfree retail location amazon drone program truth feasible large competitor replicate reasonable think walmart could build acquire capability within next year key component profitable delivery customer proximity distribution center walmart already distribution center considerably aggregate square footage amazon fulfillment center though optimized restocking store via walmart network truck walmart proven ability build distribution capacity able manage reconfiguring network ecommerce fulfillment without much difficulty amazon building full gamut critical lastmile delivery solution drone delivery robot ondemand human delivery network called amazon flex plenty thirdparty startup area could provide walmart reasonable degree parity said believe amazon defensible company earth even begun grasp scale dominance competitor amazon lead grow coming decade think much retailer stop reason bulletpoint moat talked headline culture innovation bezos vision ceo though think amazon culture incredible bezos impressive ceo fact piece amazon built serviceoriented architecture amazon using architecture successively turn every single piece company separate platform thus opening piece outside competition remember reading common pitfall vertically integrated company school usually compelling cost saving vertical integration either insourcing service acquiring supplierscustomers increased margin typically evaporate time supplier get complacent captive internal customer great example automotive industry automaker gone alternating period supplier acquisition subsequent divestiture component cost skyrocketed division get fat inefficient without external competition attempt mitigate competitiveexternal bid comparison detailed cost accounting quota usually lead increased bureaucracy little effect actual cost structure obvious example amazon soa structure amazon web service steve yegge wrote great rant beginning back timing amazon unparalleled scaling hypergrowth early enterpriseclass saas widely available amazon build technology infrastructure financial genius turning infrastructure external product aws wellcovered windfall enormous tune billion annual run rate revenue bonanza footnote compared overlooked organizational insight amazon discovered carving operational piece company platform could futureproof company inefficiency technological stagnation photo drew angerergetty image year since aws debut amazon systematically rebuilding internal tool externally consumable service recent example aws amazon connect selfservice cloudbased contact center platform based technology used amazon call center extra revenue great real value honing amazon internal tool amazon connect complete commercial failure amazon management quantifiable indicator revenue lack thereof suggests internal tool significantly lagging behind competition amazon replaced useless timeintensive bureaucracy like internal survey audit feedback loop generates cash work quickly identifies problem say money earned reasonable approximation value creating world amazon figured way measure value dozen previously invisible area much obvious know aws incredible thing strategy one herculean display effort history modern corporation permeated amazon every level amazon quietly rolled external access nook cranny across entire ecosystem long tail external service availability think nearly impossible replicate broadest example fulfillment amazon fba program ever ordered product amazon say sold company fulfilled amazon seen fba action fba amazon allows thirdparty seller ship bulk inventory amazon amazon store inventory seller still owns amazon fulfillment center ship product amazon customer order placed even handle return customer service rate incredibly competitive fba limited item sold amazon seller also use amazon multichannel fulfillment option ship nonamazon order seller customer example would hydro flask operated separate ecommerce store shopify customer place order shopify store hydro flask send order fba guessed via external api fba ship directly customer benefit hydro flask obvious product manufactured china use freight forwarder like flexport ship product directly factory amazon warehouse thus avoid headache overhead operating warehouse amazon surface benefit numerous better utilization excess capacity b increased shipping volume leverage upsfedex c revenue fulfillment service combined marketplace commission thirdparty seller service totaled whopping billion percent amazon total revenue enduring benefit improvement come opening amazon internal fulfillment operation outside user fulfillment shipping amazon largest cost center huge human component one susceptible bloat level discipline required operate multitenant externally facing service like fba yield tremendous benefit amazon internal operation hackedtogether homegrown tool hardcoded amazon need thus nearly impossible improve xit relatively clean abstracted servicebased interface owned separate team team responsibility external customer bezos imbued sense customer worship within amazon earth customercentric company amazon three distinct group customer worship ecommerce shopper amazoncom developer aws seller amazon marketplacefba side note think fba extraordinarily difficult another retailer replicate technological organizational complexity commingling inventory literally hundred thousand seller mindboggling particularly factor permutation result different setting toggled sellerbyseller basis automated system routing splitting inbound shipment optimal fulfillment center based capacity historical geographic distribution customer know else nontrivial task amazon year fba launched think investment fba substantial part reason failure make profit really choice make profit large part last decade error rate tremendous start personally know several seller reimbursed ten thousand dollar inventory amazon lost question asked setting aside massive technical challenge retailer stomach sustain incredible loss program many year seems obvious amazon move smallparcel shipping upsfedexusps within next five year thumbing income statement picking largest category productize first technology aws fulfillment fba cog actual product via amazon various private label program next shipping already started operating fleet cargo plane thousand tractortrailers built dozen parcelsorting center reduce fee pay existing small parcel carrier natural fit service model tremendous internal demand existing service customer perfect early adopter key advantage amazon enterprise service provider ups fedex rackspace forced use service ups step removed backlash due lostdestroyed package shipping delay terrible software poor holiday capacity planning angry customer blame retailer retailer scream ups turn amazon service provider permanently dogfooding nowhere poor performance hide amazon built feedback loop moat incredible watch flywheel start pick speed amazon committed idea granular level even come service sold amazon still making push expose service externally perfect example amazon marketplace web service mws api set service amazon marketplace seller use programmatically exchange data amazon amazon built service call subscription api give seller instant notification price change competitor including amazon amazon externally exposing tool us set price order guarantee price listed amazon low possible customer spawned whole ecosystem thirdparty priceoptimization tool called repricers use mws api automatically respond price change order maximize sale marketplace seller wsj published great piece back march aptly likening highfrequency trading beauty amazon care seller undercut amazon price amazon take percent commission sale regardless collect fba fee boot could go example email list update aws amazon marketplace amazon vendor program handful customerfacing program systemically productizing entire company honing work fixing killing everything else reminds bezos quote frequently get question going change next year interesting question common one almost never get question going change next year submit second question actually important two build business strategy around thing stable time n retail business know customer want low price know going true year want fast delivery want vast selection impossible imagine future year customer come say jeff love amazon wish price little higher love amazon wish deliver little slowly product assortment low price fast delivery key amazon winning retail game amazon marketplace seller list million hottest new product far faster amazon vendor team could ever hope discover built highfrequency trading platform guarantee price competitiveness delivers guaranteed minimum percent margin amazon fast delivery come operational excellence exceptionally low cost shipping accomplished opening fba external customer think anyone understands innovator dilemma better amazon implemented systemic solution maintain unbeatable advantage competitive retailer opinion amazon uncatchable took amazon year perfect fba even walmart could amazon time roll even begun touch surface amazon lesserknown industryshattering program like seller fulfilled prime direct fulfillment sure see massmarket retailer compete successfully amazon within lifetime though still think substantial opportunity verticalspecific retailer like chewycom spin gain ground short term amazon brought antitrust case though long way given small percentage total retail volume today paradigm shift consume physical product scenario come mind widespread adoption massively immersive vr combined intravenous nutrition soylent year universal basic income would obviate need physical product altogether way
300,Lobsters,scaling,Scaling and architecture,"An engineering year in review, kubernetes, grpc and more",https://www.sajari.com/blog/an-engineering-year-in-review-kubernetes-grpc-and-more,engineering year review kubernetes grpc,monolith service protocol buffer json grpc grpc v rest protobuf flatbuffers mobile stream binary data bidirectionally kubernetes read background introduction chose kubernetes federated across multiple data centre analysis learning index freshness performance scoring learn rank pipeline summary,massive development year team post quick look larger change made learnt change set foundation futuremonolith servicespeople debate benefit moving service definitely downside building large scale distributed system almost force service route different component scale different way distributed state need extra attention prior running two main monolith system frontend service search backend frontend handled request auth configuration crawling outward facing service backend home grown search engine handled search index language processing machine learning disk backup autocomplete much moreover time complexity backend unintentionally grown enormously order chase performance performance chase succeeded also left application difficult work difficult distribute core goal mind towards index becoming fully distributed began rebuilding backend several servicesthis rebuild great success much cleaner codebase actually increased performance older version component boundary clear scaling service largely fully automated one downside management multiple repos definitely slows development somewhat also provides great separationprotocol buffer jsona lot smart people put lot time protocol buffer show one point profiling search engine code noticed cpu time spent encoding decoding json seemed crazy hey everyone love json seeing talk grpc getting insight google use stubby internally seemed like might great fit u see previous grpc v rest article spent long time still dealing aspect overall great move encoding dominating cpu profile anymore internal system coupled nicely contextual tracing system sdks use single definition come steaming tracing cancellation lot boxi would note use binary encoding package internal data representation beat throughput protobuf several order magnitude also show slow json encoding really suggesting build database protobuf people go flatbuffers however protobuf handle much encoding added benefit make ideal connecting distributed system always keep mind added feature nt free mobilegrpc allows single connection remain open stream binary data bidirectionally using mobile device huge save battery cpu speed data transfer transmitting binary instead jsonxmletc b allows mobile client run function directly inside backend servicesthe benefit grpc generate mobile sdks easy u keep date versioned api work great mobilekubernetesseeing talk google borg kubernetes one mind blown moment change way think everything read background also previously wrote introduction chose kubernetesbefore kubernetes used deploy script vms seems pretty backwards kubernetes abstract away vms provides single resource plane deploy many service via container also handle scheduling container service connecting dns using simple label manages scaling load balancing much soon single resource plane federated across multiple data centre different continentsthis sample interface console showing service one cluster console proxy directly cluster localhost incredibly useful quickly see happening cluster label used connect everythingkubernetes make thing easy put thing perspective first started testing kept trying kill container see would happen could switch screen see happened already back running luckily kubernetes built logging easy see fact kill themthis brings second interesting side effect using go based docker container tiny normally le majority service dependency outside executable allows u use scratch docker template o jvm bloat consequently moved around started extremely quickly sub second scratch expose copy synd entrypoint synd listen show typical dockerfile one service ca nt get much easieranalysisin past text analysis baked engine firstly nt support language secondly lot parsing regexes generally bit brittle outside nt matter much dealt english b thing like stemming relatively symmetric query nt matter fast forward amazing tokeniser handle language even extended easily information like programming language also allows u write custom analyser based built amazing capability using word embedding recognise word actually mean group tag accordinglylearning indexesone significant change switching index updated place realtime main implication result improved constantly without creating new index long term extends much furthersearch index created handle deficiency running text based query database order improve retrieval quality logic added parse interpret score unstructured text storing way favourable text query fast query speed higher compression disk data written immutable structure unchanging could read without write locking database prevent read data update fast forward year still fundamental approach search index fact search today still powered code originating late downside immutable index freshness world realtime bad outdated result tolerated like used beperformance merging compacting index brutal resource perspective imagine every time edited paragraph book throw book print entire book scoring relationship query result becomes obvious time would useful make scoring update without needing write entirely new indexthere serious effort last year address freshness issue mostly hiding people even sell product based lucene realtime database amusing lucene great product never designed high update frequency people try wonder us much cpu io obvious underlying structure understoodscoring another area people trying improve learn rank technique becoming prominent performance finally looped back ranking algorithm unstructured information many result still tied ranking score particularly short query large data set index score termdocument intersection normally set using statistical distribution across whole collection document completely ignores context term used b performed previously particular query highly useful make score variable based performance machine learning model done allows u update importance result individual query without writing new index open whole new world experimentation ranking performance optimisationthis allows u update importance result individual query without writing new indexespipelinespipelines template running complicated data processing ranking model adding record querying end user perspective provides simple api interface integrationbreaking everything many smaller piece great u develop test found produced api complex example search may may autocompleted boosted variety way synonym added classified clustered many thing end user want problem solved largely nt care time work worked hard create new approachthe go processing record nt allow record added transformed modify create new information power feature autocomplete eg product title may used train autocomplete service also allow classification model create new field automatically categorising record much type templating pluggable allow u add almost data processing step query record processing pipelinesby labelling pipeline provide extremely simple external interface hide powerful series data processing step query understanding algorithm prioritisation secondly label provide easy way run analytics machine learning optimisation validate piece impact search amazing year technical team extremely low latency search infrastructure result ranking optimising realtime improved availability scalability simpler integration sdks better data processing much
301,Lobsters,scaling,Scaling and architecture,CockroachDB 1.0,https://www.cockroachlabs.com/blog/cockroachdb-1-0-release/,cockroachdb,cockroachdb distributed sql multiactive availability flexible deployment commercial offering cockroachdb enterprise future rubber meet road san francisco thursday may nyc wednesday may,get blog post inbox today pleased announce release cockroachdb first open source cloudnative sql database also announcing series b fundraise investor share vision launch mark graduation beta productionready database designed power business scale startup enterprise brief introduction order database generally considered thrilling subject technology news ignoring would mistake understanding ongoing evolution database brings focus ruthless arm race business need data existing technology struggle provide insistent pressure forced database alternatively become faster bigger reliable faster bigger reliable cycle continues seeing evolution folk refer newsql combination making general purpose relational database system rdbmss bigger reliable cockroachdb earns newsql stripe providing distributed sql accommodate everlarger data size multiactive availability new model high availability ha capability establish fundamentally better standard deploying global cloud service customer already production including baidu heroic lab downloads since beta release happy additionally announce series b raise led satish dharmaraj redpoint venture joined benchmark firstmark capital gv formerly google venture index venture workbench satish make valuable addition investor advisory team among many thing founded ran zimbra open source company successfully competed industry goliath microsoft latest round provides capitalization match size challenge opportunity ahead cockroachdb cockroachdb cloudnative sql database building global scalable cloud service survive disaster cloudnative actually mean believe term implies horizontal scalability single point failure survivability automatable operation platformspecific encumbrance realize product goal development past year focused three critical area distributed sql support small large use case alike scale seamlessly multiactive availability alwaysconsistent high availability flexible deployment automatable operation virtually environment distributed sql relational database offering sql traditionally constrained singlenode deployment difficulty scaling sql workload clear meteoric rise nosql database past decade nosql database promised scale simplicity delivered however adoption remains small fraction total database market growth slowed one reason nosql solution sacrifice guarantee common sql make life simpler programmer acid transaction read consistency along requirement learn custom query data modification language significant barrier adoption cockroachdb provides scale without sacrificing sql functionality offer fullydistributed acid transaction zerodowntime schema change support secondary index foreign key work box many popular orm framework supporting industry standard sql dialect addition release introduces distributed query execution engine enabling distributed join support analytics query speed linearly node added cluster recently demonstrated use case baidu processing billion insert day one top ten largest global internet company rate transaction continued regardless artificiallyinduced chaos event caused significant concurrent rereplication rebalancing across cluster multiactive availability high availability ha crucial capability database available service rely either database ha solution sacrificed consistency performance however network node fail real world application read incorrect data write value disappear thin air speed make great benchmark must secondary consideration working missioncritical data cockroachdb take different approach ha putting consistency first belongs call multiactive availability evolution high availability activeactive replication instead eventual consistency employ stronglyconsistent consensusbased replication us three active replica begin serving readwrite client traffic unlike primarysecondary replication maintains resource idle replication target multiactive availability dynamically utilizes available resource unlike activeactive replication multiactive availability read write inconsistently require conflict resolution one customer one largest online gaming company world looking solution serve consistent read writes across continent multiactive availability node geographically closest user able serve query significantly reducing latency flexible deployment even best technology useful weakest operational link easily deployed operated trouble thriving production made architectural operational simplicity priority start simplicity mean cockroachdb managed popular datacenter orchestration technology like kubernetes docker swarm even manually meaning flexibly fit custom operational environment cockroachdb run premise public cloud deployed hybrid crosscloud configuration migrated cloud zero downtime give company powerful degree flexibility distinctly absent vendor lockin associated using proprietary database service openstack interop conference yesterday boston cockroachdb deployed live across private cloud demonstrating significant portability flexibility video demo video commercial offering cockroachdb enterprise working enterprisescale production mean operational workload must run minimal downtime multiple failsafes addition support zerodowntime rolling upgrade certificate rotation happy announce feature available specifically part cockroachdb enterprise offering distributed incremental backup restore feature intended serve customer large data set exacting production requirement distributed backup restore parallelizes backup restore task across node cluster incremental backup enable efficient periodic update full backup data written restored configurable storage sink blob store offered aws gcp azure periodic backup restore best practice anyone running service top database nondistributed option backup restore available free cockroachdb core offering future come significant milestone release productionready debut world first open source cloudnative sql database table stake interesting future challenge tackle problem bedeviling world fastest growing largest company struggling build global service build data architecture able serve customer multiple continent presenting operator application developer look like single logical database minimize latency provide foundation compliance rapidly evolving data sovereignty law first significant step direction geopartitioning feature enable rowlevel control geographic replication keeping customer data next customer cockroachdb already support flexible geographic replication table database granularity geopartitioning allow column column table serve partition key make possible relocate customer sydney london simple sql update statement transaction query span partition handled transparently cockroachdb expect beta version capability late rubber meet road taken pain test cockroachdb many environment different workload various manufactured unmanufactured chaos event also made significant progress performance however paraphrase donald rumsfeld known unknown worse unknown unknown immediate future point rubber meet road cockroachdb expect wider set user use case environment highlight deficiency product accordance expect devote significant resource ruggedization phase release ready october point release along way stabilize release release expect additional sql coverage better diagnostic tool significant performance improvement look forward welcoming new user cockroachdb release committed making successful future continually expanding cockroachdb capability keep business well ahead curve san francisco thursday may nyc wednesday may cockroachdb user group cto ben darnell cover went come give u feedback
302,Lobsters,scaling,Scaling and architecture,CPU Utilization is Wrong,http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html,cpu utilization wrong,really cpu utilization tell cpu really linux perf instruction per cycle cloud pmcs measuring ipc interpretation actionable item ipc ipc cpu flame graph performance monitoring product tell reason cpu utilization misleading update cpu utilization actually wrong previously conclusion previous post,metric use cpu utilization deeply misleading getting worse every year cpu utilization busy processor measure yes talking cpu metric used everywhere everyone every performance monitoring product top may think cpu utilization mean might really mean stalled mean processor making forward progress instruction usually happens waiting memory io ratio drew busy stalled typically see production chance mostly stalled nt know mean understanding much cpu stalled direct performance tuning effort reducing code reducing memory io anyone looking cpu performance especially cloud auto scale based cpu would benefit knowing stalled component cpu really cpu utilization metric call cpu utilization really nonidle time time cpu running idle thread operating system kernel whatever usually track context switch nonidle thread begin running stop millisecond later kernel considers cpu utilized entire time metric old time sharing system apollo lunar module guidance computer pioneering time sharing system called idle thread dummy job engineer tracked cycle running v real task important computer utilization metric wrote wrong nowadays cpu become much faster main memory waiting memory dominates still called cpu utilization see high cpu top might think processor bottleneck cpu package heat sink fan really bank dram getting worse long time processor manufacturer scaling clockspeed quicker dram scaling access latency cpu dram gap levelled around ghz processor since processor scaled using core hyperthreads plus multisocket configuration putting demand memory subsystem processor manufacturer tried reduce memory bottleneck larger smarter cpu cache faster memory bus interconnects still usually stalled tell cpu really using performance monitoring counter pmcs hardware counter read using linux perf tool example measuring entire system second perf stat sleep performance counter stats system wide taskclock msec cpu utilized contextswitches ksec cpumigrations ksec pagefaults msec cycle ghz supported stalledcyclesfrontend supported stalledcyclesbackend instruction insns per cycle branch msec branchmisses branch second time elapsed key metric instruction per cycle insns per cycle ipc show average many instruction completed cpu clock cycle higher better simplification example sound bad busy realize processor top speed ipc also known referring instruction fetchdecode path mean cpu retire complete four instruction every clock cycle ipc system mean cpu running top speed newer intel processor may move hundred pmcs use dig measuring stalled cycle directly different type cloud virtual environment might access pmcs depending whether hypervisor support guest recently posted pmcs measuring ipc showing pmcs available dedicated host type aws xenbased cloud interpretation actionable item ipc likely memory stalled software tuning strategy include reducing memory io improving cpu caching memory locality especially numa system hardware tuning includes using processor larger cpu cache faster memory bus interconnects ipc likely instruction bound look way reduce code execution eliminate unnecessary work cache operation etc cpu flame graph great tool investigation hardware tuning try faster clock rate coreshyperthreads rule split ipc get made based prior work pmcs get value custom system runtime write two dummy workload one cpu bound one memory bound measure ipc calculate mid point performance monitoring product tell every performance tool show ipc along cpu break cpu instructionretired cycle v stalled cycle eg in stl top tiptop linux show ipc process tiptop root task total displayed screen default pid cpu sys p mcycle minstr ipc miss bmi bus command java nmapplet dbusdaemo reason cpu utilization misleading memory stall cycle make cpu utilization misleading factor include temperature trip stalling processor turboboost varying clockrate kernel varying clock rate speed step problem average utilized minute hiding burst spin lock cpu utilized high ipc app making logical forward progress update cpu utilization actually wrong hundred comment post elsewhere thanks everyone taking time interest topic summarize response talking iowait disk io actionable item know memory bound see cpu utilization actually wrong deeply misleading think many people interpret high cpu mean processing unit bottleneck wrong said earlier point nt yet know often something external metric technically correct cpu stall cycle ca nt used anything else nt therefore utilized waiting sound like oxymoron case yes could say cpu oslevel metric technically correct deeply misleading hyperthreads however stalled cycle used another thread cpu may count cycle utilized fact available wrong post wanted focus interpretation problem suggested solution yes technical problem metric well might say utilization metric already broken adrian cockcroft discussed previously conclusion cpu utilization become deeply misleading metric includes cycle waiting main memory dominate modern workload perhaps cpu renamed cyc short cycle figure cpu really mean using additional metric including instruction per cycle ipc ipc likely mean memory bound ipc likely mean instruction bound covered ipc previous post including introduction performance monitoring counter pmcs needed measure performance monitoring product show cpu also show pmc metric explain mean mislead end user example show cpu ipc andor instructionretired cycle v stalled cycle armed metric developer operator choose better tune application system
303,Lobsters,scaling,Scaling and architecture,How Many Data Centers Needed World-Wide,http://perspectives.mvdirona.com/2017/04/how-many-data-centers-needed-world-wide/,many data center needed worldwide,fortune asked mark hurd redundancy big fail blast radius latency speed light data center ny hong kong tokyo communicating data beyond speed light pop pop networking ecosystem inefficiency close separate location around world social political factor,last week fortune asked mark hurd oracle coceo oracle going compete cloud computing capital spending came whereas aggregate spending three cloud player essentially question assume big three spending roughly equally compete come serving customer pretty good question mark answer interesting one twotimes faster computer need many data center speed database maybe need one fourth may data center course believe oracle ever get server faster big three cloud provider also would argue speeding database something oracle uniquely positioned offer major cloud provider deep database investment ignoring extraordinary database performance change factor force successful cloud provider offer large multinational data center footprint serve world still hurd offhand comment raise interesting question many data center required successful international cloud service provider argue number considerably bigger deployed even largest provider today yes represents massive cost given even medium sized data center likely exceed provider focused cost none want open massive number facility predict let look deeper myriad driver large data center count redundancy efficient number data center per region one scaling gain single large facility one facility serious difficulttoavoid fullfacility fault mode like flood lesser extent fire absolutely necessary two independent facility per region actually much efficient easy manage three redundancy cheaper facility single facility experience fault without eliminating redundancy system consequently whenever aws go new region usual three new facility opened rather one rack different power domain big fail even building three new data center opening new region good reason three data center region grows absolute data center size facility becomes big fail line gray open debate limiting factor big facility operator lose lost resource massive network access pattern change failure hidden customer aws easily build facility cost saving scaling single facility without bound logarithmic whereas negative impact blast radius linear facing seriously sublinear gain linear risk make sense cap maximum facility size time cap may change technology evolves aws currently elect build right around instead built pocketed slight gain unlikely anyone would notice slim chance fullfacility fault elect limit blast radius current build around grouping multiple data center redundancy group often referred region region scale avoid allowing facility make region become big fail number data center easily escalate far beyond ten aws already region scaled far beyond data center factor drive large scale operator offer single region big might number region get successful international operator clearly efficient number region one covering entire planet one efficient number data center factor ignored significant scaling cost gain achieved deploying single region blast radius discovered single facility eventually get big fail thing happens large megaregion operator concentrate worldwide capacity single region would quickly become big fail proud say aws regional failure recent history industry continues see rarely never common still within realm possibility single region deployment model seem ideal customer mega region would also suffer decaying economics case single large data center gain scaling become ever smaller downside risk continue climb eventually incremental cost reduction scaling region become quite small downside risk continues escalate megaregion downside risk least partially mitigated essentially dividing region smaller independent region increase cost decrease scaling gain eventually make better sense offer customer alternative region rather attempting scale single region argument favor multiple region become even stronger factor considered latency speed light speed light remains hard exceed round trip time across north america nearly m data center ny hong kong tokyo low latency important success factor many industry latency reason alone world well served single data center single region actually turn speed light fiber le speed light medium actually possible run faster communicating data beyond speed light without fundamental solution speed light problem many region practical way effectively serve entire planet many workload many factor beyond latency push cloud provider offer large number region going argue latency prime driver large number region latency driver number required region would likely range akamai world leading content distribution network cdn report pop point presence many expert see bigger would strictly required latency another major cdn limelight report pop number closer one would come number pop required latency concern however latency concern upward pressure factor appears dominate latency networking ecosystem inefficiency world telecom market bit mess many region served state sponsored agent monopoly small number provider variety reason compete efficiently many region underserved provider trouble capital investment roll needed capacity provider lack technical ability roll capacity needed rate factor conspire produce order magnitude difference cost sort competitive u market important worldwide market imagine car one market costing far another market network transit world one reason major cloud provider private worldwide network sensible step certainly help fully address market inefficiency around lastmile network user served single access network lastmile network provider often interconnection network link different access network together access network must reached cloud provider access network face challenge sometimes unreasonable interconnection fee increase cost especially video content netflix took interesting approach access network cost problem approach help netflix customer time help access network serve customer better netflix offer place caching server essentially netflixspecific cdn node central office access network allows access network avoid pay cost transit provider move bit required serve netflix customer also give customer access network potentially higher quality service netflix content advantage netflix reducing netflix dependence large transit provider reduces control transit provider netflix netflix customer brilliant move another data point many point presence might required serve world netflix report close separate location around world social political factor seen good reason order region deliver latency required demanding customer also looked economic anomaly networking cost requiring region fully serve world economically talked yet potentially important social political factor cloud computing user really want serve customer local data center impact cloud provider choice addition national jurisdiction put place legal restriction make difficult fully serve market without local region even within single nation sometimes local government restriction allow certain type data housed outside jurisdiction even within country meet need customer political body social political driver require point presence perhaps many full region percentage serversside computing hosted cloud swing closer factor cause largest international cloud provider several hundred many thousand region region require least three data center largest run ten independent facility taking number region number data center required region account argues total data center count world largest cloud operator rise current may case many regional cloud provider rather small group international provider see argument factor supporting outcome whatever outcome number worldwide cloud data center far exceed medium large data center competitor argues fast computer database save outcome believe oracle hardly unique semiconductor team amazon custom asics google acquired arm team done custom asic machine learning microsoft done significant work fpgas also arm licensee big player major custom hardware investment underway even custom asics hard call company delivering customer value investment certainly look like oracle ahead work hard eliminate every penny unneeded infrastructure investment escaping massive data center count outlined billion deployment cost short cut way achieve excellent worldwide cloud service deploy massive scale
304,Lobsters,scaling,Scaling and architecture,1.1 Billion Taxi Rides with MapD 3.0 & 2 GPU-Powered p2.xlarge EC2 Instances,http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-8xlarge-mapd.html,billion taxi ride mapd gpupowered instance,rebranded gpu wrapping software aws google cloud azure singlepage summary gpupowered aws cluster downloading billion taxi journey billion taxi ride redshift mapd running recommendation info mapdcom importing billion trip mapd nvidia tesla benchmarking mapd benchmark recap board linkedin,update mapd rebranded omnisci april mapd released one big announcement release support clustering across multiple machine native software motherboards support one graphic card point run pcie bus bottleneck add nvidia driver limit number gpus visible one machine able use multiple machine help get around limitation scaling mapd past used gpu wrapping software cluster several graphic card separate machine together one virtual graphic card still useful gpuaccelerated application mapd clustering support added advantage able use gpus cpu memory disk drive across cluster well allows horizontally scalable deployment three major cloud provider aws google cloud azure include nvidia tesla instance offering nvidia graphic card based kepler microarchitecture rather maxwell pascal nonetheless sport two gpus combined cuda core x memory bus gb ram access memory quickly aggregated throughput speed gb whereas conventional cpu might able access ram gb might nvidia latest telsaseries offering still beast benchmark see well tesla card spread across two instance perform querying billion taxi trip using dataset used benchmark amazon athena bigquery clickhouse elasticsearch emr kdbq postgresql redshift vertica singlepage summary benchmark gpupowered aws cluster using two instance amazon oregon region benchmark machine come nvidia virtual cpu reported intel xeon cpu clocked gb ram gbps network connection launched ondemand instance cost hour check spot price history putting blog post together last week common price hour availability zone output nvidia system management interface instance see list gpus across card individually nvidiasmi driver version gpu name persistencem busid dispa volatile uncorr ecc fan temp perf pwr usagecap memoryusage gpuutil compute tesla na default tesla na default tesla na default tesla na default tesla na default tesla na default tesla na default tesla na default first instance launched cluster mapd leaf aggregator node second instance mapd leaf string dictionary node machine belong security group allows communicate one another tcp port aggregator server tcp port leaf communication tcp port string dictionary server using ami image machine instance tb eb volume filesystem size used avail use mounted devtmpfs dev tmpfs devshm downloading billion taxi journey instance set aws cli tool use concurrent request better saturate network connection downloading taxi trip dataset aws configure set download gb csv data created billion taxi ride redshift blog post onto instance data sits across gzip file decompresses around gb raw csv data mkdir csvdata cd csvdata aws sync csv gunzip tripsx csvgz mapd running everything unless otherwise noted run instance going install nvidia driver along two requirement gcc kernel development package provides header needed compile kernel module sudo yum install gcc kerneldevel uname r curl http sudo binbash per amazon recommendation going switch nvidia driver persistent mode turn autoboost set gpu compute clock speed mhz gpu memory clock speed mhz sudo nvidiasmi pm sudo nvidiasmi sudo nvidiasmi ac mapd commercial software disclose full url downloaded selfextracting archive nonetheless step needed install mapd interested copy mapd info mapdcom able help mkdir p prodinstalls cd prodinstalls curl server chmod x ln prodmapd add mapd path environment variable java installation library path export path export ldlibrarypath first instance create folder setup system cluster configuration file following content mkdir p prodmapdstorage data vi prodmapdstorageclusterconf host port role dbleaf host port role dbleaf host port role string set leaf node configuration following vi port httpport data readonly false quiet false stringservers web port frontend set aggregator node configuration following vi port httpport data readonly false quiet false cluster web port frontend initialise leaf aggregator database launch respective service cd initdb data mapdserver config mapdconf cd initdb data mapdserver config mapdconf second instance setup string dictionary server configuration mkdir p prodmapdstorage string data vi prodmapdstoragemapdsdsconf port path set leaf node configuration following vi prodmapdstoragemapdconf port httpport data readonly false quiet false stringservers web port frontend set second node clusterconf file following vi prodmapdstorageclusterconf host port role dbleaf host port role dbleaf host port role string initialise leaf node database launch two service cd prodmapdstorage initdb data mapdserver config mapdconf stringdictionaryserver config mapdsdsconf importing billion trip mapd following schema taxi trip table fragment size tell mapd many record spread across gpu set million exercise vi createtripstablesql create table trip tripid integer vendorid varchar encoding dict pickupdatetime timestamp dropoffdatetime timestamp storeandfwdflag varchar encoding dict ratecodeid smallint pickuplongitude decimal pickuplatitude decimal dropofflongitude decimal dropofflatitude decimal passengercount smallint tripdistance decimal fareamount decimal extra decimal mtatax decimal tipamount decimal tollsamount decimal ehailfee decimal improvementsurcharge decimal totalamount decimal paymenttype varchar encoding dict triptype smallint pickup varchar encoding dict dropoff varchar encoding dict cabtype varchar encoding dict precipitation smallint snowdepth smallint snowfall smallint maxtemperature smallint mintemperature smallint averagewindspeed smallint smallint pickupctlabel varchar encoding dict pickupborocode smallint pickupboroname varchar encoding dict varchar encoding dict varchar encoding dict pickupcdeligibil varchar encoding dict pickupntacode varchar encoding dict pickupntaname varchar encoding dict pickuppuma varchar encoding dict smallint dropoffctlabel varchar encoding dict dropoffborocode smallint dropoffboroname varchar encoding dict varchar encoding dict varchar encoding dict dropoffcdeligibil varchar encoding dict dropoffntacode varchar encoding dict dropoffntaname varchar encoding dict dropoffpuma varchar encoding dict create environment variable credential mapd read mapdpassword export mapdpassword execute sql script aggregator table created leaf node mapdql p mapdpassword port createtripstablesql cutting dataset roughly half node holding around record node distinctive version loadsql script load different part dataset script first node copy trip csv headerfalse copy trip headerfalse copy trip headerfalse script second node copy trip csv headerfalse copy trip headerfalse copy trip headerfalse avoid going aggregator become bottleneck following command coordinated first server executed independently individual leaf node time mapdql p mapdpassword server time mapdql p mapdpassword server first node loaded dataset minute second second node loaded dataset minute second load time clustered approach really begin shine last june benchmark nvidia tesla single machine took minute second import dataset great see two machine able break apart job complete roughly half time data loaded connect via aggregator see billion record visible mapdql p mapdpassword port mapdql select count trip benchmarking mapd time quoted lowest query time seen series run benchmark use lowest query time way indicating top speed mapdbinmapdql mapd u mapdusername p mapdpassword following completed second select cabtype count trip group cabtype following completed second select passengercount avg totalamount trip group passengercount following completed second select passengercount extract year pickupdatetime pickupyear count trip group passengercount pickupyear following completed second select passengercount extract year pickupdatetime pickupyear cast tripdistance int distance count thecount trip group passengercount pickupyear distance order pickupyear thecount desc although result place benchmark place benchmark recap board worth mentioning recap board sorted fastest query time query time outperformed one benchmark query time fastest ever query seems scale linearly number node cluster run query past single machine number execution took almost twice long first heard mapd would natively support clustering wondered impact network overhead would implementation great see mapd still optimises data locality extremely well network chatter nt seem noticeable impact query time launched cluster using spot instance would spent hour pretty sure best price performance seen benchmark benchmark ran twogenerationsold microarchitecture nvidia ca nt imagine kind number would seen pascalpowered gpus time would look like used two instance performance result impressive say least impressive know lot performance yet unlocked thank taking time read post offer consulting handson development service client north america europe like discus offering help business please contact via linkedin
305,Lobsters,scaling,Scaling and architecture,Building a r/place [clone] in a weekend,http://josephg.com/blog/rplace-in-a-weekend/,building rplace clone weekend,challenge rplace live made original code work subscribe version snapshot rest endpoint pngjs writing server kafka serversent event need polyfill ie making pixel editor browser steam dance using cpu unnecessarily idle stole line code past project hour starting put site live day optimization polish fast enough yet nginxpushstream w thor final thought fighting botnets used draw giant picture angela merkel followed along twitter,friday accepted challenge clone reddit rplace weekend live amazing able build weekend nt genius possible programming made activity making decision typing reddit wrote wonderful blog post made original lot decision already made much load need handle big make palette ui using directly nt copy reddit architecture though simply nt agree technical decision place disagree based decade programming experience still nt lot decision left make clear building reddit weekend would nt enough time code mess monitoring logging ci test access control mobile support also nt load tested thoroughly reddit would need weekend enough time make production ready site like reddit thats ok sure work quick benchmarking show scale past reddit user mark work think wanted data flow app even accepted challenge important know could figure could actually build time favorite architecture sort thing use event sourcing make data flow one way though site ever used redux react appreciate simple make everything every part system responsibility get data send data sephsplace edits start browser hit server go kafka get read kafka server get sent user simple enough edits globally ordered kafka two edits location happen time everyone see final result based order come back kafka could make fancier event log load balanced edits across multiple kafka partition reddit spec say need handle edits per second single kafka partition able manage much load ensure consistency attach version number edit coming kafka event stream first edit edit edit time writing edit check windowversion site curious subscribe version genius system image date long know version catch sending edits example get disconnected version reconnect download operation version snapshot client need load page quickly without downloading entire history operation made rest endpoint simply return png current page make work server store copy page array memory array get updated edits come kafka rendering image png slow hooked pngjs render image take time per render single cpu render page time per second way slow keep probably possible optimize reddit apparently spent lot time bit packing image redis sound like waste time configured nginx fetch image every second time nginx return stale image client catch cached image version number embedded header client connects realtime feed immediately get copy edits happened since image generated make server restarts fast every edits store copy disk current snapshot version server start work like client grab snapshot disk update recent operation kafka good go data flow end looking something like although server keep recent snapshot memory disk data flow one way part easy reason isolation writing server friday knew building started work never actually used kafka rule project like new technology put first case unknown unknown affect design kafka dreamy work kafka event working server rendering image video edit set top left pixel image random color triggering edits via curl browser code yet edits published kafka server get sent event kafka update image refresh page actually image url server return updated png new content endpoint allowing client subscribe live stream edits used serversent event instead websockets first simpler elegant websockets work sse nt support binary data need polyfill ie moved away later later anyway done server basically complete added caching header snapshot put behind nginx strict caching policy moved client making pixel editor browser luckily already written browser game scrollable pannable pixellike editor editor blatantly stole bunch code steam dance client work using canvas one invisible canvas image canvas basically image whole drawing space really simple efficient image known fixed size x fit comfortably gpu memory second canvas drawable area see page canvas dom get resized resize browser time reddit us cs render rplace page fall back using canvas browser nt like needing multiple renderers avoid draw function nt seen wrapper requestanimationframe improves code two way tab background wo nt draw let call draw impunity code time need something redrawn page redrawn matter many call draw make rendering game animation usually render regardless want sephsplace able sit background without using cpu unnecessarily idle panning zooming stole line code past project done added palette swatch ui working client spent hour fighting java zookeeper systemd script kafka nginx hour starting put site live nt manage sleep though day optimization polish good rule thumb want spend time polishing need twice much time pleased whole day tweak optimization project aiming able support reddit number concurrent user edits per second making number go really fun always like measure optimizing really see performance metric shoot initial benchmark pretty depressing video script running sending edits second kafka every part system slower want chrome using entire core cpu rendering animation server using core simply receiving operation kafka sending even kafka embarrassingly slow using cpu process opssecond expect better kafka really small number modern computer reason using much cpu bookkeeping kafka process edit individually kafka sends network packet per edit think server decodes edit individually using msgpack separate javascript object server reencodes edit json string send browser client process edit individually edit need talk gpu upload one lonely pixel whew tired thinking staggering amount work computer fix made change first nt need work peredit much better batch edits second block process together way need pay bookkeeping time second matter many message secondly moved everything binary message binary encoding beautiful edit fit perfectly byte look math edit x color triple x coordinate integer almost perfectly represented bit integer bit different value color fit exactly bit x bit bit color bit bit byte perfect batch hundred edits efficiently byte array great byte array blazing fast much faster easy optimize hardware software gcefficient compared j list cheap access c code like say nodejs networking stack also writing bitwise code always make feel like hacker third change move serversent event websockets needed sse nt support binary message send edits sse need encode string would slow increase size message websockets support sending binary message directly easier use done edits per second looked like notice chrome cpu node process server using cpu kafka nt listed using cpu handle larger message every second threw minor optimization video well adding batching tweaking w parameter stuff like love optimizing code feel cleansing fast enough yet actual performance bottleneck need fast turn big scaling challenge actually getting data kafka client well even naive version code could handle required writes per second easily thats tiny number remember need support active client server get edits per second need send million edits per second paper writes byte data sending data per second client traffic large manageable well optimized c program able send network traffic per second sweat really want something like nginx designed event log written c node wo nt fast enough closest found nginxpushstream look perfect designed exactly use case nt like nt guarantee message order delivery remember need consistent message order everyone see result two people edit pixel time effectively nginxpushstream udp want tcp definitely good enough project nt want write code replay reorder message use need worker process simply tail kafka log forward nginx need need special catchuponreconnect logic stream sends would nt support subscribing specified version number another approach would send event using long polling sound wild make url block edits client could request next block edits directly nginx nginx configured hold request client send request backend data server hold request data available ie second passed get nginx cache edits support catchup fine sad way long polling pretty weird way use nginx case lucky say nt need turn binary message handling w fast enough anyway laptop manages client using one cpu core client take modern cpu core several time faster expected would grateful performance optimization gone w websocket library nodejs last year horizontal scaling like put extra load kafka kafka handle order magnitude load need worry node process see screenshot websocket testing library thor modify little work though day declared challenge complete feeling virtuous better rested would rented aws machine set full cluster thought spending hour setting zookeeper enough convince declare victory play computer game instead fine fix live need right final thought wild ride nt gotten much sleep spent altogether much time deleting nazi symbol penis fighting botnets used draw giant picture angela merkel think make something like like live stream whole thing one enjoyable part process going online seeing people drawn sort project real community thing like involve community future site nt know leave worried people start drawing child porn something nt keep eye working feel like interesting design question policy rate limiting bot cool way powerful human limit edits per minute enough community keep going maybe could make version every user energy bar different area space either le volatile draw volatile section free easy others draw top want art stay long time draw slow region take age draw first place maybe space start small one small tile community slowly add tile tile edited hour get locked forever forming slowly growing mosaic sure lot cool variant thankyou reddit making rplace inspiring thanks everyone drawn awesome stuff page followed along twitter fun ugh feel weak forget eat oops
307,Lobsters,scaling,Scaling and architecture,Scaling Unsplash,https://medium.com/unsplash-unfiltered/scaling-unsplash-with-a-small-team-fbdd55571906,scaling unsplash,scaling unsplash small team request million event update million image team build boring obvious solution focus solving user problem technology problem throw money technical problem,scaling unsplash small team fun thing building unsplash sheer size scale popularity producton average day api handle request unsplashcom thousand party application data pipeline process million event feed add update serve million imagesat time team relatively small designer frontends backends data engineer one tasked devops everyone spends majority time experimentation new feature development fuel growthwhile accomplished lot unsplash still beginning journey product business still lot prove meaning need entire team focused solving problem unique unsplash one every company share like deployment network security infrastructure dependency management etcover past year developed set principle allow u focus growth away scaling challenge unfortunately looking magic bullet none common sense set principle liberally borrowed build boring obvious solutionsaka reaching introduce new tool whether new database rethinkdb rocksdb etc new pattern functional everything new architecture microservices rescue exhaust obvious option firston backend problem made good enough using standard workhorse tool triedandtrued pattern like caching batching asynchronous operation prerequest focus solving user problem technology problemsunsplash product company technology company given lot money investor specifically focus solving product market problem trying eek improvement operating cost application common technologyinstead focus time connecting prebuilt technology way solves user problem expands unsplash community part unique unsplash succeed building something unique value tackle optimization heavy customization later date optimization may main source growth leftthis confusing prospective teammate hear unsplash scale small team usage imagery ai future feature realize use lot shelf technology service framework paying small premium punt inhouse development technology line future teammatesdeployment pipeline server configuration system dependency data processing data analysis image processing personalization name example area chose focus investing engineering resource choosing instead service handle throw money technical problemsthe flip side focusing technology problem paying premium access prebuilt technology serviceit become bit joke within team first response problem tried throwing money joke one favourite approach problem solvingoptimizing cost associated infrastructure technology common problem simple repeatable solution investorbacked product company concern feel growth topline metric longer number one prioritythrowing money technological problem free team focus nonrepeatable hard problem like figuring collectively grow user base quarter
309,Lobsters,scaling,Scaling and architecture,Solving Our Slow Query Problem,http://www.scalingsaas.com/posts/solving-our-slow-query-problem/,solving slow query problem,click redirected,click redirected
312,Lobsters,scaling,Scaling and architecture,Pattern: Using Pseudo-URIs with Microservices,http://philcalcado.com/2017/03/22/pattern_using_seudo-uris_with_microservices.html,pattern using pseudouris microservices,basic need place thinking going microservices route understand object identity classic book value object entity pattern using simple numeric identifier monolith first enumerability distributed generation data type coupling tech medium dubbed partially sortable still allowed client make assumption identifier integer every time execute change like make sure client ready like one try use javascript parse identifier mixed collection wrote detail new iteration search platform still see attribute several endpoint puris borrowing solution internet uniform resource name urn uniform resource identifier uris url pseudouris curie creating goodenough puri spec rfc percentencoding rule two object namespace collection identifier considered equal benefit namespaces isbn curie syntax namespaces collection bound system discussed blog data tends live much longer code microservices code tends live even shorter period ubiquitous language storing puris efficiently welldocumented scott ambler many year ago tolerant reader bff acknowledgement kristof adriaenssens bora tunca rahul goma phulore hannes tydn vitor pellegrino sam starling deep kapadia greg gigon thompson marzago,spent time talking basic need place thinking going microservices route even place mean going find new surprise microservices impose distributed architecture requires u revisit many concept considered solved problem traditional scenario one concept implement identity object usually nobrainer monolithic architecture becomes interesting challenge distribution collaboration service understand object identity discus thing change distributed service scenario let try build working definition object identity first thing clarify mean word object text going use objectoriented literature text object mean bag data cohesive attribute likely model abstract concept include behaviour besides state messagepassing relevant discussion classic book bertrand meyer describes challenge working unique identity object two object different identity might identical field field certain object may change execution system affect object identity would like add third challenge something becomes commonplace code store object persistent storage like database even minimal support parallelism two distinct object instance without identical field might identity hence represent object basically modern system always assume one instance object memory given time meyer defines object identity property object help deal challenge property uniquely identifies object independently current content field something tricky idea automated way define meaningful identity make two object identity depend business domain application logic programming language programmer define identity usually overriding equal method implementing protocol type class used code thirdparty library runtime testing identity although represents work programmer also allows lot flexibility language runtimes make assumption equality implement sophisticated technique value object entity pattern using simple numeric identifier even value object useful certain situation important object system tend entity mean object able change state eg user change home address changing marital status keeping identity changing address getting married divorced change user historically implementing identity entity simple use primary key similar unique identifier whatever persistence technology use attribute identifies unique object usually called id direct relationship databasespecific identifier object identity lead several problem one potential issue coupled object become specific database implementation another challenge might raise potential security privacy concern even challenge strategy usually considered goodenough case fact default identity implementation various popular framework tool monolith first approach common way go object identity move architecture towards distribution component new issue arise let discus challenge one might find number distributed service scale usage increase enumerability way database framework use automatic identifier monotonically incrementing integer ie sequence go like enumerable identifies many benefit including providing cheap way sort data recency leaking detail client user cause serious headache building identifier enumerable sequence make simple enumerate object tell comment available http write shell script try download content url http http also make easy guess big dataset write new comment blog id reasonably sure comment across whole blog mentioned making easy guess url identifier may bring question privacy security avoid proper authentication authorisation control experience real problem easy guess identifier business reason industry made mostly venture capitalbacked private company use growth engagement number currency allowing thirdparties guess many object many active etc never great distributed generation like underlying infrastructure system one way structure microservices segment shard system many different way split shard one popular strategy distribute globally besides benefit brings storage large data set also allows architecture make sure user client system served service geographically closer tends improve quality service let illustrate oversimplified picture kind architecture described common system million user global footprint organisation never reach scale justify case privacy regulation patent law various external factor may require engineering team deal constraint actual scalability need irrespective need driven towards geographic distribution one characteristic sharded system like different instance service different database creating instance object diagram instance user service would creating deleting user still want perform operation across user organisation need create list contains user shard using simple autoincrement strategy generate identifier likely allocate different range system eg brazilian user id canadian managing range become nightmare especially add new shard system data type coupling integer common data type used database implement automatic primary key field database support many different type integer larger others database expert often try make sure use smallest type fit need making possible database engine efficient store query data common apis internal public let client application know data type identifier tht make sure system also optimised advantage sharing data type information client clear leaking implementation detail lead challenge common successful product reach point optimal data type chosen first enough anymore almost literally run integer probably wellknown occurence problem twitter faced around tech medium dubbed twitpocalypse twitpocalypse name given bug exposed apparently similar bug nature stem fact every tweet sent unique numeric identifier identifier hit number signed integer limit apparently thirdparty twitter client start hitting identifier start turning negative apps likely crash result back twitter depended lot thridparty client tool needed make sure change implement fix problem would break client end near first step ask client small change changed internal system architecture cope scalability issue twitter made several change generates store identifier discussed previous section something move away sequential identifier even able keep new identifier partially sortable change twitter asked folk use larger integer type still allowed client make assumption identifier integer next year converted identifier new numeric format even four year first incident every time execute change like make sure client ready even client would adapt changed implementation detail eventually larger size integer caused interesting problem like one try use javascript parse identifier thirdparty developer always harder steer collaborate internal client working organisation nevertheless experience larger scale microservices architecture implementation detail leak service client depend big refactorings even service really hard orchestrate execute mixed collection last issue want explore simpler identifier database integer implementing object identity distributed system often complicated intuitively think back early interviewing soundcloud talking cto cut short skype session fourth time week website search one would expect one important project joined team week later fix search eventually wrote detail new iteration search platform included extracting search indexing logic microservices one relatively small change never previously discussed relevant object identity topic list contains object different type one new feature introduced new search good experience universal search ie single search would return track comment user playlist matching specified criterion single result apart many searchspecific infrastructure application architecture challenge face public internal apis changed previously search result like illustrative example result id createdat userid duration sharing public description couple field recording genre title field recording id createdat userid duration sharing public description sound banana make genre bananatronic title big b given client say type object interested explicitly guess result case track list containing one type object clear anymore maybe could try guessing type based attribute object even way end many case object similar shape example trying distinguish track playlist solution found search team add new attribute called kind result soundcloud public api changed lot past year writing still see attribute several endpoint several challenge approach something personally failed area make single way identify type object sometimes type object defined kind attribute endpoint would use attribute like type even fewer thirty programmer working berlin office one never underestimate conway even single attribute term used across system apis new field fundamentally change way think object identity mentioned previously object identity allows u identify object uniquely verify two instance object object different type id enough distinguish object anymore example identity becomes combination id kind simplicity single id field gone every time service api receives identifier input return one output must also return description object type puris borrowing solution internet facing problem team soundcloud started exploring alternative would allow u simple scalar value still rich enough act good identifier across hundred microservices reading decade industry work matter found something simple could help u uniform resource name urn urn type uniform resource identifier uris opposed url concerned identifier resource locate specification allowed u create structured identifier would contain information object type mean instead confusion created id would id looked like urn urn urn urn urn iterated approach decided follow recent recommendation limit identifier deprecated concept urn instead decided make type uri private schema led u identifier look like soundcloud soundcloud soundcloud soundcloud soundcloud turn swedishborn startup trying disrupt music industry fact spotify uris strong part user experience integration many year pseudouris uris owned managed ietf documented specified countless rfcs document deal sort usecases impedance mismatch happen govern standard used open ecosystem internet properly understand one need learn xml qnames curie many concept standard would never suggest consciously break compatibility uris use internally standard strongly suggest make explicit tradeoff prioritising productivity simplicity versus compatibility practical term mean probably make sure wellknown followed rule model uris invest lot time trying cater possible feature corner case ietf handle make guarantee implementation compatible internet standard know standard hopefully find usage intuitive like call usage uri concept pseudouri puri trying make clear usage different one would expect creating goodenough puri spec rfc defines syntax uris following structure scheme describes namespace theory scheme could define rule rest uri formed puris recommendation organisation standardise single format puris follow irrespective scheme experience goodenough specification puri specify many segment team service owner add new segment encoding escaping expected text identifier treated opaque data opposed part consumer make assumption data format far favourite format reasonably simple namespace collection identifier composed three segment separated segment composed stream caseinsensitive character following normal percentencoding rule segment added removed team service might end defining subschemes using special character eg bobsburgers meat chickenprime bobsburgers meat chickenleftovers bobsburgers meat beefprime first segment scheme prefer calling namespace used describe organisation owns concept segment used identify two object belong owner second segment collection describes type object segment used identify two object type third segment defines identifier object identifier stream allowed character identifier must unique within namespace collection two object namespace collection identifier considered equal benefit namespaces although team service owner degree freedom defining subgrammars within segment feature seldom useful like call first segment namespace instead scheme believe biggest benefit component syntax rule defining scope object might sound like overkill first let look spotify example instead spotify case paste text channel like would slack know referring spotify track opposed something music platform find similar challenge even within organisation let say example news organisation publishes article article usually picture writer usually take picture illustrate piece produce one interesting challenge arises also need deal picture coming thirdparty source like stock photo provider simple picture namespace need find way figure given picture right belong even interesting thirdparty entity generate identifier way eg using auto increment integer conflict whenever situation like arise usually see programmer trying fix issue creating two different namespaces example could every time see solution like though upcoming feature use case evolve manner find parsing namespace across many different service instead trying hack extra bit information limited shape would much rather add namespace third component avoid parse segment altogether simpler namespaces like work well global standard like isbn fall short single organisation assigns identifier object main reason use curie syntax allow one level namespacing namespaces collection bound system first implementing concept common team think puris shaped like myorg mybillingsystem company call homegrown billing system valid reason team try add prefix collection surprised domain deal different kind bill main problem example necessarily existence prefix collection prefix strongly coupled specific system discussed blog term microservices well defined still something happens lot architecture described based microservices new system come go time general data tends live much longer code microservices code tends live even shorter period soon enough mybillingsystem replaced myevenbetterbillingsystem either legacy puris go big useless migration two collection effectively example one type bill system avoid using technical term distinguish instead use domain concept something straight ubiquitous language something like myorg storing puris efficiently team introducing puris first reaction database expert typically get angry one good reason use database primary key efficient tend reasonably small integer database programming language deal well even sophisticated type like uuids usually handled native level database reality necessarily lose storage efficiency use puris one solution problem provide clear mapping function puris stored database example puris like myorg myorg need provide set function able map format whatever optimal way want store issue different general impedance mismatch objectoriented system relational database solve applying method like welldocumented scott ambler many year ago one way puris used across microservices different object monolithic objectoriented system latter often control many type usually class exist control using microservices make assumption many collection namespaces exist suggest implementing replace type code booleans pattern suggested ambler dealing microservices need apply tolerant reader pattern way storage notice similar mapping function help minimise amount data transmitted client application especially dealing customer bad network like using mobile phone bff great place store mapping function acknowledgement time soundcloud pseudouriurns architecture maintained core engineering team composed kristof adriaenssens bora tunca rahul goma phulore hannes tydn vitor pellegrino sam starling deep kapadia greg gigon bruno carvalho thompson marzago gave feedback draft article
313,Lobsters,scaling,Scaling and architecture,Scaling Financial Reporting at Airbnb,https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040,scaling financial reporting airbnb,tracking money scaling financial reporting airbnb historically payment team focus implement new feature currency payment method make payment local global business trip experience prior financial system mysqlbased data pipeline mysql database trigger scaling challenge goal introducing new financial reporting pipeline apache spark hdfs scala brief overview work double entry accounting key takeaway future looking software engineer data scientist,tracking money scaling financial reporting airbnbat airbnb payment team responsible everything related moving money airbnb global marketplace build technology power airbnb massive daily transaction volume collect payment guest distribute payouts host goal make payment experience airbnb delightful magical intuitivehistorically payment team focus implement new feature currency payment method make payment local global business sphere grown include compliance sale tax earnings tax license well reconciliation financial accounting according generally accepted accounting principlescurrently airbnb payment financial accounting system complex ecosystem transacts country currency processor airbnb transaction volume experienced exponential transaction growth every year also rapidly increased feature product platform airbnb hope become premier endtoend travel service helping people accommodation also trip experience wellthe challenge maintain existing financial accounting system support new product well increasing data volume become mission impossible sort taskairbnb finance infrastructure engineering team responsible delivering accurate reliable comprehensive businessfinancial data stakeholder blog post talk manage keep track money move scalable way face exploding data size complexity well support new airbnb initiative payment product share workflow deprecated finance system illustrate challenge issue describe new system built replace itthe prior financial system mysqlbased data pipelinebuilt early retired late previous financial system mysql data pipeline parameterized mysql etl ran nightly provide financial reporting served u faithfully past year workflow follows enabled mysql database trigger main table able capture change airbnb reservation payment record happen real time per row basis way enforced immutability ensured financially relevant event would captured meant temporary solution deal inherent mutability production data acknowledge great pattern necessary timeby able replay history event database trigger built set intermediate helper table assist calculating different report example kept track recognized revenue guest receivables future host payouts liability essential component financial reportingbased helper table built financial reportingthe scaling challengethere advantage approach since relied mysql db trigger mysql guarantee data accuracy engineer lot flexibility change business logic move fast sql based report written fast logic simplehowever sqlbased etl approach scaling well right language programming modelsql good lightweight data transformation designed handle complicated business data flow modern software design principle easily applied decompose complexitythe original logic tightly coupled core reservation logic airbnb grew company needed support product logic original reservation flow eg needed pay professional photographer translator every product change unique money flow thus financial accounting impact result build many additional report meet business need mysql logic structured around reservation logic hard modify extremely error prone adding new logic productsvalidation testing became impossible get comprehensive result pulled data separately different report finance team combined discretion built report separately could difficult tell change report caused issue number mismatch scalable company wanted change product add new product frequentlyas added reporting logic transaction volume grew developing report using sql script became complex prohibitively became extremely difficult time consuming test logic accuracythe nightly run taking much timeas transaction volume grew transformation logic became complex nightly pipeline took time relational database hard scale difficult shard data difficult leverage distributed system process massive amount data towards end life able run every day due runtime hoursour goal grow needed able cope dramatically increasing data size frequent addition new airbnb product payment channel thus two goal new system new system give u enough flexibility support product well deal product change accounting logic change need decouple financial logic product logic representation product behavior generic financial system eg book receivable payable revenue tax etc thus build sustainable financial report based highly normalized datathe new system scale horizontally volume grows able scale system adding machinesintroducing new financial reporting pipelineour event based financial report designed support current future product type platformhave holistic view event financial impact platformhorizontally scale transaction volume growsit powered apache spark stored hdfs cluster written scala spark fast general engine largescale data processing implicit parallelism faulttolerance chose scala language wanted latest feature spark well benefit language like type closure immutability lazy evaluation etcthis huge considering previous language sqla brief overview worksour new financial reporting system concept different product type reservation one product type set platform payment event corresponding set financial event thus address product type individually systematically build holistic reportthe system thought many event handler calculate accounting impact different product different point life cycle scala strong static type system providing full support functional programming easy design write handler different product process thembelow diagram data flow system worry explain everythingplatform event event provide information product related change like reservation reservation alteration photography cancellation etc usually financial expectation associated always case time product created updated derive event product example reservation booked emit booking event reservation product type day reservation start consider reservation service rendered service delivered customer guest case recognize revenue event important financial accounting implication talk belowpayment event describe money movement event real money move airbnb bank account payment event also describe stored value like someone buy us gift card kind payment event money may actually move still account lack cash movement someone sends someone else gift card person claim consider balance transfer virtual movement example money movement would guest us coupon money coupon funded marketing budget money actually moved account need account somewhere money must equal money coupon guest side impact original host payout amount likewise host side operation need take account money equation balancesthese event currently generated examining aforementioned accounting audit row change change accounting impact platform payment event generated system designed central place data pas different system financial reporting system process platform payment event event handler produce accounting event describe accounting impact eventsaccounting event generated event handler build payment platform event introduced layer abstraction represent relationship different platform payment event product well accounting logic sometimes see single platform event generate one accounting event multiple accounting impact different time event basically keep track happened assigning unique identifier product type product id set activity u consider product type id smallest accounting unit operate onfrom accounting event generate subledger basis financial accounting entry detailed accounting record includes information time transaction occurred payment reservation booking amount currency direction monetary impact credit debit account impactsthe subledger generated using double entry accounting double entry accounting allows u sure everything accounted properly system mean money appears disappears without sourceeven though product type may behave differently found generic life cycle product type share let walk reservation would look frameworkan event happens introduces accounting liability contract created money moved point expectation future money flow set accounting event point describe contract createdhere guest confirmed two night reservation refer time booking date treat contract start date price stay fee price breakdown example purpose reflect real reservation also left host fee simplify explanation guest host entered contract airbnb acting platform payment collection agent exchange staying listing two night guest pay airbnb host receive time checkin date reservation confirmation guest receivable future host payable due host consider reservation fulfilled expectation set soon reservation confirmedan event money flow occurs happen anytime contract created accounting event describe direction liability amount fulfillinghere guest successfully pay reservation would consider guest receivable fulfilledthe event happens contracted service fulfilledthis checkin time point would recognize revenue loss example also time scheduled payout host delivered host mean future host payable host payable duemore money flow may occurthis successfully deliver payout host airbnb host payable particular hostsometimes alteration product payment example would guest adding date reservation inducing reservation price change properly account price difference alteration unbook rebook reservation entirely time alterationfor example reservation previously booked guest extended stay later date could either book additional later date unbook reservation rebook day think chose latter alteration add much easier unbook rebook instead computing delta every time cleanest way deal product alteration data backfills imagine alteration long term reservation would look system built subledgers platform payment event handler easily query financial impact different account event generatesan example revenue would queried subledger follows key takeawaysit scale maintaining quality performanceour financial reporting pipeline scale product basis runtime basis easily support new product financial engineering side built framework around right abstraction instead tying closely one specific product life cycle also scale horizontally much better limited amazon rds instance matter beefy may nightly runtime hour growing much march made troubleshooting much simplerbefore finance team needed comprehensive report pulled data separately different report finance team combined discretion built report separately could difficult tell change report caused unexpected deviance scalable company wanted change product add new product frequently issue investigate data single source truth significantly simplifying troubleshooting processit made coding much simplergoing declarative programming functional programming powerful paradigm shift u think financial processing accounting think system straightforward actorhandler system rather getting mired complicated sqljoin logicthe nightly run timely well monitoredoriginally mysql etl scheduled via crontab dependent data arriving via different pipeline instead taking upwards day complete nightly run take around hour complete longer babysit legacy system quite frequently encounter snag caused upstream change taking hour developer time week resolvewe built comprehensive test frameworkthis perhaps important part picture financial processing reporting longer sql also able write extensive suite unit test specific handler together integration test smoke test easily identify regression error smoke test rule expect data follow rule violation occur logged addressed give u high degree confidence quality data let u quickly vet new change roll built extensive ever expanding test suite real transaction check expect look system individually well aggregate test framework critical time sensitive request various partner finance legal well audit partner need confident reportingfuture lookingin future moving towards entirely eventbased system financial reporting system consume event emitted system stay tuned read future blog post help u even greater financial integrity richer vocabulary express different product payment flowsin end want airbnb complete accurate extensible financial reporting current future product airbnb believe designed financial reporting system strong foundation financial processing airbnb clean decoupling business logic accounting logic system product agnostic extensible futureproof give u confidence serve u well many year come start back office financial system airbnbif enjoyed reading thought interesting challenge payment team always looking talented people join team whether software engineer data scientistplease stay tuned payment ecosystem airbnb many thanks sarah hagstrom lou kosak shawn yan brian wey jiangming yang ian logan reading many draft helping write post
314,Lobsters,scaling,Scaling and architecture,Using strace to understand a 10x Java performance improvement,https://blog.packagecloud.io/eng/2017/03/14/using-strace-to-understand-java-performance-improvement/,using strace understand java performance improvement,tl dr ibm jvm executes piece code faster sun jvm thorough bug report fix setting command line parameter posix prerequisite information measuring time linux system man page tsc madness posix compliant clock single unix specification man page series patch hotspot java vm read code vm check java program comparison disabled enabled conclusion related post,tl drin blog post examine uselinuxposixthreadcpuclocks command line flag jvm starting patch update sun jvm default value switch changed true yielding nice performance boost roughly test user recent jvms get behavior default exhuming performance bug interesting show strace useful even java programsexamining old performance bug important instructive u programmer sort computer archaeology allows examine use tool available u today detect problem significant past also learn interesting history lesson development evolution computer system help u understand got today easy use maven repository freeibm jvm executes piece code faster sun jvmthis thorough bug report sun jvm show small piece java code run slower sun jvm v ibm jvm according person wrote report note test modern jvm show improvement result analysis code simply attempting get current thread cpu time calling getcurrentthreadcputime function tight loop like private final static long cputime return javalangmanagementmanagementfactorygetthreadmxbean getcurrentthreadcputime fix setting command line parameterthe performance issue mitigated earlier jvm version simply setting command line flag xx uselinuxposixthreadcpuclocks flag instructs jvm attempt obtain thread timing information using system call flag outlined posix result bug report option default value changed truewhy option enabled sooner exactly dive specific flag purpose serf let go important prerequisite informationprerequisite informationthe internals measuring time linux changed kernel glibc sometime around kernel need understand detail change entailed fully understand jvm uselinuxposixthreadcpuclocks flagmeasuring time linux systemsthere series function user program use measure time process thread understanding function related important understanding performance bug investigatingthe key function particular case clockgettime system call implement several different type clock specified clock idclockgetcpuclockid glibc function get clock id specified procces process id pthreadgetcpuclockid glibc function get clock id specified thread pthreadt depending whether program want get process time thread time call clockgetcpuclockid pthreadgetcpuclockid pas returned clock id clockgettime note api provides streamlined method getting current process thread cpu time clockprocesscputimeid clockthreadcputimeid respectively read man page detailsallowing cpu time obtained process thread part make api linux posix compliant always way time clockgettime returned total time since thread process created instead total cpu time tsc madness posix compliant clocksoriginally glibc implementation posix clock meet definition put forth single unix specification particular specification dictate clockgettime return amount cpu processing time original glibc implementation simply returning amount time passed since process thread started executionmoreover original implementation based reading value timestamp counter register tsc cpu reading value problematic numerous reason many cpu time still today stable tscs mean tsc cpu may increment rate respect cpu frequency may adjusted various power saving systemsthe tsc various cpu smp system may synchronized process thread migrates one cpu another value returned clockgettime may bogusthis documented note section clockgettime man page entertaining interesting readeventually series patch proposed fix making clockgettime posix compliant safe use even thread migrated cpu different tsc valuesthis well good related back uselinuxposixthreadcpuclocks uselinuxposixthreadcpuclocks since glibc linux kernel implement posix compliant cpu timing jvm added flag uselinuxposixthreadcpuclocks control two cpu timing method would used linuxduring initialization hotspot java vm function called o linux fastthreadclockinit called begin first checking uselinuxposixthreadcpuclocks flag void o linux fastthreadclockinit uselinuxposixthreadcpuclocks return see flag serf way bail fastthreadclockinit function exactly flag protecting read code find jvm use value uselinuxposixthreadcpuclocks determine whether attempt detect pthreadgetcpuclockid function exists work systemif check pas internal flag set true supportsfastthreadcputime vm check whenever java program attempt check cpu time flag true thread cpu time checked call pthreadgetcpuclockid clockgettimeif flag false thread cpu time checked opening file proc pid stat parsing outputas original bug report suggests using pthreadgetcpuclockid clockgettime much faster test opening file reading content parsing cleaning returning cpu timejava program strace comparisonlet compare strace output test program provided original bug report openjdk uselinuxposixthreadcpuclocks flag enabled disabled class test public static void main string args long base cputime int k k k cputime systemoutprintln took cputime base m private final static long cputime return javalangmanagementmanagementfactorygetthreadmxbean getcurrentthreadcputime build program javac testjavawith uselinuxposixthreadcpuclocks disabledon jvm default recent jvms option need explicitly disabled let expect see lot call open read etc proc filesystem consulted trying determine cpu timerun program strace fceclockgettime open read close fstat java xx uselinuxposixthreadcpuclocks test strace fc java xx uselinuxposixthreadcpuclocks test took m time second usecscall call error syscall read close open fstat clockgettime totalthe program completed second thousand call read thousand call read close open fstatjust get current thread cpu time wownote thousand clockgettime call unrelated test program recommend motivated reader take look find come interesting uselinuxposixthreadcpuclocks enabledon jvm version default need specify anything expect see far fewer call open read et al many call clockgettime let explicit bout enabling uselinuxposixthreadcpuclocks incase anyone supporting legacy jvm want see itrun program strace fceclockgettime open read close fstat java xx uselinuxposixthreadcpuclocks test took m time second usecscall call error syscall clockgettime read open close fstat totalthe program completed second nearly faster call clockgettime far far fewer call open read et alconclusionas mentioned post big fan using strace anything everything including java program strace free run constantly however run strace time time ensure program expect often highlight interesting behavior possibly reveal incorrect assumption made underlying system librariesrelated postsif enjoyed post may enjoy lowlevel technical post
315,Lobsters,scaling,Scaling and architecture,The Back-end for Front-end Pattern (BFF),http://philcalcado.com/2015/09/18/the_back_end_for_front_end_pattern_bff.html,backend frontend pattern bff,soundcloud understanding evolution software component breaking monolith described previous article people like fred wilson predicted challenge dogfooding dogfooding backend frontend pattern bff new io application nick fisher presentation model bff part application way acknowledgement kristof adriaenssens bora tunca rahul goma phulore hannes tydn vitor pellegrino sam starling fabio kung paul osman emerson macedo caio filipini,soundcloud transparent architecture evolution part technology strategy something talked countless occasion never really described detail application backend frontend architecture pattern bff post document understanding developed applied technique understanding evolution software component fully distributed architecture became feasible organisation would usually build application one tier tier highlycoupled fairly independent component application coupled sense opposed service meant used one application independent run part process often even machine let illustrate three fictional application larger company would develop back architecture could get complicated overall easy draw line different application clearly demarcating one start end back application copy data duplicated implementation common business process time organisation acquired built application realised needed something different needed application share data reuse logic simple architecture became bit complicated need reuse consolidation collective mindset software industry settled quite abstract concept called service practical term mean diagram changed something akin selling point architecture flexibility reusable service offer theory building application platform matter selecting service need write glue code call service merge data get back something familiar enduser render data way enduser consume time computer internet becoming popular customer used interact clerk system operator started directly interacting application design thinking user experience research moved u away complicated user interface focused making expert user efficient richer usable experience would understood read manual website richer experience require rich data mean aggregating information various source following example end something like diagram instead user interface lineofbusiness system ended user interface application right application often written jsp php asp code contained user interface applicationspecific backend logic breaking monolith oversimplified example different lot modern tech organisation evolved architecture soundcloud website looked like logic logic one place one system system application described previous article found many problem architecture decided extract logic microservices successful extracting backend service longest time mothership still critical path every single request main motivation behind architecture change making reducing timetomarket new feature detected worst bottleneck change touch monolith considering often user interface change extracting code monolith intuitive way boost productivity extracted ui layer component made fetch data public api back architecture change happening vast majority user web people like fred wilson predicted eventually changed user base started using mobile apps way often web interface soundcloud mobile client android io long time similarly new web application talked directly public api challenge dogfooding modern software engineering dogfooding usually considered good thing building product top api perceived best way make sure api highquality always uptodate practice experienced several problem approach first issue necessarily related technology fundamental challenge product development use public api nothing could offer platform available thirdparty api client much wanted thriving ecosystem soundcloud integration advertisement business needed make sure people using property data creating feature exclusive application meant constantly check oauth scope many place make hard people spoof official app key technical problem public apis almost definition generic empower thirdparty developer build interesting integration need design api make assumption data going used result finegrained endpoint require large number http request multiple different endpoint render even simplest experience see many request used make monolithic day versus number make new web application generate single profile page would make many call different api endpoint eg get author track get track recommend related get information track author get usersmejson information current user web application would merge create user profile page problem exists platform even worse growing mobile user base often used unreliable slow wireless network third even annoying problem architecture even without monolith still bottleneck api every time team needed change existing endpoint needed make sure change would break existing client including important thirdparty integration whenever added something new invest lot time making sure new endpoint overspecialised specific app client could easily use coordination made daytoday much harder also made almost impossible u ab testing slow rollouts new feature backend frontend pattern bff almost one year debut architecture started gearing develop would new io application massive project would ultimately change user experience across property high stake experimentation iteration development crucial engineering team started thinking application architecture saw challenge described would become blocker project needed rethink way thing first proposed solution would different apis mobile web idea team working client api would allow move much quicker required coordination part original idea different backends different frontends term bff coined tech lead web nick fisher initial suggestion beffe dutchspeaking team mate vetoed option first incarnation backends still looked much look like public api many generic endpoint required many call client render single screen time though saw something interesting happening using user profile page example previously concept existed client side web mobile application would fetch data various endpoint use create object called user profile applicationspecific object point client team realised since owned api could push object api could extract logic made many call different service mashed together user profile backend would ultimately simplify code improve performance instead making multiple different call many endpoint described client needed request single resource get experimented model found writing much presentation model bff stage realised bff api used application bff part application eventually property including apis started following pattern way point five different bffs production begun looking increase productivity even following user profile example something became obvious u given every single application equivalent user profile page lot duplicated code across bffs fetching merging data duplication exact larger screen like web browser would much information user profile page tiny mobile apps nevertheless saw duplication bad smell indicating missing object domain model fix created userprofileservice would deal duplicated logic time found situation like started consciously moving towards architecture core object understood user microservice backing acknowledgement time soundcloud bff maintained core engineering team composed kristof adriaenssens bora tunca rahul goma phulore hannes tydn vitor pellegrino sam starling fabio kung paul osman emerson macedo caio filipini gave feedback draft article
316,Lobsters,scaling,Scaling and architecture,Micro-optimizations matter: preventing 20 million system calls,https://blog.packagecloud.io/eng/2017/03/06/micro-optimizations-matter/,microoptimizations matter preventing million system call,tl dr setting tz environment variable avoids thousand system call noticeable improvement environment variable application set avoid thousand extra system call nonvdso enabled kernel code look deeper test case million real world example puppet bug filed puppet comment complaining stackoverflow question similar bug report happen easy fix vdso pas vdso changing configure flag make go away conclusion related post,tl drthis blog post followup previous post setting tz environment variable avoids thousand system call post explore particularly prominent case microoptimization like removing system call hot path drastic effect software performance setup npm registry freewhat noticeable improvement previously described environment variable application set avoid thousand extra system callsour post met reasonable question bit skepticism neat removing single system call result noticeable performance improvement program seems unnecessary system call fast linux anyway difficult say noticeable might mean developer application kernel driver developer often devote considerable amount time microoptimize code data structure way maximize usage cpu cache reduce amount cpu usage even programmer might consider tiny amount application programmer pay le mind sort optimization one might consider microoptimizations noticeable eitherfor purpose article let take noticeable mean easily measurable obvious show real world example removing slow nonvdso enabled system call code path easily measurable obvious turn infact many instance wild everything packet sniffer programming language runtimes let look particularly infamous case sigprocmask effect ruby language runtimewhat sigprocmask sigprocmask system call used check set current process signal mask allows program block permit signal useful program need execute critical section code interruptedit particularly complex system call kernel code sigprocmask show writes sigsett c structure contains current process state called taskstruct kernel fast operationlet create simple test program call sigprocmask tight loop use strace time measure include stdlibh include signalh int main int argc char argv int sigsett test sigprocmask sigsetmask null test return compile gcc test testc execute first time strace timeon test system running simply time test yield order real user system time respectivelyrunning time stace ttt test show overhead imposed strace quite clearly real user system time respectivelythe strace output show approximate execution time sigprocmask call likely show rtsigprocmask system approximate time small test system get something like time occasional jump way turn measuring exact system call execution time difficult number reason far outside scope article assume though measurement equally incorrect let review know sigprocmask system call used set check current process signal maskthe kernel side code simple execute quicklythe time strace output show executing million sigprocmask call tight loop terminates quickly call taking small amount timewith number like could removing extra call sigprocmask ever worth effort look deepersometimes code use application like system library kernel glibc etc thing quite expect side effect seem obvious front illustrate point going show severe performance penalty incurred indirectly using sigprocmask test program show manifested real world applicationone prominent example extra sigprocmask call leading directly obvious easily measurable performance degradation old friend ruby performance loss occurs ruby compiled one particular configure flagwe start using ruby default configure flag used operation system debian ubuntu et al time ruby widely used examine test case illustrating performance loss followed real world example explanation happened rubysigprocmask test caselet take look following test code def makethread threadnew apop end end end makethread makethread tjoin code simple creates two thread add remove data array million time run default ruby configure flag strace see surprising result strace ce rtsigprocmask tmptestrubyusrlocalbinruby tmptestrb process attached process detached time second usecscall call error syscall rtsigprocmaskover million system call sigprocmask generated ruby vm code pasted may think look little time consumed way matter mentioned measuring system call time bit complicated let rerun test program time strace get reading long test program take complete system time tmpgogousrlocalbinruby tmptestrb real user sys second real time execute mean code making roughly million sigprocmask call per second wowif adjust one configure flag building ruby slightly get ruby vm build system avoid making call sigprocmask let rerun strace time test ruby adjusted slightly avoid making sigprocmask call see get strace ce rtsigprocmask tmptestrb time second usecscall call error syscall nan rtsigprocmaskwhoa reduced call sigprocmask million look like strace trouble computing system call execution time let see time say time tmptestrb real user sys faster real time previous example le sigprocmask call per secondso cool result raise important question example bit contrived would actually matter real world going adjusting configure flag actually causing let look real world example mattered dive detail actually going onreal world example puppeta bug filed puppet show precisely impact extra sigprocmask system call ruby vm currently facing consider serious performance issue fact puppet seems slow generallet show first example time puppet user sys time hundred thousand rtsigprocmask sigblock null call made show versionif follow thread find comment complaining poor performance puppet seaches show stackoverflow question expressing concernit ruby puppet though find similar bug report project user reporting cpu usage hundred thousand call sigprocmask melting cpushow happen easy fix really going call sigprocmask coming inside pair glibc function system call getcontext setcontextthese two function used save restore cpu state commonly used program library implementing exception handling userland thread case ruby setcontext getcontext used part userland thread implementation context switch threadsyou imagine pair function would execute quickly need save restore small set cpu register turn addition saving set cpu register fast operation implementation function glibc sigprocmask system call used save restore signal maskrecall linux provides mechanism vdso certain system call executed userland instead kernerl order reduce cost associated making system call unfortunately sigprocmask one system call provided vdso sigprocmask system call cause transition userland kernelthe cost transition much higher cost operation setcontext getcontext simple memory writes calling function often performing slow operation case system call sigprocmask pas vdso every time intended something quickly like saving restoring cpu register set switch thread changing configure flag make go away time ruby widely used default configured enablepthread enabled separate oslevel timer thread would occasionally run preempt ruby vm ruby vm knew time switch userland thread mapped thread created ruby program using enablepthread also caused configure script search use function getcontext setcontext use enablepthread configure script would instead search use setjmp longjmp note underscore instead function save restore signal mask thus thus make system call sigprocmaskso enablepthread disablepthread intended used control whether ruby vm userland threading implementation would use o level thread simple vtalrm signal notify ruby vm time switch execution different ruby threadthe unintended side effect switching two preemptiong method underlying primitive used implement context switching swapped setcontextgetcontext setjmplongjmpthe additional side effect low level function swap either additional call sigprocmask setcontextgetcontext pair chosen notthe additional side effect reduced performance pair us sigprocmask chosenthe real world result user puppet software poor experiencephew one configure flag essentially enables disables single system call hot pathconclusionmicrooptimizations matter extent matter course application specific aware library code depend thing might expect like make slow system call hot path knowing tool use detect correct sort issue vast impact user case user usersrelated postsif enjoyed post may enjoy lowlevel technical post
318,Lobsters,scaling,Scaling and architecture,How setting the TZ environment variable avoids thousands of system calls,https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/,setting tz environment variable avoids thousand system call,tl dr quick summary problem linux system call vdso system call method sample program showing issue verifying effect production system conclusion related post,tl drthis blog post explains setting environment variable save thousand case ten thousand unnecessary system call generated glibc small period timethis tested ubuntu precise ubuntu xenial likely applies flavor linux well easy test applies correct keep reading detail setup npm registry freequick summaryto avoid extra system call server process updating timezone restart process simply set tz environment variable etclocaltime timezone file choice process cause glibc avoid making extra unnecessary system callsto understand test process benefit read problemin previous blog post linux system call explained different system call method highlighted interesting method particular vdso system call methodthe purpose vdso system call method create way certain frequently used system call like gettimeofday time clockgettime etc avoid needing actually enter kernel cause context switch user land kernel land result method certain system call like listed used program much much lower costwhat would happen every time called one fast vdso system call like time also called normal system call like say stat pas vdso essentially negating performance improvement meant gain vdso optimization first place making slow system call oftenit turn situation happens rather often pair function commonly used together time vdsoenabled system call used obtain number second since epoch andlocaltime glibc provided function convert output time local time user timezone system call internally localtime make system call casesthe time vdsoenabled system call localtime function glibc often used together application either directly programmer lower level unbeknownst programmer formatting date time everything log message sql query pair commonly used ruby railsit turn localtime function glibc check tz environment variable set set two ubuntus tested set glibc use stat system call every time localtime calledin word system support calling time system call via linux kernel vdso avoid cost switching kernel soon program call time call localtime immediately invokes system call anyway eliminated one system call vdso replaced anotherlet see sample program show behavior use strace detect finally prevent setting tz environment variablesample program showing issuelet start creating simple test program reproduces issue include timeh include stdioh int main int argc char argv int timet timep printf greeting n time timep localtime timep printf godspeed dear friend n return compile program simply running gcc test testc see program simply call time localtime loop timesverifying straceevery single call localtime glibc function generate system call stat believe let use strace prove using program shown gcc test testc strace ttt test truncated output write greeting n open etclocaltime ordonlyocloexec fstat fstat read lseek seekcur read close stat etclocaltime stat etclocaltime stat etclocaltime stat etclocaltime stat etclocaltime stat etclocaltime stat etclocaltime stat etclocaltime stat etclocaltime write godspeed dear friend n strace output see thing etclocaltime file openedfstat called twice followed two read file pull timezone datanext call stat made passing etclocaltime overnotice strace output show call time expected system call made via vdso appear strace output see need use ltrace insteadwhat going first call localtime glibc open read content etclocaltime subsequent call localtime internally call stat ensure timezone file changedon many system etclocaltime symlink timezone file conceivable program might running etclocaltime symlink updated happen glibc would notice localtime called reread file time conversionsmany production system use utc timezone ever need want change use case reason stat etclocaltime file symlink localtime called timezone never going change utc application restarted easiest way prevent stat call set tz environment variable tz environment variable set glibc notice told explicitly timezone file use programread file cache internallynever stat read file path long value tz environment variable left unchangedlet set tz variable check strace tz etclocaltime strace ttt test write greeting n open etclocaltime ordonlyocloexec fstat fstat read lseek seekcur read close write godspeed dear friend n see simply setting tz cause glibc read etclocaltime single time never program run fewer system call single environment variableeffect production systemsthe effect setting tz production system depend mostly often localtime called application specific vary request load said changing timezone often ever may worth simply eliminating unnecessary call even system making many themon test system real life app example shown without setting tz normal operation yield approximately call stat second period roughly stats per second tz set time period result call stat second periodso eliminated order extra system call associated context switch without changing anything environment variable pretty coolconclusionusing strace questioning pattern system call emerge infrastructure help better understand exactly system may even able remove unncessary system call save system resource toorelated postsif enjoyed post may enjoy lowlevel technical post
320,Lobsters,scaling,Scaling and architecture,$5 Showdown: Linode vs. DigitalOcean vs. Amazon Lightsail vs. Vultr,https://joshtronic.com/2017/02/14/five-dollar-showdown-linode-vs-digitalocean-vs-lightsaild-vs-vultr/,showdown linode v digitalocean v amazon lightsail v vultr,updated march since vultr updated offering data table updated applicable section update near end section usually wait run fresh benchmark provider comment section blowing vultr update vps hosting showdown linode need plan linode offering per month instance overview march update cpu info march update cpu benchmark march update memory benchmark memory read memory writes march update file io benchmark march update mysql benchmark march update apache benchmark march update network benchmark march update conclusion ubuntu march update digitalocean linode upcloud vultr,updated march since vultr updated offering data table updated applicable section update near end section usually wait run fresh benchmark provider comment section blowing vultr update every time post vps hosting showdown flooded comment people want focus solely price regardless benchmark result always say thing linode need plan somebody mention always say thing given enough time probably offer plan said thing people barking linode even plan linode offering per month instance also added high memory instance added ssd storage per month plan expect showdown soon overview linode digitalocean lightsail vultr memory processor core core core core storage ssd ssd ssd ssd transfer overage network network price right gate linode offering twice much ram digitalocean amazon lightsail vultr even though vultr offering bit ram offer le storage server built running ubuntu lts exception lightsail virginia rest instance new york new jersey area even though company price except linode hourly slightly highter offer special amazon offer first month free without promo code everybody else offer credit enough cover least month case linode digitalocean since bonus code expired vultr also back month quick note running benchmark vultr borked software update unsure cause received error get lock seen error running apt never brand new server person logged march update vultr bumped plan ram storage comparable ram offering linode slightly storage competitor cpu info exactly underneath hood abridged data proccpuinfo model name cpu mhz cache size bogomips linode intel r xeon r cpu digitalocean intel r xeon r cpu lightsail intel r xeon r cpu vultr virtual cpu versed hardware look like linode digitalocean lightsail comparable cpu linode fastest bunch even appears older cpu vultr speed came second march update vultr originally reported intel xeon ivy bridge clocking box spun run updated benchmark cpu reported virtual cpu cpu benchmark linode digitalocean lightsail vultr number event total time event execution minimum request average request maximum request percentile part everybody performed percentile linode max request nearly high guy march update vultr clocked notable difference maximum request jumped memory benchmark memory read sysbench testmemory run linode digitalocean lightsail vultr number event total time execution time minimum request average request maximum request percentile operationssec mbsec memory writes sysbench testmemory memoryoperwrite run linode digitalocean lightsail vultr number event total time execution time minimum request average request maximum request percentile operationssec mbsec somewhat expected provider offering ram ended faster speed ram important get bang buck linode vultr price point march update vultr remained spot slipped bit since previous set benchmark file io benchmark sysbench testfileio prepare sysbench testfileio filetestmoderndrw run sysbench testfileio cleanup linode digitalocean lightsail vultr number event total time execution time minimum request average request maximum request percentile requestssec mbsec quite mixed bag file io benchmark digitalocean performed decent percentage linode request per second vultr lightsail march update vultr saw slight improvement enough change outcome benchmark mysql benchmark mysql uroot e create database sbtest sysbench testoltp mysqluserroot prepare sysbench testoltp mysqluserroot run sysbench testoltp mysqluserroot cleanup linode digitalocean lightsail vultr number event total time execution time minimum request average request maximum request percentile readwrite requestssec linode best performance percentile readswrite request per second maximum request time nearly second vultr second readwrites also significantly better maximum request time march update vultr saw another bit improvement enough move needle since last time apache benchmark ab kc n http linode digitalocean lightsail vultr concurrency level time taken completed request failed request requestssec time per request transfer rate kbytesec somebody mentioned apache benchmark nginx may future enough interest nginx webserver choice using apache apache benchmark utility make sense sake review march update slight improvement vultr really network benchmark mentioned chose datacenters new york new jersey linode digitalocean vultr lightsail instance life virginia speedtestcli linode digitalocean lightsail vultr distance ping download mbitsec upload mbitsec like include metric always feel like crap shoot varying location server never apple apple comparison digitalocean upload time squeaked lightsail vultr linode able blow past everybody term download speed march update decent bump download speed vultr benchmark distance server ended le though weird spun vultr instance int new york nj data center last time used speed test server houston conclusion linode new plan offering consistently better performance seen past dollar dollar comparison also single digit price point people craving linode still bit behind curve come thing like block storage volume default ssh key yeah ui patience virtue given enough time linode always seems deliver deliver tends blow everybody offering water said digitalocean also big announcement today offering load balancer per month linode offered version called node balancer price point vultr lightsail current offer could also spin instance serve selfmanaged load balancer linode announcement today something expected birthday later year tend make announcement wait see store june like review ymmv need pick provider fit need end making decision based review much appreciate used referral link linode linode digitalocean digitalocean andor vultr vultrspecial amazon referral could always donate ubuntu linux distro use review oh yeah happy valentine day march update vultr latest offering definitely help compete linode offering amount ram looking storage buck would way go otherwise metric saw improvement nothing made offering better everybody else interesting offering plan par digitalocean amazon lightsail plan start seem like race bottom certain point time tell need showdown future digitalocean new account receive credit good day linode new account receive credit good day upcloud new account receive credit vultr new account receive credit good day
323,Lobsters,scaling,Scaling and architecture,Build a serverless app with the first serverless database,https://fauna.com/blog/serverless-cloud-database,build serverless app first serverless database,connect faunadb moment faunadb getting started serverless framework serverless crud service installation usage instruction go social graph tutorial defining function serverlessyml readall example difference faunadb dynamodb interface try via sign form moving beyond demo conclusion,faunadb first truly serverless database post use serverless framework connect aws lambda application faunadb serverless cloud say serverless referring functionasaservice pattern serverless system must scale dynamically per request require capacity planning provisioning instance connect faunadb moment scale seamlessly prototype runaway hit serverless system must scale dynamically per request current popular cloud database support level pay capacity use additionally often lack support join index authentication capability necessary build rich application faunadb faunadb serverless cloud globally distributed database requires provisioning capacity metered available pay use faunadb globally distributed database requires pay use faunadb perfect fit serverless development let code getting started serverless framework serverless leading serverless framework market right clean system configuring writing deploying serverless application code different cloud infrastructure provider took afternoon port one storage example dynamodb faunadb incredibly easy accomplish looking code show u simple set serverless environment crud service porting simple rest api allows creation updating deletion todo item well listing todo time toy example look code describe go adding multilist multiuser data model user invite member read update todo list would lot productive push logic faunadb example limited capability dynamodb also provides readme contains installation usage instruction go get instant access faunadb set running play around faunadb interesting feature like implementing social graph check social graph tutorial defining function first file start reading app using serverless framework serverlessyml defines service link function event handler readall example see function definition one function readall handler handlerreadall event http path todos method get cors true configuration mean readall function handlerjs called http get received todos path look configuration see function linked handlerjs look next moduleexportsreadall event context callback todosreadall event error result const response statuscode header accesscontrolalloworigin body jsonstringify result contextsucceed response everything handlerjs concerned managing http actual logic imported module function case todosreadalljs include entire file place faunadb come play use strict const faunadb require faunadb const q faunadbquery const client new faunadbclient secret processenvfaunadbsecret moduleexports event callback return clientquery qpaginate qmatch qref indexesalltodos response callback false response catch error callback error case run query todos using faunadb secret passed via configuration serverlessyml faunadb us http query need worry sharing connection module invocation difference faunadb dynamodb interface main difference dynamodb faunadb version return dynamodbscan tablename todos error data becomes return clientquery qpaginate qmatch qref indexesalltodos try follow readme instruction launch run service create todo item ready explore data experiment query faunadb dashboard open dashboard via sign form look something like look closely screenshot get hint faunadb temporal capability power everything social activity feed auditing mobile sync moving beyond demo productionworthy version application request would contain list id query would validate list visible user returning matching item security model similar collaboration apps may familiar supported natively faunadb source dynamodb example list todos using scan operation move secure list sharing model dynamodb would add index run multiple query serverless handler validate list visible requesting user build real application faunadb also create index list id able pas authentication credential database query instead requiring multiple call faunadb return data need single query including list item metadata watch blog updated serverless application mature multiuser data model using faunadb security feature conclusion build example using serverless architecture choose serverlesscom framework look forward example serverless code dynamically provisioning resource using fauandb multitenant qos feature integrate faunadb serverless component time think provides better agility database requires rethinking data layout requirement change implementing big piece policy application database flexible query security awareness hope agree answer faunadb
324,Lobsters,scaling,Scaling and architecture,keepalived DNS health checker revamp,http://fanf.livejournal.com/149116.html,keepalived dns health checker revamp,bind rpz catatonia rpz spamhaus domain block list bind lost listening socket holy health check script bat man fix previously recursive dns server failover daemonic decoupling much complex http wwwyoutubecomwatch winning race back hold result http wwwyoutubecomwatch,today rolled significant improvement automatic recovery system cambridge university recursive dns server change three bug bind rpz catatonia first bug sometimes bind lock second rpz maintenance work happen large frequently updated response policy zone spamhaus domain block list happens server keepalived start failover process couple second deliberately configured respond quickly however bind soon recovers second later keepalived fails back bind lost listening socket brief keepalived flap unfortunate effect bind see service address disappear close listening socket service address reappear try reopen listening socket server fairly busy nt time clean state old listening socket bind try open new one bind get address already use error sadly bind give point keep trying periodically reopen socket might hope holy health check script bat man point bind still listening interface address except tcp socket public service ip address ideally spotted health check script told keepalived fail gaping hole health checker coverage test loopback interface fix ideally three bug fixed expert enough fix bind bug since gnarliest bit code leave good folk iscorg even fixed still need fix health check script actually check userfacing service address noone else leave previously wrote setup recursive dns server failover keepalived set couple year ago recent work leaf keepalived configuration bascially unchanged concentrate health check script purpose article key feature keepalived configuration run health checker script many time per second order fake dynamically reconfigurable server priority old script dns query inline ok checking loopback address new script need make typically query getting bit much daemonic decoupling new health checker split two script called keepalived examines content status file run predictably fast regardless speed dns response separate daemon performs actual health check writes result status file speed thing nice really important daemon naturally stateful way old health checker could started knew statefulness necessary clearly needed kind hysteresis flap damping holddown something much complex http wwwyoutubecomwatch theory mbius twist fabric space time becomes loop bind observes list network interface open close listening socket address come go health check daemon verifies bind responding properly network interface address keepalived poll health checker brings interface depending result without care inevitable unexpected interaction component destroy enterprise winning race health checker get race daemon interface deleted added deletion case simpler health checker get list address check turn keepalived deletes address process checker detect failure actually ok nt get respose missing address fortunately distinctive error message case health checker treat alternative successful response new interface tricky health checker need give bind little time open socket would really bad server appears healthy keepalived brings address health checker test bind ready causing immediately fail huge flap back main technique new health checker us suppress flapping exponential backoff normally everything working health checker query every network interface address writes ok status file sleep second looping query fails immediately writes bad status file sleep looping sleep time increase exponentially failure occur repeated failure cause longer longer interval server try recover exponential backoff handle original problem somewhat indirectly flap cause bind lose listening socket hopefully short series slower slower flap eventually flap slow enough bind able reopen socket server recovers probably tune backoff parameter minimize disruption kind event hold another way suppress flapping avoid false recovery test query succeed new health checker decrease failure sleep time rather zeroing failure occur exponential backoff continue still report success immediately keepalived want true recovery fast instance server accidentally crash restarted holddown mechanism linked way health checker keep track network interface address interface go away checker decrease sleep time several second even query working ok holddown supposed cover flap interface immediately return case want exponential backoff continue similarly avoid tricky race also record time interface brought ignore failure occur first second result took quite lot headscratching trial error end think came something resonably simple rather targeting specifically failure observed production tried use general purpose robustness technique hope mean behave ok new weird problem crop actually hope new weird problem crop p st tng quote recently listening old orbital album http wwwyoutubecomwatch
325,Lobsters,scaling,Scaling and architecture,Every Possible Scalability Limit Will Be Reached,http://aras-p.info/blog/2017/02/05/Every-Possible-Scalability-Limit-Will-Be-Reached/,every possible scalability limit reached,suggested every possible scalability limit reached eventually background shader variant unity almost shader variant unity add variant shader keywords unity add variant unity thing getting hand new import pipeline marmoset skyshop robertcupisz shader compilation unity unity need make variant optional standard shader alloy shaders wrote unity people need shader keywords long forum thread hash function unity million variant expand data hd render pipeline unity half billion variant anyone search data hundred billion variant dont search data moral story,wrote day mccloudstrife suggested call ara law ok every possible scalability limit reached eventually concrete example happened work bit year shader combinatorial variant explosion dealing retrospect skipped step recognized oh improvement enough people start oh well live learn boring story background shader variant gpu programming model day still solved compose piece together problem cpu land function call function pointer goto virtual function elaborate way based shaders either exist cumbersome use terribly performant many time people resort writing many slightly different variant shader pick one another use depending processed called ubershaders megashaders often done stiching piece source code together using clike preprocessor thing slowly improving move away madness eg specialization constant vulkan function constant metal take time get shaders variant thing end problem especially number variant large turn get large really easily unity almost shader variant many year ago shaders unity many variant dealing simple forward shading would write autolight shader would compile internal variant compile directive behind c style comment variant know like probably bitmask light type three bit directional spot point support sure value besides worked five variant light needed support light cooky v light cooky back unity supported one graphic api opengl five variant shader problem count hand compiled shader import time five always included game data build unity add variant unity changed syntax pragma multicompile look le commnt like proper compile directive point got ability user add variant called shader keywords forget version exactly happened think series people could make shaders one another thing choice eg use normalmap v use normal map control behavior based shader keywords set big problem since way custom inspector uis material complex datadependent shader variant practical use feature much unity builtin shaders many thought something advanced maybe shader variant graphic apis time opengl always compiled shader import time always included game build data think limit maximum shader keywords used case crazy amount shader variant happening yet unity add variant unity added builtin lightmapping support meant shader variant without lightmaps added deferred lighting shader variant game build pipeline got ability include well surely needed shader variant game data compilation variant present shader still happening shader import time making impractical go couple dozen variant maybe hundred simple enough unity thing getting hand new import pipeline little little people started adding shader variant think marmoset skyshop gave u wow something need done realization either thing many shadervariant based system number possible shader variant always much much higher number actually used shader variant imagine simple shader thing multivariant fashion normal map specular map emission map detail map alpha cutout feature essentially bit two state feature total possible shader variant many actually used likely lot le particular production usually settle standard way authoring material example material end using normal map specular map occasional one also putting either emission map alpha cutout feature handful shader variant needed instead full set point compiling every possible shader variant shader import time terribad people wanted ten toggleable feature get really large number really fast also help opengl graphic apis also opengl e flash etc compiling variant shader backends import time even never ever needed result robertcupisz started rewriting shader compilation pipeline written shader compilation unity basically changed several thing shader variant would compiled ondemand whenever needed editor game data build cached shader compiler split separate process per cpu core work around global mutexes shader compiler backends preventing effictive multithreading big improvement compared previous state done yet far unity need make variant optional new compilation pipeline meant editing shader thousand potential variant longer coffee break however variant still always included build unity launch developing new builtin standard shader possible variant always including problem added way indicate know include variant material use authoring shader variant never used anything never even compiled included game build either done december plenty time ship unity time people started going crazy shader variant count eg alloy shaders million variant needed optimization wrote managed get time unity launch went five variant two million possible variant limit yet quite unity people need shader keywords sometime along way amount shader keywords toggleable shader feature control variant picked support went still enough see long forum thread looked increasing keyword count turn doable especially fiddling around hash function side effect investigating various hash function replaced almost hash function used across whole codebase ok neither improve scalability shader variant part make except shader keywords people started adding even potential shader variant give someone thing start using expected unexpected way yet nope still possible scalability cliff near future unity million variant expand data team working new hd render pipeline shaders million possible variant turn step editor still expanding data possible variant inmemory structure million potential variant taking gob memory lot time spent searching time fix stop expanding data instead search directly fairly compact unexpanded data unblocked team import time minor shader edit went minute couple second yay unity half billion variant anyone search data course went half billion possible variant another problem surfaced path editor looking okay shader variant use right code enumerating possible variant checking one closest want unity shader keywords exactly match variant present shader better previous step made table possible variant expanded memory purely enumerating half billion variant fairly simple check still taking long time half billion turn big number course search like fairly stupid know searching shader variant keyword normalmapon little use enumerating one keyword cut search space half optimization done nicely got timing dozen second feel instant case half billion shader variant done right hundred billion variant dont search data somehow team ended shader almost hundred billion possible variant ask guess adding everything kitchen sink quick look layered lit tessellation shader seems usual optional texture normal map specular map emissive map detail map detail mask map one two three four layer map mixed together mixing based vertex color height something else tessellation displacement phong displacement parallax occlusion mapping several double sided lighting mode transparency alpha cutout option lightmapping option oddball thing thing need individually toggleable feature get hundred billion variant range feature lot feature imaginable problem ran game data build time code similar previous case looping possible shader variant deciding whether one included data file looping hundred billion simple thing long process like oh wanted build check performance console gave waiting good course stupid thing loop inverted since already info material included game build know shader variant needed need augment set variant always build got build time forever ten second done yet know see future bring moral story code used something five thing year ago might turn problematic deal hundred thousand million hundred billion kinda obvious
326,Lobsters,scaling,Scaling and architecture,Rewriting Duolingo's engine in Scala,http://making.duolingo.com/rewriting-duolingos-engine-in-scala,rewriting duolingo engine scala,meet duolingo engine session generator duolingo incubator overview rewrite technical debt system redesign overview aws codebase refactor overview performance memory management threadsafe dynamic typing scala java virtual machine putting together functional programming scala referential transparency side effect referential transparency immutability case class pattern matching monad developing scala le verbosity le boilerplate fewer bug stack infrastructure finatra guice mockito aws elastic beanstalk testing launching new session generator final thought,recently profoundly refactored engine drive duolingo lesson post talk engineering choice experience pain point rewriting highly complex system highlight redesigned architecture refactored code python scala latency dropped engine uptime increased meet duolingo engine session generator duolingo world popular language learning app million user time writing core duolingo experience user learn bitesized lesson consists several interactive exercise internally call challenge given lesson exercise user see order responsibility session generator backend module get data one language course counting duolingo incubator sprinkle machine learning magic proceeds serve sequence exercise tailored need million user overview rewrite rewriting code complicated necessary process even though halt development new feature may take several month eventually technical debt built must addressed case familiar term technical debt like financial debt borrow money making engineering decision let develop something quickly long term though development start stall accumulated shortcut point time pay session generator existed since day one duolingo lived pain come rapid growth startup unsurprisingly built lot technical debt next section talk indicator made u decide time come rewrite decision made address issue system redesign overview course session generator history often added dependency shared resource needed piece data data store needed cache expensive computation decision ultimately evolved architecture resembling following many problem architecture numerous hard dependency marked red session generator would fail respective network connection failed addition represent additional network call added total latency redesigning architecture main guiding principle remove many shared resource possible resulting design simple robust new architecture course data need processing step shared among user course processed offline serialized file aws amazon cloud storage service need fetch file stable cheap data store cache hand user data leverage personalization requires fewer processing step shared fetched api server injected request sent session generator codebase refactor overview one major decision made rewrite session generator scala duolingo backend session generator originally written python python easy understand especially useful developer sort background however come drawback performance python considerably slower c java example memory management python generally threadsafe prevents developer using inmemory cache full potential dynamic typing actual problem complex system dynamic typing increase number runtime bug since definition interface module weaker interpreter able catch typerelated bug result excessive number deploy find bug fix iteration turn slows engineering effort decided port codebase scala staticallytyped functional programming language built top java virtual machine mean use existing java library scala widely used big data application high complexity characteristic session generator seemed like good fit one main concern scala steep learning curve however highly complex system learning curve system steeper programming language improving readability maintainability higher priority regard scala good job present next section putting together let dive specific part session generator implementation development process scala functional programming scala aside learning new syntax main challenge learning scala python java background getting used functional programming mention thing developer know order get started functional programming note scala tutorial mean able write imperative code scala since multiparadigm although might using full potential referential transparency side effect one pillar functional programming referential transparency referentially transparent function one output always input like algebraic function nothing else example function referentially transparent io function example fetch user database get data time user might changed email print something code something computing output called side effect method return unit type scala equivalent void certainly side effect since computation done order fulfill sideeffected procedure return nothing immutability referential transparency go hand hand immutability side effect mutate state idea dealing immutability scala code idea use val instead var defining variable since val reassigned val x x error reassignment val x particular use case class instance case class nonreassignable attribute val hashable allows nice thing pattern matching straight box idea use immutable collection package scalacollectionimmutable default collection scala therefore need explicitly import scalacollectionmutable want use corresponding mutable collection val x set immutable x error value member scalacollectionimmutableset int x val x scalacollectionmutableset mutable x x scalacollectionmutableset int set idea use control structure need mutable state particular avoid loop prefer use transformation method map flatten flatmap filter collect fold etc val x list xmap creates new immutable list scalacollectionimmutableset int set idea write loop control structure try recursive approach monad name monad come category theory want go mathematical formality instead start example illustrates usage option monad option monad wrapper object null example first try fetch value x map yielding x existed entry none otherwise short gracefully dealing nullpointerexception val somemap map string int val optionalx option int stringtointmapget x val optionaly option int stringtointmapget x int optionalx int optionaly yield x yield x none pure functional programming requires u separate evaluation execution code monad express code chain data transformation evaluation ultimately run middleware execution particularly useful achieving purity io operation inherently sideeffected let see example future monad trying fetch data data access layer asynchronously futurex futurey hold state async call waiting fetched error foryield chain sum operation receive response yield future x futurex futurey successful propagate error otherwise val futurex future int dalgetasync x val futurey future int dalgetasync x int futurex int futurey yield x yield futurevalue x futureexception developing scala scala language seems learned pain point language proposes solve le verbosity scala number thing remove verbosity example infers type whenever possible write val x instead val x int functional nature also allows developer write oneliners many situation write list comprehension list map even write entire class definition class person val name string getters setter name created hood one last thing worth mentioning scala many thing implicitly told example passing argument implicitly write every time def greet name string implicit greeting greeting unit println greetingvalue name implicit val hello new greeting value hello greet john hello john greet mary hello mary le boilerplate scala remove lot boilerplate code example provides tool developer need write errorprone control structure loop mutating state addition scala code need handle trivial edge case everywhere example treat option monad make sense instead treating null pointer place go exception future monad fewer bug static typing allows compiler catch number error compile time exception enhanced fact scala powerful generic typing system java moreover using option future monad common runtime exception explicitly treated code finally le verbosity le boilerplate ultimately make complex code easier read since code tends representative actual application logic together immutability referential transparency code easier reason debug stack infrastructure stack chose us finatra http server guice dependency injection framework mockito testing framework stack deployed aws elastic beanstalk amazon orchestration service automatically handle rolling deployment load balancing autoscaling monitoring testing mixture functional programming le boilerplate stack offer dependency injection unitintegrationfeature test box result le time spent writing application code logic time writing test rewrite able write test virtually every single method improves stability serf documentation thorough test suite also helped u seamlessly integrate component developed separately launching new session generator launch rewrite obtained decrease average latency going old session generator rewrite duolingo lesson time spent downloading course data whenever inmemory cache expires rewrite also considerably stable degraded performance old infrastructure around hour per quarter rewrite dropped zero downtime first couple month obviously expect keep availability service forever number indicates new infrastructure much robust additional positive result number bug go developer confident deploying code without breaking site thanks improved testing suite ability detect compiletime error development final thought rewrite able produce faster reliable session generator fewer bug cleaner codebase initially thought would take long time onboard developer practice time taken considerably le anticipated two main pain point process documentation certain scala library often either missing scattered around across web trouble java integration example integration would support nice scalaspecific feature using option future monad supporting optional parameter finally still early say well rewrite help reduce future technical debt far thing looking good
330,Lobsters,scaling,Scaling and architecture,Million requests per second with Python,https://medium.com/@squeaky_pl/million-requests-per-second-with-python-95c137af319,million request per second python,enter japronto japronto fast scalable lightweight synchronous asynchronous asyncio fast erratum faster fasthttp slower http http wrk http pipelining gory detail optimization nagle algorithm read system call write one system call scattergather io japronto japronto japronto open source contributor could use help japronto japronto contact directly twitter japronto github project repository final word inada naoki,possible hit million request per second python probably recentlya lot company migrating away python programming language boost operation performance save server price need really python right tool jobthe python community lot around performance lately cpython boosted overall interpreter performance new dictionary implementation cpython going even faster thanks introduction faster call convention dictionary lookup cachesfor number crunching task use pypy justintime code compilation also run numpy test suite improved overall compatibility c extension later year pypy expected reach python conformanceall great work inspired innovate one area python used extensively web microservice developmententer japronto japronto brand new microframework tailored microservices need main goal include fast scalable lightweight let synchronous asynchronous programming thanks asyncio shamelessly fast even faster nodejs gopython microframeworks blue dark side force green japronto purple erratum user heppu point go stdlib http server faster graph show written carefully also awesome fasthttp server go apparently slower japronto particular benchmark awesome detail see http http also see meinheld wsgi server almost par nodejs go despite inherently blocking design great performer compared preceding four asynchronous python solution never trust anyone say asynchronous system always speedier almost always concurrent much thati performed micro benchmark using hello world application clearly demonstrates serverframework overhead number solutionsthese result obtained aws instance vcpus launched so paulo region default shared tenancy hvm virtualization magnetic storage machine running ubuntu lts xenial xerus linux kernel o reporting cpu cpu used python freshly compiled source codeto fair contestant including go running singleworker process server load tested using wrk thread connection simultaneous pipelined request per connection cumulative parallelism request http pipelining image credit wikipedia http pipelining crucial since one optimization japronto take account executing requestsmost server execute request pipelining client fashion would nonpipelining client try optimize fact sanic meinheld also silently drop request pipelining client violation http protocol simple word pipelining technique client need wait response sending subsequent request tcp connection ensure integrity communication server sends back several response order request receivedthe gory detail optimizationswhen many small get request pipelined together client high probability arrive one tcp packet thanks nagle algorithm server side read back one system calldoing system call moving data kernelspace userspace expensive operation compared say moving memory inside process space important perform necessary system call le japronto receives data successfully par several request try execute request fast possible glue response back correct order write back one system call fact kernel aid gluing part thanks scattergather io system call japronto use yetnote always possible since request could take long waiting would needlessly increase latencytake care tune heuristic consider cost system call expected request completion timejapronto give rps median grouped continuous data calculated percentile using interpolationbesides delaying writes pipelined client several technique code employsjapronto written almost entirely c parser protocol connection reaper router request response object written c extensionsjapronto try hard delay creation python counterpart internal structure asked explicitly example header dictionary created requested view token boundary already marked normalization header key creation several str object done accessed first timejapronto relies excellent picohttpparser c library parsing status line header chunked http message body picohttpparser directly employ text processing instruction found modern cpu extension almost cpu quickly match boundary http token io handled super awesome uvloop wrapper around libuv lowest level bridge epoll system call providing asynchronous notification readwrite readinesspicohttpparser relies cmpestri intrinsic parsingpython garbage collected language care need taken designing high performance system needlessly increase pressure garbage collector internal design japronto try avoid reference cycle allocationsdeallocations necessary preallocating object socalled arena also try reuse python object future request longer referenced instead throwing awayall allocation done multiple internal structure carefully laid data used frequently together close enough memory minimizing possibility cache missesjapronto try copy buffer unnecessarily many operation inplace example percentdecodes path matching router processopen source contributor could use helpi working japronto continuously past month often weekend well normal work day possible due taking break regular programmer job putting effort projecti think time share fruit labor communitycurrently japronto implement pretty solid feature set http implementation support chunked uploadsfull support http pipeliningkeepalive connection configurable reapersupport synchronous asynchronous viewsmastermultiworker model based forkingsupport code reloading changessimple routingi would like look websockets streaming http response asynchronously nextthere lot work done term documenting testing interested helping please contact directly twitter japronto github project repositoryalso company looking python developer performance freak also devops open hearing going consider position worldwidefinal wordsall technique mentioned really specific python could probably employed language like ruby javascript even php interested work sadly happen unless somebody fund iti like thank python community continuous investment performance engineering namely victor stinner victorstinner inada naoki methane yury selivanov entire pypy teamfor love python
331,Lobsters,scaling,Scaling and architecture,Is upgrading RDS like a shit-storm that will not end?,http://www.iheavy.com/2015/02/12/is-upgrading-rds-like-a-shit-storm-that-will-not-end/,upgrading rds like shitstorm end,follow sean hull twitter hullsean changing parameter mysql mongo beautiful baby called aurora much longer automation killing oldschool operation amazon force o upgrade manager underestimate operational cost happening server difference dev ops fourletter word surgery blunt instrument zero downtime even possible rds upside rds,join others follow sean hull twitter hullsean rds worsen outage another way think question experience clearly increase outage tying one hand behind back believe say terribly frustrating putting fire changing parameter everyday occurance need change database parameter want enable login great problem except rds becomes problem ok thinking regular mysql login shell issue set global parameter value nice easy straightforward server restarting nonsense parameter requires reboot mysql tell rds process waay complex first edit parameter group copy existing one change one using parameter group applies many server careful ok next apply new parameter group immediately next maintenance window tricky part amazon going restart instance something bos manager surely ask well might think would parameter question required tried enable general log recently amazon tell status pendingreboot change require sitting scared amazon might suddenly decide reboot production server reason feel lost control dig doc want ever say sure managed service behave predictably already layer software relational database want also mysql mongo beautiful baby called aurora much longer another question ask long maintenance take mysql command line run test test time process go perform task offhours already clear picture rds thing predicted server restarted rebuilds take forever progress bar eb performance hiccup snapshot time double trouble go related automation killing oldschool operation amazon force o upgrade another surprise ran managed solution amazon must take opportunity pay unpredictability going perform mysql upgrade run test test advance timed process minute went production amazon decided throw o upgrade adding minute surprise time worse progress bar either upgrade nerve wracking enough without kind stuff scaring daylight read manager underestimate operational cost happening server question progress opaque rds lack command line watch process disk io granular stuff surgery analogy though touch patient find pulse guage skin cold clammy pale also difference dev ops fourletter word surgery blunt instrument end day rds feel like surgery blunt instrument command line scalpel window gui tool may remote video surgery worse still rds would like surgery opportunity mar rover landed stuck valley everything delayed hard tell going worst environment work emergency database operation experience deploy mysql instance thank later also zero downtime even possible rds upside rds upside people use pushbutton replication check pushbutton multiaz check great dba automated backup shoot foot check guess something love
332,Lobsters,scaling,Scaling and architecture,The probability of data loss in large clusters,http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html,probability data loss large cluster,probability data loss large cluster mathjax bunch disk easy lose data la la laaa permanently losing three replica three twice great consistent hashing calculating probability data loss disk failure binomial distribution conditional probability fixed number partition per node fixed create cluster little ruby program union bound switched partition per node stephan mat clayton alastair beresford stephan kollmann christopher meiklejohn daniel thomas,probability data loss large cluster published martin kleppmann jan blog post us mathjax render mathematics need javascript enabled mathjax work many distributed storage system eg cassandra riak hdfs mongodb kafka use replication make data durable typically deployed bunch disk jbod configuration without raid handle disk failure one disk node dy disk data simply lost avoid losing data permanently database system keep copy replica data disk node common replication factor database keep copy every piece data three separate disk attached three different computer reasoning go something like disk die disk dy bit time replace still two copy restore data onto new disk risk second disk dy restore data quite low risk three disk die time tiny likely get hit asteroid backoftheenvelope calculation probability single disk failing within time period pick arbitrary number probability two disk failing probability three disk failing one billion calculation assumes one disk failure independent another disk failure actually true since example disk manufacturing batch may show correlated failure good enough approximation purpose far common wisdom sound reasonable unfortunately turn untrue many data storage system post show easy lose data la la laaa database cluster really consists three machine probability three dying simultaneously indeed low ignoring correlated fault datacenter burning however move larger cluster probability change node disk cluster likely lose data counterintuitive idea surely think every piece data still replicated three disk probability disk dying depend size cluster size cluster matter calculated probability drew graph looked like clear probability single node failing probability permanently losing three replica piece data restoring backup one remaining way recover data bigger cluster likely haemorrhaging data probably intended decided pay replication factor axis graph bit arbitrary depends lot assumption direction line scary assumption node chance dying within time period graph show cluster chance permanently losing three replica piece data within time period yes read correctly risk losing three copy data twice great risk losing single node point replication intuition behind graph follows cluster almost certain node always dead given moment normally problem certain rate churn node replacement expected part routine maintenance however get unlucky piece data whose three replica happen three node died case piece data gone forever data lost small fraction total dataset cluster still great use replication factor generally mean really want lose data mind occasionally losing bit data long much maybe piece lost data particularly important one probability three replica dead node depends crucially algorithm system us assign data replica graph calculated assumption data split number partition shard partition stored three randomly chosen node pseudorandomly hash function case consistent hashing used cassandra riak among others far know system sure replica assignment work appreciate insight people know internals various storage system calculating probability data loss let show calculated graph using probabilistic model replicated database let assume probability losing individual node pp text node loss going ignore time model simply look probability failure arbitrary time period example could assume probability node failing within given day would make sense take day replace node restore lost data onto new disk simplicity distinguish node failure disk failure consider permanent failure ignoring crash node come back reboot let n number node cluster probability f n node failed assuming failure independent given binomial distribution p f text node failed binom n f pf nf term pf probability f node failed term nf probability remaining nf failed binom n f number different way picking f n node binom n f pronounced n choose f defined binom n f frac n f nf let r replication factor typically assume f n node failed probability particular partition r replica failed node well system us consistent hashing partition assigned node independently randomly pseudorandomly given partition binom n r different way assigning r replica node assignment equally likely occur moreover binom f r different way choosing r replica f failed node way r replica assigned failed node work fraction assignment result replica failed p text partition lost mid f text node failed frac binom f r binom n r frac f nr fr n vertical bar partition lost pronounced given indicates conditional probability probability given assumption f node failed probability replica one particular partition lost cluster k partition one partition lost lost data thus order lose data require k partition lost begin align p text data loss mid f text node failed p text partition lost mid f text node failed k left frac f nr fr n right k end align cassandra riak call partition vnodes instead thing general number partition k independent number node n case cassandra usually fixed number partition per node default n configured numtokens parameter also assumed graph riak number partition fixed create cluster generally node also mean partition place work probability losing one partition cluster size n replication factor r number failure f le replication factor sure data lost thus need add probability possible number failure f r le f le n begin align p text data loss sum fr n p text data loss cap f text node failed sum fr n p f text node failed p text data loss mid f text node failed sum fr n binom n f pf nf left left frac f nr fr n right k right end align bit mouthful think accurate plug n vary n get graph wrote little ruby program calculation get simpler approximation using union bound begin align p text data loss p getext partition lost pleft bigcup k text partition text lost right le k p text partition lost k pr end align even though one partition failing independent another partition failing approximation still applies seems match exact result quite closely graph data loss probability look like straight line proportional number node approximation say probability proportional number partition equivalent since assumed fixed partition per node moreover plug number node approximation get p text data loss le cdot cdot match result ruby program closely problem practice know mostly think interesting counterintuitive phenomenon heard rumour causing real data loss company large database cluster seen issue documented anywhere aware discussion topic please point calculation indicates order reduce probability data loss reduce number partition increase replication factor using replica cost ideal large cluster already expensive however number partition present interesting tradeoff cassandra originally used one partition per node switched partition per node year ago order achieve better load distribution efficient rebalancing downside see calculation much higher probability losing least one partition think probably possible devise replica assignment algorithm probability data loss grow cluster size least grow fast nevertheless good load distribution rebalancing property would interesting area explore context colleague stephan pointed expected rate data loss constant cluster particular size independent replica assignment algorithm word choose high probability losing small amount data low probability losing large amount data latter better need fairly large cluster effect really show cluster thousand node used various large company interested hear people operational experience scale probability permanently losing data node cluster really per day would mean chance losing data year way higher one billion gettinghitbyanasteroid probability talked start designer distributed data system aware issue got right something taken account designing replication scheme hopefully blog post raise awareness fact three replica automatically guaranteed safe thank mat clayton bringing issue attention alastair beresford stephan kollmann christopher meiklejohn daniel thomas comment draft post
333,Lobsters,scaling,Scaling and architecture,Amazon Web Services secret weapon: Its custom-made hardware and network,http://www.geekwire.com/2017/amazon-web-services-secret-weapon-custom-made-hardware-network/amp/,amazon web service secret weapon custommade hardware network,announced may us switch keynote bought investor powerhungry,james hamilton aws vp distinguished engineer rev talk invent la vega geekwire photo dan richman unusual internet software giant design even make hardware increase efficiency build competitive advantage google customdesigns server filling million chip intel announced may designed applicationspecific integrated circuit asic use neural network facebook us switch data center marketleading publiccloud company amazon web service may gone farthest path designing router chip storage server compute server also highspeed network got company digital designer working chipsets hardware designer working nics network interface card software developer said james hamilton aws vp distinguished engineer keynote aws invent conference november horizontal vertical get move pace used get make change pace used get respond customer requirement pace used think really big deal geekfest replete closeup photo server rack cable hamilton posited networking world going way mainframe getting chopped vertical stack different company innovating competing stack helping make networking gear commodity said one aws save money building via amazon video run custommade router made specification protocoldevelopment team hamilton said cost caused u head path though big cost improvement biggest gain reliability custommade gear one requirement u show judgment keep simple fun would lot tricky feature want reliable aws using standard commercial router problem arose committed serious company would take six month resolve issue said terrible place love right aws standardized ethernet gbe fiber networking transfer speed even though look like crazy decision hamilton said heavily involved decision defend industry standard noted gbe gbe gbe representing single optical wave gbe representing four wave almost four time optic cost said well gig almost price gig mean run gig much le cost gig optic standpoint absolutely right answer believe industry end hamilton display custommade asic via amazon video aws router run customized broadcom tomahawk asic billion transistor port gbe total flowthrough terabit tbit absolute monster hamilton said holding one aloft similar chip tbit tbit capacity coming around price said another key piece aws networking strategy softwaredefined networking ability network administrator change managed network behavior interface part moving process software hardware extent possible sometime made obvious important observation whenever workload repetitive better taking hardware said people say hey reason aws go customer networking gear could never bandwidth data center hamilton related true could give bandwidth want anyone equipment absolutely hard know hard latency physic tell software people thing measuring millisecond one thousandth second hardware measure nanosecond one billionth second microsecond one millionth second right place u go via amazon video aws also produce customized chipsets emblazoned name annapurnalabs use every server deploy hamilton said amazon bought israeli chipmaker annapnurna last january reported million first time aws made clear using chip company believe semiconductor business hamilton exclaimed building hardware built said showing chipset big deal right trend told hardware implementation latency fairly confident one mean get implement silicon aws us powerswitching gear custom firmware data center ensuring fault occur outside center facility continues operate occur inside load dropped hamilton said avoids problem like airline lost million switching gear locked reserve generator gap super bowl coverage occurred reason aws recent custom storage server hold petabyte one million gb data disk contained single standardsize rack pb model hamilton showed said company policy prohibited showing latest model custom aws compute server via aws company custom compute server occupying one slot rack sparsely populated hamilton conceded turn implemented thermal power efficiency oems selling customer probably three four five time dense le efficient make cost aws compute server power supply voltage regulator operate greater percent efficiency aws spends hundred million dollar electricity power supply percent better get pretty interesting number aws data center region worldwide linked private network controlled exclusively aws interconnection site administered company many many parallel link way single link ever impact anyone room capacity survive link failure engineer way crazy via aws every link gig absolutely everywhere hamilton said pretty important asset started little concerned really really really expensive networking team percent committed right thing aws us short longterm lease dark fiber lit contract several case laying cable said whatever costeffective get resource need amazon investor hawaiki submarine cable cable linking australia hawaii new zealand oregon via aws hamilton provided detail amazon data center usually surrounded secrecy already known availability zone within region contains least one data center hamilton added new data center consume megawatt mw power fairly modest size powerhungry data center world china telecom inner mongolia information park consumes mw ongoing basis get really big gain cost advantage get bigger could easily build facility go good place said take right mw right size facility cost u tiny bit go path think right thing customer watch full video hamilton talk
334,Lobsters,scaling,Scaling and architecture,Postgres Parallel indexing in Citus,https://www.citusdata.com/blog/2017/01/17/parallel-indexing-with-citus/,postgres parallel indexing citus,advanced index type citus command github archive citus cloud email protected email protected email protected email protected documentation slack downloading citus cloud,index essential tool optimizing database performance becoming ever important big data however volume data increase index maintenance often becomes write bottleneck especially advanced index type use lot cpu time every row get written index creation may also become prohibitively expensive may take hour even day build new index terabyte data postgres citus made creating maintaining index much faster parallelization citus used distribute postgresql table across many machine one many advantage citus keep adding machine cpu keep increasing write capacity even index becoming bottleneck citus create index also performed massively parallel fashion allowing fast index creation large table moreover copy command write multiple row parallel used distributed table greatly improves performance usecases use bulk ingestion eg sensor data click stream telemetry show benefit parallel indexing walk small example indexing row containing large json object github archive run example set formation using citus cloud consisting worker node core running postgresql citus download sample data running following command wget http csvgz gzip githubevents gz next let create table github event regular postgresql table distribute across node create table githubevents eventid bigint eventtype text eventpublic boolean repoid bigint payload jsonb repo jsonb actor jsonb org jsonb createdat timestamp distributed table shard table repoid select createdistributedtable githubevents repoid initial data load event copy githubevents program cat githubevents csv format csv event github data set detailed payload object json format building gin index payload give u ability quickly perform finegrained search event finding commits specific author however building index expensive fortunately parallel indexing make lot faster using core time building many smaller index create index githubeventspayloadidx githubevents using gin payload regular table distributed table speedup create index row test well scale took opportunity run test multiple time interestingly parallel create index exhibit superlinear speedup giving speedup despite core likely due fact inserting one big index le efficient inserting small pershard index following log n n row give additional performance benefit sharding regular table distributed table speedup create index row create index row create index row index created copy command also take advantage parallel indexing internally copy sends large number row multiple connection different worker asynchronously store index row parallel allows much faster load time single postgresql process could achieve much speedup depends data distribution data go single shard performance similar postgresql copy githubevents program cat githubevents csv format csv regular table distributed table speedup copy row index copy row gin finally worth measuring effect index query time try two different query one across repos one specific repoid filter distinction relevant citus githubevents table sharded repoid query specific repoid filter go single shard whereas query parallelised across shard get commits email protected repos select repoid jsonbarrayelements payload commits githubevents eventtype pushevent payload commits author email email protected get commits email protected single repo select repoid jsonbarrayelements payload commits githubevents eventtype pushevent payload commits author email email protected repoid row give u query time time marked query executed parallel citus parallelisation creates fixed overhead also allows heavy lifting either much faster bit slower query regular table regular table distributed table select index repos select gin payload repos select index single repo select index single repo index postgresql dramatically reduce query time time dramatically slow writes citus give possibility scaling cluster get good performance side pipeline particular sweet spot citus parallel ingestion singleshard query give querying performance better regular postgresql much higher scalable write throughput would like learn parallel index way citus help scale consult documentation give u ping slack also get started citus minute downloading citus open source spin cluster desktop deploying citus cloud
335,Lobsters,scaling,Scaling and architecture,IPv6-only at Microsoft,https://blog.apnic.net/2017/01/19/ipv6-only-at-microsoft/,microsoft,driving push experimenting testing challenge trouble worth code conduct,microsoft running fashion corporate network many year nowwe substantial footprint across country probably surmised title post dualstack meet needswhat driving push first obvious reason exhaustion address space depletion public space wellknown microsoft exhausted almost space small discontiguous pocket remaining enough current future needsthis situation exacerbated two main factor integration corporate acquisition example nokia azure expansion even worse last factor resulted overlap spaceas interconnectivity required corporate network azure acquired company network necessitated suboptimal nat solution course potentially fragile certainly operationally challengingthe second reason consider move complexity dualstack helpdesk well network system operation staff dualstack double complexity dealing issue equally consider network engineering design process make life complicated need bewith mind last couple year experimenting experimenting started couple test network redmond usa one wireless guest network one building segment wired corporate network latter network optin configured two network various address acquisition scheme slaac dhcp stateful stateless get picture worked notsurprisingly scenario worked also interesting gain experience various platform served input shortlist platform selectionfollowing initial pilot decided needed move aggressively widen deployment production network elected concentrate guest network gave u flexibility minimum risk soon encountered problem howevertesting challengesthe router building wanted deploy support rdnss current code aware rdnss facility provide dns resolver information ra needed support android device guest network android support feature problemour temporary solution move default gateway function centralized pair router supported rdnss router reached using overlaythis ideal operationally currently testing new code router vendor support rdnss allow u move default gateway back building highlight fact feature consistent among platformsthis change enabled guest network encountered two issue slowed u one vendorbased homegrownthe first new wireless design allows employee authenticate network access using azure ad turn requires reauthorization acls wireless controller acls dynamic namebased supported one vendor second vendor code finished testing clearly feature parity vendor equipment difficult achievewe expect complete testing wireless controller code router code supporting rdnss next month meaning able roll guest network redmond campus building next six monthsthe second issue slowing u bug window affected stateful stateless scheme needle say expansion impossible resolved issue reported product group duly working fixthe final bit puzzle corporate network complicated disparate requirement different region even building cityspecifically type user developer vastly different requirement corporate function hr finance technically mean assessing whether break application usethere also question placement complicated ass idea sdwan meaning site local instead centralized internet egress environment centralizing function easier manage le flexible however distributing function allows flexibility manage issue grapple plan move corporate networkso trouble worth hopefully migrating dualstack uncontroversial stage u moving soon possible solves problem depletion address oversubscription also move u simpler world network operation concentrate innovation providing network service instead wasting energy battling fundamental resource addressing interesting journey get marcus keane principal network engineer microsoft corporationthe view expressed author blog necessarily reflect view apnic please note code conduct applies blog
336,Lobsters,scaling,Scaling and architecture,How Discord Stores Billions of Messages,https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7,discord store billion message,announced million message day announced million choosing right database linear scalability automatic failover low maintenance proven work predictable performance blob store open source data modeling snowflake dark launch eventual consistency ap,discord continues grow faster expected usergenerated content user come chat message july announced million message day december announced million blog post well past million decided early store chat history forever user come back time data available device lot data ever increasing velocity size must remain available cassandra doingthe original version discord built two month early arguably one best database iterating quickly mongodb everything discord stored single mongodb replica set intentional also planned everything easy migration new database knew going use mongodb sharding complicated use known stability actually part company culture build quickly prove product feature always path robust solutionthe message stored mongodb collection single compound index channelid createdat around november reached million stored message time started see expected issue appearing data index could longer fit ram latency started become unpredictable time migrate database suited taskchoosing right databasebefore choosing new database understand readwrite pattern problem current solutionit quickly became clear read extremely random readwrite ratio chat heavy discord server send almost message mean send message two every day year kind server unlikely reach message problem even though small amount message make harder serve data user returning message user result many random seek disk causing disk cache evictionsprivate text chat heavy discord server send decent number message easily reaching thousand million message year data requesting usually recent problem since server usually member rate data requested low unlikely disk cachelarge public discord server send lot message thousand member sending thousand message day easily rack million message year almost always requesting message sent last hour requesting often data usually disk cachewe knew coming year would add even way user issue random read ability view mention last day jump point history viewing plus jumping pinned message fulltext search spell random read next defined requirement linear scalability want reconsider solution later manually reshard dataautomatic failover love sleeping night build discord self heal much possiblelow maintenance work set add node data growsproven work love trying new technology newpredictable performance alert go api response time percentile go also want cache message redis memcachednot blob store writing thousand message per second would work great constantly deserialize blob append themopen source believe controlling destiny want depend third party companycassandra database fulfilled requirement add node scale tolerate loss node without impact application large company netflix apple thousand cassandra node related data stored contiguously disk providing minimum seek easy distribution around cluster backed datastax still open source community drivenhaving made choice needed prove would actually workdata modelingthe best way describe cassandra newcomer kkv store two k comprise primary key first k partition key used determine node data life found disk partition contains multiple row within row within partition identified second k clustering key clustering key act primary key within partition row sorted think partition ordered dictionary property combined allow powerful data modelingremember message indexed mongodb using channelid createdat channelid became partition key since query operate channel createdat make great clustering key two message creation time luckily every id discord actually snowflake chronologically sortable able use instead primary key became channelid messageid messageid snowflake meant loading channel could tell cassandra exactly range scan messageshere simplified schema message table omits column cassandra schema unlike relational database cheap alter impose temporary performance impact get best blob store relational storewhen started importing existing message cassandra immediately began see warning log telling u partition found size give cassandra advertises support partition apparently done mean large partition put lot gc pressure cassandra compaction cluster expansion large partition also mean data distributed around cluster became clear somehow bound size partition single discord channel exist year perpetually grow sizewe decided bucket message time looked largest channel discord determined stored day message within bucket could comfortably stay bucket derivable messageid timestampcassandra partition key compounded new primary key became channelid bucket messageid query recent message channel generate bucket range current time channelid also snowflake older first message sequentially query partition enough message collected downside method rarely active discord query multiple bucket collect enough message time practice proved fine active discord enough message usually found first partition majorityimporting message cassandra went without hitch ready try productiondark launchintroducing new system production always scary good idea try test without impacting user setup code double readwrite mongodb cassandraimmediately launching started getting error bug tracker telling u authorid null null required field eventual consistencycassandra ap database mean trade strong consistency availability something wanted antipattern readbeforewrite read expensive cassandra therefore everything cassandra essentially upsert even provide certain column also write node resolve conflict automatically using last write win semantics per column basis bite u
337,Lobsters,scaling,Scaling and architecture,0 to 1 Million: Scaling my side project to 1 million requests a day,http://www.re-cycledair.com/0-to-1-million-scaling-my-side-project-to-1-million-requests-a-day,million scaling side project million request day,http kernlus moved file storage aws moved mongo composeio moved nginx server added node server,beginning late decided needed side project technology wanted learn experience building actual project best way sat couch trying figure build remembered idea back still junior dev wordpress development idea people building commercial plugins theme able use automated update system wordpress provides selfmanaged solution thought building saas product would good way learn new tech getting started programming history looked something like lamp php mysql apache ruby rail django nodejs becoming extremely popular mongodb started become mature technology interested decided use new project get overwhelmed learning thing decided use angular fronted since already familiar month getting started finally deployed http kernlus world see give idea expectation project deployed digital ocean droplet mean everything mongo nginx node single machine next month two sufficed since traffic low first wave december thing started get interesting kernl moved kernl closed alpha beta led rise sign ups traffic steadily started climb high handled single droplet around december customer large install base start use kernl see graph scale completely change kernl went request per day request per hour seems like lot time still well within single droplet could handle thats le request per second scaling first month kernl experienced steady growth started charging february helped fuel growth made customer feel comfortable trusting something important update starting march noticed resource consumption droplet getting bit hand wanting keep cost low development time actual money opted scale kernl vertically per month droplet ram core seemed like plenty knew permanent solution lowest friction one time scaling period kernl went also ran issue apache started using apache reverse proxy familiar started fall would occasionally receive request rate instead tweaking apache switched using nginx yet run issue sure apache handle far request simply know enough tweaking setting make happen scaling increasing availability rest kernl saw continued steady growth kernl grew customer started rely update bitbucket github pushtobuild knew time make thing far reliable resilient currently course month made following change moved file storage aws one thing occasionally brought kernl resulted dropped connection large customer would push update lot connection would stay open file download made hard request get without timing moving uploaded file nobrainer make scaling file downloads stupidsimple moved mongo composeio one thing learned mongo managing cluster huge pain as tried run mongo cluster month much work correctly end paying composeio best choice also awesome highly recommend moved nginx server beginning nginx lived box node application better scaling separation concern moved nginx droplet eventually would end nginx server implemented floating ip address added node server nginx living server mongo living composeio file served able finally scale node side thing kernl currently node app server handle request rate final thought past year wondered taking time build thing right first time would worth come conclusion optimizing simplicity probably kept interested kernl long enough make profitable deal enough complication day job deal fun side project feel like great way kill passion
338,Lobsters,scaling,Scaling and architecture,Provisioning and Deploying Cloud Native Systems in AWS with Terraform and Shell Scripts,https://github.com/18F/cloud-native-aws-terraform-workshop,provisioning deploying cloud native system aws terraform shell script,provisioning deploying cloud native system aws terraform shell script prerequisite workshop participant virtualbox aws cli terraform vagrant running slide locally revealjs slide revealjs documentation instructor preparation default vpc limit vpcs per region codescripts acknowledgement blog post terraform public domain public domain contributing universal public domain dedication revealjs,provisioning deploying cloud native system aws terraform shell script handson training class participant learn aws guest network architecture region availability zone vpcs subnets security group route gateway element aws identity access management role policy configure simple highly available aws application environment aws autoscaling group load balancer rds cloudwatch infrastructureascode aws terraform configure manage aws infrastructure using terraform simple automated deployment using shell script bucket introduction architecture distributed system basic infrastructure build shown system diagram prerequisite workshop participant must bring laptop support bash linux macos window window subsystem linux installed fine alternatively stand virtual linux box using tool virtualbox must comfortable using shell commandline please install aws cli terraform coming class note may also interested installing vagrant optional running slide locally workshop run series slide deck deck written html use revealjs view slide local machine first go slide directory install npm nt already run following command npm install grunt serve run http server locally spawn browser window first slide workshop check revealjs documentation using revealjs instructor preparation workshop designed scheduled class including lunch break two break word ran material without break would take class requires handson coding recommend maximum participant per instructor assuming participant comfortable shell scripting text editor participant nt familiar editing running command shell need lower ratio student instructor time recommend using empty aws account workshop participant set vpc course workshop created new aws account need submit request raise default vpc limit vpcs per region done several day advance delivering class instructor upload content codescripts directory bucket code slide assume bucket called running want upload bucket change reference bucket repo find using grep r acknowledgement charity major blog post terraform proved valuable preparing workshop thanks charity public domain project worldwide public domain stated contributing project public domain within united state copyright related right work worldwide waived universal public domain dedication contribution project released dedication submitting pull request agreeing comply waiver copyright interest note part project taken revealjs repository copyright c hakim el hattab
339,Lobsters,scaling,Scaling and architecture,oklog/ulid: Universally Unique Lexicographically Sortable Identifier in Go,https://github.com/oklog/ulid,oklogulid universally unique lexicographically sortable identifier go,universally unique lexicographically sortable identifier alizainulid background install usage commandline tool specification component timestamp entropy encoding crockford binary layout byte order string representation test benchmark prior art,universally unique lexicographically sortable identifier go port alizainulid binary format implemented background guiduuid suboptimal many usecases nt character efficient way encoding bit uuid impractical many environment requires access unique stable mac address uuid requires unique seed produce randomly distributed id cause fragmentation many data structure uuid provides information randomness cause fragmentation many data structure ulid however compatible uuidguid unique ulids per millisecond exact lexicographically sortable canonically encoded character string opposed character uuid us crockford better efficiency readability bit per character case insensitive special character url safe monotonic sort order correctly detects handle millisecond install go get usage ulid constructed timetime ioreader entropy source design allows greater flexibility choosing tradeoff please note randrand math package safe concurrent use instantiate one per long living goroutine use syncpool want avoid potential contention locked randsource frequently observed package level function func exampleulid timeunix entropy ulidmonotonic randnew randnewsource tunixnano fmtprintln ulidmustnew ulidtimestamp entropy output commandline tool repo also provides tool generate parse ulids command line installation go get githubcomoklogulidcmdulid usage usage ulid hlqz f format parameter f format format parsing show time format default unix m h help print help text l local parsing show local time instead utc q quick generating use noncryptograde entropy z zero generating fix entropy allzeroes example ulid ulid z ulid sun mar utc ulid local specification current specification ulid implemented repository component timestamp bit unixtime millisecond wo nt run space till year ad entropy bit user defined entropy source monotonicity within millisecond ulidmonotonic encoding crockford used shown alphabet excludes letter l u avoid confusion abuse binary layout byte order component encoded octet component encoded significant byte first network byte order string representation timestamp entropy char char test benchmark intel core ivy bridge ghz macos go nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop nsop mb bop allocsop prior art
340,Lobsters,scaling,Scaling and architecture,The NIST Definition of Cloud Computing (2011),http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf,nist definition cloud computing,,obj endobj xref n n n n n n n n n n n n n n n n n n trailer prev startxref eof obj stream b vc  endstream endobj obj metadata routlines rpagelayoutonecolumnpages rstructtreeroot rtypecatalog endobj obj extgstate font xobject rotate endobj obj endobj obj iccbased r endobj obj endobj obj stream iv w n j endstream endobj obj endobj obj endobj obj endobj obj endobj obj endobj obj endobj obj stream u v endstream endobj obj endobj obj iccbased r endobj obj filterccittfaxdecodeheight stream r x endstream endobj obj font rotate endobj obj stream
341,Lobsters,scaling,Scaling and architecture,The Cure for Architectural Amnesia,https://www.promptworks.com/blog/the-cure-for-architectural-amnesia,cure architectural amnesia,make architectural decision software never done rfcs rfcs great request comment rfcs venue propose change rfcs designed collect input rfcs let propose multiple idea rfcs also terrible rfcs historical record rfcs never get finished rfcs usually huge huge doc terrible must better way architecture decision record howto put version control choosing record choosing record reading post michael nygard yan pritzker interpretation,often find introducing codebase new teammate ca nt quite remember something one way really sure good reason perhaps trying explain architecture software best come well way always guess architectural decision record provides history acting collection architecturally significant decision succinct structured format describes exactly something changed given point time provide way understand architecture evolved grownhow make architectural decision common scenario come making architectural decisionsfirst somebody proposes new idea lucky written somewhere shown team maybe shouted meeting instead discussion somebody else take general consensus order distill input team determine preferred approach problemnext someone else implement often person proposing idea person implement especially idea large one person actually implement ownand finally change get approved merged done yeah kidding software never donethis process leaf team potential architectural amnesia nothing record decision actually made furthermore nothing refer trying understand remember decision made mean leaving future developer following option look architecture either say well nt understand author seems pretty smart say well nt understand author seems pretty dumb something different hopelessly wander outdated doc old email chat archive choose one abovenone leave teammate good place continue making significant architectural decisionswhat rfcs rfcs great team might say nt need anything else already request comment process like internet engineering task force first using rfc process internally sure creating anything nearly meticulous detailed ietf write rfcs sometimes pale comparison ok currently using rfcs workflow keep rfcs greatrfcs venue propose change rfcs appropriate venue proposing change architecture great way collect idea team member centralized place designates right place itrfcs designed collect input rfcs great mean collect input proposed change heck even name goal rfc collect feedback proposal anyone everyone may interested itrfcs let propose multiple idea rfcs way propose multiple alternate possibly incompatible idea let lob couple idea see get shot teammate stand flakrfcs also terriblehowever familiar rfc process also know rfcs terrible well rfcs historical record ca nt refer back rfc historical record nt designed one proposal change plus comment change nt accurately represent actually changed true ietf rfc well write rfc tomorrow intertwine real language around message structure http get accepted still reflect folk actually using historical record rfcs never get finished often rfc provides enough idea discussion someone start implementation new idea happens rarely necessary go back finish rfcrfcs usually huge unless ietf probably necessary distill idea something succinct instead better get flakey idea teammate know exactly thinking downside huge doc terribleany document rfc something else becomes terrible larger get due transitive property doc doc huge harder readdocs hard read harder updatedocs hard update usually datedocs date terribleqed huge doc terrible obviously huge quantity doc system beget lot documentation mean large individual doc like actual separate file hard maintain go source code well must better way need record decision ideaswe need consistentwe need keep simplethe solution architecture decision recordsthis change architecture triviality might create lot work change code process nt actually change architecture going start using tab instead space significant architectural change may necessarily userfacing might usually solve problem improve aspect architecturealso meant historical record especially disparate team never change created one small exception supersededhowtogreat decided want make first architecture decision record walk step followfirst create file name something like want sequential coursenext give title short point title adopting python short enough fit commit messagenext even though record short give summary example summary decided upgrade python python support unicode betterthis super short concise maybe even character lessnext context describes state system decision made also record external force come play context currently using python everywhere user want start sending u unicode character apithis eventually become date still contextually relevant without context decision record might make sense reviewed hindsight usually implementation particular architectural decision make context date fine next decision made full sentence active voice decision migrate python service x zafter next section list consequence benefit decision including could go wrong consequence able handle unicode character might provide backwardscompatible support dependency nt ported python yetfinally status section one following proposedaccepteddeprecatedsupersededthe last two time decision record modified creation unless something found factually wrong record furthermore case superseded may useful provide link decision record supersedes status superseded put version controlfinally either start new repository decision record make directory another repository docsadrs easy see l adrs put decision record review like code change teammate check accuracy merged record live version control perpetuity serve point reference future decision new developerschoosing recordone concern heard regard decision record difficult know change significant enough recordobviously ca nt record every single little change make architecture thing truly small discrete enough actually obviousmy recommendation find weighing two option research extending unknown domain feel need inform teammate change decision record relevantchoosing recordanother concern write decision record like code need write started implementation change started last step implementing change good argument putting decision record repository code live pull request change required implementation really scale across multiple repository however reading main inspiration article post michael nygard another flavor thought decision record also read yan pritzker interpretationthanks mike nicholaides patrick smith andrew croce ryan hinkel reading draft post
342,Lobsters,scaling,Scaling and architecture,How I Installed Kubernetes From Scratch on a Bytemark VM,http://blog.willmer.org/2016/11/kubernetes-bytemark/,installed kubernetes scratch bytemark vm,luzme kubernetes bytemark getting started http setup master node remove install docker clone kubernetes code check network config master worker edit run worker install code finished,run luzme ebook search system run virtual machine country continent article explains setup kubernetes scratch set vms bytemark improve way use infrastructure behind luzme using new containerbased technology docker kubernetes provide better maintainable scalable system luzme us diverse stack technology including percona mysqlcompatible database firebase realtime nosql redis celery django solr angularjs python majority system run vms provided excellent bytemark uk hosting company vms running country necessary develop support system need system work need selfhealing possible would really like scale gracefully next time find luzme front page cnet lifehacker system scale cope rather falling whimpering knee liked idea using kubernetes docker containerise aspect infrastructure although documentation time made easy get single node running proved extremely hard get multinode system running documentation needed help others avoid pain discovery possible included listing network configuration stage since cause almost problem encountered please note done january current documentation procedure may different getting started followed documentation http first step create ubuntu trusty node one master one worker using usual bytemark setup process add worker node would replicate procedure first one note documentation say please install docker docker setup master node created standard bytemark vm using base configuration gb memory gb disk space choosing ubuntu lts trusty o install saved root password prefer use le powerful account possible configured machine let ssh key mkdir ssh chmod ssh touch sshauthorizedkeys chmod sshauthorizedkeys copied public key new machine adding sshauthorisedkeys remove cause many problem due confusion install script different result returned asking programmatically ip address hostname bytemark vms come standard sufficient time understand different result returned obvious reason decided disable ideal time would investigated code nano etcsysctlconf code add line sysctlconf file sysctl p install docker aptget install dockerio note package name docker ubuntu network config looked like ifconfig link encap ethernet hwaddr inet broadcast multicast rx tx rx b tx b link encap ethernet hwaddr fe inet addr scope global addr fcff ff scope link broadcast running multicast rx tx rx mb tx kb lo link encap local loopback inet addr scope host loopback running rx tx rx b tx b clone kubernetes code git clone href http githubcomkuberneteskubernetesgit http githubcomkuberneteskubernetesgit git checkout cd kubernetesdocsgettingstartedguidesdockermultinode need kubectl program part repo download using link kubernetes documentation put usrlocalbin check network config run command ifconfig route docker p docker h unix varrundockerbootstrapsock p kubectl get node get output similar ifconfig link encap ethernet hwaddr inet broadcast multicast rx tx rx b tx b link encap ethernet hwaddr fe inet broadcast running multicast rx tx rx mb tx mb lo link encap local loopback inet loopback running rx tx rx b tx b route kernel ip routing table destination gateway genmask flag metric ref use iface default ug u u docker p container id image command created status port name docker h unix varrundockerbootstrapsock p fata get http dial unix varrundockerbootstrapsock file directory trying connect tlsenabled daemon without tl kubectl get node error could nt read version server get href http http dial tcp connection refused tell u docker interface ifconfig docker route routing table standard docker daemon running nt live container docker daemon needed kubernetes yet running neither kubernetes server master install kubernetes master node cd kubernetesdocsgettingstartedguidesdockermultinode mastersh whereupon result command look like ifconfig link encap ethernet hwaddr inet broadcast multicast rx tx rx b tx b link encap ethernet hwaddr fe inet broadcast running multicast rx tx rx mb tx mb link encap ethernet hwaddr inet broadcast running multicast rx tx rx b tx b lo link encap local loopback inet loopback running rx tx rx kb tx kb route kernel ip routing table destination gateway genmask flag metric ref use iface default ug u u u docker p container id image command created status port name href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube schedule minute ago minute href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube apiserve minute ago minute href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube controll minute ago minute href http pause minute ago minute href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube proxy minute ago minute hopefuldavinci href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube kubelet minute ago minute ecstaticnobel docker h unix varrundockerbootstrapsock p container id image command created status port name href http optbinflanneld minute ago minute tenderkowalevski href http usrlocalbinetcd minute ago minute romanticmccarthy kubectl get node name label status href http ready worker add ip address worker node firewall master follow initial setup look similar root ifconfig link encap ethernet hwaddr inet broadcast multicast rx tx rx b tx b link encap ethernet hwaddr fe inet broadcast running multicast rx tx rx mb tx mb lo link encap local loopback inet loopback running rx tx rx b tx b root route kernel ip routing table destination gateway genmask flag metric ref use iface default ug u u root docker p container id image command created status port name edit run worker install code line workersh file kubernetes repo cause confusion mentioned earlier edit workersh remove overrider line run work installation script export masterip public ip master workersh worker node ifconfig link encap ethernet hwaddr inet broadcast multicast rx tx rx b tx b link encap ethernet hwaddr fe inet broadcast running multicast rx tx rx mb tx mb link encap ethernet hwaddr inet broadcast running multicast rx tx rx b tx b lo link encap local loopback inet loopback running rx tx rx b tx b route kernel ip routing table destination gateway genmask flag metric ref use iface default ug u u u docker p container id image command created status port name href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube proxy minute ago minute modestmclean href http gcriogooglecontainershyperkube gcriogooglecontainershyperkube hyperkube kubelet minute ago minute goofybabbage docker h unix varrundockerbootstrapsock p container id image command created status port name href http optbinflanneld minute ago minute serenehoover kubectl get node name label status href http ready href http ready finished done wrote ansible script meant figured possible install new node short time took month digging kubernetes install get working hope help someone
343,Lobsters,scaling,Scaling and architecture,CockroachDB Stability Post-Mortem: From 1 Node to 100 Nodes,https://www.cockroachlabs.com/blog/cockroachdb-stability-from-1-node-to-100-nodes/,cockroachdb stability postmortem node node,run cockroachdb cluster tl dr achieved stability goal still working chaos scenario system easily stable many node cluster tested successfully node background hypothesis root cause rapid pace development obscuring contributing instability faster solution could developed many engineer paying attention stability focused team clear leader instability localized core set component undergoing many disparate change anyone fully understand review communication exit criterion must measurable achievable blog post announce stability code yellow hacker news commentator technical detail rebalancing via snapshot simulation lock refactoring tracing tool lightstep corruption raft simulation also directly contributed another key product goal constrain network disk cpu usage directly proportional amount data read written outcome working two branch cockroachdb stability team fewer people scrutiny stream chart gossip kv storage conclusion cockroachdb stability could avoided instability hacker news commentary proving distributed system correct slow allocsim zerosum,get blog post inbox august published blog post entitled run cockroachdb cluster post outlined difficulty encountered stabilizing cockroachdb cockroachdb stability lack become significant enough designated code yellow issue concept borrowed google mean problem pressing merit promotion primary concern company u code yellow warranted database program worth byte store binary lack stability post set stage background cover hypothesis root cause instability communication strategy interesting technical detail outcome stabilization effort conclusion long post bear tl dr achieved stability goal still working chaos scenario system easily stable many node cluster tested successfully node background better set stage announced cockroachdb beta release april year development five month progress beta concern correctness performance general push new feature dominated focus incorrectly assumed stability would emergent property forward progress long everyone paying attention fixing stability bug whenever encountered august despite team developer stand cluster two week without major performance stability issue nothing could effectively convey gulf opened stability expectation reality increasingly frequent mention instability punchline office despite feeling one two pull request away stability inevitable chuckle nevertheless morphed insidious critique blog post chronicle journey establish baseline stability cockroachdb hypothesis root cause caused instability obviously technical oversight unexpectedly complex interaction system component better question preventing u achieving stability perhaps surprisingly hypothesis came mix mostly process management failure engineering identified three root cause rapid pace development obscuring contributing instability faster solution could developed imagine delicate surgery excessive amount blood welling incision must stop bleeding first order operate analogy suggested need work stability fix isolation normal development accomplished splitting master branch two branch master branch would dedicated stability freezing exception pull request targeting stability development would continue develop branch many engineer paying attention stability focused team clear leader imagine many cook kitchen working independently dish without anyone responsible result tasting right complicate matter imagine many chef making experimental time designate head chef chose peter mattis one cofounder lead engineering particularly good diagnosing fixing complex system obvious choice instead previously diffuse set goal develop myriad functionality review significant amount code agreed would largely limit focus stability become le available duty key objective enable focus establish accountability instability localized core set component undergoing many disparate change anyone fully understand review perhaps smaller team could apply scrutiny fewer careful change achieve eluded larger team downsized team working core component transactional distributed keyvalue store composed five engineer familiarity part codebase even changed seating arrangement felt dangerous countercultural normally randomly distribute engineer project team naturally resist balkanization communication decision something stability happened quickly come home august vacation blithely assuming instability solved problem unfortunately new seemingly worse problem cropped finally provided enough perspective galvanize u action engineering management team discussed problem earnest considered likely cause laid course action course weekend proceed communicate decision internally team cockroach lab soul searching externally community large one value cockroach lab transparency internally open stability goal success failure meet transparent problem enough fell honest magnitude problem meant company decided drafted detailed email announcing code yellow team succeeded clearly defining problem risk action taken importantly code yellow exit criterion exit criterion must measurable achievable decided cluster running two week chaos condition without data loss unexpected downtime succeed precipitous decision communication seemed member team received feedback decision lacked sufficient deliberation implementation felt railroaded decide immediately communicate code yellow externally although consensus quickly formed around necessity one thing building open source project make effort use gitter instead slack engineering discussion community large participate would step backwards withhold important change focus another thing community surely aware stability problem opportunity clarify set expectation nevertheless task actually writing blog post announce stability code yellow easy free misgiving raise hand like airing problem unsurprisingly criticism hacker news commentator also supportive voice end maintaining community transparency right decision hope established trust technical detail change process team structure decided necessary communication undertaken embarked intense drive address factor contributing instability priority order course idea long would take anywhere one three month general consensus end achieved code yellow exit criterion five week fix well instability appeared various guise including cluster slowing precipitously deadlocking outofmemory panic ooms data corruption detected via periodic replica checksum comparison rebalancing via snapshot generation communication replica snapshot used rebalance repair data cockroachdb cluster persistent adversary battle stability snapshot use significant disk network io mechanism limit memory consumption processing time holding important lock originally considered unnecessary beta stability much work tame snapshot occurred month leading stability code yellow hint significance course addressing snapshot reduced memory usage streaming rpcs made structural change avoid holding important lock generation however true cause snapshot instability proved trivial oversight simply visible fog cluster stabilization least mostly eliminated obvious symptom snapshot badness snapshot used node replicate information node repair node lost rebalancing spread load evenly node cluster rebalancing accomplished straightforward algorithm node periodically advertise number range replica maintain node computes mean replica count across node decides node underfull compared mean nothing overfull rebalances via snapshot underfull node error making judgement literally without applying enough threshold around mean order avoid thrashing see animated diagram show two scenario simulation left exact mean simulation rebalance within replica mean never stop rebalancing notice far rpcs sent simulation never reach equilibrium right threshold mean simulation rebalance within threshold mean quickly reach equilibrium practice continuously rebalancing crowded salient work done cluster lock refactoring tracing tool invaluable diagnosing lockcontention cause excessively slow deadlocked cluster symptom caused holding common lock processing step could sometimes take order magnitude longer originally supposed pileup common lock resulted rpc traffic jam excessive client latency solution lock refactoring lock held raft processing particular proved problematic command range executed serially holding single lock per range limited parallelization caused egregious contention longrunning command notably replica snapshot generation garbage collection replica data rebalancing previously protected common lock order avoid tricky consistency issue replica gc work time consuming impractical holding pernode lock covering action store case expedient solution coarsegrained locking proved inadequate required refactoring tracing tool ironically tracing tool used diagnose degenerate locking behavior stability culprit internal tracing tool pedantically storing complete dump kv raft command span held trace ring buffer fine small command quickly caused outofmemory oom error larger command especially prestreaming snapshot silver lining various oomrelated difficulty development finegrained memory consumption metric tight integration go c heap profiling tool integration lightstep distributed tracing system inspired google dapper corruption ooms deadlock often diagnosed fixed honest labor pay honest wage keep u night seemingly impossible corruption error occur replica ie replica agree common checksum content others visible system invariant broken kind problem rare though found one stability code yellow cockroachdb us bilevel index access data system first level life special bootstrap range advertised via gossip node contains addressing information second level life arbitrary number subsequent range second level finally contains addressing information actual system data life remaining range addressing record updated range split rebalanced repaired updated like data system using distributed transaction always consistent however second level index addressing record went unexpectedly missing luckily ben darnell resident coding sherlock holmes able theorize gap model could account problem despite requiring obscure unlikely sequence event perfect timing amazing brilliant engineer intuit code inspection alone also ought maxim sufficiently large distributed system anything happen happen raft last certainly least waged epic struggle tame raft distributed consensus algorithm resonant theme technical explanation originally concluded improvement raft drawing board could wait general availability release seen necessary much larger cluster raft algorithm impedance mismatch cockroachdb architecture could simply ignored time proved faulty assumption impedance mismatch yes turn raft busy protocol typically suited application small number distinct instance raft group required however cockroachdb maintains raft group per range large cluster hundred thousand million range raft group elect leader coordinate update leader engages periodic heartbeat follower heartbeat missed follower elect new leader large cockroachdb cluster meant huge amount heartbeat traffic proportional total number range system range actively read written causing massive amount network traffic conjunction lock contention snapshot would cause chain reaction example many heartbeat would fill network queue causing heartbeat missed leading reelection storm thus bringing overall progress halt causing node panic due unconstrained memory usage fix dynamic undertook two significant change first lazy initialization raft group previously cycle every replica contained node startup time causing participate respective raft group follower lazy dramatically eased communication load node startup however lazy free raft group require time respond first read write request still cold leading higher latency variance still benefit outweighed cost success lazy initialization led insight raft group need active immediately startup simply decommissioned use called process quiescence applied raft group participant fully replicated pending traffic remaining final heartbeat raft group contains special flag telling participant quiesce instead ready campaign new leader leader fails heartbeat simulation left naive raft simulation notice near constant sequence heartbeat denoted red rpcs raft group constant despite slow trickle writes application right quiescing raft simulation raft heartbeat occur order quiesce write traffic addition change raft batching managed meaningfully reduce background traffic also directly contributed another key product goal constrain network disk cpu usage directly proportional amount data read written never proportional total size data stored cluster outcome process management initiative fare addressing three hypothesized root cause working two branch splitting master branch without cost added significant overhead neardaily merges master branch develop order avoid conflict maintain compatibility stability fix effectively excluded change core package develop branch order avoid massive merge road held developer effort refactorings particular making unpopular particular tamir duberstein martyr stability cause suffering daily merge master develop first quietly mounting frustration split branch necessary look data suggests significant churn develop branch counted commits split branch epoch despite regression stability branch merged suspect successful merge result limit change core component split branch probably psychological benefit working isolation stability branch nobody arguing crucial factor cockroachdb stability team designating team stability specific focus putting single person charge proved invaluable case drafted experienced engineer may led productivity hit area since temporary easy justify given severity problem relocating team member closer proximity felt like meaningfully increased focus productivity started however ended conducting natural experiment efficacy proximity first two three five stability team member ended working remotely despite increasing ratio remote engineer notice adverse impact execution ended important proximity daily stability sync stand ups served backbone coordination required minute morning agenda remains status test cluster working group discussion clearing blocking issue also held twiceweekly stability war room pressed interested engineer role production monkey week production monkey engineer dedicated overseeing production deployment monitoring many contribution came beyond stability team war room central point coordination larger engineering org everyone pitching production duty raised awareness familiarized engineer deployment debugging tool fewer people scrutiny smaller team mandate greater scrutiny crucial success factor testament structure become le permanent analogy achieving stability maintaining imagine swimming ocean night little sense direction shoreline pretty sure far spot could put foot stop swimming every time tried touch bottom finally found stable place proceed confidence step nothingness swim back pace reassess position safety merge nontrivial change core component oneatatime deploying immediatelyprior sha verifying course several hour load deploying nontrivial change verify expected behavior without regression process work proven dramatically effective smaller stability team instituted obsessive review gatekeeping change core component effect went state significant concurrency decentralized review smaller number clearly delineated effort centralized review somewhat counterintuitively smaller team saw increase per engineer pull request activity see stream chart stream chart stream chart show pull request changed code line addition deletion major cockroachdb component three blue area bottom contain core component gossip kv storage vertical black line show start end code yellow branch split graph see stabilization effort saw significant tightening pace pull request change near merge time effect traced increase embargoed pull request affecting core component mostly refactorings performance improvement considered risky merge increasingly stable master conclusion cockroachdb stability hindsight hacker news commentary seems negligent allowed stability become pressing concern realized earlier problem going away without changing approach one explanation analogy frog slowly heating pot water working closely system day day failed notice stark contrast become stability expectation prebeta reality month followed many distraction rapid churn code base new engineer starting contribute team stability primary focus end jumped pot water gotten pretty damn hot many u cockroach lab worked previously complex system took sweet time stabilize enough hold deepseated belief problem tractable posited stopped work system small group dedicated engineer could fix stability matter week stress enough powerful belief achievable solution could avoided instability ah big question going use instead hacker news commentary previous blog post reveals differing viewpoint going say next simply conjecture assert counterfactual possible nobody assert impossible however since unaware complex distributed system avoided period instability weakly assert quite unlikely present argument experience clear knowledge fallacy enough disclaimer worked several system mold cockroachdb none required le month stabilize chalk one personal anecdote work spanner google understanding took long time stabilize heard estimate long month many popular nondistributed database sql nosql open source commercial took year stabilize chalk several anecdotal hearsay proving distributed system correct possible likely apply kind stability problem plagued cockroachdb system worked designed case emergent behavior result complex interaction like conclude several practical suggestion mitigating instability future effort define le ambitious minimally viable product mvp hope suffer le emergent complexity smaller period instability proceed incremental fashion preventing instability careful process catch regression system functionally complete proceed immediately laser focus stability form team experienced technical lead make stability sole focus resist everyone working stability clearly define accountability ownership system like cockroachdb must tested real world setting however significant overhead debugging cluster aws cycle develop deploy debug using cloud slow incredibly helpful intermediate step deploy cluster locally part every engineer normal development cycle use multiple process different port see allocsim zerosum tool building distributed sql system untangling part sound like ideal tuesday morning hiring check open position
345,Lobsters,scaling,Scaling and architecture,"How Heap Works, Part 1: 10 Million Indexes And Counting",http://blog.heapanalytics.com/how-heap-works-part-1-10-million-indexes-and-counting/,heap work part million index counting,wanted migrate react mobx architecture afford spend six month rewriting codebase expense delivering feature rewrite migrating frontend typescript rendering dom structure give u lot control scope exactly much work put refactoring given part interface without getting sucked converting world react worrying le application state mobx atom legacy code interacts heavily collection go business new development react interact higherlevel store without bending fit backbone collection underneath storybook please reach hiring,year ago frontend written cumbersome combination backbone typescript custom state management layer maintainable wanted ship feature faster would let u wanted migrate react mobx architecture afford spend six month rewriting codebase expense delivering feature cooked technique piece piece let u build new component react lazily migrate existing chunk interface logic past eight month migrated frontend shipping big new feature process arriving codebase engineer excited contribute rewrite introducing architectural change complex application touch state view layer easy accidentally end rewriting whole thing start rewriting one thing run dependency muddying design start rewriting dependency repeat workflow attractive work feel productive building thing attention detail quality afford compromise architectural purity interoperability old code fortune feeling burned migrating frontend typescript project ballooned scope took longer expected locking lot really cool feature cautious started thinking improve architecture increase feature velocity knew fall trap rewriting functionality shipping new feature clear make fundamental change piecebypiece realistic alternative found facing new exciting engineering challenge migrate new architecture operating inside old architecture identified couple thought guide decision migrating api thing working influenced existing architecture way happened given rewrite everything else around rewritten perfectly hard would incorporate thing architecture long question respected building migrating component engineer feel confident work last outside transition period transition end found order build consistently according guideline needed toolkit shim pattern ensured could pave sustainable path forward rendering dom first thing want npm install react reactdom find place code call reactdomrender considering already using marionette manage lifecycle view nice opportunity tie react component view hierarchy pretty contained way wrote wrapper take component return marionette view manages mounting unmounting component normal marionette lifecycle hook function wrapinmarionetteview p component reactcomponentclass p reactsfc p prop p return class extends marionetteview null get classname string return reactview onshow reactdomrender reactcreateelement component prop thisel ondestroy reactdomunmountcomponentatnode thisel wrapper served u pretty well build react component already rendered full react state prop leave glue code legacy view rendering component one small caveat marionette view like add div dom el property might want titlebar buttonsavereport titlebar reactview buttonsavereport considering react expects full control root element anyway div necessary matter implementation diagram used inside view hierarchy eventually encountered area piece interface reused several place app would take time willing spend duplicate functionality new tool ready commit rewriting piece interface still wanted migrate view contained maybe see going way used marionette lifecycle hook create shim react component decided use react lifecycle hook create shim marionette view ended tad complicated react shim interface imarionetteviewprops viewfn marionetteview backbonemodel regionref element htmldivelement void class marionetteview extends reactcomponent imarionetteviewprops id string regionmanager marionetteregionmanager constructor prop imarionetteviewprops super thisregionmanager new marionetteregionmanager thisid marionetteview uuid componentdidmount void thisregionmanageraddregion entrypoint thisid thisregionmanagerget entrypoint show thispropsviewfn componentwillunmount void thisregionmanagerremoveregion entrypoint shouldcomponentupdate boolean return false render jsxelement return div id thisid ref thispropsregionref marionette expose regionmanager responsible view lifecycle management use provide enough context render marionette view like normal without modification bit hacky work component rendering marionetteview little bit work componentdidmount componentwillunmount hook listen state change view wrap found useful encapsulate hook inside separate component responsible rendering view put together component look like diagram form two shim couple place code rendering marionette view inside react component inside marionette view seems little crazy first structure give u lot control scope exactly much work put refactoring given part interface without getting sucked converting world react want point particularly revolutionary glamorous work shim foundational reasonable migration strategy worrying le application state started migrating feature react manipulate thing existed essentially global state needed pattern allowed new component stay sync old architecture minimal flimsy glue code whole class bug around user change thing one part interface see previous version thing separate page look pretty silly degrades user trust application wanted avoid introducing many bug change state management part broader architectural change opted use mobx state management dead simple api graceful require change access data model mobx work react tracking property access rerendering component access property render function change pretty easily spruce model needed observable property use make clean react component since mobx change type signature model still use like backbone view hook right event keep sync necessary progressively refactoring backbone collection business object like report event definition segment definition singleton store expose observable property store allow u retrieve gold copy object perform general mutation persistence operation usually look something like implementation detail omitted since particularly interesting class reportstore reportsbyid observablemap report action addreport report report promise report action savereport report report promise report action deletereport report report promise void const singleton new reportstore export singleton reportstore reportstore compatibility backbone view perform manipulation use computed property create collection view expect class reportstore reportsbyid observablemap report computed get ascollectiondeprecated backbonecollection report return new backbonecollection thisreportsbyidvalues work well enough flawless especially view refer backbone collection depend event might fire rerender fortunately one view interface depended collection work case backbone view perform heavy manipulation resource potentially handle persistence done something bit weirder goal create mobxready store resource without rewrite large amount tricky logic old view preserve backbone collection area solution make map object exposed store computed property derived original backbone collection expose persistence method interact backbone collection hood class eventdefinitionstore computed get eventdefinitionsbyid observablemap eventdefinition return observablemap cachedefinedeventsmodels groupby event eventid mapvalues first value action saveeventdefinition event eventdefinition promise eventdefinition return eventsave action event replace event backbone collection high level allows u build robust futureready component store without interact backbonecollection update state trick backbonecollection observable eventdefinitionsbyid recompute event definition added removed cachedefinedevents collection dug bit mobx doc found little thing called atom essentially tool make thing look observable mobx atom fire reportobserved inside computed value observer mobx remember rerun observer atom fire reportchanged much hackery followed class cache private definedeventsatom mobxatom private realdefinedevents backbonecollection eventdefinition get definedevents backbonecollection eventdefinition thisdefinedeventsatomreportobserved wow easy return thisrealdefinedevents set definedevents val backbonecollection eventdefinition thisrealdefinedevents val constructor thisdefinedeventsatom new mobxatom defined event thisdefinedevents new backbonecollection eventdefinition thisdefinedeventson add remove thisdefinedeventsatomreportchanged nice const singleton new cache export singleton cache work like charm legacy code interacts heavily cachedefinedevents collection go business new development react interact higherlevel store without bending fit backbone collection underneath see take bit hacking shuffling thing table inside dynamic getters setter build api architecture around fully exists work bit unsavory super excited enabled u build new functionality within complex application speed comparable working fresh slate stay tuned next post describe using storybook make suite presentational component wrangle errant stylesheets midmigration prepare browser testing also spent time migrating application flight please reach love compare note oh way hiring come help make easier company big small make datadriven decision u
346,Lobsters,scaling,Scaling and architecture,How Riot Games manages docker containers in production,https://engineering.riotgames.com/news/running-online-services-riot-part-ii,riot game manages docker container production,first post series thing scheduling write mesos marathon lmctfy kubernetes fleet admiral overview go consul reconciler scheduling indepth resource constraint resource constraint conclusion first post dcos part introduction part ii scheduling part iii networking opencontrail docker part iii part deux networking opencontrail docker part iv dynamic application microservice ecosystem part v dynamic application developer ecosystem part vi product service,name kyle allan carl quinn work infrastructure team riot welcome second blog post multipart series describing detail deploy operate backend feature around globe post going dive first core component deployment ecosystem container scheduling jonathan first post series discussed riot history deployment struggle faced particular outlined difficulty experience deploying software grown especially due manual server provisioning perapplication added infrastructure support league legend along came tool called docker altered approach server deployment among thing outcome iteration became admiral internal tool cluster scheduling management found yesterday vmware similar software name great mind think alike important note journey application deployment far constantly evolving preparing next leg potentially adoption dcos discussed later article present story arrived point made decision hopefully others learn well scheduling docker released linux containerization became widely understood technology recognized could benefit moving towards containerized implementation infrastructure docker container image provide immutable deployable artifact built deployed dev test production additionally dependency image running production guaranteed exactly testing purpose article another benefit especially important docker allows decoupling unit deployment container unit compute host leveraging scheduler allocate container host hopefully intelligent manner eliminates coupling server application given container run number possible server backend service packaged docker image deployable time scale fleet server able adapt change quickly add new player feature scale feature getting heavy traffic rapidly roll update fix consider deploying service inside container production three major problem need solve address article given cluster host specific set chosen receive set container container actually started remote host happens container die answer three question need scheduler service operates cluster layer executes container strategy scheduler key component maintaining cluster ensuring container running right place restarting die example might want launch service hextech crafting need six container instance handle load scheduler responsible finding host enough memory cpu resource support container whatever operation needed get container running one server ever go scheduler also responsible finding replacement host affected container decided leverage scheduler wanted ability prototype quickly order understand containerized service would work well u production additionally needed ensure existing opensource option would work environment maintainer would willing accept adaptation write began writing scheduler became admiral surveyed landscape existing cluster manager scheduler scheduling container across cluster docker host could technology solve problem well looked project initial research mesos marathon technology fairly mature oriented large scale complicated tricky install made hard experiment evaluate time limited container support nt track rapid evolution docker nt play well docker ecosystem support container group pod felt need bundling sidecar container many service lmctfy kubernetes kubernetes evolved lmctfy looked promising clear future evolution would match needed kubernetes yet constraint system could container placement like needed fleet also prototyped small command line tool spoke docker api rest successfully demonstrated could use tool orchestrate deployment decided move forward writing scheduler borrowed best bit researched system including core idea behind kubernetes pod marathon constraint system vision track architecture functionality system influence possible eventually try converge one future admiral overview began writing admiral creating foundational deployment jsonbased metadata language called cudl cluster description language became language admiral spoke restful api two major component cudl cluster pack two different aspect spec live aspect represents description different phase container lifecycle spec representing desired state element posted admiral external source truth source control immutable delivered admiral spec cluster host describe resource available cluster spec pack describe resource constraint metadata required run service live representing realized state element admiral written go compiled packaged docker container run production datacenter admiral several internal subsystem shown diagram user perspective interaction admiral take place using provided admiralctl commandline tool communicates admiral rest api admiralctl user access admiral functionality via standard verb post ing new spec pack schedule delete ing old pack get ing current state production admiral store spec state using hashicorp consul regularly backup case catastrophic failure event complete data loss admiral also able partially rebuild spec state using information live state retrieved individual docker daemon reconciler life heart admiral key subsystem drive scheduling workflow reconciler periodically compare actual live state desired spec state discrepancy schedule action required bring live state back line livestate driver package support reconciler caching live host container state providing communication docker daemon cluster host rest apis scheduling indepth admiral reconciler operates spec pack effectively transforming live pack spec pack submitted admiral reconciler act create container start using docker daemon mechanism reconciler achieves first two highlevel scheduling goal described earlier reconciler receives spec pack evaluates cluster resource pack constraint finding appropriate host container know start container remote host using data spec let walk example starting container docker host example using local docker daemon docker host interacting local instance admiral server first use admiral pack create cluster name pack file command start pack command target specific cluster submits spec pack json admiral server notice almost immediately running command container started machine container started using parameter pack file shown name datblogscout description pack scout service usage engineering blog post service location devlocaltest discovery appname datscout container image datdscout version port internal external count next called admiral pack create use show command view live pack created admiral command admiral pack show cluster name pack name finally verify pack working hitting service within container using information admiral pack show command piece together simple curl hit service within admiral reconciler always running ensuring cluster live state always match desired spec state allows u recover container fails exit due crash whole server becomes unavailable due hardware failure reconciler working ensure state match player never experience interruption function solves third final problem described earlier container unexpectedly exit quickly recover impact remains minimal show existing container started via admiral pack create command proceed kill container stopping execution within second new container different id started reconciler realizes live state match spec state resource constraint best allocate container scheduler must insight cluster host two key component solving problem resource representation server available resource including memory cpu io networking among others constraint set condition included pack give scheduler detail restriction pack placed example might want place instance pack every host entire cluster specific host named myhostriotgamescom labeled zone cluster defining resource host give scheduler flexibility deciding place container defining constraint pack limit scheduler choice enforce specific pattern cluster conclusion riot admiral crucial piece continuing evolution deployment technology leveraging power docker scheduling system able deliver backend feature player much faster article examined feature admiral indepth showed scheduling container across cluster machine jonathan mentioned first post opensource world moved quickly similar model moving forward transitioning work admiral focusing deploying dcos become one leading opensource application scheduling container workload similar journey feel like something add conversation love hear comment information check rest series part introduction part ii scheduling article part iii networking opencontrail docker part iii part deux networking opencontrail docker part iv dynamic application microservice ecosystem part v dynamic application developer ecosystem part vi product service
347,Lobsters,scaling,Scaling and architecture,Do not use offset and limit for pagination in PostgreSQL [2014],http://chrisdone.com/posts/postgresql-pagination,use offset limit pagination postgresql,fast pagination postgresql update much ircbrowse,fast pagination postgresql update people asked much creating index event channel id help answer much implementation ircbrowse discovered builtin offset fast characteristic data ircbrowse event table publicevent column type timestamp timestamp time zone type text nick text text text network integer channel integer id bigint index eventunique unique constraint btree network channel timestamp nick type text eventuniqueid unique constraint btree id eventchannelidx btree channel eventnickidx btree nick eventtimestampidx btree timestamp eventtypeidx btree type size ircbrowse select count event count channel biggest ircbrowse select count event channel count working data scale large postgresql handle beautifully speed offsetlimit great ircbrowse explain analyze select event channel order id offset limit query plan limit actual index scan using eventuniqueid event actual filter channel think index scan simply expensive notice ordering id unique btree index check speed ircbrowse select event channel order id offset limit time m ircbrowse select event channel order id offset limit time m might think le second sift row row table pretty good think suck also deceptive increase row ircbrowse select event channel order id offset limit time m getting worse worse probably linear poor performance however solution use index table separate table contains foreign key pointing table ircbrowse eventorderindex table publiceventorderindex column type modifier id integer null origin integer null idx integer null index eventorderidorigin unique constraint btree id origin eventorderidx btree id eventorderidxidx btree idx eventorderorigindx btree origin pagination index channel ircbrowse select eventorderindex idx limit id origin idx used channel channel etc would space numerical index channel make efficient query data ircbrowse select idxid etimestamp enetwork echannel ircbrowse etype enick etext event e ircbrowse eventorderindex idx ircbrowse eid idxorigin idxidx ircbrowse idxid idxid ircbrowse order eid asc ircbrowse limit time m le constant time see action site take load render page run server time curl http ircbrowsenetbrowsehaskell real user sys course sending request browser take longer due connection overhead asset generally goal snappy old ircbrowsecom another individual kindly let name slow indeed see page loading data incrementally database anyhoo thought decent practical postgresqlspecific optimization regarding pagination hope worth writing
348,Lobsters,scaling,Scaling and architecture,What Am Container,https://glyph.twistedmatrix.com/2016/10/what-am-container.html,container,function call address space port specific optin docker sandstorm carina shared mutable state root evil,perhaps software developer perhaps developer recently become familiar term container perhaps heard container described something like lxc better applicationlevel interface cgroups like virtual machine lightweight perhaps even le usefully function call probably heard docker wonder whether container different part docker bewildered blisteringly fastpaced world container maybe trouble understanding fact might familiar half dozen orchestration system container runtimes already frustrated seems like whole lot work nt see point article like lay exactly point container people excited make ecosystem around confusing unlike previous writing topic going assume know anything ecosystem general basic understanding unixlike operating system separate process file network dawn time computer singletasking machine somehow load program main memory turn would run program lucky spit output onto paper tape program running computer looked around could see core memory computer running attached device including console printer teletypes later networking equipment course powerful program full control everything attached computer also somewhat limiting mode addressing hardware limiting meant program would break instant moved new computer rewritten accommodate new amount type memory new size brand storage new type network program contain within full knowledge every piece hardware might ever interact would expensive indeed also resource computer dedicated one program could nt run second program without stomping first one crashing mangling structure memory deleting data overwriting data disk programmer cleverly devised way indirecting virtualizing access hardware resource instead program simply addressing memory whole computer got little space could address memory address space program wanted memory would ask supervising program today call kernel give memory made program much simpler instead memorizing address offset particular machine kept memory program would simply begin saying hey operating system give memory would access memory little virtual area word memory allocation virtual ram virtualizing memory ie ephemeral storage nt enough order save transfer data program also virtualize disk ie persistent storage whereas wholecomputer program would seek position disk start writing data however pleased program writing virtualized disk might call today file first needed request file operating system word file system virtual disk networking treated similar way rather addressing entire network connection program could allocate little slice network port way program could instead consuming network traffic destined entire machine ask operating system deliver traffic say port number seven word listening port virtual network card getting bored obvious stuff yet good one thing frustrates container incredibly obvious idea logical continuation trend programmer intimately familiar different virtual resource exist reason said earlier two program need resource function properly try use without coordinating break horribly unixlike operating system le virtualize ram correctly one program grab ram nobody else modulo superpowered administrative debugging tool get use without talking program extremely clear memory belongs process program want use shared memory specific optin protocol basically impossible happen accident however abstraction use disk filesystems network card listening port address significantly limited every program computer see filesystem program data program store live filesystem every program computer see network information query everything receive arbitrary connection permission remove certain part filesystem view ie program optout far le clear program owns certain part filesystem access must carefully controlled sometimes mediated administrator particular way unix manages filesystems creates environment installing program requires manipulating state place filesystem program might require different state popular package manager unixlike system apt rpm rarely way separate program installation even convention let alone strict enforcement want recompile software configure prefix hardcode new location fundamentally package manager nt support installing different place program tell difference different installation location developer thought go one place file system hard code work machine order address shortcoming unix process model concept virtualization became popular idea virtualization simple write program emulates entire computer storage medium network device install operating system completely resolve oversharing resource process inside virtual machine real sense running different computer program running different virtual machine physical device however virtualiztion also extremly heavyweight blunt instrument since virtual machine running operating system designed physical machine ton redundant hardwaremanagement code enormous amount operating system data could shared host since form disk image totally managed virtual machine operating system host ca nt really peek inside optimize anything also make kind intentional resource sharing hard software manage host need installed host since installed guest wo nt full access host hardware hate using term heavyweight talking software often bandied contentfree criticism difference overhead running virtual machine process difference gigabyte kilobyte somewhere order magnitude huge difference mean need treat virtual machine multipurpose since one vm big run single small program mean often manage almost physical harware run program unixlike operating system running grant address space call entity created process understand container container get run program give memory whole virtual filesystem whole virtual network card metaphor process nt perfect container contain multiple process different memory space share single filesystem also container ecosystem fervor begin creep people interested container religiously exhort treat container single application run multiple thing inside ssh whole point container lightweight far closer overhead size process virtual machine process inside container query operating system see computer running owns entire filesystem mounted disk explicitly put administrator ran container word want share data another application given shared data optin optout way memorysharing optin unixlike system exciting sense really loweroverhead way run virtual machine long share kernel super exciting reason container exciting process reason using filesystem exciting use whole disk sharing state always inevitably lead brokenness optin better optout give program whole filesystem sharing data explicitly eliminate even possibility program scribbling shared area filesystem might break nt need package manager package installers removing function package manager inventory removal radically simplified le complexity mean le brokenness give program entire network address exposing port explicitly eliminate even possibility rogue program expose security hole listening port nt expecting eliminate possibility might clash program host hardcoding port number autodiscovering address addition exciting thing runtime side container rather thing run get container image present compelling improvement buildtime side linux window building software artifact distribution endusers quite challenging challenging clear specify depend certain software installed clear conflicting version software may version already available user computer clear put thing filesystem linux often mean getting software operating system distributor notice said linux window usual linux window mac desktop platform nt say anything mobile os macos android io window metro application already run container rule macos container bit weird different docker container mac check librarycontainers see view world application running see io look much something nt get discussed lot container ecosystem partially everyone developing technology breakneck pace many way linux serverside containerization continuation trend started mainframe operating system already picked full force mobile operating system one build image one building picture entire filesystem container see image complete artifact contrast package linux package manager fragment program leaving dependency integrated later image run machine except extremely unusual circumstance run target machine everything need run fully included build software image requires image implication server management longer need apply security update machine get applied one application time get applied normal process deploying new code since one update process delete old container run new one new image update roll much faster build image run test image security update applied confident wo nt break anything scheduling maintenance window managing reboots least security update application library kernel update different kettle fish exciting confusing fundamentally confusion caused way many tool many tool accepted software live image none old tool work almost every administrative monitoring management tool unixlike os depends intimately upon ability promiscuously share entire filesystem every program running container break assumption new tool need built nobody really agrees tool work wide variety force ranging competitive pressure personality conflict make difficult panoply container vendor collaborate perfectly many company whose core business nothing infrastructure gone reasoning process container much better process need start using right away even tooling pain adopting old tool nt work new tool tool vendor nt ready new tool community nt work usecase time write tool usecase nobody else cause problem somebody else course le fundamental reason much focus scale running smallscale web application stable userbase nt expect lot growth many great reason adopt container opposed automating operation fact keep thing simple fact software run container might obviate need systemmanagement solution like chef ansible puppet salt totally adopt try ignore complex involved part running orchestration system however container even useful significant scale mean company significant scaling problem invest container heavily write prolifically many guide tutorial container assume expect running multimillionnode cluster fully automated continuous deployment bluegreen zerodowntime deploys operation team great got stuff building component nontrivial investment leave dear reader absolutely adopting container technology say probably least using docker build software radically different container system like sandstorm might make sense depending kind service create course huge ecosystem tool might want use many mention although shout employer dockerasaservice carina delivered blog post among thing nt feel though need container absolutely right way value containerization derived adopting every single tool value container come four simple thing reduces overhead increase performance colocating multiple application hardware force explicitly call shared state required resource creates complete build pipeline result software artifact run without special installation setup instruction least software installation side still might require configuration course give way test exactly deploying benefit combine interact surprising interesting way enhanced wide growing variety tool underneath hype buzz real benefit containerization basically fixing old design flaw unix container let share le state shared mutable state root evil
349,Lobsters,scaling,Scaling and architecture,"Microservices: Please, dont",http://basho.com/posts/technical/microservices-please-dont/,microservices please,anyways fallacy cleaner code fallacy easier fallacy faster fallacy simple engineer fallacy better scalability use microservices takeaway,blog post adapted lightning talk gave boston golang meetup december seemed like everyone crazy microservices open favorite news aggregator choice without company never heard touting move microservices saved engineering organization may even worked one company got swept hype around tiny magical little service going solve problem big ailing legacy codebase course hindsight nothing could truth beauty hindsight often much closer vision thought looking forward month ago going cover major fallacy microservices movement coming someone worked company also got swept idea breaking apart legacy monolithic application going save day want takeaway blog post ideally anyone reading walk away series issue think deciding move microservice based architecture right anyways really perfect definition constitute microservice although people really champion approach codified fairly reasonable set requirement tautologically monolith actually mean practice microservice deal limited area domain possible thing necessary serve defined purpose stack give concrete example bank last thing want access record financial transaction push kind keep mind naming thing hard additionally people talk microservices often implicitly talking service need speak others remotely since distinct process quite often running location remote common build service speak network using rest kind rpc protocol outset actually seems pretty simple wrap tiny piece domain rest api kind everyone talk network experience people believe approach always true keep code cleaner easy write thing one purpose faster monolith easy engineer work codebase simplest way handle autoscaling plus docker somewhere fallacy cleaner code need introduce network boundary excuse write better simple fact matter microservices approach modeling technical stack requirement writing cleaner maintainable code true since le piece involved ability write lazy poorly thought code decrease however like saying solve crime removing desirable item store front fixed problem simply removed many option popular approach architect internals code around logical piece domain mirror concept microservice help keep dependency needed managing domain explicit well help keep key business logic sprawling multiple place additionally using service longer incurs excess use network potential error case arise benefit approach given closely mirror service oriented architecture built around microservices decide move microservice approach already done good deal design work front likely understand domain well enough able extract solid soa approach begin code itself move physical topology stack time move fallacy easier transaction never might seem simple outset domain especially newer company need prototype pivot generally redefine domain many time lend neatly carved little box often time given piece domain need reach get data part job correctly becomes even complex need delegate responsibility writing data outside domain broken area influence need involve others request flow store modify data land distributed transaction sometimes known saga lot complexity wrapped problem involving multiple remote service given request call parallel must done serially aware possible error application network level could arise point chain mean request often distributed transaction need approach handling failure could arise lot work understand error determine handle recover fallacy faster could gain lot performance monolith simply applying little extra tough one dispel truth often make individual system faster paring number thing number dependency load etc etc ultimately anecdotal claim doubt folk pivoted microservices saw individual code path isolated inside service speed understand also adding network inbetween many call network never fast coresident code call although often time additionally many story performance gain actually touting benefit new language technology stack entirely concept building code live microservice rewriting old ruby rail django nodejs app language like scala go two popular choice microservice architecture going lot performance improvement inherent choice technology language really chose describe process run simply perform better due thing like compilation majority apps startup space starting raw cpu memory performance almost never problem io additional network call adding io profile fallacy simple engineer bunch engineer working isolated codebases lead tin might seem simpler smaller team focused one small piece puzzle ultimately often lead many problem dwarf gain might see smaller problem space tackle biggest simply anything run everincreasing number service make even smallest change mean invest time effort building maintaining simple way engineer run everything locally thing like docker make easier someone still need maintain thing change additionally also make writing test difficult write proper set integration test mean understanding different service given interaction might invoke capturing possible error case etc etc there even time spent simply understanding system could better spent continuing develop would never tell engineer time spent understanding system time wasted would definitely warn people away prematurely adding level complexity know need finally also creates social problem well bug span multiple service require many change languish multiple team need coordinate synchronize effort fixing thing also breed situation people feel responsible push many issue onto team possible engineer work together codebase knowledge system grows kind willing capable working together tackle problem opposed king queen isolated little fiefdom fallacy better scalability scale microservice outward easily scale incorrect say packaging service discrete unit scale via something like docker good approach horizontal scalability however incorrect say something like microservice monolithic application work approach well create logical cluster monolith handle certain subset traffic example inbound api request dashboard front end background job server might share codebase need handle subset work every box benefit like exists microservice approach tune individual cluster given workload well scale individually response surge traffic given workload microservice approach guide approach get go apply exact method scaling stack monolithic process well use microservices ready engineering like close going could right time pivot approach starting know right way start single important step path solid workable approach microservices simply understanding domain working understand still trying figure microservices could harm good deep understanding know boundary dependency microservices approach could right move another important thing handle workflow specifically might relate idea distributed transaction know path category request make system understand path might fail could start build distributed model handling request alongside understanding workflow monitoring workflow monitoring subject greater v something core engineering effort may need lot data fingertip various part system understand one underperforming even throwing error solid approach monitoring various piece system begin understand system behavior increase footprint horizontally finally actually demonstrate value engineering organization business well moving microservices help grow scale make money although fun build thing try new idea end day important thing many company bottom line delay putting new feature make company revenue blogpost told monolith going need justify business sometimes tradeoff worth sometimes knowing pick battle spend time right technical debt earn lot credit long run takeaway hopefully new series condition question go next time someone suggesting microservices approach opened aim tell microservices bad rather jumping without thinking concern recipe problem road ask advocate building service via cleanly defined module code carve distinct service true need arises time approach necessarily way also panacea bad code get faster trying deal handful microservices ready sean kelly affectionately known software engineer year inaugural member riak developer program currently work komand security orchestration automation platform empowers security team quickly automate streamline security operation need code komand enables team connect tool build dynamic workflow utilize human insight accelerate incident response move forward faster reach twitter stabbycutyou
350,Lobsters,scaling,Scaling and architecture,"How Airbnb operate at scale with Rails, Java, and MySQL",https://medium.com/airbnb-engineering/unlocking-horizontal-scalability-in-our-web-serving-tier-d907449cdbcf,airbnb operate scale rail java mysql,engineering blog post problem problem airbnb maxscale database proxy connection pooling request throttling query blocklist blog post database proxy service smartstack latency impact result github,airbnb web application powered ruby rail java webfacing application monolithic rail application run many web server web request handled rail application talk several different java service instance search service listing pricing service airbnb technology stack mysql database play critical role storing core business data partition database application ease capacity planning example user messaging thread listing calendar management separate core booking flow managed database couple database operation spirit partitioning core database functionality see engineering blog post site traffic grew amazing rate every year infrastructure team responded horizontally scaling application server tier compute capacity vertically partitioning database database headroom summer peak season architecture working pretty nicely however one notable resource issue mysql database increasing number database connection application serversproblemswe use aws relational database service rds run mysql instance rds us community edition mysql server employ onethreadperconnection model connection management thread model mysql server likely hit famous problem problem upper bound number connection mysql server accept serve without dramatically increasing number thread running severely degrades mysql server performance mysql threadsrunning counter measure number query executing concurrently however due limited innodb storage engine thread concurrency spike metric really mean client query piling mysql database happens mysql query latency increase request queue across entire stack error rate spikesin production several severe database incident manifested large spike active running mysql server thread graph illustrate threadsrunning spike simultaneous application error rate spike root cause could attributed poorly written query underlying storage system outage usually would take long time mysql database server recover often time engineer resort manually killing connection stabilization trickas bad thread running spike pressing problem even fire fighting database incident brought site downtime database connection limitation application server run webfacing application direct connection core rds database mysql server allocates thread stack resource client connection although thread stack size could tuned allow handling client connection limited resource large number thread would cause scheduling context switch issue rds mysql server hit resource limitation client problem creating connection database connection limitation put cap application server capacity handle growing traffic end summer foresight hitting scaling bottleneck summer traffic engineer infrastructure scalability team started look viable solutionsairbnb maxscale database proxyto fair mysql dynamic thread pool feature however available mysql enterprise edition percona server mysql mariadb similar offering since airbnb us aws mysql rds access mysql thread pool feature look external proxy address connection limitation investigated several different open source technology chose mariadb maxscale mariadb maxscale mysql database proxy support intelligent query routing client application set backend mysql server however maxscale solve connection limitation problem would require establishing one backend mysql server connection client connection since looking connection pooling decided fork mariadb maxscale implement ourselvesconnection poolingin airbnb maxscale connection pooling implemented multiplexing n client connection connection backend mysql server client connection request completes successful authentication backend mysql server proxy severs link backend connection client connection park connection pool backend server server connection pool size configurable typically small number since forked mariadb maxscale developer branch able leverage persistent connection feature implement server connection pool receiving query client connection maxscale pick backend connection pool link client connection forward query backend mysql server maxscale understands transaction context client session therefore know keep linked backend connection transaction commits link must kept used forwarding query result back clientone challenge connection pooling implementation knowing unlink return backend connection pool query response consists one mysql packet maxscale keep onetoone link client connection backend connection forward response packet come connection pooling mode unlinking backend connection prematurely would cause client wait indefinitely complete set mysql packet correct mysql query response forwarding implemented mysql packet following mysql client server protocol comqueryresponse way airbnb maxscale unlink backend connection seen complete mysql packet query response aside forwarding response allows u measure query response size monitoringrequest throttlingthe typical server connection pool size configured production many instance airbnb maxscale proxy server number database connection mysql server several hundred normal situation small portion connection active use underlying storage outage happens bad expensive query hit fan query execution becomes slow noticeable server connection pool run maxscale proxy server instance take symptom signal backend mysql server may run concurrent thread running spike problem proactively throttle client request killing client connection production request throttling proven useful preventing database incident due transient storage system outagequery blocklistmaxscale us embedded mysql parser query routing actually build parse tree every mysql query query classifier module leveraged query parse tree bad query blocklisting airbnb maxscale motivation feature protect u ruby vm heap memory corruption memory corruption cause mysql query statement generated rail activerecord become corrupted way conditional predicate completely removed blog post detailed explanation nasty ruby heap corruption problemthe mysql parse tree make easy inspect predicate list mysql query query blocklisting feature leverage mysql parse tree look existence malformed predicate condition update delete statement reject statement protective coverage block mysql update delete statement without predicate condition well deployed airbnb maxscale query blocklist feature protected u least one instance scary corrupted query could caused damage one core database tablesdatabase proxy servicemariadb maxscale support multiple worker thread model practical reason chose use single worker thread airbnb maxscale proxy server deploy many instance achieve concurrency airbnb use smartstack service discovery deploy cluster airbnb maxscale server database proxy service core production mysql database application discover connect database proxy service instead mysql database airbnb maxscale database proxy service application server mysql server web application tier scaled horizontally per capacity demand new tier core architecture database proxy service scaled horizontally well launching new airbnb maxscale proxy server instancesthe diagram illustrates architecture different database proxy service deployed front different core mysql databaseslatency impactthe airbnb maxscale database proxy introduces additional network hop computation maxscale proxy lightweight request routing response forwarding connection pooling feature added airbnb maxscale straightforward add overhead concern possible latency impact extra network hop led u implement availability zone aware request routing smartstack smartstack used route request backend server random availability zone az az aware routing allows application server send request database proxy server availability zone extensive stress testing using database workload replay framework found latency concern really minimal negligible time ready launch high confidence airbnb maxscaleresultearlier deployed airbnb maxscale production smooth operation switched monolithic rail application connection direct connection mysql server database proxy service tier maxscale mysql protocol compliant switching connection airbnb maxscale require client application change following graph show drastic drop number database connection one core database performed operationcorrespondingly connection application shifted new airbnb maxscale database proxy service instead time deployed new database proxy mysql database verge database connection limitation made production right critical timewith connection pooling database proxy able scale application server tier addition server without increase mysql server thread went summer peace mind would service capacity required handling yet another record traffic peak summertoday airbnb maxscale database proxy service production different core mysql database different provisioned capacity many hundred maxscale server instance churning request web application server mysql database automatic request throttling effectively worked back pressure mechanism essentially replaces engineer killing connection manually running reliably one instance crash productionat airbnb engineering believe open source strongly far great production experience airbnb maxscale like share community therefore announce open sourced airbnb maxscale complete source documentation found github please try feel free share comment pull request u find working kind problem interesting hiring talented infra engineer please come work u
351,Lobsters,scaling,Scaling and architecture,How Uber Scales Their Real-time Market Platform,http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html,uber scale realtime market platform,uber time bigger four year matt ranney scaling uber realtime market platform matching dynamic demand dynamic supply realtime supply demand productive power enthusiasm node developer mapseta service database post trip pipeline money starting limit growth company supply service demand service geo supply geo demand handle million writes per second supply every state must tracked run hundred process reduce extra driving reduce waiting lowest overall eta ap system scaled thousand node far tchannel already time faster http uber getting http json business availability matter lot make everything retryable make everything killable crash small piece kill everything break smaller chunk total datacenter failure periodically sends encrypted state digest driver phone backup request cross server cancellation,reportedly uber grown astonishing time bigger four year think first time matt ranney chief system architect uber interesting detailed talk scaling uber realtime market platform tell u lot uber software work interested surge pricing covered talk learn uber dispatch system implement geospatial indexing scale system implement high availability handle failure including surprising way handle datacenter failure using driver phone external distributed storage system recovery overall impression talk one rapid growth many architectural choice made consequence growing fast trying empower recently assembled team move quickly possible lot technology used backend major goal team get engineering velocity high possible understandably chaotic successful start seems uber learned lot business really need succeed early dispatch system typical make work type affair assumed deep level moving people uber mission grown handle box grocery well people dispatch system abstracted put solid smart architectural foundation though matt think architecture might little crazy idea using consistent hash ring gossip protocol seems spot use case hard captivated matt genuine enthusiasm working talking disco dispatch system say excited tone like traveling salesman problem school cool computer science thing even though solution optimal traveling salesman interesting scale realtime realworld built fault tolerant scalable component cool let see uber work inside gloss matt talk stats platform nodejs python java go native application io android microservices redis postgres mysql riak twitter twemproxy redis google geometry library ringpop consistent hash ring tchannel network multiplexing framing protocol rpc thrift general uber transportation platform connecting rider driver partner challenge matching dynamic demand dynamic supply realtime supply side driver free whatever want demand side rider need transportation whenever want uber dispatch system realtime market platform match driver rider using mobile phone new year eve busiest time year uber easy lose track quickly industry made enormous progress technology getting good fast thing recently amazing quickly fade background twentythirty year ago mobile phone internet gps barely science fiction barely notice architecture overview drive rider driver mobile phone running native application backend primarily servicing mobile phone traffic client talk backend mobile data best effort internet imagine year ago basing business mobile data awesome sort thing private network used fancy qos quality service open internet client connect dispatch system match driver rider supply demand dispatch written almost entirely nodejs plan move iojs since iojs nodejs merged interesting distributed system work javascript hard underestimate productive power enthusiasm node developer enthusiastic get lot done quickly whole uber system probably seems pretty simple need subsystem people long seems way mark success lot long seems simple done job mapseta estimated time arrival dispatch make intelligent choice necessary get map routing information street map historical travel time used estimate current travel time language depends lot system integrated python c java service vast amount business logic service database lot different database used oldest system written postgres redis used lot behind twemproxy behind custom clustering system mysql uber building distributed column store orchestrating bunch mysql instance dispatch service keeping state riak post trip pipeline lot processing must happen trip completed collect rating send email update database schedule payment written python money uber integrates many payment system old dispatch system limitation original dispatch system starting limit growth company change mostly rewrote whole thing despite joel spolsky said system touched even service within dispatch system survived old system designed private transportation made lot assumption one rider per vehicle work uber pool idea moving people baked deep data model interface constrained moving new market new product like moving food box original version sharded city good scalability city run independently city added however became increasingly hard manage city big small city big load spike much built fast single point failure multiple point failure new dispatch system fix city sharding support product idea supply demand generalized supply service demand service created supply service track capability state machine supply track vehicle many attribute model number seat type vehicle presence car seat child wheelchair fit allocation need tracked vehicle example may three seat two occupied demand service track requirement order aspect demand rider requires car seat requirement must matched inventory rider mind sharing car cheaper rate must modeled box need moved food delivered logic match supply demand service called disco dispatch optimization old system would match currently available supply mean car road currently awaiting work disco support planning future making use information becomes available example revising route progress trip geo supply geospatial index required disco make decision based supply expected geo demand geo index also required demand better routing engine required make use information dispatch vehicle move around location update sent geo supply match rider driver display car map disco sends request geo supply geo supply make coarse first pas filter get nearby candidate meet requirement list requirement sent routing eta compute eta nearby geographically road system sort eta send back supply offer driver airport emulate virtual taxi queue supply must queued order take account order arrive geospatial index must super scalable design goal handle million writes per second write rate derived driver send update every second move around read goal many read writes per second everyone open app read old geospatial index worked well making simplifying assumption tracked dispatchable supply majority supply busying something subset available supply easy support global index stored memory handful process easy enough naive matching new world supply every state must tracked addition projected route must also tracked much data new service run hundred process earth sphere hard summarization approximation based purely longitude latitude uber divide earth tiny cell using google library cell unique cell id using every square centimeter earth represented uber us level cell km km depending earth box change shape size depending sphere give coverage shape want draw circle radius centered london tell cell needed completely cover shape since cell id id used sharding key location come supply cell id location determined using cell id shard key location supply updated sent replica disco need find supply near location circle worth coverage calculated centered rider located using cell id circle area relevant shard contacted return supply data scalable even though efficient might like since fanout relatively cheap write load always scaled adding node read load scaled use replica read capacity needed replica factor increased limitation cell size fixed level size dynamic cell size might supported future tradeoff smaller cell size greater fanout query routing answer geospatial option must ranked several high level goal reduce extra driving driving people job desire productive getting paid extra driving around ideally driver would continuous trip bunch work would queued would get paid reduce waiting rider wait little possible lowest overall eta old system let demand search currently available supply match done easy implement simple understand worked pretty well private transportation good choice made looking current availability idea driver currently transporting rider may better match customer asking ride currently idle driver farther away picking ontrip driver minimizes customer wait time minimizes amount extra driving time remote driver dynamic condition better handled model trying see future example driver come online near customer another driver already dispatched away way change dispatch decision another example involved customer willing share ride possible lot optimization trying predict future complicated scenario decision become interesting considering delivering box food case people generally something else different tradeoff involved scaling dispatch dispatch built using nodejs building stateful service stateless approach scaling work node run single process method must devised run node multiple cpu machine multiple machine joke reimplementing erlang javascript solution scaling node ringpop consistent hash ring gossip protocol implementing scalable faulttolerant applicationlayer sharding cap terminology ringpop ap system trading consistency availability better explain away inconsistency service better occasionally make error ringpop embeddable module included node process node instance gossip around membership set node agree make lookup forwarding decision independently efficiently really scalable add process work get done used shard data distributed locking system coordinating rendezvous point pubsub longpoll socket gossip protocol based swim improvement made improve convergence time list member gossiped around node added scalable swim scalable really work scaled thousand node far swim combine health check membership change part protocol ringpop system node process containing ringpop module gossip around current membership externally disco want consume geospatial every node equivalent random healthy node selected wherever request land responsible forwarding request right node using hash ring lookup look like may sound crazy hop peer talking yield really nice property like service scaled adding instance machine ringpop built uber rpc mechanism called tchannel bidirectional requestresponse protocol inspired twitter finagle important goal control performance across lot different language especially node python lot existing rpc mechanism work well wanted redis level performance tchannel already time faster http wanted high performance forwarding path intermediary could make forwarding decision easily without understand full payload wanted proper pipelining headofline blocking request response could sent either direction time every client also server wanted bakein payload checksum tracing first class feature every request traceable wends way system wanted clean migration path http http encapsulated naturally tchannel uber getting http json business everything moving thrift tchannel ringpop gossip tchannel based persistent connection persistent connection used fanout forward application traffic tchannel also used talk service dispatch availability availability matter lot uber competitor switching cost low uber even briefly money go someone else product sticky customer try later necessarily true uber make everything retryable something work retryable route around failure requires request idempotent retrying dispatch example dispatch twice charge someone credit card twice make everything killable failure common case killing process randomly damage crash graceful shutdown graceful shutdown need practiced need practiced unexpected thing break small piece minimize cost thing failing break smaller piece might possible handle global traffic one instance happens dy pair one fails capacity cut half service need broken sound like technology problem cultural problem easier pair database natural thing pair bad randomly killing really risky able automatically promote one restart new secondary kill everything even kill database make sure possible survive kind failure required changing decision database usethey chose riak instead mysql also mean using ringpop instead redis killing redis instance expensive operation usually pretty big expensive go away break smaller chunk talking cultural shift typically service talke service b load balancer load balancer dy going deal ever exercise path never know kill load balancer route around load balancer load balancing logic put service client required intelligence know route around problem philosophically similar finagle work make whole system scale handle back pressure service discovery routing system created cluster ringpop node total datacenter failure happen often could unexpected cascading failure upstream network provider could fail uber maintains backup datacenter switch place route everything backup datacenter problem data inprocess trip may backup datacenter rather replicate data use driver phone source trip data happens dispatch system periodically sends encrypted state digest driver phone let say datacenter failover next time driver phone sends location update dispatch system dispatch system detect know trip ask state digest dispatch system update state digest trip keep going like nothing happened downside downside uber approach solving scalability availability problem potentially high latency node process forwarding request sending message big fanouts fanout system tiny blip glitch surprisingly large impact higher fanout system better chance high latency request good solution backup request cross server cancellation bakedin tchannel first class feature request sent service b along information request also sent service b delay later request sent service b b completes request cancel request b delay mean common case b perform work b fail b process request return reply lower latency b tried first timeout occurred b tried see google latency tolerant system making predictable whole unpredictable part background related article
352,Lobsters,scaling,Scaling and architecture,The Little Story How Helm Was Born,https://rimusz.net/the-little-story-how-helm-was-born/,little story helm born,helm one year old helm kubesolo xhyve glide design document helm workflow happy birthday helm,helm one year old helm born october quite journey helm since let tell story helm project born personal experience involvement started deis offsite october internal hackathon started working deis week ago happy finally meet brain behind deis paas project let skip boring part jump straight hackathon hackathon hackathon told assemble small team guy siting next matt butcher jack francis whole day already looked said ok let make team started discus work working kubesolo prototype xhyve built top hypervisorframework showed guy jack working nodejs project matt glide got puzzled nice idea make became clear need package manager kubernetes easy way install set kubernetes manifest one go hmm also would nice share team member community idea started bump head next day fun whiteboarding idea making prototype matt wrote client kph interact jack written package management server worked first chart also put together design document look really similar today helm testing stuff mac kubesolo app prototype help allowed boot coreos vm kubernetes le minute virtualbox virtualisation software needed real fun working soon named helm package manager kubernetes also needed tool deis workflow time name deis easy install kubernetes helm looked right tool course mentioned project presentation day october day present project big surprise wow next next morning gabe monroy deis cto brings u coffee ah boulder many nice coffee shop tell u hackathon project approved leadership team become real deis project seen eye hehe next course pick right name project end helm tiller wheel steering ship boat also server side ditched back helm tiller homebrew git repository model picked store chart easy chart sharing etc small team formed matt butcher leading guy started work hard something show first kubecon today today helm part kubernetes project recently helm alpha released big community behind make proud involved even real programming project helm course used install deis workflow paas intended many project us let helm guide kubernetes ocean container story written untouched editor came love heart also must read blog post happy birthday helm written matt butcher
353,Lobsters,scaling,Scaling and architecture,How Kafkas Storage Internals Work,https://medium.com/the-hoard/how-kafkas-storage-internals-work-3a29b02e026,kafka storage internals work,kafka storage internals work kafka golang kafka storage unit partition retention policy governs kafka retains message partition split segment segment log message stored zero copy segment index map message offset position log kafka wrap compressed message together let review,kafka storage internals workin post going help understand kafka store datai found understanding useful tuning kafka performance context broker configuration actually inspired kafka simplicity used learned start implementing kafka golangso kafka storage internals work kafka storage unit partitiona partition ordered immutable sequence message appended partition split across multiple broker even multiple disksthe retention policy governs kafka retains messagesyou specify much data long data retained kafka purge message whether message consumedpartitions split segmentsso kafka need regularly find message disk need purged single long file partition message operation slow error prone fix problem see partition split segmentswhen kafka writes partition writes segment active segment segment size limit reached new segment opened becomes new active segmentsegments named base offset base offset segment offset greater offset previous segment le equal offset segmenton disk partition directory segment index file log file tree kafka head n log message storedeach message value offset timestamp key message size compression codec checksum version message formatthe data format disk exactly broker receives producer network sends consumer allows kafka efficiently transfer data zero copy binkafkarunclasssh kafkatoolsdumplogsegments deepiteration printdatalog file head n offset position isvalid true payloadsize magic compresscodec nocompressioncodec crc payload name travis msg hey offset position isvalid true payloadsize magic compresscodec nocompressioncodec crc payload name wale msg starving segment index map message offset position logthe segment index map offset message position segment logthe index file memory mapped offset look us binary search find nearest offset le equal target offsetthe index file made byte entry byte store offset relative base offset byte store position offset relative base offset byte needed store offset example let say base offset rather store subsequent offset wrap compressed message togetherproducers sending compressed message compress batch together send payload wrapped message data disk exactly broker receives producer network sends consumerslet reviewnow know kafka storage internals work partition kafka storage unitpartitions split segmentssegments two file log indexindexes map offset message position log used look messagesindexes store offset relative segment base offsetcompressed message batch wrapped together payload wrapper messagethe data stored disk broker receives producer network sends consumer
354,Lobsters,scaling,Scaling and architecture,Erlang and millions of concurrent gamers (2011),http://www.erlang-factory.com/upload/presentations/395/ErlangandFirst-PersonShooters.pdf,erlang million concurrent gamers,,obj endobj obj endobj obj font procset pdftextimagebimagecimagei mediabox content rgroup tabssstructparents endobj obj stream  wl x endstream endobj obj stream jfif ii
355,Lobsters,scaling,Scaling and architecture,Yandex Mail Success Story,https://www.youtube.com/watch?v=-SS4R1sFH3c,yandex mail success story,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature yandex mail success story youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature yandex mail success story youtube
356,Lobsters,scaling,Scaling and architecture,GOTO 2016  What I Wish I Had Known Before Scaling Uber to 1000 Services  Matt Ranney,https://www.youtube.com/watch?v=kb-m2fasdDY,goto wish known scaling uber service matt ranney,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature goto wish known scaling uber service matt ranney youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature goto wish known scaling uber service matt ranney youtube
357,Lobsters,scaling,Scaling and architecture,What is the best free Docker hosting? - Quora,https://www.quora.com/What-is-the-best-free-Docker-hosting,best free docker hosting quora,,please enable javascript refresh page continue
358,Lobsters,scaling,Scaling and architecture,Yandex.Mail success story [Oracle to PostgreSQL conversion],https://www.pgcon.org/2016/schedule/attachments/426_2016.05.19%20Yandex.Mail%20success%20story.pdf,yandexmail success story oracle postgresql conversion,,obj length r filter flatedecode stream c r q  bo h b jt r  e jdf j f endstream endobj obj endobj obj type page parent r resource r content r mediabox endobj obj procset pdf colorspace r r extgstate r endobj obj type extgstate opm endobj obj length r n alternate devicergb filter flatedecode stream vdf c n z v l r dgry j e e n c l q wt w endstream endobj obj endobj obj iccbased r endobj obj length r n alternate devicergb filter flatedecode stream vdf c n z v l r dgry j e e n c l q wt w endstream endobj obj endobj obj iccbased r endobj obj length r filter flatedecode stream x aqd endstream endobj obj endobj obj type page parent r resource r content r mediabox endobj obj procset pdf text imageb imagec imagei colorspace r font r xobject r endobj obj length r type xobject subtype image width height interpolate true colorspace r intent perceptual smask r bitspercomponent filter flatedecode stream l f lm ig  u p endstream endobj obj endobj obj length r type xobject subtype image width height colorspace devicegray interpolate true bitspercomponent filter flatedecode stream b k l v g xl r v w  endstream endobj obj endobj obj length r filter flatedecode stream w  endstream endobj obj endobj obj type page parent r resource r content r mediabox annots r endobj obj procset pdf text colorspace r font r r r endobj obj r endobj obj length r filter flatedecode stream z endstream endobj obj endobj obj type page parent r resource r content r mediabox endobj obj procset pdf text colorspace r font r r r endobj obj length r filter flatedecode stream p endstream endobj obj endobj obj type page parent r resource r content r mediabox endobj obj procset pdf text colorspace r r font r r r endobj obj length r n alternate devicegray filter flatedecode stream q ex bzh  g w c sp r  ul ci n x c b r endstream endobj obj endobj obj iccbased r endobj obj length r filter flatedecode stream n c b j endstream endobj obj endobj obj type page parent r resource r content r mediabox endobj obj procset pdf text imageb imagec imagei colorspace r font r xobject r endobj obj length r type xobject subtype image width height interpolate true colorspace r intent perceptual bitspercomponent filter flatedecode stream x   zx x q r z p h p b g zd c u k f p j
359,Lobsters,scaling,Scaling and architecture,Kubernetes The Hard Way,https://github.com/kelseyhightower/kubernetes-the-hard-way,kubernetes hard way,kubernetes hard way google kubernetes engine getting started guide copyright creative common attributionnoncommercialsharealike international license target audience cluster detail lab google cloud platform,kubernetes hard way tutorial walk setting kubernetes hard way guide people looking fully automated command bring kubernetes cluster check google kubernetes engine getting started guide kubernetes hard way optimized learning mean taking long route ensure understand task required bootstrap kubernetes cluster result tutorial viewed production ready may receive limited support community nt let stop learning copyright work licensed creative common attributionnoncommercialsharealike international license target audience target audience tutorial someone planning support production kubernetes cluster want understand everything fit together cluster detail kubernetes hard way guide bootstrapping highly available kubernetes cluster endtoend encryption component rbac authentication lab tutorial assumes access google cloud platform gcp used basic infrastructure requirement lesson learned tutorial applied platform
360,Lobsters,scaling,Scaling and architecture,Goods: organizing Googles datasets,https://blog.acolyer.org/2016/07/12/goods-organizing-googles-datasets/,good organizing google datasets,good organizing google datasets build data bazaar infrastructure google also put place tracking input signal stream datasets automated feature management system machine learning google built good hyperloglog algorithm,good organizing google datasets havely et al sigmod try build data cathedral build data bazaar data cathedral referring centralised enterprise data management solution everyone company buy pay homage making pilgrimage edm every time want publish retrieve dataset data bazaar hand abandon premeditated centralised control alternative approach enable complete freedom within enterprise access generate datasets solve problem finding right data posthoc paper describe google dataset search good posthoc system built order organize datasets generated used within google everyone within company carry creating consuming datasets using whatever mean prefer good work background figure datasets exist gather metadata word surprise google built crawling turn far easy problem start current catalog index billion datasets whose access permission make readable google engineer approach problem understanding data exists provenance find good hugely appealing v reality everlosing battle strict central control within enterprise let first examine benefit building data bazaar dive detail go build data bazaar big picture good system good crawl datasets google extract much metadata possible join metadata inferred source eg log source code make catalog available google engineer good quickly became indispensable nice segue material looking last week example given team working nlu good us catalog provide google engineer service dataset management illustrate type service powered good imagine team responsible developing natural language understanding nlu text corpus say news article engineer team may distributed across globe maintain several pipeline add annotation different text corpus pipeline multiple stage add annotation based various technique including phrase chunking partofspeech tagging coreference resolution team consume datasets nlu team generates nlu team pipeline may consume datasets team based information catalog good provides dashboard nlu team case dataset producer display datasets enables browsing facet eg owner data center schema even team datasets diverse storage system engineer get unified view datasets dependency among good monitor feature dataset size distribution value content availability alert owner feature change unexpectedly good also track dataset provenance figuring datasets used creation given dataset datasets consume downstream provenance visualisation useful figuring upstream change may responsible problem working potential consequence change considered reminds much infrastructure google also put place tracking input signal stream datasets automated feature management system machine learning full set metadata good track illustrated dataset consumer good provides search mechanism finding important andor relevant datasets every dataset profile page help user understand schema user provenance find datasets contain similar content good system allows user provide data annotation also indexed mechanism facilitated application built top good profile page dataset crosslinks metadata specialized tool example profile page link provenance metadata job generated dataset page detail job jobcentric tool similarly link schema metadata codemanagement tool provide definition schema correspondingly tool link back good help user get information datasets profile page also provides access snippet different language eg c java sql access content dataset customtailor generated snippet specific dataset example snippet use path schema dataset known user copypaste snippet respective programming environment good profile page dataset become natural handle bookmarking sharing dataset information google file system browser provides direct link good page datasets within directory example google built good challenge creating central data repository posthoc manner without relying collaboration engineer need piece together overall puzzle multiple source information even may never certain google scale sheer number datasets question mean need little bit savvy often crawl process datasets even spend one second per dataset many datasets large process one second per dataset going catolog billion datasets using thousand parallel machine still requires around moreover datasets created deleted time datasets catalog deleted every day two tactic used help deal volume datasets twintrack approach crawling processing dataset clustering twintrack approach designates certain datasets important high provenance centrality user taken effort provide additional metadata annotation one instance schema analyzer heavyweight job pipeline run daily important datasets get quickly second instance process datasets may get fraction within given day practice web crawling ensuring good coverage freshness head importance distribution enough user scenario clustering datasets help make cognitive overload user lighter well reducing processing cost consider dataset produced every day saved eg abstracting date portion possible get generic representation daily scan file shown single toplevel entity good also save processing time assumption example file series share schema composing hierarchy along different dimension construct granularity semilattice structure node corresponds different granularity viewing table list abstract dimension currently use good contains entry topmost element semilattice avoids many cluster help keep set cluster stable time cluster large good us variety technique try infer metadata dataset good explicitly identifies analyzes datasets posthoc noninvasive manner often impossible determine type metadata complete certainty instance many datasets consist record whose schema conforms specific protocol good try uncover implicit association several signars instance match dataset content agains registered type protocol buffer within google consult usage log may recorded actual protocol buffer provenance metadata especial interest help understand data flow company across boundary internal team organisation provenance mined production log contain information job read write dataset make tractable log sampled downstream upstream relationship computed hop opposed full transitive closure find schema associated dataset good need find protocol buffer used read write record since nearly always checked google source code repository good also crawl code repository discover protocol buffer possible produce short list protocol buffer could perform matching scanning record file going protocol message definition determine whether could conceivable generated byte see matching procedure speculative produce multiple candidate protocol buffer candidate protocol buffer along heuristic score candidate become part metadata facilitate searching datasets good also collect metadata summarizing content dataset record frequent token find sampling content analyze field determine contain key data individually combination find potential key use hyperloglog algorithm estimate cardinality value individual field combination field compare cardinality number record find potential key also collect fingerprint checksum individual field localitysensitive hash lsh value content use fingerprint find datasets content similar identical given dataset column datasets similar identical column currentdataset also use checksum identify field populated record dataset many implementation detail paper wanted focus writeup big idea find compelling one significant challenge still open according author improving criterion ranking datasets identifying important datasets know user feedback must improve ranking need able distinguish production test development datasets datasets provide input many datasets datasets user care finally hope system good provide impetus instilling data culture datadriven company today general google particular develop system enable enterprise treat datasets core asset dashboard monitoring hopefully become natural much data discipline code discipline
361,Lobsters,scaling,Scaling and architecture,Sun goes Hollywood (1995),http://sunsite.uakom.sk/sunworldonline/swol-11-1995/swol-11-pixar.html,sun go hollywood,sidebar plot synopsis pixar renderman typestry sidebar sun cpu farm resource webmaster sunworldcom url last modified toy story story pixar renderman typestry editor note,click sponsor help support sunworld fall ambitious collaboration hollywood silicon valley date hit screen world first featurelength computeranimated film toy story marketed financed disney toy story see sidebar plot synopsis written produced directed pixar animation studio point richmond calif formerly unit lucasfilm percent owned apple cofounder steve job also film executive producer pixar best known many graphic community renderman toolkit used create dinosaur jurassic park liquid metal cyborg terminator typestry software package see pixar renderman typestry sidebar company also numerous academy award clio award short film commercial toy story company began odyssey territory owned hollywood two way look film one culmination year research brought u point said pixar founder chief technical officer ed catmull whole new way making feature animated film technology start long period improved productivity quality toy story also something coup sun microsystems movie final image rendering accomplished farm dualprocessor quadprocessor sparcstation representing computing power cray computer power applied film film history catmull said even rendering film frame required equivalent day continuous processing put another way rendering frame took one three hour sparc processor time could use time power know exactly use limited budget appetite catmull said like someone spreadsheet want two three time power faster turnaround nt going anything fundamentally different advertisement sun silicon graphic dominant mind share among animator film video producer important graphic software tool yet ported platform toy story sgis used handle modeling animation shading lighting sparcstation offered clear priceperformance win using data series typical picture pixar developed benchmark called rendermark measure rendering performance different hardware running pixar renderman software evaluated different platform list price per rendermark sun le expensive anybody else almost factor two catmull said sun smart pricing multiprocessor technology company make pay premium saving sharing power supply common element catmull said sun fourprocessor version much cheaper per processor sparcstations also size advantage sparcstation four processor lot cheaper small package got processing per cubic foot odd way measure turn important catmull said sense pixar come full circle back lucasfilm day pixar wrote first operating system sun used first sun delivered ran version unix wrote first year sun existence group largest customer catmull recall cpu farm farm essentially wall sparcstations configured headless server megabyte ram processor average megabyte ram three five gigabyte local disk storage local disk hold copy solaris proprietary job control software pixar renderman huge swap space space download model said david ching pixar manager computer operation job fed sparcstation sparcserver contains basic information model texture shaders needed complete rendering frame pixar propriety job control software feed frame one time workstation ensures even distribution work sparcstations connected server though grand junction hub connect sparcstations via sparcserver via megabyte link sparcserver connected rest company network comprised mostly sgis via via fddi sgi challenge indigo act master server master server store model different object retrieved needed sparcserver essentially sparcstations need refer model get get model master server ching said pixar call collection modelling data digital back lot shifting around terabyte data generated course production enough stress network improve performance pixar initially considered placing sparcstation ethernet segment exotic high speed technology economic consideration ruled choice ching said company looking go connection throughout hippi fibrechannel expensive really nt want look pay highspeed interface ching said course gob money would done fiber back january nt economical pixar two large disk farm sgi challenge gigabyte sun array gigabyte together nt enough hold entire movie image finalled received final approval director recorded onto film backed exabyte tape drive rendered data essentially generated sun working data backed back important data ca nt replaced rest always rerendered ching said one power outage cost u data truth data replaced within day future pixar planning number improvement looking automated cartridge backup system handle gig data quickly ching said also like know atm like switching hub maybe video server atm switching hub used improve network performance video server speed movie production directly letting animator work interactively efficiently rather wait view work special light room video server allow view clip right desk plot line subject matter aside toy story mark departure brand new world movie making computer industry eager new business studio desire cut cost annoying percentage gross going top actor steve job arrival hollywood scene graphic geek rest assured world domination matter time click sponsor help support sunworld resource technical problem magazine contact webmaster sunworldcom url http last modified toy story story positioned disney christmas megamovie toy story comedyadventure toy take life one watching toy story feature voice tom hank woody pullstring talking cowboy doll tim allen buzz lightyear superhero space action figure two learn put aside rivalry work together become separated owner boy named andy character reincarnated interactive cd produced pixar numerous licensing deal toy joint marketing campaign burger king arranged disney well known toy mr potato head voice rickles slinky dog voice jim varney also make appearance four year making toy story directed john lasseter also directed pixar tin toy seminal computer animation short made living toy tin toy first computer animated film win academy award toy story much descendent film many doubling computer power later pixar renderman typestry editor note october editor received following message joy gipson joy pixarcom marketing communication manager pixar longer sell support pixar typestry referenced web site addition movie pixar also sell two software package renderman typestry renderman run sun silicon graphic macintosh computer produce photorealistic image tiff pict eps tga format mathematical model shape shading texture lighting like digital camera control focus exposure viewing angle level detail renderman computes final image partly market since late partly run many platform raft thirdparty rendermancompatible program available fact real production thirdparty product necessary renderman nt modeling animation among renderman credit dinosaur jurassic park cyborg terminator water creature abyss image tin toy free willy cliffhanger casper pixar typestry let user create threedimensional type manipulate variety way wrapping text around sphere tube onto rubber sheet also support animation change appearance type surface even handle particle emission type surface
363,Lobsters,scaling,Scaling and architecture,Context aware MySQL pools via HAProxy,http://githubengineering.com/context-aware-mysql-pools-via-haproxy/,context aware mysql pool via haproxy,click redirected,click redirected
364,Lobsters,scaling,Scaling and architecture,Pull doesn't scale - or does it?,https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/,pull nt scale,prometheus chooses pull push prometheus nagios nt matter initiate connection prometheus eventbased system monitoring need know service instance accidentally ddosing monitoring realworld proof monitor million machine digitalocean problem pull via pushgateway restructuring setup good comment powered disqus,let talk particularly persistent myth whenever discussion monitoring system prometheus pullbased metric collection approach come someone inevitably chime pullbased approach fundamentally nt scale given reason often vague apply system fundamentally different prometheus fact worked pullbased monitoring largest scale claim run counter operational experience already faq entry prometheus chooses pull push focus specifically scaling aspect let closer look usual misconception around claim analyze whether would apply prometheus prometheus nagios people think monitoring system actively pull often think nagios nagios reputation scaling well part due spawning subprocesses active check run arbitrary action nagios host order determine health certain host service sort check architecture indeed scale well central nagios host quickly get overwhelmed result people usually configure check executed every couple minute run serious problem however prometheus take fundamentally different approach altogether instead executing check script collect time series data set instrumented target network target prometheus server simply fetch current state metric target http highly parallel way using goroutines execution overhead would pullrelated brings u next point nt matter initiate connection scaling purpose nt matter initiate tcp connection metric transferred either way effort establishing connection small compared metric payload required work pushbased approach could use udp avoid connection establishment altogether say true tcphttp overhead prometheus still negligible compared work prometheus server ingest data especially persisting time series data disk put number behind single big prometheus server easily store million time series record incoming sample per second measured real production metric data soundcloud given scrape interval time series per host allows monitor machine single prometheus server scaling bottleneck never related pulling metric usually speed prometheus server ingest data memory sustainably persist expire data diskssd also although network pretty reliable day using tcpbased pull approach make sure metric data arrives reliably monitoring system least know immediately metric transfer fails due broken network prometheus eventbased system monitoring system eventbased report individual event http request exception name central monitoring system immediately happens central system either aggregate event metric statsd prime example store event individually later processing elk stack example system pulling would problematic indeed instrumented service would buffer event pull pull would happen incredibly frequently order simulate liveness pushbased approach overwhelm event buffer however prometheus eventbased monitoring system send raw event prometheus store prometheus business collecting aggregated time series data mean interested regularly collecting current state given set metric underlying event led generation metric example instrumented service would send message http request prometheus handled would simply count request memory happen hundred thousand time per second without causing monitoring traffic prometheus simply asks service instance every second whatever configure current counter value store value together scrape timestamp sample metric type gauge histogram summary handled similarly resulting monitoring traffic low pullbased approach also create problem case monitoring need know service instance pullbased approach monitoring system need know service instance exist connect people worried extra configuration requires part monitoring system see operational scalability problem would argue escape configuration effort serious monitoring setup case monitoring system nt know world look like monitored service instance would able tell instance never report due outage really longer meant exist acceptable never care health individual instance like run ephemeral worker sufficient largeenough number report result environment exclusively like monitoring system need know desired state world anyway pushbased approach actually requires configuration total monitoring system need know service instance exist service instance also need know reach monitoring system pull approach requires le configuration also make monitoring setup flexible pull run copy production monitoring laptop experiment also allows fetch metric tool inspect metric endpoint manually get high availability pull allows run two identically configured prometheus server parallel lastly move endpoint monitoring reachable pull approach require reconfigure metric source practical front prometheus make easy configure desired state world builtin support wide variety service discovery mechanism cloud provider containerscheduling system consul marathon kubernetes dnsbased sd azure zookeeper serversets prometheus also allows plug custom mechanism needed microservice world multitiered architecture also fundamentally advantage monitoring system us method discover target monitor service instance use discover backends way sure monitoring target serving production traffic one discovery mechanism maintain accidentally ddosing monitoring whether pull push timeseries database fall send sample handle however experience slightly likely pushbased approach accidentally bring monitoring control metric get ingested instance centralized monitoring system run danger experimental rogue job suddenly pushing lot garbage data production monitoring bringing still plenty way happen pullbased approach control pull metric size nature metric payload risk lower importantly incident mitigated central point realworld proof besides fact prometheus already used monitor large setup real world like using monitor million machine digitalocean prominent example pullbased monitoring used successfully largest possible environment prometheus inspired google borgmon partially still used within google monitor critical production service using pullbased approach scaling issue encountered borgmon google due pull approach either pullbased approach scale global environment many ten datacenters million machine hardly say pull nt scale problem pull indeed setup hard monitor pullbased approach prominent example many endpoint scattered around world directly reachable due firewall complicated networking setup infeasible run prometheus server directly network segment quite environment prometheus built although workarounds often possible via pushgateway restructuring setup case remaining concern pullbased monitoring usually scalingrelated due network operation difficulty around opening tcp connection good article address common scalability concern around pullbased monitoring approach prometheus pullbased system used successfully large environment pull aspect posing bottleneck reality result clear pull nt scale argument real concern hope future debate focus aspect matter red herring please enable javascript view comment powered disqus
365,Lobsters,scaling,Scaling and architecture,ZFS High-Availability NAS,https://github.com/ewwhite/zfs-ha/wiki,zfs highavailability na,zfs highavailability na zfs filesystem zfs thumperthor zfs linux stackexchangeserverfault lvm adherence best practice option high availability costly tightly associated commercial storage solution may work well nexentastor quantastor zetavault particularly unattractive licensing scheme interesting sharednothing architecture nimble tegile objective hp storageworks dualported sa drive critical success design pcie slot connectivity ram speed number cpu cost nehalem westmere example budget part manifest example highend part manifest note physical setup cluster configuration assumption requirement zfs linux repository corosyncpacemaker setup rhel high availability addon http githubcomzfsonlinuxzfswikirhelandcentos kabitrackingkmod zfs zpool pacemaker ocf agent lacp bonding interface script file determine drive layout wwn note consistently outperformed recommended zfs filesystem setting file space reclamation bug compression cluster initialization nfs export video zfs ha cluster failover vmware tuning multiple queue io scheduling operation monitoring zfswatcher netdata mmonit monit new relic cli operation web gui http,zfs highavailability na zfs filesystem gamechanger way approach local application data storage shared storage solution replication general data backup longtime proponent zfs storage variety scenario going back first experience opensolaris buying zfs thumperthor adopting zfs linux production use continued contribution stackexchangeserverfault community zfs advantage intelligent storage application server serious replacement lvm great shared storage option back virtualization environment useful largescale backup target atomic snapshot flexible replication transparent filesystem compression zfs downside good zfs implementation sometimes require specialized knowledge adherence best practice easy make irreversible mistake often due poor design inappropriate hardware choice lot contradictory zfs information online due filesystem appeal home lab user option high availability costly tightly associated commercial storage solution may work well expanding final point traditional method achieving high availability zfs storage array mean paying licensing commercial product suite common option marketplace nexentastor quantastor zetavault product proven work existing user base particularly unattractive licensing scheme notably nexentastor quantastor capacitybased licensing model nt work well use case highavailability licensing addons solution expensive yet still place onus hardware build validation deployment ongoing support customer zetavault made positive stride providing interesting sharednothing architecture may work situation consultant work small business simple vmware estate host need robust storage back solution typically le terabyte usable storage requirement cost lower tier san storage way line technology budget licensed highlyavailable build raw commercial zfs array built atop commodity hardware approach without ha solution may le ha option close price fullyintegrated storage array like nimble tegile value proposition commercial zfs solution low placed context objective build highlyavailable dualcontroller storage array using opensource technology solution capable presenting shared storage nfs client zfs equivalent dualcontroller array like hp storageworks key highavailability storage design shared sasattached storage enclosure jbod example include external sa enclosure feature redundant power supply fan sa expander logic backplane redundant sa controller io module endtoend multipathing accommodate dualported sa disk dualported sa drive critical success design scsi enclosure service s sensor communication enclosure component status complement shared jbod enclosure need two server head node controller provide client connectivity computeram resource zfs array make sense scale specification head node meet anticipated workload installation variable may include pcie slot connectivity nics sa host bus adapter future expansion ram zfs leverage system ram read write caching maximizing ram amount within reason help accelerate io workload speed number cpu impact data compression performance cost recommendation use intel nehalem westmere newer cpu four core example budget part manifest lowcost build manifest simple usable storage array hp storageworks fully disassembled front view hp proliant head node jbod rear view server jbod sa cabling example highend part manifest build manifest usable storage array using new component x hp proliant rackmount server total x hp sa hba total x sa external cable total x hp storageworks sa enclosure total x hp dualport sa hard drive total x readoptimized ssd total x lowlatency ssd zilslog note zilslog device mirrored depending requirement tend use stec zeusram ssd purpose failure wearout uncommon due nature device architecture using non drambased ssd may make sense mirror slog front view hp proliant head node jbod rear view server jbod sa cabling physical setup purpose shared jbod setup multipath configuration provides cabling controller host adapter path redundancy basic recommended setup two head node single jbod enclosure single sa hba host four sa cable following arrangement hba port jbod port hba port jbod port hba port jbod port hba port jbod port example scale multiple enclosure sa cabling ring cluster configuration assumption requirement root access two server system rhel centos installed local disk zfs filesystem configured via zfs linux repository familiarity basic zfs operation zpoolzfs filesystem creation modification understanding network bonding rhel linux step describe construction two host single jbod cluster manages single zfs pool comprised data drive poolhot spare separate slog zil drive corosyncpacemaker setup additional information rhel high availability addon simplification disable firewalld build process definitely modified later systemctl stop firewalld systemctl disable firewalld install zfs filesystem downloading zfs kerneldevel package yum localinstall http consider making yum repo modification use kabitracking module versus dkm http githubcomzfsonlinuxzfswikirhelandcentos kabitrackingkmod yum install kerneldevel zfs install rhel cluster suite multipath software yum install pc fenceagentsall devicemappermultipath start pacemaker corosync service systemctl enable pcsd systemctl enable corosync systemctl enable pacemaker systemctl start pcsd multipath daemon start without configuration file present mpathconf enable systemctl start multipathd systemctl enable multipathd download zfs zpool pacemaker ocf agent allows zfs pool exported imported cluster node failover cd usrlibocfresourcedheartbeat wget http githubcomskiselkovstmfharawmasterheartbeatzfs chmod x zfs networking depends client consume data na build use lacp bonding storage array switch maintain ip data network system management basic requirement ip host plus virtual ip vip float active node nfs client use virtual ip prefer segregate traffic vlan create single master lacp bond interface separate vlan interface eg example etcsysconfignetworkscripts interface script file networkmanager nt necessary since bond interface likely require handediting systemctl stop networkmanager systemctl disable networkmanager populate etchosts file cluster member hostnames add second heartbeat address ring corosync management address node cluster ring address heartbeat ensure address reached either host determine drive layout due use dmmultipath want build zfs pool using device mapper disk identifier rather normal dev entry way determine drive layout identify disk depend hbas jbod enclosure use make sense record sa wwn disk using case possible enumerate drive identifying wwns programmatically examining sysfs entry example hp sa hba cd sysclassenclosure root sysclassenclosure l root total drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul drwxrxrx root root jul r r r root root jul component lrwxrwxrwx root root jul device drwxrxrx root root jul power lrwxrwxrwx root root jul subsystem classenclosure rwr r root root jul uevent represent drive slot hp storageworks using build root total rwr r root root jul active lrwxrwxrwx root root jul device rwr r root root jul fault rwr r root root jul locate drwxrxrx root root jul power rwr r root root jul status r r r root root jul type rwr r root root jul uevent note device directory locate option running echo locate illuminate drive beacon device present slot root cd device root cat sasaddress output multipath show path selection policy scsi device name devmapper device name eg drive enclosure root multipath hp feature hwhandler wprw policyroundrobin statusactive sde active ready running policyroundrobin statusenabled sdq active ready running hp feature hwhandler wprw policyroundrobin statusactive sdd active ready running policyroundrobin statusenabled sdp active ready running hp feature hwhandler wprw policyroundrobin statusactive sdt active ready running policyroundrobin statusenabled sdh active ready running lsscsi command also help display topology note drive appear twice sa layout root lsscsi disk hp devsdb disk hp devsdc disk hp devsdd disk hp devsde disk hp devsdf disk hp devsdg disk hp devsdh disk hp devsdi disk hp devsdj disk hp devsdk disk hitachi devsdl disk stec zeusram devsdm enclosu hp sa disk hp devsdn disk hp devsdo disk hp devsdp disk hp devsdq disk hp devsdr disk hp devsds disk hp devsdt disk hp devsdu disk hp devsdv disk hp devsdw disk hitachi devsdx disk stec zeusram devsdy enclosu hp sa disk hp logical volume devsda storage hp idea sa address layout drive create zpool create zpool using devmapper device critical cluster pool creation option cachefilenone allows ha agent manage pool versus o set using disk set nvme certain ssd drive zpool create autoexpandon autoreplaceon cachefilenone spare result root zpool status v pool state online scan scrub repaired error sun aug config name state read write cksum online online online online online online online online online online online online online log online cache online spare avail note show x pool specific design layout depend application redundancy performance requirement zfs mirror always option particular setup though x rpm sa disk x setup consistently outperformed group disk arranged mirror recommended zfs filesystem setting zfs filesystem setting make sense set pool level working rhelcentos disable access time zfs set atimeoff change acltype enable filesystem acls zfs set acltypeposixacl enable xattrsa workaround file space reclamation bug selinux prevent certain performance issue edge case zfs set xattrsa enable compression almost zfs filesystems default set pool level descendant automaticallycreated filesystems inherit property impact low essentially trading cpu storage efficiency zfs set cluster initialization corosync pacemaker package install service account named hacluster assign password user used later cluster authorization management gui set password cluster node passwd hacluster enable start pacemaker corosync system service systemctl enable pcsd systemctl enable corosync systemctl enable pacemaker systemctl start pcsd authorize cluster node use name zfscluster choose something appropriate pc cluster auth pc cluster setup start name zfscluster set quorum policy ignore since twonode cluster pc property set noquorumpolicyignore create stonith device using scsi reservation zfs pool disk list device associated specific pool pc stonith create fencescsi pcmkmonitoraction metadata pcmkhostlist device meta providesunfencing create zfs pool resource correspond zpool configuration pool name pc resource create zfs pool importargs devmapper op start timeout op stop timeout define virtual ip float node associated zfs pool benefit give option run dualactive clustering mutual failover pinning zfs pool head node pc resource create group set default stickiness value prevent flapping node failover event pc resource default result pc status command look like root pc status last change tue jul root via crmattribute stack corosync current dc version partition quorum node resource configured online full list resource stonith fencescsi started resource group ocf heartbeat zfs started ocf heartbeat started pcsd status online online daemon status corosync activeenabled pacemaker activeenabled pcsd activeenabled nfs export elegance design come eschewing traditional nfs cluster resource instead relying zfs sharenfs property export old way would mean separate resource zfs zpool zpool virtual ip nfs server daemon exportfs resource nfs server export nfs notify resource inform client failover pacemaker cluster constraint maintain ordering resource design use fencingstonith resource resource zpool virtual ip nfs export define sharenfs property filesystem wish export zfs set sharenfsrw sync norootsquash nowdelay condenses nfs daemon export nfs notify step zpool exportimport process speed failover time demonstration following action array centos vmware virtual machine installed average failover time second vmware virtual machine unaffected video zfs ha cluster failover vmware tuning progress tuned profile advanced sector disk zfs filesystem parameter etcmodprobedzfsconf option multiple queue io scheduling operation progress monitoring zfswatcher zfs filesystem pool statushealth netdata realtime system analytics mmonit monit thresholding daemon management alert new relic external monitoring thresholding cli operation get cluster status pc status place cluster node standby manual failover pc cluster standby node name remove cluster node standby state pc cluster unstandby node name clean cluster resource pc resource cleanup web gui red hat high availability add also present web gui cluster management enabled default accessible via http port either cluster node http log using hacluster account corresponding password add node hostname cluster monitoring management function available possible initiate planned failover maintenance using interface
366,Lobsters,scaling,Scaling and architecture,Me Too! (2004),https://blogs.technet.microsoft.com/exchange/2004/04/08/me-too/,,,imagine could book office work anywhere world imagine could done outlook well
367,Lobsters,scaling,Scaling and architecture,"A Little Story about Amazon ECS, systemd, and Chaos Monkey",https://medium.com/production-ready/a-little-story-about-amazon-ecs-systemd-and-chaos-monkey-8bc7d1b5778,little story amazon ec systemd chaos monkey,building running web system covering topic sre chaos engineering system thinking,building running web system covering topic sre chaos engineering system thinking
368,Lobsters,scaling,Scaling and architecture,How to Setup a Highly Available Multi-AZ Cassandra Cluster on AWS EC2,http://highscalability.com/blog/2016/8/1/how-to-setup-a-highly-available-multi-az-cassandra-cluster-o.html,setup highly available multiaz cassandra cluster aws,happens full availability zone go learn setup cassandra survive full availability zone outage cluster running az node per az run cassandra node different az replica factor second setup affect capacity two test scenario using enhanced networking enabled saw little difference single az multi az deployment dynamic snitching read query cassandra hit node different availability zone request handled within local availability zone using enhanced networking result consistently lower interinstance latency chosen use replication factor different availability zone placing node across multiple availability zone make cassandra cluster available resilient availability zone outage additional storage needed run multiple az cost increase minimal replication factor combined using availability zone good starting point use case aws done great job keeping latency availability zone low,guest post alessandro pieri software architect stream try minute interactive tutorial learn stream api originally built facebook apache cassandra free opensource distributed database designed handle large amount data across large number server stream use cassandra primary data store feed cassandra stand able already using cassandra cluster likely configured handle loss node however happens full availability zone go article learn setup cassandra survive full availability zone outage afterwards analyze moving single multi availability zone cluster impact availability cost performance recap availability zone aws operates geographically isolated location called region region composed small amount usually physically independent availability zone availability zone connected low latency network region completely independent shown diagram order achieve high availability aws resource hosted multiple availability zone hosting multiple availability zone allows ensure one go app stay running recap cassandra high availability one primary benefit cassandra automatically shard data across multiple node even manages scale almost linearly doubling number node give nearly double capacity cassandra setting called replication factor defines many copy data exist replication factor set node go lose data stored place replication factor insure data always stored different node ensuring data safe single node break configuring cassandra multi az availability covered basic let explain setup cassandra multiaz availability new cassandra want learn setup cluster article good starting point part snitch first step make sure cassandra know region availability zone handled snitch keep track information related network topology cassandra provides several builtin snitch work well aws meant single region deployment meant cluster span multiple region cassandra understands concept data center rack snitch treat region data center availability zone rack change snitch setting cassandrayaml beware changing snitch setting potentially destructive operation planned care read cassandra documentation changing snitch setting change snitch data inserted cluster must run full repair since snitch affect replica placed box cassandra provides appropriate deployment single region load region availability zone information api region treated datacenter availability zone rack private ip used work across multiple region us public ip broadcastaddress allow crossregion connectivity thus set seed address public ip well need open storageport sslstorageport public ip firewall intraregion traffic cassandra switch private ip establishing connection use custom snitch setting full class name snitch assumed classpath endpointsnitch snippet cassandrayaml part replication factor replication factor determines number replica exist cluster replication strategy also known replica placement strategy determines replica distributed across cluster setting keyspace property default cassandra us simplestrategy replication strategy strategy place replica cluster ignoring region availability zone networktopologystrategy rack aware designed support multidatacenter deployment create keyspace mykeyspace replication class networktopologystrategy useast code snippet declared keyspace called mykeyspace networkreplicationstrategy place replica useast datacenter replication factor change existing keyspace use example beware changing replication strategy running cassandra cluster sensitive operation read full documentation alter keyspace mykeyspace replication class networktopologystrategy useast part consistency level read write cassandra ability specify consistency level clientside word specify many node cassandra cluster required agree read write request valid ask higher consistency level cassandra able answer node local availability zone query zone stay availability zone outage need use consistency level remaining node able satisfy next section discus failure scenario consistency level detail handling az outage cassandra cluster behaves availability zone go depends several factor let look diagram show couple scenario figure consistency level affect availability first scenario shown left show cluster running az node per az az go half cluster offline az guarantee entire dataset still present least node see table next cluster diagram outcome query depends requested consistency level example query clone succeed still least node available hand query higher cl requirement quorum always fail require response node second scenario run cassandra node different az replica factor deployment cluster clearly resilient event az failure cassandra still able satisfy query clquorum worth noting event availability zone outage capacity left service cluster different first cluster setup lose capacity second setup affect capacity much latency introduced multiazs setup estimating query latency introduced multiaz setup easy due nature cassandra number factor fluctuate cloud environment eg network latency disk io host utilization etc test used cassandrastress tool generate read write load cluster running single multiple az order keep variance low possible lower deviation disk io used instance ephemeral storage instead network attached storage eb came two test scenario first used cluster instance aws network performance moderate running without enhanced networking median percentile write single az multi az az read single az multi az az table scenario performance test single az v multi az time millisecond setup cassandra second scenario used cluster aws network performance high enhanced networking turned median percentile write single az multi az az read single az multi az az table scenario performance test single az v multi az time millisecond setup cassandra interesting enough networking performance varies two instance type using enhanced networking enabled saw little difference single az multi az deployment therefor recommend enabling enhanced networking selecting instance type high network performance another interesting fact cassandra read certain extent rackaware coordinating query cassandra node route request peer lowest latency feature called dynamic snitching part cassandra since version thanks dynamic snitching read query cassandra hit node different availability zone give cassandra sort rackawareness could reproduce behavior read test shown following chart figure number local readrequests per node multiaz setup replica az preferred set node cluster spanning across az read performed consistency levelone figure show read request distributed across cluster see request handled within local availability zone enhanced networking aws offer enhanced networking recent instance family using enhanced networking result consistently lower interinstance latency information topic please follow link guideline deciding number availability zone use cassandra configured way every availability zone least entire copy dataset cassandra refers scenario making az selfcontained achieve need place node across number az le equal replication factor also recommended number node running every az general beneficial availability zone replication factor stream chosen use replication factor different availability zone ensures every availability zone copy data enough capacity left handle read write request unlikely event az outage conclusion cassandra amazing database stream rely heavily keep feed running ten million end user short cassandra ability post explained configure cassandra highly available multiaz setup aws cost performance almost identical single availability zone deployment key takeaway placing node across multiple availability zone make cassandra cluster available resilient availability zone outage additional storage needed run multiple az cost increase minimal traffic az free use case major concern replication factor combined using availability zone good starting point use case enables cassandra cluster selfcontained aws done great job keeping latency availability zone low especially use instance network performance set high enhanced networking enabled
369,Lobsters,scaling,Scaling and architecture,Managing Elasticsearch time-based indices efficiently,http://www.elastic.co/blog/managing-time-based-indices-efficiently,managing elasticsearch timebased index efficiently,index template many forcemerged index template rollover api create index api nt support constraint cluster health api cat recovery api forcemerging field stats api rollover shrink forcemerge fieldstats curator index management tool xpack,anybody us elasticsearch indexing timebased data log event accustomed indexperday pattern use index name derived timestamp logging event rounded nearest day new index pop existence soon required definition new index controlled ahead time using index template easy pattern understand implement gloss complexity index management following achieve high ingest rate want spread shard active index many node possible optimal search low resource usage want shard possible shard big become unwieldy index per day make easy expire old data many shard need one day every day end many shard one day enough next blog post going introduce new rollover pattern apis support simpler efficient way managing timebased index rollover pattern work follows one alias used indexing point active index another alias point active inactive index used searching active index many shard hot node take advantage indexing resource expensive hardware active index full old rolled new index created indexing alias switch atomically old index new old index moved cold node shrunk one shard also forcemerged compressed let assume cluster hot node pool cold node ideally active index one receiving writes one shard hot node order split indexing load many machine possible want one replica primary shard ensure tolerate loss node without losing data mean active index primary shard giving u total shard one per hot node could also use primary shard total including replica two shard node first create index template active index put templateactivelogs template activelogs setting numberofshards numberofreplicas routingallocationincludeboxtype hot routingallocationtotalshardspernode alias searchlogs index created template allocated node tagged boxtype hot totalshardspernode setting help ensure shard spread across many hot node possible set instead still allocate shard node fails use activelogs alias index current active index searchlogs alias search across log index template used inactive index put templateinactivelogs template inactivelogs setting numberofshards numberofreplicas routingallocationincludeboxtype cold codec bestcompression archived index allocated cold node use deflate compression save space explain set replica later create first active index put put pattern name recognised counter rollover api use activelogs alias index current active index searchlogs alias search across log indiceswhen created index also created activelogs alias point index using alias document sent current active index post activelogslogbulk create text log message timestamp create text log message timestamp create text log message timestamp create text log message timestamp create text log message timestamp stage active index going become big old want replace new empty index rollover api allows specify big old index allowed rolled big big always depends depends hardware type search perform performance expect see long willing wait shard recover etc etc practice try different shard size see work start choose arbitrary number like million billion adjust number depending search performance data retention period available space hard limit number document single shard contain plan shrink active index single shard must fewer billion document active index document fit single shard shrink index one shard long target number shard factor original eg rolling index age may convenient allows parcel log hour day week etc usually efficient base rollover decision number document index one benefit sizebased rollover shard approximately weight make easier balance rollover api would called regularly cron job check whether maxdocs maxage constraint breached soon least one constraint breached index rolled since indexed document example specify maxdocs value completeness maxage one week post activelogsrollover condition maxage maxdocs request tell elasticsearch rollover index pointed activelogs alias index either created least seven day ago contains least document response look like oldindex newindex rolledover true dryrun false condition maxdocs true maxage false index rolled index maxdocs constraint met mean new index called created based activelogs template activelogs alias switched way want override value index template setting mapping pas rollover request body like would create index api nt support maxsize constraint given intention produce evenly sized shard nt support maxsize constraint addition maxdocs answer shard size le reliable measure ongoing merges produce significant temporary growth shard size disappears soon merge completed five primary shard process merging one shard would temporarily increase index size doc count hand grows predictably longer used writes move cold node shrink single shard new index called shrinking make index readonly move one copy shard single node choose whichever node like probably whichever cold node available space achieved following request put indexblockswrite true indexroutingallocationrequirename somenodename allocation setting ensures least one copy shard moved node name somenodename move shard replica shard allocated node primary ensure least one primary replica shard move index finished relocating use cluster health api check issue following request shrink index post kick shrink process long filesystem support hard link shrink almost instantaneous filesystem support hard link well wait segment file copied one index monitor shrink process cat recovery api cluster health api get waitforstatusyellow soon done remove searchlogs alias old index add new post alias action remove index alias searchlogs add index alias searchlogs index reduced single shard still contains number segment file bestcompression setting kicked yet made writes improve situation forcemerging singleshard index single segment follows post request create single new segment replace multiple segment existed also elasticsearch write new segment bestcompression setting kick segment written deflate compression point running forcemerge primary replica shard template inactive log index set numberofreplicas forcemerge finished increase number replica gain redundancy put numberofreplicas replica allocated use cluster health api waitforstatusgreen check sure redundant copy data safely delete index old indexperday pattern easy decide old index could dropped rollover pattern quite obvious index contains data time period fortunately field stats api make easy determine need look index highest value timestamp field older cutoff get searchlogsfieldstats levelindices field timestamp indexconstraints timestamp maxvalue lt format yyyymmdd index returned request deleted rollover shrink forcemerge fieldstats apis provided primitive manage timebased index efficiently still number step could automated make life simpler step easy automate within elasticsearch need able inform somebody thing go planned role utility application built top elasticsearch expect see work flow based provided curator index management tool nice ui xpack take advantage scheduling notification feature provide simple reliable index management tool
370,Lobsters,scaling,Scaling and architecture,Monitorama 2016 - A (Biased) Recap,http://blog.librato.com/posts/monitorama-2016-recap,monitorama biased recap,,solarwinds worldwide llc right reserved
371,Lobsters,scaling,Scaling and architecture,Amazon Elastic File System  Production-Ready in Three Regions | AWS Blog,https://aws.amazon.com/blogs/aws/amazon-elastic-file-system-production-ready-in-three-regions/,amazon elastic file system productionready three region aws blog,amazon amazon elastic block store eb announced built efs hpc big data amazon elastic file system elastic file system mount target general purpose max io percentiolimit elastic file system action aws management console aws command line interface cli aws tool window powershell create file system corpvpcmounttarget create file system mount instruction efs cloudwatch metric burstcreditbalance clientconnections datareadiobytes datawriteiobytes metadataiobytes totaliobytes permittedthroughput percentiolimit efs bursting workload performance file system performance permittedthroughput average io size simultaneous connection request model async sync nfs client configuration mount instance amazon elastic file system efs efs pricing aws free tier jeff,portfolio aws storage product grown increasingly rich diverse time amazon started single storage class grown include storage class regular infrequently accessed archived object similarly amazon elastic block store eb began single volume type offer choice four type sanstyle block storage designed great particular set access pattern data type object storage block storage capably addressed eb turned attention file system announced amazon elastic file system efs last year order provide multiple instance shared lowlatency access fullymanaged file system happy announce efs available production use u east n virginia u west oregon europe ireland region launching today extended preview period gave u insight extraordinarily wide range customer use case efs preview great fit largescale throughputheavy processing workload along many form content web serving preview received lot positive feedback performance efs workload along request provide equally good support workload sensitive latency andor make heavy use file system metadata working address feedback today launch designed handle wide range use case based heard far customer really excited efs plan put use right away built efs many aws customer asked u way easily manage file storage scalable basis customer run farm web server content management system benefit common namespace easy access corporate departmental file hierarchy others run hpc big data application create process delete many large file resulting storage utilization throughput demand vary wildly time customer also insisted high availability durability along strongly consistent model access modification amazon elastic file system efs let create posixcompliant file system attach one instance via nfs file system grows shrink necessary fixed upper limit grow petabyte scale preprovision storage space bandwidth pay storage use efs protects data storing copy file directory link metadata multiple availability zone order provide performance needed support large file system accessed multiple client simultaneously elastic file system performance scale storage say later elastic file system accessible single vpc accessed way mount target create within vpc option create mount target desired subnet vpc access mount target controlled usual via security group efs offer two distinct performance mode first mode general purpose default use mode unless expect ten hundred thousand instance access file system concurrently second mode max io optimized higher level aggregate throughput operation per second incurs slightly higher latency file operation case start general purpose mode watch relevant cloudwatch metric percentiolimit begin push io limit general purpose mode create new file system max io mode migrate file enjoy even higher throughput operation per second elastic file system action easy create mount access elastic file system used aws management console could used efs api aws command line interface cli aws tool window powershell well opened console clicked create file system button selected one vpcs created mount target public subnet security group corpvpcmounttarget allows instance access mount point port inbound rule outbound one added name owner tag opted general purpose performance mode confirmed information clicked create file system file system ready right away mount target took another minute clicked mount instruction learn mount file system instance mounted file system efs copied bunch file spent time watching nfs stats console report amount space consumed file system information collected every hour displayed hour collected cloudwatch metric file system delivers following metric cloudwatch burstcreditbalance amount data transferred burst level throughput clientconnections number client connected file system datareadiobytes number byte read file system datawriteiobytes number byte written file system metadataiobytes number byte metadata read written totaliobytes sum preceding three metric permittedthroughput maximum allowed throughput based file system size percentiolimit percentage available io utilized general purpose mode see metric cloudwatch console efs bursting workload performance throughput available efs file system grow file system grows filebased workload generally spiky demand high level throughput short amount time low level rest time efs designed burst high throughput level asneeded basis file system burst mb per second throughput tb burst additional mb per second tb stored example tb file system burst mb per second tb file system burst mb per second throughput file system larger tb always burst time inactive efs us credit system determine file system burst one accumulates credit baseline rate mb per tb storage determined size file system spends whenever read writes data accumulated credit give file system ability drive throughput beyond baseline rate example give better idea mean practice gb file system burst mb per second minute day drive mb per second continuously tb file system burst gb per second hour day drive mb per second continuously learn credit system work read file system performance efs documentation order gain better understanding feature spent couple day copying concatenating file ultimately ending using well tb space file system watched permittedthroughput metric grow concert usage soon file collection exceeed tb saw case file system throughput see dependent characteristic workload average io size number simultaneous connection efs file access pattern random sequential request model synchronous asynchronous nfs client configuration performance characteristic instance running nfs client effect positive negative briefly average io size work associated managing metadata associated small file via nfs protocol coupled work efs make data highly durable highly available combine create peroperation overhead general overall throughput increase concert average io size since peroperation overhead amortized larger amount data also read generally faster writes simultaneous connection efs file system accommodate connection thousand client environment drive highly parallel behavior multiple instance benefit ability efs support multitude concurrent operation request model enable asynchronous writes file system including async option mount time pending writes buffered instance written efs asynchronously accessing file system mounted sync option opening file using option bypass cache eg odirect turn issue synchronous request efs nfs client configuration nfs client use laughably small today standard value read write buffer default consider increasing mib option mount command use nfs client efs latter provide better performance instance application perform large amount io sometimes require large amount memory andor compute power well sure plenty choose appropriate instance size type performing asynchronous read writes kernel use additional memory caching side note performance characteristic efs file system dependent use ebsoptimized instance benchmarking file system blend art science make sure use mature reputable tool run make sure examine result light consideration listed also find detailed data regarding expected performance amazon elastic file system efs page available efs available u east n virginia u west oregon europe ireland region start using today pricing based amount data store sampled several time per day charged gigabytemonth prorated usual starting per gb per month u east n virginia region minimum fee setup cost see efs pricing page information eligible aws free tier use gb efs storage per month charge jeff
372,Lobsters,scaling,Scaling and architecture,SATURN 2016 Keynote: Abstracting the Unknown with Grady Booch,https://www.youtube.com/watch?v=yBm7MDEODUw,saturn keynote abstracting unknown grady booch,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature
373,Lobsters,scaling,Scaling and architecture,CloudFlare is ruining the internet (for me),http://www.slashgeek.net/2016/05/17/cloudflare-is-ruining-the-internet-for-me/,cloudflare ruining internet,follow cloudflare making internet little bit faster select group people cloudflare multiple datacenter location security feature recaptcha aggressive partnership cheap hosting service kuala lumpur new delhi scheduled maintenance,follow cloudflare making internet little bit faster select group peoplecloudflare helpful service website owner want deal separate service cdn dns basic ddos protection superficial security need service onestopshop free hard pas offer go commercial solution generally speaking cloudflare service stable come downtime service interruption within margin similar service least experience know used two website recentlybut user live first world country part probably notice much difference better speed response time website using cloudflare service happy know multiple datacenter location mostly usa canada europe china short downtime result service interruption automatically rerouted nearest cloudflare data center plenty go around within first world countriesbut rest u talk experience live south east asia sea normally case u treated second class citizen internet often premium sometimes freemium service limited developed nation resort proxy vpns enjoy freedom internet user developed nation used part enjoy basic internet freedom rest world unless course website using cloudflare servicesyou see cloudflare security feature prompt user belonging certain ip block country blacklist behavioral pattern either automatically block visit site rare often done site owner discretion prompt recaptcha much better ridiculous captcha visit site recaptcha prompt happens lot know one click away deal annoying pseudo security measure entirety stackexchange site cloudflare firewall often case lot search often end one stackexchange site visit link google search result prompted captcha curiously get prompted directly visit site wish limited stackexchange cloudflare ludicrous free onestopshop offering aggressive partnership cheap hosting service every joe website using cloudflare one click per site per day often result click per day morei wish something ip see coown isp four ip block ip get chance test ip personally checked last year get prompted captchas limited ip block happens phone using telco internet service couple dozen isps tried country often market research service quality competition happens lot second part cloudflare annoyance cf cdn strength lie redundancy around world theory service work even closest node go data center node evenly distributed around world closest cloudflare node becomes unavailable traffic get rerouted node could lot location result increased latency invalidating whole point cdn first place exactly happened may two closest node sea kuala lumpur new delhi scheduled maintenance whatever reason instead traffic rerouted closest node close first place cloudflare hosted site would either work heavily broken static file loading includes reddit stackexchange first time happened probably longest duration far approximately hour worth disruptionthe idea single company negatively influence experience large portion internet user kinda scary would urge everyone reconsider using cloudflare cdndnsddos solution free good enough reason use something concerned site speed important thing look optimization considering cdn ddos might good reason choose cloudflare small portion total user probably get affected ddos anyways mention free tier cover complex ddos attack really concerned something configured free use relatively easy set integration cheap host turn nuisance large part internet user
374,Lobsters,scaling,Scaling and architecture,Choosing Metrics that Matter (Beyond CPU/Disk/Network),http://blog.librato.com/posts/metrics-beyond-cpu,choosing metric matter beyond cpudisknetwork,apm metric cpu memory network disk metric great teacher good metric test system hypothesis choose meaningful metric scratch distributed system step measure space service written length step extract knowledge latency data step repeat monitoring tool sign free trial,note article originally published librato since merged learn apm metric track using appoptics imagine moment suddenly promoted regular old developer senior architect tasked overseeing telemetry effort engineering team organization recently conversation devops day toronto someone living exactly experience put charge figuring stabilize effort different development team stabilize meant fix erratic behaviour product beginning large blocking outage team working different part single large microservices architecture grown large enough individual development effort service growing apart becoming siloed known friendly talented engineer contributed many service individually business decided devops ie snap current team focus making entire system work together better happy help hard time figuring begin knew wanted get data would give good feel problem question specifically measure cpu memory network disk metric great teacher much written theory choosing good metric written example good metric test system hypothesis mean think system build act eg queue handle writes per second balancer always choose least loaded worker good metric confirm valid assumption discredit error judgement show u edge case teach u thing build actually work yardstick classic cpumemorynetwork triumvirate mediocre best may meaningful hypothesis much ram cpu process use may learn something creation likely underlying interpreter o garbage collector assumption borne practice however metric measure thing like long particular database call take count total number worker thread queue element reflect assumption make meaningful understanding thing care let give example say built process expect consume cpu kilobyte ram per pool db connection production system running process using order magnitude ram cpu hypothesized learned something process learned something host running process problem metric like cpu memory utilization measure system rather process metric teach o runtime overhead teach actually care choose meaningful metric scratch building distributed system thing understanding experienced engineer pretty often intuit well architecture understood team running evidence everywhere deeply test code specifically monitor service precisely derive capacity plan even repeatably deploy change newly appointed architect asked start question experienced engineer knew made good metric knew team put charge syncing understand constructed need lecture theory going waste time asking team identify metric get hook question quite simply well let tell worked past whenever put charge large churning wad software write draw picture picture inevitably come looking something like figure fact one actual picture drew first hired trying wrap head around librato microservices architecture work practice figure step measure space service normally focus attention box end want know service work derive metric key indicator well job however case going ignore box completely fact going delete box label instead label line line going place label identifies protocol line represents give u figure figure check previously incomprehensible microservices architecture became handful commodity network protocol every application build balanced equation work fine long balance eventually root thing throw whack best way detect balance timing interaction component space service strategy figure way time interaction represented line made sound easy getting number collectively refer interservice latency data going require lot engineering knowhow almost every case get source add instrumentation wrap api db call sometimes need reconfigure set web server proxy every need write glue code api wrapper wind slew number order ten hundred millisecond something go wrong application number tell problem service node note thing telling problem actually get minute course need actually put data somewhere sort thing many people written length worth mentioning going need scalable telemetry system help store analyze stuff step extract knowledge latency data play around number get running note baseline value search pattern behavior thing strike odd service latency raise fall together appear dependent others vary time day day week discover pattern talk engineer run service see pattern confirm notion service work take long someone else notice something data cause say something like huh sound scientific discovery dig service behavior help engineer run likely encounter meaningful metric two something go wrong look interservice latency data see early identify thing going sideways number tend get big upstream service actually trouble share data engineer running service dig number together figure went wrong likely encounter good metric two step repeat sound labor intensive slower installing whizbang fully automatic apm form insight glean service metric embody absolutely worth squeeze effective metric require attention patience take effort identify manifestation insight teach something know service maintain thing prized shared talked life every engineer deal telemetry data come point want stop messing around monitoring tool start messing around monitoring data ready transition time away managing metric infrastructure toward using metric infrastructure sign free trial today let u help identify track key operational metric
375,Lobsters,scaling,Scaling and architecture,Linkedin's feed technology,https://engineering.linkedin.com/blog/2016/03/followfeed--linkedin-s-feed-made-faster-and-smarter,linkedin feed technology,guava cache avro,caching realized deserialization major bottleneck serving request optimize latency node maintains readwritethrough cache deserialized content record implementation us open source guava cache leverage functionality readthrough lru behaviour limiting total memory utilization size keyspace computing weight entity cached utilizing cache effectively missioncritical followfeed translates two requirement cache provide high performance usually deploy index node large jvm footprint cache much data possible experiment showed due large size cache concurrent eviction read became expensive using guava configs partitioned cache internally multiple subcaches optimization allows concurrent access subcaches also speed eviction good control kind data get cached get evicted memory used index node cache depends two factor total number timeline key total number record cached many timeline key record associated cache member may shared new content feed past day may shared content certain type last day desirable good control timeline key get evicted nonempty list record associated otherwise achieve use two instance guava cache one instance called fat cache called skinny cache fat cache hold timeline key nonempty list record associated one skinny cache contains key nt record associated fat cache holding capacity defined term weight total number record holding whereas skinny cache holding capacity defined term size total number key holding sizing two instance well could reach right effectiveness caching eviction schema awareness followfeed us avro serialization protocol data storage since avro need reader writer schema instance serdeser needed schema registry stored version schema data serialized persisted chose make index node schema aware mean along timeline index node also persists schema registry avro schema rocksdb registry used serialize deserialize content record also inmemory deserialized object representation registry allow fast access schema object filtering index node support business logicspecific filtering using parameter privacy georestrictions content type etc viewer specific data required filtering viewer privacy geolocation obtained helper service also followfeed defines custom grammar filtering allows client specify different filtering criterion easily comprehend followfeed filtering logic sqllike example client specified filtering criterion please note followfeed actually use sql query language
376,Lobsters,scaling,Scaling and architecture,GDC Vault - Parallelizing the Naughty Dog Engine Using Fibers,http://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine,gdc vault parallelizing naughty dog engine using fiber,,please enter search word
377,Lobsters,scaling,Scaling and architecture,Scaling Jarvis from 5 to 5000 users in forty-eight hours,http://lawrencewu.me/2016/05/21/scaling-jarvis-from-five-to-five-thousand-users-in-forty-eight-hours.html,scaling jarvis user fortyeight hour,scaling jarvis user fortyeight hour daniel jarvis cool site like hour lesson learned hour lesson learned hour lesson learned hour lesson learned,scaling jarvis user fortyeight hour may last saturday daniel ready launch side project jarvis world building dogfooding three week excited people try posted link reddit product hunt much happened people briefly messaged jarvis left went home dejected slept two day later jarvis user got featured cool site like sent reminder user migrated parser witai crucial first fortyeight hour thing went hour woke middle night checked product hunt right trending front page people actually trying jarvis checked heroku log holy shit log scrolling like matrix well good stopped responding message checked log ing place someone sent weird unicode decode properly woke daniel got debugging issue messaged new user tell issue could please hold second minute later pushed fix thank god could rest five minute something else broke ran google map geocoding api limit request per day quickly generated new api key google said could take meanwhile ton people messaging jarvis probably bouncing unresponsive palm sweaty knee arm alright exciting time life except time played really good guy smash tournament minute later seeing luckily facebook buffer request could still backfill missed request went back sleep soon little incident lesson learned careful around unicode hour grabbed brunch nearby started talking future billion upgraded dynos went home created test framework continued writing good test like good software developer always writes good test lesson learned nothing much really thing pretty good point hour monday morning woke everything fire checked log goddammit getting hit request per minute thanks lifehacker article luckily fix super straightforward missed unicode conversion pushed like five minute waking mlgskills realtalk jarvis total minute point probably lost bunch traffic oh well much redis cluster started getting really slow like second per query slow may something u stuffing data two key lost connection cluster dropped user good contacted redistogo told investigating eventually managed recover cluster upgrade memory lost thousand user time started building postgres backend redis thing clearly getting unsustainable lesson learned use redis caching message brokering real database database thing stuff data redis list iterate list find thing inefficient hour thing getting stable looked like going hit api limit google attach credit card developer account pay cent every extra request pretty good deal also finished migration redis postgres problem still getting shitton user ex started talking thing looking pretty good lesson learned make tiny immediately useful thing validate market
379,Lobsters,scaling,Scaling and architecture,ZeroMQ-backed API for building HTTP servers with distributed processing back-ends,https://mapzen.com/blog/zmq-http-server,zeromqbacked api building http server distributed processing backends,introduction point tutorial please note impetus software path public service anouncement documentation warning blog martin sstrik blog post api b u u h e r u p valhalla primeserver future picohttpparser hintjen thought conclusion one production system,introduction primeserver api building http soas le acronym library executables marry http server talk client distributed computing backend work example made sample application tell given number prime like give shot try installing running sudo addaptrepository ppa kevinkreiserprimeserver sudo aptget update sudo aptget install libprimeserverdev primeserverbin primeserverd tcp curl http killall primeserverd point wanted make tool let build system pipelined parallelized ie zmq butterfly parallel pipeline pattern see tutorial get bit first look like input requestproducer v v v parallelize worker worker worker v v v rebalance intermediaterouter v v v parallelize worker worker worker v v v output responsecollector seems like pretty good pattern offline scientific code pump job system wait result land bottom however useful online system face user need kind loopback get result back requester something like client server proxy proxy worker worker client browser separate thread make request server server listens new request reply backend part send back result backend comprised load balancing proxy layer worker pool real life may run different process different machine use thread example server conveniently simulate within single process please note lack mutexlocking pattern thank zmq system let handle request decomposed single pipeline multiple step useful certain step take longer others since scale independently one another really handle request would need multiple pipeline branching pipeline unless worker one job fix could upgrade worker able forward work one proxy would allow heterogeneous workflow without make smarterlarger pluripotent worker therefore would allow scaling various workflow independently sound great configuration would mess easier approach would run separate cluster per workflow pro con talk option later may asking earth worker pool hooked loopback two important function request could enter error state stage pipeline important able signal back client soon possible generally certain request known result error otherwise without going stage pipeline impetus toy example http service computes whether number prime simple illustration someone might want setup described prime yes prime prime give actual usecase drove creation project worked service oriented archtectures noticed compiled amounted wishlist architectural feature buzzword form roughly simplicity flexibility fault tolerance separation concern throughput load balancing fair queuing quality service nonblocking web scale kidding needed handle http request would widely varying degree complexity specifically writing software shortest value short path computation large graph example user would able make request get best route carfootbikeetc london edinburgh may take millisecond contrast user would also able ask route capetown beijing could take thousand millisecond ie several second different request vary computation time order magnitude worse might imagine finding path graph sound pretty straight forward making path useful client requires bunch extra work conveniently sometimes great effort decomposing problem discrete step help wish list context http server api requires little extra consideration basically catering hoping many simultaneous user make buzzword relavant especially last one kidding path fun build many reason first thing prove zmq butterfly pattern tiny github gist idea could put http server front pattern hit pesky wishlist item separate stage pipeline surprisingly learning pattern figuring would work concrete scenario another delight public service anouncement read zeromq documentation usually huge fan technical writing absolute joy read warning may cause rethink many many past code design decision worry though feeling embarassment past poor decision indicator making better informed one future massive kudos pieter hintjens wrote amazing document hungry good writing checkout blog another zmq author martin sstrik fantastic reading back gist little setup around writing running debugging code throw away easily work left came parallelized pipeline whose stage loopback common entrypoint next scoured internet http server zmq binding put front pipeline surprise lot good option spent time prototyping using different one stumbled upon interesting search result blog post pieter hintjens zmqstream socket type describes example socket work end making small web server using hintjens go say lot work would needed full fledged http server describes missing part bit detail idea enticing could build minimal http server zmq sit front pipeline threw stuff gist started testing long little code something though writing http state machine handle streaming nature socket type writing state machine especially couple protocol version time torture term code reuse get future work section api api consists essentially part clientserver stuff bit make answer request server stand client pipeline worker load balancing proxy proxyworker stuff bit fulfill request proxy sits pool worker given stage pipeline next stage proxy know worker available work send request worker available take worker responsible either send result another stage pipeline next proxy sensible protocol specific response back server forward client protocol stuff bit parse serialize request response respectively prearranged format make possible client speak something worker understands viceversa http pretty useful protocol exist even create like also note intermediate stage worker talk whatever protocol like want make web service sake example let say want real life ie production want well wishlist yeah let learn easy way shall well want build let make beautiful unicode text transmission service api featuring artisanal text art say sure like going hey service let get library installed sudo addaptrepository ppa kevinkreiserprimeserver sudo aptget update sudo aptget install libprimeserverdev ok great let write program call artcpp start including thing need primeserver gut include primeserverprimeserverhpp include primeserverhttpprotocolhpp using namespace primeserver nut bolt required include thread include functional include chrono include string include list include vector include csignal configuration constant various socket const std string serverendpoint tcp const std string resultendpoint ipc tmpresultendpoint const std string proxyendpoint ipc tmpproxyendpoint assortment artisional content const std vector std string art first including couple bit libprimeserver mainly setup pipeline guessed stuff need talk http protocol include bunch standard data structure make use throughout program pretty obvious configuration basically tell different thread socket find first one tcp socket webclients connect u normal mean two unix domain socket could tcp want run different part different machine example could run one stage pipeline machine fat graphic card gpgpu win whereas another stage might run better machine metric ton ram finally super sweet text art alright actually return response someone looking textual unicode stuff high intellectual excitement actually serve content workert resultt artwork const std list zmq messaget job void requestinfo false mean going back client next stage pipeline workert resultt result false type differs per protocol hence void fun auto info staticcast httprequestt infot requestinfo httpresponset response try todo actually usevalidate request parameter auto request httprequestt fromstring staticcast const char jobfront data jobfront size get art response httpresponset ok art infoid artsize catch const std exception e complain response httpresponset bad request ewhat tricky stuff header different version http responsefrominfo info format response protocal client understand resultmessagesemplaceback responsetostring return result basically define work functionobjectlambda whose signature match api expects workert resultt bit primeserver shuttling around architecture bulk function stuff every stage pipeline need unpack message previous stage case server work function assumes valid httplooking byte want formulate response either sent back client forwarded next stage pipeline case worker respond client scenario always initialize workert resultt intermediate false true worker would attempt forward result stage proxy next pool worker end simply formatting response client make sense store workert resultt message ok left much hook plumbing basically constructing pipeline int main void zmq contextt context http server false turn requestresponse logging std thread server std thread std bind httpservert serve httpservert context serverendpoint proxyendpoint upstream resultendpoint false load balancer std thread proxy std bind proxyt forward proxyt context proxyendpoint upstream proxyendpoint downstream proxydetach worker auto workerconcurrency std max sizet std thread hardwareconcurrency std list std thread worker sizet workerconcurrency worker function could defined inline via lambda could std bind instance method simply free function like workersemplaceback std bind workert work workert context proxyendpoint downstream ipc tmpnoendpoint resultendpoint artwork workersback detach listen sigint terminate hear std signal sigint int std thisthread sleepfor std chrono second exit serverjoin return first thing first zmq communication requires context get one pas around bit setup really simple run httpservert one thread server keep track forward request load balancing proxyt another thread proxy keep queue request shuttle fifo style first nonbusy workert inventory spawn bunch workerts constantly handshaking proxy done message let proxy know worker bored busy minimize latency far greedy scheduler notice program pretty much meant run daemon wait sigint ctlc away done get responsive unicode message portal shaking err cracking err running g artcpp lprimeserver art art another terminal hit curl bask glorious yet somewhat inappropriate art work k k curl echo done interested sample code check valhalla sample daemon program src dcpp primeserver source future first thing make use proper http parser impressive one notibly picohttpparser used one webservers came across searching may issue streaming nature zmqstream socket worth working maintain mess code required properly parse http second thing want work zbeacon perk api currently setup pipeline somewhat cumbersome stage must know previous next optional stage well loopback clunky requires decent understanding get right also manual zbeacon application broadcast endpoint peer connect eachother discovery rather via manual configuration see hintjen thought detailed description automatic service discovery pretty great really interesting part pipeline pipeline graph client server v proxy worker v proxy worker may reaching limit ascii art bear implication huge remove shackle rigid fixedorder pipeline instead application code determine stage request forwarded fly example imagine request requires bunch iteration specific stage traditional pipeline option perform iteration single worker basically locking worker iteration done would monopolize worker certain request ask n iteration worth work next request may need one allowing stage connected graph structure including cycle would give application option load balance portion larger request entire request fulfilled problem could broken task equal size least equal size would potential handle request much fairly balanced fashion graph structure various stage also potential better organize functionality worker pool one drawback fixed pipeline mentioned earlier handle heterogeneous request type would either need worker knew one thing indeed run different type request different cluster limitation would longer exist graph structure based request type application forward relevant stage example say wanted offer math service ma course might worker compute derivative worker summation worker compute integral course could implement client side library sake argument ignore impracticality second want write worker three thing would nicer isolate worker based type work perform wishlist requires forwarding specific worker pool based url example brings another todo probably want allow server forward request worker pool based url lot server furthermore operation complex others watched system statistically relevant amount traffic could look amount cpu spent per stage reallocate proportionally sized worker pool could even dynamically size worker pool based current traffic really slick conclusion fantastic little experiment worked even better successful claim used least one production system taking excellent tool zmq mostly building new tool help others build yet tool rewarding experience think may interested building projectservicetool using work let u know find something wrong submit issue better yet pull request fix
380,Lobsters,scaling,Scaling and architecture,Reflections: The ecosystem is moving,https://whispersystems.org/blog/the-ecosystem-is-moving/,reflection ecosystem moving,ecosystem moving stuck time ecosystem moving extensible federation issue template thorough issue template federation control address book social network facebook want get involved signal hiring,open whisper system developing open source consumerfacing software past four year want share thing learned itas software developer envy writer musician filmmaker unlike software create something really done forever recorded album year later software changesoftware exists part ecosystem ecosystem moving platform change network evolve security threat countermeasure constant shift collective ux language rarely sits still money time focus gone ecosystem faster whole thing begun travelall mean set expectation user social communication feature evolving rapidly anyone building software today know possible stand stillone controversial thing signal early build unfederated service nothing protocol developed requires centralization entirely possible build federated signal protocolbased messenger longer believe possible build competitive federated messenger allstuck timein circle popular opinion someone recently asked federating unrelated communication platform signal network told thought unlikely ever federate client server control retort dumb far would internet gotten without interoperable protocol defined party thought got first production version ip trying past year switch second production version ip limited success got http version stuck likewise smtp irc dns xmpp similarly frozen time circa late answer question far internet got got late taken u pretty far undeniable federate protocol becomes difficult make change right application level thing stand still fare well world ecosystem movingindeed cannibalizing federated applicationlayer protocol centralized service almost sure recipe successful consumer product today slack irc facebook email whatsapp done xmpp case federated service stuck time centralized service able iterate modern world beyondso nice able host email also reason email endtoend encrypted probably never contrast whatsapp able introduce endtoend encryption billion user single software update long federation mean stasis centralization mean movement federated protocol going trouble existing software climate demand movement todayearly thought federate signal velocity subsided realize thing probably never slow anything velocity entire landscape seems steadily increasingextensible federationxmpp example federated protocol advertises living standard despite capacity protocol extension however undeniable xmpp still largely resembles synchronous protocol limited support rich medium realistically deployed mobile device xmpp extensible extension quickly brought speed modern world like federated protocol extension mean much unless everyone applies almost impossible task truly federated landscape instead complicated morass xeps consistently applied anywhere implication severe someone choice use xmpp client server support video arbitrary feature affect affect everyone try communicate creates climate uncertainty never knowing whether thing work consumer space fractured client support often worse client support consistency incredibly important creating compelling user experiencefor example even github problem consistency control right introduced issue template number thirdparty github client support even creating thorough issue template signal android repository still get people post work please help client never even showed template make annoyed github even though use official github client potential opportunity github competitor display issue template consistentlyone potential benefit federation ability choose provider get access metadata however someone selfhosts email never felt particularly relevant given every email send receive seems gmail end anyway federated service always seem coalesce around provider bulk people use long tail small scattered selfhosting across internet make sense running reliable service easy outcome sadly worst worldsif anything protecting metadata going require innovation new protocol software change likely possible centralized environment control rather le making change consistently deploy endtoend encryption federated protocol like email proved difficult likely see emergence enhanced metadata protection centralized environment greater controlfederation controlon level federation appealing precisely freeze protocol time great centralized client server roll feature benefit u could easily roll feature federation give u collective control change accept come unacceptable inability adaptgiven federated service always seem coalesce around provider bulk people use federation becomes sort implicit threat nobody really want run server know might possible current host something egregious enough make worth efforthowever past six year also seen user cost switching centralized communication service reduced substantially particularly given tendency towards addressing userowned identifier like phone number device address book social network using phone number identifier reduced switching cost putting user social network control way notification center mobile device become federation point communication apps similar older desktop im client unified communication across multiple im networksthe effect visible messaging space market leader come gone new popular apps come nowhere even successful player seem compelled continue iterating improving service quickly possiblethis reduced user friction begun extend implicit threat used come federated service centralized service well could switch host even decide run server user simply switching entire network many case cost much lower federated switching cost changing email address use different email provideran open source infrastructure centralized network provides almost level control federated protocol without giving ability adapt centralized provider open source infrastructure ever make horrible change disagree software need run alternative instead may beautiful federation point seems facebookwant get involved signal hiring
381,Lobsters,scaling,Scaling and architecture,Spinning Up Your First Kubernetes Cluster on GKE,https://deis.com/blog/2016/first-kubernetes-cluster-gke/,spinning first kubernetes cluster gke,common us azure kubernetes service ak,common us azure kubernetes service ak migrate existing application cloud build complex application us machine learning take advantage agility offered microservices architecture
382,Lobsters,scaling,Scaling and architecture,"Kubernetes Overview, Part Two",https://deis.com/blog/2016/kubernetes-overview-pt-2/,kubernetes overview part two,common us azure kubernetes service ak,common us azure kubernetes service ak migrate existing application cloud build complex application us machine learning take advantage agility offered microservices architecture
383,Lobsters,scaling,Scaling and architecture,Towards Hard RealTime Erlang,http://www.erlang.se/workshop/2007/proceedings/05nicosi.pdf,towards hard erlang,,obj goto endobj obj motivation endobj obj goto endobj obj scheduling erlang endobj obj goto endobj obj harte proposal rt erlang endobj obj goto endobj obj test endobj obj goto endobj obj open issue endobj obj goto r fit endobj obj length filter flatedecode stream u zz vkd h k lu txo n k endstream endobj obj type page content r resource r mediabox trans r parent r annots r r r r r r r r r r r r r r r r r r r r r r r r r endobj obj type xobject subtype form bbox formtype matrix resource r length filter flatedecode stream endstream endobj obj shading sh shadingtype colorspace devicergb domain coords function functiontype domain function functiontype domain n functiontype domain n functiontype domain n functiontype domain n bound encode extend false false procset pdf endobj obj type xobject subtype form bbox formtype matrix resource r length filter flatedecode stream endstream endobj obj shading sh shadingtype colorspace devicergb domain coords function functiontype domain n extend true false procset pdf endobj obj type xobject subtype form bbox formtype matrix resource r length filter flatedecode stream endstream endobj obj shading sh shadingtype colorspace devicergb domain coords function functiontype domain function functiontype domain n functiontype domain n bound encode extend true false procset pdf endobj obj type xobject subtype form bbox formtype matrix resource r length filter flatedecode stream endstream endobj obj shading sh shadingtype colorspace devicergb domain coords function functiontype domain function functiontype domain n functiontype domain n bound encode extend false false procset pdf endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link endobj obj type annot border hnc rect subtype link endobj obj type annot border hnc rect subtype link endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj type annot border hnc rect subtype link goto endobj obj r xyz null endobj obj r xyz null endobj obj r xyz null endobj obj font r xobject r r r r procset pdf text endobj obj length filter flatedecode stream x
384,Lobsters,scaling,Scaling and architecture,"OpenMP 3.1 Summary Card C/C++ (2011) (supported since GCC 4.7, LLVM/Clang 3.7)",http://openmp.org/mp-documents/OpenMP3.1-CCard.pdf,openmp summary card cc supported since gcc llvmclang,, obj endobj xref n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n trailer prev startxref eof obj stream hb b g c  b   l bx    endstream endobj obj endobj obj extgstate font procset pdftext property shading xobject rotate typepage endobj obj endobj obj endobj obj endobj obj endobj obj stream  xue ri t  t a jsabuah g l m  tq endstream endobj obj r endobj obj iccbased r endobj obj stream  p m hja p x d bw  fncf n
386,Lobsters,scaling,Scaling and architecture,The Stack That Helped Medium Drive 2.6 Millennia of Reading Time,https://medium.com/medium-eng/the-stack-that-helped-medium-drive-2-6-millennia-of-reading-time-e56801f7c492,stack helped medium drive millennium reading time,current stack production environment amazon virtual private cloud ansible node go java cloudflare fastly cloudfront nginx haproxy datadog pagerduty elasticsearch logstash kibana database dynamodb hotkey issue redis amazon aurora data platform amazon redshift apache spark protocol buffer image go groupcache textshots phantomjs custom domain web frontend closure nick io android guava party tool aim solve narrow problem mockito robolectric internal version medium hatch beta group ab testing feature flag variant misc algolia sendgrid urban airship sqs bloomd pubsubhubbub superfeedr,current stackfor site seemingly simple medium may surprising much complexity behind scene blog right could probably knock something using rail couple day anyway enough snark let start bottomproduction environmentwe amazon virtual private cloud use ansible system management allows u keep configuration source control easily roll change controlled waywe serviceoriented architecture running dozen production service depending count micro others primary choice whether deploy separate service specificity work performs likely dependent change made across service boundary resource utilization characteristicsour main app server still written node allows u share code server client something use quite heavily editor post transformation node worked pretty well u performance problem emerged block event loop alleviate run multiple instance per machine route expensive endpoint specific instance thus isolating also hooked runtime get insight tick taking long time generally due object reification json deserializationwe several auxiliary service written go found go easy build package deploy like typesafety without verbosity jvm tuning java personally fan using opinionated language team environment improves consistency reduces ambiguity ultimately give le rope hang yourselfwe serve static asset using cloudflare though send traffic fastly cloudfront keep cache warm need cut emergency recently turned cloudflare application traffic well primarily ddos protection happy performance gainswe use combination nginx haproxy reverse proxy load balancer satisfy venn diagram feature needwe still use datadog monitoring pagerduty alert heavily use elk elasticsearch logstash kibana debugging production issuesdatabasesdynamodb still primary datastore completely smooth sailing one perennial issue hit hotkey issue viral event fanouts millionfollower user redis cache cluster sitting front dynamo mitigates issue read optimizing developer convenience production stability often seemed odds working close gapwe starting use amazon aurora newer data allows flexible querying filtering dynamowe use store relation entity represent medium network running master two replica people post tag collection node graph edge created entity creation people perform action follow recommend highlight walk graph filter recommend postsdata platformfrom early data hungry investing analytics infrastructure help u make business product decision recently able use data pipeline feed back production system power datadriven feature explorewe use amazon redshift data warehouse providing scalable storage processing system tool build continuously import core data set eg user post dynamo redshift event log eg post viewed post scrolled redshiftjobs scheduled conduit internal tool manages scheduling data dependency monitoring use assertionbased scheduling model job executed dependency satisfied eg daily job depends entire day event log production proved indispensable data producer decoupled consumer simplifying configuration system predictable debuggablewhile sql query running redshift work well u need get data redshift increasingly turned apache spark etl flexibility ability scale growth time spark likely become tool choice data pipelineswe use protocol buffer schema schema evolution rule keep layer distributed system sync including mobile apps web service data warehouse using custom option annotate schema configuration detail like table name index validation constraint like max length string acceptable range numberspeople need remain sync mobile web app developer log way product scientist interpret field way help people work data treating schema spec rigorously documenting message field publishing generated documentation proto filesimagesour image server written go us waterfall strategy serving processed image server use groupcache provides memcache alternative helping reduce duplicated work across fleet inmemory cache backed persistent cache image processed demand give designer flexibility change image presentation optimize different platform without large batch job generate resized imageswhile mainly used resizing cropping earlier version site allowed color wash blurring image effect processing animated gifs huge pain reason another posttextshotsthe fun textshots feature powered small go server interface phantomjs renderer processi always imagined switching rendering engine use something like pango practice ability lay image html way flexible convenient frequency feature used mean handle throughput quite easilycustom domainswe allow people set custom domain medium publication wanted single signon http everywhere super trivial get working set dedicated haproxy server manage cert route traffic main fleet application server manual work required setting domain automated large swathe custom integration namecheap cert provisioning publication linking handled dedicated serviceweb frontendon web tend want stay close metal single page application framework us closure standard library use closure template rendering client server use closure compiler minify code split module editor complex part web app nick written aboutiosboth apps native making minimal use web viewson io use mixture homegrown framework builtin component network layer use nsurlsession making request mantle parsing json model caching layer built top nskeyedarchiver generic way render item list common styling allows u quickly build new list different type content post view built uicollectionview custom layout use shared component render full post post previewevery commit built pushed medium employee try app quickly possible cadence release appstore beholden review cycle try keep pushing fast even minimal updatesfor test use xctest ocmockandroidon android stay current latest edition sdk support library use comprehensive framework preferring instead establish consistent pattern repeated problem use guava thing missing java otherwise tend use party tool aim solve narrow problem define api response using protocol buffer generate object use appwe use mockito robolectric write highlevel test spin activity poke around create basic version first add screen prepare refactoring grow reproduce bug shield regression write lowlevel test exercise particular single class create build new feature help u reason class interactevery commit automatically pushed play store alpha build go medium staff right away includes another flavor app internal version medium hatch friday promote latest alpha beta group play thing weekend monday promote beta production since latest code always ready release find bad bug get fix production immediately worried new feature let beta group play thing little longer excited release even frequentlyab testing feature flagsall client use serversupplied feature flag called variant ab testing guarding unfinished featuresmiscthere lot thing fringe product mentioned algolia allowed u iterate quickly searchrelated functionality sendgrid inbound outbound email urban airship notification sqs queue processing bloomd bloom filter pubsubhubbub superfeedr r etc etc
388,Lobsters,scaling,Scaling and architecture,"A comparison between Misultin, Mochiweb, Cowboy, NodeJS and Tornadoweb (2011)",http://www.ostinelli.net/a-comparison-between-misultin-mochiweb-cowboy-nodejs-and-tornadoweb/,comparison misultin mochiweb cowboy nodejs tornadoweb,nginx patched version httperf misultin mochiweb cowboy nodejs tornadoweb test result tornadoweb nodejs mochiweb cowboy misultin update may,already know author misultin erlang http lightweight server library interested http server spend quite time trying always interested comparing different perspective today wanted try benchmark various http server library chosen library one currently interest misultin obviously since wrote mochiweb since solid library widely used production afaik used still used empower facebook chat amongst thing cowboy newly born lib whose programmer active erlang community nodejs since bringing javascript backend opened new whole world possibility code reusable frontend ease access various programmer finally tornadoweb since python still remains one favourite language tornadoweb excelling load benchmark production empowering friendfeed two main idea behind benchmark first want hello world kind test static server nginx wonderfully perform task benchmark needed address dynamic server second wanted socket get periodically closed since load socket scarcely correspond real life situation latter reason decided use patched version httperf widely known used benchmark tool hp basically try send desired number request server report many actually got replied many error experienced process together variety piece information great thing httperf set parameter called set amount call per session ie socket connection socket get closed client command issued test httperf server uri valuebenchmarks rate value rate set incrementally since number requestssec rate numcalls test conducted desired number responsessec incrementing total number request numconns rate therefore fixed value along every test iteration test basically asks server check get variable set variable set reply xml stating error variable set echo inside xml therefore tested header parsing querystring parsing string concatenation socket implementation server virtualized uptodate ubuntu lts cpu ram etcsysctlconf file tuned parameter maximum tcp receive windownetcorermemmax maximum tcp send windownetcorewmemmax give kernel memory tcp need many open socket etcsecuritylimitsconf file tuned ulimit n set hard soft limit code different server misultin module misultinbench export start port misultin startlink port port loop fun req handlehttp req end stop misultin stop handlehttp req get value parameter args req parseqs value misultinutility getkeyvalue value args case value undefined req ok contenttype textxml value specified req ok contenttype textxml value end mochiweb module mochibench export start port mochiwebhttp start port port loop fun req handlehttp req end stop mochiwebhttp stop handlehttp req get value parameter args req parseqs value misultinutility getkeyvalue value args case value undefined req respond contenttype textxml value specified req respond contenttype textxml value end note using misultinutility function inside code since proplists much slower cowboy module cowboybench export start port application start cowboy dispatch host list path handler opts cowboybenchhandler name nbacceptors transport transopts protocol protoopts cowboy startlistener http cowboytcptransport port port cowboyhttpprotocol dispatch dispatch stop application stop cowboy module cowboybenchhandler behaviour cowboyhttphandler export init tcp http req opts ok req undefinedstate handle req state ok case cowboyhttpreq qsval value req undefined cowboyhttpreq reply contenttype textxml value specified req value cowboyhttpreq reply contenttype textxml value req end ok state terminate req state ok nodejs var http require http url require url httpcreateserver function request response responsewritehead contenttype textxml var urlobj urlparse requesturl true var value urlobjquery value value responseend value specified else responseend value listen tornadoweb import tornadoioloopimport tornadoweb class mainhandler tornadowebrequesthandler def get self value selfgetargument value selfsetheader contenttype textxml value selfwrite value specified else selfwrite value application tornadowebapplication r mainhandler name main applicationlisten tornadoioloopioloopinstance start took code run misultin erlang mochiweb erlang cowboy master erlang nodejs tornadoweb python library run standard setting erlang launched kernel polling enabled smp disabled single cpu used library test result raw printout httperf result got downloaded note graph logarithmic scale according see tornadoweb top around responsesseconds nodejs mochiweb cowboy misultin misultin cowboy experience little error server seem funnel load please note error timeout error second without reply total response response time speak say surprised result point like feedback code methodology alternate test performed input welcome available update post correct eventual error made ongoing discussion whomever want contribute however please refrain flame war welcomed published post exactly surprised result got opinion update may due success benchmark want stress important point read including mine benchmark often misleading interpreted higher graph best libofthemomentnamehere everything absolutely wrongest way look stress point enough fast n feature desire webserver library definitely want consider stability feature ease maintenance low standard deviation code usability community development speed many factor whenever choosing best suited library application thing generic benchmark one related specific situation fast application computational time load connection small data transfer therefore please use grain salt jump generic conclusion regarding cited library clearly stated beginning post find interesting valuable still open criticized described methodology thing might missed
389,Lobsters,scaling,Scaling and architecture,10M - goroutines,http://goroutines.com/10m,goroutines,problem load balancer mtcp dpdk,concurrent websockets problem modern server able easily handle concurrent connection solid throughput low jitter handling level traffic generally requires specialized approach offered stock linux kernel using stock image go server handle concurrent connection low throughput moderate jitter connection mostly idle server design example simplest websocket server useful anything similar push notification server like io apple push notification service without ability store message client offline server accepts websocket connection port avoid exhaustion ephemeral port client testing url client specifies channel connect w server channel websocket connection setup server never read data connection writes message client publishing channel handled redis using publishpsubscribe command unnecessary single server machine nice multiple server need sort central place handle message routing whenever message published channel server send message connected client subscribed channel make sure client still connected server also send ping message every minute client use missing ping message detect disconnected func handleconnection w websocketconn channel string sub subscribe channel timenewticker pingperiod var message byte select case tc message nil case message sub wssetwritedeadline timenow add timesecond err wswritemessage websockettextmessage message err nil break tstop wsclose unsubscribe channel sub go get run server like aptget update aptget install redisserver echo bind etcredisredisconf systemctl restart redisserver sysctl w sysctl w ulimit n sysctl w sysctl w sysctl w client connects server specified command line make number connection also specified command line start port increment port every connection func createconnection url string w err dialerdial url nil err nil return wssetreadlimit maxmessagesize wssetreaddeadline timenow add idletimeout message err wsreadmessage err nil break len message fmtprintln received message url string message wsclose go get run client like sysctl w sysctl w ulimit n sysctl w sysctl w ip address number connection server run instance gce machine memory sending ping every minute connection roughly limit server could handle end ping per second terribly high number seems limited kernel network setting using machine number core could handle least ping per second single machine could since connection mostly idle channel message traffic assumed insignificant compared ping full connection server cpu load memory half used default likely hardware could handle connection much higher ping rate without fancy optimization server code garbage collector surprisingly performant even memory allocated process using smaller instance instance connection putting behind google excellent load balancer easily scale whatever maximum number connection allowed load balancer limited likely larger number concurrent connection could handled using user space tcp stack mtcp direct interface network card like dpdk though unclear hard would integrate go since may require pinning thread specific core instance
390,Lobsters,scaling,Scaling and architecture,Caching in datacenters (Pelikan),https://twitter.github.io/pelikan/2016/04/03/caching-in-datacenters.html,caching datacenters pelikan,blog series ubiquitous performance caching datacenters infrastructure ramcloud problem storing data accessing data requirement hidden requirement production readiness challenge coming,first post blog series design implementation usage caching datacenters many different definition caching indeed caching ubiquitous long locality taken advantage cpu cdn hardware controller wide area network caching varies greatly medium location result speed srambased cpu cache clock mere couple nanosecond per operation downloading image nearest cdn server take second one invariant people use cache way get data faster cheaply lifeline caching performance sometimes surprising see often people would take tradeoff slightly incorrect data exchange fast data access economy caching also answer scalability intended optimization plenty system collapse cache pulled little paranoia thing may slow caching thus understandable realize whole existence ie competitive advantage caching hinge squarely caching datacenters care caching datacenters mean caching unless otherwise specified term applies whenever data stored outside canonical source goal find fastest cheapest way data start one need understand underlying infrastructure problem infrastructure datacenters filled server network largely homogeneous centrally controlled compared broader internet good solution take homogeneity predictability advantage caching datacenters aim limit reality physic networking fabric software stack send forward receive data take send byte end end request wait least get response kernel network stack take process tcp packet request tcp pay overhead end system violate speed light defined environment wise choose topological placement storage medium accordingly datacenter network built ethernet available network bandwidth range edge increasingly becoming mainstream setup endtoend latency usually order storage side ssds seek time similar endtoend network latency bandwidth comparable ethernet well somewhere spinning disk hand one two order magnitude slower seek thus slow cache dram used main memory offer bandwidth order access latency making way ahead persistent medium come performance following figure capture relative closeness data different location typical datacenter infrastructure implication local memory access significantly faster remote memory access also offer much higher throughput ssd ethernet comparable term latency throughput depending specific product setup thus choice two always obvious however getting data remotely open door scaling horizontally dataset distributed probably explains dominance distributed inmemory store ssd cache datacenters getting remote data stored ssd usually slower memory dramatically larger object transfer time increasingly dominates endtoend performance rendering difference even le significant faster communication andor local storage medium gamechangers example infiniband lower endtoend latency two order magnitude data storage system built top see relationship local remote data differently see ramcloud nonvolatile memory becomes reality next year blur boundary volatile persistent storage forcing architect rethink storage including caching hierarchy problem caching simple problem concept core two fairly universal functionality storing data accessing data storing data get performance edge cached data overwhelmingly stored memory however infrastructure indicates ssd nvram future considered right condition met faster storage expensive caching solution must also good control data layout chosen medium make efficient use storage real estate accessing data great deal consider come data access important difference whether network involved ie local versus remote locally data access often cheaper happens address space give u several mode caching mode network comm protocol inprocess local yes remote yes yes array communication protocol one used presenting different performance characteristic example udp generally boast lower overhead tcp remote access locally one choose use unix domain socket pipe messagingpassing shared memory considerably lighterweight compared networking counterpart requirement marrying problem hand underlying constraint caching datacenters usually combination inprocess caching local remote inmemory caching commonality among good caching solution deliver latency throughput close limit baremetal operating system often use lightweight protocol available scenario primarily store data memory using persistent storage extension directly manage memory use data structure memoryefficient hidden requirement far completely ignoring operational aspect caching anybody try keep service know operation arguably biggest hidden criterion system running datacenters experienced engineer want sleep night always choose productionready system exact definition production readiness still open question essentially productionready system operationsfriendly system offer customization optimization configuration stability predictability runtime adaptive various scale mean monitor debug logging statistic longterm maintainability ease development case noticed list productionreadiness long functionality furthermore requirement logging may stand way achieving others delivering optimal throughput latency challenge weighing option balancing goal odds main challenge facing anybody want build good caching solution coming next post zoom server aspect caching explore design principle allows u satisfy obvious hidden requirement
391,Lobsters,scaling,Scaling and architecture,Microservices Weekly (Issue #20): This week's news/articles on microservices,http://microservicesweekly.com/issue/20,microservices weekly issue week newsarticles microservices,article video event discussion,issue apr article article defines microservices architecture outline best practice designing one insinghtful article microservices architecture solve performance issue peter lawrey allegro cache service written go handle handle rps writes read time team spent figuring gc pause huge impact application responsiveness million object control simple limited scale application monolithic architecture still make sense microservices different approach application development deployment one perfectly suited agility scale reliability requirement many modern cloud application arguably one difficult microservices pattern apply implement bounded context march towards microservices technology post centered around uber built service go handle problem geofencing simple limited scale application monolithic architecture still make sense microservices different approach application development deployment one perfectly suited agility scale reliability requirement many modern cloud application article donnie berkholz give overview microservices critical future software get built informationweek article charles babcock summarizes concept cloud native enterprise shift also described new stack microservices environment bounded context service may may backed single database scenario generalized asynchronous messaging mechanism provide support distributed environment oracle view microservices build microservices oracle second article series discus building microservices using api gateway video talk phil calcado explore reference model microservices architecture built speaker personal experience building architecture tech startup square manik surtani joined apigee alan ho discus mobile payment merchant service provider collaboration google launch grpc open source rpc framework backed protocol buffer based realworld experience operating microservices scale great talk peter elger one node j meetups dublin event goto amsterdam practitionerdriven enterprise software development conference designed team lead architect project management organized developer developer discussion
394,Lobsters,scaling,Scaling and architecture,Reducing System Jitter,http://epickrram.blogspot.com/2015/09/reducing-system-jitter.html,reducing system jitter,lmax exchange disruptor busyspinwaitstrategy example hdrhistogram systemjitter baseline cpu speed perfevents intel cpufreq driver script process migration thread affinity cpu isolation conclusion github repository followup post,next instalment series lowlatency tuning lmax exchange going talk reducing jitter introduced operating system application typically execute many thread running within jvm turn run atop linux operating system linux generalpurpose multitasking o target phone tablet laptop desktop serverclass machine due broad reach sometimes necessary supply guidance order achieve lowest latency lmax exchange service rely heavily disruptor fast interthread communication handful hot thread wish always oncpu simplified diagram one lowlatency path exchange receive fix request customer gateway request multiplexed disruptor instance consumer thread sends message onto network via udp message arrive matching engine processed response sent customer via fix gateway focussing simplified matching engine see thread execution affect endtoend latency jitter present system reality diagram illustrative purpose thread polling network inbound traffic thread executes business logic generating response journaller thread writes inbound message disk publisher thread responsible sending response back gateway ensure data consistency businesslogic thread process message written journal covered detail previous post seen jitter experienced thread affect endtoend latency experienced customer aim reduce jitter absolute minimum use disruptor busyspinwaitstrategy messagepassing publisher consumer instantaneous allowed platform disruptor different strategy waiting suited different situation case since want reduce latency busyspinning best choice however come caveat thread always runnable need access cpu resource time mentioned linux multitasking generalpurpose operating system whose default mode schedule wide variety task different latency requirement operating system decides run another task cpu currently executing one busyspinning thread unwanted unpredictable latency creep system enter dark art cpuisolation threadpinning linux tracing tool example order demonstrate technique used lmax exchange achieve consistent low interthread latency going refer example application used measure latency introduced host platform application three thread lowlatency requirement producer thread read message datasource writes disruptor instance logic thread performs arbitrary logic journaller thread writes message disk logic journaller thread consumer disruptor instance using busyspin wait strategy producer thread performs call systemnanotime writes result message passing disruptor logic thread read message disruptor immediately call systemnanotime delta two timestamps time taken transit disruptor delta stored hdrhistogram reported application exit given little work done logic thread reasonable expect interthread latency low consistent reality however case running test laptop operating system scheduling jitter magnified would le pronounced serverclass machine instance technique used investigate reduce jitter effectively systemjitter baseline executing example application period time inspecting result show large variation time taken transit disruptor accumulator message transit latency n mean min max count fastest message get disruptor nanosecond thing rapidly degrade message took longer millisecond pas thread longest delay millisecond difference several order magnitude clearly something happening system negatively affecting latency pause introduced runtime jvm ruled application garbagefree performs warmup cycle exercise jit guaranteed safepoints disabled confirmed enabling safepoint logging looking gc log stdout output xx printcompilation enabled cpu speed modern cpu especially laptop designed power efficient mean o typically try scale clock rate activity intel cpu partially handled using powerstates allow o reduce cpu frequency meaning le power draw le thermal overhead current kernel handled cpu scaling governor check current setting looking file one directory entry sysdevicessystemcpu per available cpu machine laptop set powersave mode see available governor cat tell two choice performance powersave making change though let make sure powersave actually causing issue perfevents used monitor cpu pstate application running perf record e power cpufrequency command sample cpufrequency trace point written intel cpufreq driver cpu information come msr chip hold fsb speed filtering entry include sample taken java executing show variation reported frequency clearly ideal achieving lowest latency java power cpufrequency java power cpufrequency java power cpufrequency java power cpufrequency script used set scaling governor performance mode reduce variation sudo bash setcpugovernorsh performance running application performance governor enabled produce better result interthread latency monitoring perf show cpufrequency event longer emitted accumulator message transit latency n mean min max count though still max latency reduced previous value process migration another possible cause scheduling jitter likely o scheduler moving process around different task become runnable important thread application mercy scheduler invoked decide run another process current cpu happens running thread context saved shifted back scheduler runqueue possibly migrated another cpu entirely find whether happening thread application turn perf sample trace event emitted scheduler case sampling schedstatruntime event show cpu playing host application thread one row output perf script show java thread executed duration millisecond java sched schedstatruntime commjava n n bit sorting counting show exactly process moved around available cpu lifetime perf script grep java awk print sort uniq c thread mostly ran cpu also spent time cpu moving process around going require context switch produce cache invalidation effect unlikely source maximum latency order start improving worstcase necessary stop migration process thread affinity thread affinity used force process run specific cpu set cpu achieved either using taskset launching program schedsetaffinity system call within application using technique stop process migration latencysensitive thread positive effect latency jitter experienced application result implies forcing thread run single cpu help reduce interthread latency whether scheduler making better decision run process simply le context switching clear one thing look fact effort made stop scheduler running task cpu still multimillisecond delay message passing could process run cpu application thread restricted returning perf time capturing schedstatruntime event specific cpu case show process scheduled application running perf record e sched schedstatruntime c stripping everything process name counting occurrence event trace show java application running time plenty process scheduled application execution time java rngd rcusched gmain goadaemon chrome rtkitdaemon cpu isolation point time remove target cpu o scheduling domain done isolcpus boot parameter ie add isolcpus cpulist grubconf using cset command cpuset package using method scheduler restricted running userspace process cpu hosting latencysensitive application thread combination setting thread affinity mean application thread always cpu resource effectively always running difference interthread latency dramatic maximum latency microsecond accumulator message transit latency n mean min max count difference great necessary use logscale yaxis chart note difference great serverclass machine lot spare processing power effect magnified fact o cpu laptop work desktop distribution linux much scheduling pressure would present serverclass machine using perf confirm process running reserved cpu show still contention deal java swapper process starting k kernel thread deal housekeeping task behalf o swapper linux idle process scheduled whenever work executed cpu conclusion cpu isolation thread affinity powerful tool help reduce runtime jitter introduced o scheduler linux tracing tool perfevents invaluable inspecting state running process determining source jitter lowlatency application ordersofmagnitude reduction jitter achieved applying technique post introduced technique observing fixing system jitter example post generated using application available github repository also walkthrough step used generate data post post describes start journey towards tuning linux lowlatency application taken lmax exchange dealing cause runtime jitter covered followup post
395,Lobsters,scaling,Scaling and architecture,Eliminating Large JVM GC Pauses Caused by Background IO Traffic,https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic,eliminating large jvm gc pause caused background io traffic,background io activity cfengine solution evaluation putting gc log ssd tmpfs,gc logging ie write call blocked o blocking time contributes stw pause new question buffered writes blocked digging various resource including kernel source code realized buffered writes could stuck kernel code multiple reason including stable page write journal committing stable page write jvm writing gc log file firstly dirties corresponding file cache page even though cache page later persisted disk file via o writeback mechanism dirtying cache page memory still subject page contention caused stable page write stable page write page o writeback write page wait writeback complete page locked ensure data consistency avoiding partially fresh page persisted disk journal committing journaling file system appropriate journal generated file writing appending gc log file result new block allocated file system need commit journal data disk first journal committing o io activity commitment might need wait background io activity heavy waiting time noticeably long note file system feature delayed allocation postpones certain journal data o writeback time alleviates problem note also changing data mode default ordered mode writeback really address cause journal need persisted writetoextend call return background io activity standpoint particular jvm garbage collection background io activity inevitable typical production environment several source io activity o activity administration housekeeping software colocated application io jvm instance first o contains many mechanism eg proc file system incur data writing underlying disk second systemlevel software cfengine also perform disk io third node colocated application share disk drive application contend io fourth particular jvm instance may use disk io way gc logging solution since current hotspot jvm implementation well others gc logging blocked background io activity various solution help mitigate problem writing gc log file first jvm implementation could enhanced completely address issue particularly gc logging activity separated critical jvm gc process cause stw pause problem go away instance jvm put gc logging function different thread handle log file writing independently hence contributing stw pause taking separatethread approach however risk losing last gc log information jvm crash might make sense expose jvm flag allowing user specify preference since extent stw pause caused background io depends heavy latter various way reduce background io intensity applied instance deallocating iointensive application node reducing type logging improving log rotation etc latencysensitive application online one serving interactive user large stw pause eg second intolerable hence special treatment need applied bottom line ensuring big stw pause induced o avoid gc logging blocked o io activity one solution put gc log file tmpfs ie xloggc tmpfsgclog since tmpfs disk file backup writing tmpfs file incur disk activity hence blocked disk io two problem approach gc log file lost system crash consumes physical memory remedy periodically backup log file persistent storage reduce amount another approach put gc log file ssd solidstate drive typically much better io performance depending io load ssd adopted dedicated drive gc logging shared io load however cost ssd need taken consideration costwise rather using ssd costeffective approach put gc log file dedicated hdd io activity gc logging dedicated hdd likely meet lowpause jvm performance goal fact scenario showed mimic setup since setup io activity exist gclogging drive evaluation putting gc log ssd tmpfs take dedicatedfilesystem approach putting gc log file ssd tmpfs drive run java workload background io load scenario ii ssd tmpfs observe similar result following figure show result putting gc log file ssd disk notice jvm pausing performance onpar scenario pause second result indicate background io load impact application performance
396,Lobsters,scaling,Scaling and architecture,Holistic Configuration Management at Facebook,http://muratbuffalo.blogspot.com/2016/02/holistic-configuration-management-at.html,holistic configuration management facebook,move fast break thing developer conference zuckerberg announced new facebook motto configerator tool bertrace link paper proceeding link conference presentation video configuration management configuration code approach example twitter configerator architecture canary testing proof pudding canary testing facebook monitoring tool ubertrace mystery machine performance configerator research review facebook paper facebook mystery machine endtoend performance analysis largescale internet service facebook software architecture scaling memcache facebook finding needle haystack facebook photo storage finding needle haystack facebook photo storage,move fast break thing facebook famous mantra developer facebook belief getting early feedback iterating rapidly release software early frequently three time day frontend code three time day backend code amazed facebook able maintain agile deployment process scale heard software company even relatively young one develop problem agility even point deploying trivial app would take couple month due review filing ticket routing permission etc course deploying frequently scale need discipline good process order prevent chaos developer conference zuckerberg announced new facebook motto move fast stable infra think configerator tool discussed paper big part stable infra way configerator configurator another facebook peculiarity like spelling bertrace link paper proceeding link conference presentation video configuration management even surprising daily facebook code deployment facebook various configuration changed even frequently currently thousand time day hold fast every single engineer make live configuration change certainly exceptional especially considering even minor mistake could potentially cause sitewide outage due complex interdependency possible without incurring chaos answer discipline set free disciplined deployment process built configerator facebook lower risk deployment give freedom developer deploy frequently ok reviewing cool system configerator let get clarified first configuration management involve needed turn essential many diverse set system facebook include gating new product feature conducting experiment ab test performing applicationlevel traffic control performing topology setup load balancing tao performing monitoring alertsremediation updating machine learning model varies kb gb controlling application behavior eg much memory reserved caching many writes batch writing disk much data prefetch read essentially configuration management provides knob enable tuning adjusting controlling facebook system wonder configuration change keep growing frequency outdo code change order magnitude configuration code approach configerator philosophy treating configuration code compiling generating configs highlevel source code configerator store config program generated configs git version control complex dependency across system service facebook one subsystemservice config updated enable new feature configs system might need updated accordingly taking configuration code approach configerator automatically extract dependency source code without need manually edit makefile furthermore configerator provides many foundational function including version control authoring code review automated canary testing config distribution review next part configerator architecture discussion configerator main tool configuration support tool suite gatekeeper control rollouts new product feature moreover also run ab testing experiment find best config parameter addition gatekeeper facebook ab testing tool built top configerator omit paper due space limitation packagevessel us peertopeer file transfer assist distribution large configs eg gb machine learning model without sacrificing consistency guarantee sitevars shim layer provides easytouse configuration api frontend php product mobileconfig manages mobile apps configs android io bridge backend system configerator gatekeeper mobileconfig bridged sitevars sitevars php mobileconfig bridged packagevessel currently need transfer large configs mobile device file transfer mentioned part packagevessel none bittorrent yes bittorrent find many application datacenter example twitter configerator architecture configerator application designed defend configuration error using many phase first configuration compiler automatically run developerprovided validators verify invariant defined configs second config change treated code change go though rigorous code review process third config change affect frontend product automatically go continuous integration test sandbox lastly automated canary testing tool roll config change production staged fashion monitor system health roll back automatically case problem think architecture actually quite simple even though may look complex control data flowing direction top cyclic dependency make recovery hard softstate architecture new correct information pushed top clean old bad information canary testing proof pudding paper say canary testing canary service automatically test new config subset production machine serve live traffic com plements manual testing automated integration test manual testing execute test hard automate may miss config error due oversight shortcut time pressure continuous integration test sandbox broad coverage may miss config error due smallscale setup environment difference config associated canary spec describes automate testing config production spec defines multiple testing phase example phase test server phase test full cluster thousand server phase specifies testing target server healthcheck metric predicate decide whether test pass fails example clickthrough rate ctr collected server using new config x lower ctr collected server still using old config canary testing endtoend test somewhat override trying build exhaustive static test configs course validation review sandbox test important precaution try make sure config sane tried small amount production however given facebook already canary testing good end proof correctness config somewhat obviates need heavyweight correctness checking mechanism paper give couple example problem caught canary testing hand paper make clear conclusiveexhaustive canary test canary test nt catch slowly manifesting error like memory leak also facebook detect whether abnormality canary test yes facebook monitoring tool ubertrace mystery machine sufficient abnormality detection subtle bug detection maybe nt see adverse effect configuration change application adversely affected application backend service seems like exhaustive monitoring log collection log analysis may need done detect subtle error performance configerator approximate latency configerator phase engineer save config change take ten minute go automated canary test long testing time needed order reliably determine whether application healthy new config ca nary test long take commit change propagate server subscribing config la tency broken three part take second commit change shared git repository git slow large repository git tailer see figure take second fetch config change shared git repository git tailer writes change zeus propagates change subscribing server distribution tree last step take second reach hundred thousand server distributed across multiple continent figure paper show git bottleneck configuration distribution commit throughput scalable respect repository size execution time many git operation increase number file repository depth git history configerator process migration multiple smaller git repository collectively serve partitioned global name space research configerator impressive engineering effort want focus important research take aways going forward core research idea finding push envelope futurefacing improvement consistent configuration rollouts couplingsconflicts code configuration facebook solves cleverly deploy code first much earlier config enable hiddenlatent code later config change also couplingsconflicts old new configs configuration change arrives production server different time albeit within second would cause problem server run old configuration new configuration facebook punt responsibility developer need make sure new config coexist old config peace use canary testing fraction machine use new config remaining old config sum facebook try strong consistent reset new config nt know detail system backend server config change may need stronger consistency push versus pull debate paper claim push advantageous pull datacenter config deployment convinced argument look strong configerator us push model compare pull model biggest advantage pull model simplicity implementation server side stateless without storing hard state individual client eg set configs needed client note different machine may run different application hence need different configs however pull model le efficient two reason first poll return new data hence pure overhead hard determine optimal poll frequency second since server side stateless client include poll full list configs needed client scalable number configs grows environment many server need ten thousand configs run opt push model environment may worth revisiting investigating detail pull simple stateless also mention unclear could nt adopted extend wan coordination mentioned single master ie single producerwriter would need multi master solution master regioncontinent start config update system shall need deal concurrent potentially conflicting configuration change however given canary testing order minute would practical need multimaster deployment near future review facebook paper facebook mystery machine endtoend performance analysis largescale internet service facebook software architecture scaling memcache facebook finding needle haystack facebook photo storage finding needle haystack facebook photo storage
397,Lobsters,scaling,Scaling and architecture,Design of a Modern Cache,http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html,design modern cache,benjamin mane loaddocs caffeine yield high hit rate excellent concurrency eviction policy try predict entry likely used us matrix counter multiple hash function us sketch filter admitting new entry observing often fixed duration preferred policy every access write shared state update per entry metadata borrow idea database theory writes scaled using commit log update written log replayed asynchronous batch separate buffer used cache read writes striped ring buffer read scale linearly number cpu related article,guest post benjamin mane engineery thing google engineery thing new load documentation startup loaddocs caching common approach improving performance yet implementation use strictly classical technique article explore modern method used caffeine opensource java caching library yield high hit rate excellent concurrency idea translated favorite language hopefully reader inspired eviction policy cache eviction policy try predict entry likely used near future thereby maximizing hit ratio least recently used lru policy perhaps popular due simplicity good runtime performance decent hit rate common workload ability predict future limited history entry residing cache preferring give last access highest priority guessing likely reused soon modern cache extend usage history include recent past give preference entry based recency frequency one approach retaining history use popularity sketch compact probabilistic data structure identify heavy hitter large stream event take example countmin sketch us matrix counter multiple hash function addition entry increment counter row frequency estimated taking minimum value observed approach let u tradeoff space efficiency error rate due collision adjusting matrix width depth window tinylfu wtinylfu us sketch filter admitting new entry higher frequency entry would evicted make room instead filtering immediately admission window give entry chance build popularity avoids consecutive miss especially case like sparse burst entry may deemed suitable longterm retention keep history fresh aging process performed periodically incrementally halve counter wtinylfu us segmented lru slru policy long term retention entry start probationary segment subsequent access promoted protected segment capped capacity protected segment full evicts probationary segment may trigger probationary entry discarded ensures entry small reuse interval hottest retained le often reused coldest become eligible eviction database search trace show lot opportunity improve upon lru taking account recency frequency advanced policy arc lir wtinylfu narrow gap provide near optimal hit rate additional workload see research paper try simulator trace experiment expiration policy expiration often implemented variable per entry expired entry evicted lazily due capacity constraint pollutes cache dead item sometimes scavenger thread used periodically sweep cache reclaim free space strategy tends work better ordering entry expiration time lg n priority queue due hiding cost user instead incurring penalty every read write operation caffeine take different approach observing often fixed duration preferred constraint allows organizing entry time ordered queue time live duration write order queue time idle duration access order queue cache reuse eviction policy queue concurrency mechanism described expired entry discarded cache maintenance phase concurrency concurrent access cache viewed difficult problem policy every access write shared state traditional solution guard cache single lock might improved lock striping splitting cache many smaller independent region unfortunately tends limited benefit due hot entry causing lock contented others contention becomes bottleneck next classic step update per entry metadata use either random sampling fifobased eviction policy technique great read performance poor write performance difficulty choosing good victim alternative borrow idea database theory writes scaled using commit log instead mutating data structure immediately update written log replayed asynchronous batch idea applied cache performing hash table operation recording operation buffer scheduling replay activity policy deemed necessary policy still guarded lock try lock precise shift contention onto appending log buffer instead caffeine separate buffer used cache read writes access recorded striped ring buffer stripe chosen thread specific hash number stripe grows contention detected ring buffer full asynchronous drain scheduled subsequent addition buffer discarded space becomes available access recorded due full buffer cached value still returned caller loss policy information meaningful impact wtinylfu able identify hot entry wish retain using threadspecific hash instead key hash cache avoids popular entry causing contention evenly spreading load case write traditional concurrent queue used every change schedule immediate drain data loss unacceptable still way optimize write buffer type buffer written multiple thread consumed single one given time multiple producer single consumer behavior allows simpler efficient algorithm employed buffer fine grained writes introduce race condition operation entry may recorded order insertion read update removal replayed order improperly handled policy could retain dangling reference solution state machine defining lifecycle entry benchmark cost buffer relatively cheap scale underlying hash table read scale linearly number cpu hash table throughput writes penalty contention updating hash table dominant cost conclusion many pragmatic topic covered could include trick minimize memory overhead testing technique retain quality complexity grows way analyze performance determine whether optimization worthwhile area practitioner must keep eye neglected difficult restore confidence one ability manage ensuing complexity design implementation caffeine result numerous insight hard work many contributor evolution year possible without help following people charles fry adam zell gil einziger roy friedman kevin bourrillion bob lee doug lea josh bloch bob lane nitsan wakart thomas meller dominic tootell louis wasserman vladimir blagojevic thanks nitsan wakart adam zell roy friedman chu feedback draft article related article
398,Lobsters,scaling,Scaling and architecture,Retrospective on Linodes holiday DDoS attacks,https://blog.linode.com/2016/01/29/christmas-ddos-retrospective/,retrospective linode holiday ddos attack,sign node newsletter view printable version post earlier update secondary address crossconnects lesson learned lesson two absorb larger attack lesson three let customer know happening status page future brighter past gigabit transit peering capacity final word online html editor,sign node newsletter view printable version post twelve day december january linode saw hundred denialofservice attack every major part infrastructure severely disrupting service hundred thousand linode customer like follow earlier update providing insight attacked stop happening pictured overview different infrastructure point attacked essentially attacker moved stack roughly order layer bad request attack toward publicfacing website volumetric attack toward website authoritative nameservers public service volumetric attack toward linode network infrastructure volumetric attack toward colocation provider network infrastructure attack simple volumetric attack volumetric attack common type distributed denialofservice ddos attack cannon garbage traffic directed toward ip address wiping intended victim internet virtual equivalent intentionally causing trafficjam using fleet rental car pervasiveness type attack caused hundred billion dollar economic loss globally typically linode see several dozen volumetric attack aimed toward customer day however attack almost never affect wider linode network tool use protect called remotetriggered blackholing ip address blackholed internet collectively agrees drop traffic destined ip address preventing good bad traffic reaching content network like linode hundred thousand ip blackholing blunt crucial weapon arsenal giving u ability cut finger save hand sacrifice customer attacked order keep others online blackholing fails effective mitigator one obvious important circumstance ip targeted say critical piece infrastructure go offline without taking others example usually come mind server server like api endpoint dns server make foundation infrastructure many attack server server hardest one u mitigate turned attack pointed directly toward colocation provider network infrastructure secondary address attack leveled network infrastructure relatively straightforward mitigating artifact history segment customer individual subnets meaning router must secondary ip address inside subnets customer use network gateway time gone router amassed hundred secondary address potential target attack course first time router attacked directly typically special measure taken send blackhole advertisement upstreams without blackholing core stopping attack allowing customer traffic pas usual however unprepared scenario someone rapidly unpredictably attacked many dozen different secondary ip router couple reason first mitigating attack network gear required manual intervention network engineer slow errorprone second upstream provider able accept limited number blackhole advertisement order limit potential damage case error several day playing catandmouse game attacker able work colocation provider either blackhole secondary address instead drop traffic edge transit provider network blackholing possible crossconnects attack targeting colocation provider straightforward even harder mitigate router longer able attacked directly colocation partner transit provider became next logical target specifically crossconnects crossconnect generally thought physical link two router internet side physical link need ip address two router communicate ip address targeted case infrastructure method attack novel made method effective rapidity unpredictability attack many datacenters dozen different ip within upstream network attacked requiring level focus coordination colocation partner transit provider difficult maintain longest outage far hour atlanta directly attributed frequent breakdown communication linode staff people sometimes fourdegrees removed u eventually able completely close attack vector stubborn transit provider finally acknowledged infrastructure attack successfully put measure place stop attack lesson learned personal level embarrassed something like could happened learned hard lesson experience lesson one depend middleman hindsight believe longer outage could avoided relying colocation partner ip transit two specific reason first several instance led believe colocation provider simply ip transit capacity actually several time amount attack traffic directed toward linode large colocation provider choice temporarily depeer linode network attack ended second successfully mitigating nuanced attack required direct involvement senior network engineer different tier provider holiday weekend colocation partner became extra unnecessary barrier people could fix problem lesson two absorb larger attack linode capacity management strategy ip transit simple peak daily utilization start approaching overall capacity time get link strategy standard carrier network understand inadequate content network like put real number smaller datacenter network total ip transit capacity may seem like lot capacity many context ddos blackholed worth headroom leaf u crippling packet loss duration attack lesson three let customer know happening important acknowledge fail lack detailed communication early day attack big failure providing detailed technical update time crisis done detailed knowledge current state affair usually people also one firefighting thing settled reviewed public communication came conclusion fear wording something poorly causing undue panic led u speak ambiguously status update wrong going forward designated technical pointperson responsible communicating detail major event like additionally status page allows customer alerted service issue email sm text messaging via subscribe update link future brighter past lesson mind like know putting practice first easy part mitigated threat attack publicfacing server implementing ddos mitigation nameservers protected cloudflare website protected powerful commercial traffic scrubbing appliance additionally made sure emergency mitigation technique put place holiday attack made permanent measure put u place confident type attack happened holiday happen still need today excited announce linode overhauling entire datacenter connectivity strategy backhauling gigabit transit peering capacity major regional point presence location carrier shown example purpose product name logo property respective owner overview forthcoming infrastructure improvement newark datacenter first receive capacity upgrade headliner architecture optical transport network already begun building network provide fully diverse path important pop region giving linode access hundred different carrier option thousand direct peering partner compared existing architecture benefit upgrade obvious taking control entire infrastructure right edge internet mean rather depending middleman ip transit direct partnership carrier depend service additionally linode quintuple amount bandwidth available u currently allowing u absorb extremely large ddos attack properly mitigated attack size grow future architecture quickly scale meet demand without major new capital investment final word lastly sincere apology order company host critical infrastructure customer trusted responsibility keeping infrastructure online hope transparency forwardthinking post regain trust would also like thank kind word understanding support many u holiday ruined relentless attack difficult thing try explain loved one support community really helped encourage post question comment always use online html editor compose content website easily
399,Lobsters,scaling,Scaling and architecture,Popular Off-the-Shelf Container Tooling,https://deis.com/blog/2016/container-tooling,popular offtheshelf container tooling,common us azure kubernetes service ak,common us azure kubernetes service ak migrate existing application cloud build complex application us machine learning take advantage agility offered microservices architecture
401,Lobsters,scaling,Scaling and architecture,Adventures in High Speed Networking on Azure,http://www.ageofascent.com/azure-cloud-high-speed-networking/,adventure high speed networking azure,serving million request per second gbps single azure vm age ascent million application message per second using core aspnet team smurf lab encouraging aspnet community standup found techempower benchmark test type plaintext simd net framework dual xeon ht core bounded latency receive side scaling wrk azure vm core techempower provide benchmark amazon w vcpus azure w core grand strategy mmo illyriad performance guaranteed virtual cpu forum discussion adventure high speed networking azure,serving million request per second gbps single azure vm illyriad game building new scale gaming age ascent ultrammo realtime twitch combat unprecedented scale also want instanton alwaysupdated experience rather waiting gigabyte downloads patch eschewing conventional wisdom high speed game client building javascript webgl rather cc directxopengl  tale another today talking server io throughput extreme end march held public scale test directpiloting concurrent rl player combat zone later ran automated load test throughput million application message per second using cores gave u substantially higher simulated player number however technology march looking revalidate assumption design choice little unconventional say least see thing today game server built like hft quant trading server also need dynamically scale repartition space fly based demand  dynamic scaling need good fit cloud pay use always capacity ready need whether burst sustained high throughput need prevailing wisdom would suggest need write server c use udp rather tcp run high spec bare metal box running linux already running client javascript ran tcp websockets wrote server code managed c ran window vm cloud madness aspnet team testing experimental server design smurf lab recently moved network quickly created http echo server based similar principle game server networking hoping validate whether inherent using managed c rather lower level closer metal language kindly agreed test initial feedback encouraging tweaking based feedback managed get single server performing peaking request per second whilst cpu showed weekly aspnet community standup video code shown video found extremely good performance would better base line compare luckily techempower benchmark looking two top performing server ulib cpollcppsp c linux really well however jumping conclusion caveat main one following rule test test type plaintext making change meet criterion test result follows connection pipeline depth bandwidth rps cpu mbps gbps gbps gbps gbps gbps gbps gbps gbps good coming close saturating nic additional change use simd available net framework hardware accelerated new ryujit compiler well result hopefully bring cpu network caveat testing full webserver although pas plain text requirement  however others full webservers certainly work per request  hand testing core xeon ht core whereas techempower benchmark running dual xeon ht core core play actual benchmark course numa node clock difference apply interesting direct comparison point think concluded window c good high performance networking possibly outperform best c linux far  deeper test reveal whether true translate cloud bearing mind max tcp throughput per connection bounded latency two vms connecting across datacenter perform worse two machine next directly connected via switch equally connecting east coast usa east coast usa higher bandwidth connection east coast usa europe ignoring terrible last mile connectivity wifi packet dropping reason interestingly west u east u east u europe first thing first make sure receive side scaling r switched virtual network adapter sure always default lower latency low bandwidth situation trying simd server window linux wrk load generator test earlier azure vm core give following network cpu window vm sending cpu good bandwidth even bare metal machine let alone vm plenty cpu thing looking linux measurement translates million request per second outperforming top gbe directly connected ht core peak test exceptional performance request every nano second micro second request millisecond likeforlike cloud comparison luckily techempower provide benchmark techempower benchmark server run amazon w vcpus look close enough azure w core azure vm slightly cheaper also slightly le ram get azure window vm make sure r switched using cpu respectable plenty cpu available  guessing bandwidth capped split vms base hardware fairly small affordable vm equate switching linux side get request per second come request per second looking chart exceptional performance latency avg stddev max flipping latency graph techempower test including top server throughput give result exactly sure interpret looking regular website regular current window net web stack inprogress refresh current grand strategy mmo illyriad go empty cache browser request europe east coast u back twice total distance mile though load balancer regular aspnet rather leaner newer aspnet azure cloud web page complete kb website faviconico ignored something wrong latency simple world response byte presumably vms datacenter prior applying recent learning testing shall obviously go bigger smaller cloud cheaper end basic vms feature standard counterpart believe basic cpu provisioned case outright shared cpu series preferred option faster cpu series memory local ssds core gb gb ssd disk bound d series series ability attach premium storage rdma attached ssd tb iop raid configuration w read use d series premium storage great hence testing however need exotic option hpc work want explicit cpu rather performance guaranteed virtual cpu use vms ghz vms additionally gbps infiniband network rdma technology fast interconnected cluster extreme server end core beast gb ram gb local ssd quick ootb test window azure virtual machine linux azure vm give result gbps cpu need multicore optimization looking linux wrk server request per second latency avg m stddev max however since cloud smaller machine give better scale inout granularity ymmv depending kind workload also production want give headroom burst rather trying max connection also kinder latency going continue testing setup variety configuration keep posted result far believe validates compromising performance running azure using window managed c get best performance also developer productivity takehome learning test far  doubt measure measure measure even think know performance generally lost unexpected place discus forum forum discussion adventure high speed networking azure
402,Lobsters,scaling,Scaling and architecture,Building An Infinitely Scaleable Online Recording Campaign For David Guetta,https://serverlesscode.com/post/david-guetta-online-recording-with-lambda/,building infinitely scaleable online recording campaign david guetta,parallax parallax interview parallax thisonesforyoucom serverless vanamco device lab testdroid wrapping site gitterim chatroom disclosure r mailing list ryan serverlesscodecom tweet send hackernews post reddit,paging david guetta fan week interview team built site behind latest ad campaign site fan record singing along single one build album cover go hood site built lambda api gateway cloudfront social campaign tend pretty spiky lot press stampede user bring infrastructure crawl ready team parallax chose lambda longlived server could offload work scaling app demand amazon james hall parallax going tell u built internationalized app handle level demand nothing six week interview parallax tell company size specialty etc parallax digital agency provide range service including application development security design service fulltime employee well external contractor specialise providing massively scaleable solution particular focus sport advertising fmcg fast moving consumer good tell little app problem solving app part huge marketing campaign david guetta new release one official anthem uefa euro final inviting fan hopefully million march virtual recording studio giving chance sing along guetta voice included final song also create personalised album artwork name favourite team shared among friend increasing engagement allowing fan become part campaign whole site built twelve language incorporating video content dj chance win trip paris check thisonesforyoucom lambda technology app using includes frontend mobile database whatever share image credit parallax agency ltd backend use variety technology deliver completely scaleable architecture use serverless formerly jaw cloudformation orchestrate entire platform code request routed via cloudfront static asset cached edge location nearest user requesting page first load everything entirely static uuid generated client browser used future api request allows u associate different action page together without serve cooky randomness value important library us timing crypto function available derive random seed data image credit amazon web service origin static asset simple bucket uploaded via script deployment language detection endpoint sends back acceptlanguage header also country code request received using basic lambda function another endpoint add subscriber data dynamodb well sending welcome email via amazon s make recording work endpoint issue token path named uuid provided uploads directly posted browser useful application lambda image generation endpoint take image user favourite team flag overlay picture guetta add name uploaded along facebook twitter sized graphic also upload static html file point unique graphic ensures people share url display custom artwork use open graph image tag page frontend used brunch compile handlebar template compile sas prefix cs build task recording interface three implementation agrade experience webrtc recorder us new functionality record directly microphone flash fallback experience desktop browser without webrtc support finally file input prompt user record mobile solution considered yes option decision use lambda made yes absolutely regular stack lamp built top cloudfront elastic load balancer node would scaled would much difficult scale quickly simply using lambdabased architecture build queueing system image generation spin node dedicated making image based throughput writing simple lambda function letting amazon hard work seemed like obvious choice large team anyall experience lambda already coming area expertise team parallax consisted five people tom faller account manager responsible daytoday running project backend developer creating lambda function designing cloud architecture amit singh primarily frontend worked many glue part interface backend j chris mill qa system linked serverless formerly jaw project first place jamie sefton another j developer worked integrating compatibility fallback codebase including flashbased inputbased recording experience fallback device support webrtc experience lambda new team member long spend developing app faster writing another framework express rail whatever home turf say around six seven week total though plenty research prototyping done beforehand arrive correct architecture design amount scalability required would taken much longer ensure robust using home turf deploying application using cicd service application deployed atlassian bamboo codebase life stash every branch get deployment url posted company chat built let u quickly test decide change mergeable image credit parallax agency ltd monitoring place anything want monitor dontca nt yet frontend j error use bugsnag type application easiest way see problem anywhere stack error spike bigger traffic spike know something gone wonky usually use newrelic backend backend code life lambda function opted use cloudwatch testing change go production testingstaging environment yeah absolutely every commit branch pushed stash deployed testing environment static asset frontend javascript us bamboo creates new folder every branch build number lambda function updated environment using serverless dash anything surprised along way certain task easier harder expected server lambda run lack font japanese chinese character expected something planned development operating system render particular font contain particular glyph fallback system installed multilingual unicode font happens web automatically inside imagemagick bare server happen mean ship large unicode font endpoint reduced performance impact routing nonlatin name unicode endpoint helvetica example include asian glyph arial pain point development process found would want fix issue multiple branch early serverless formerly jaw framework say adding endpoint one branch another another branch deployed environment stage something worked around looking contribute helpful tool back serverless deal tool library found particularly helpful like people know couple testing tool highly recommend camera microphone behave differently emulator sometimes even simulated mean use real device testing used vanamco device lab along five device consider good spread use alongside ghostlab allows u page open device keep sync includes web inspector tweak cs run debugging j increase android coverage used testdroid brilliant service allows use real device remotely open camera app see inside data centre eagerly awaiting testdroid engineer poke head rack far disappointed testdroid using google nexus wrapping thanks james hall inside view ci mobile testing unicode workarounds alllambda backends learn serverless application framework check site gitterim chatroom disclosure relationship parallax agency ltd build cool project interview cover one keep future post via r subscribe mailing list suggestion question comment feel free email ryan serverlesscodecom tweet send hackernews post reddit
404,Lobsters,scaling,Scaling and architecture,Evolving from Machines to Services,http://blog.raintank.io/evolving-from-machines-to-services/,evolving machine service,,
405,Lobsters,scaling,Scaling and architecture,Fast Threaded Polling,http://geocar.sdf1.org/fast-servers.html,fast threaded polling,fastservers epoll kqueue creating thread pool creating listening socket acceptloop requestloop,fastservers networkserver programming pattern popular canonical approach towards writing network server design easy recognise main loop wait event dispatch based file descriptor state file descriptor one point vogue actually fork file descriptor could handled different thread worker thread usually created perform task rely kernel schedule file descriptor much better design possible epoll kqueue however people use new system call using wrapper like libevent encourages slow design people using twenty year design currently use recommend involves two major point one thread per core pinned affinity separate cpu epollkqueue fd major state transition accept reader handled separate thread transitioning one client one state another involves passing file descriptor epollkqueue fd thread design decision point simple blockingio call make simple onepage performant server easily get requestssecond territory modern system creating thread pool ask operating system many core sometimes reserving core make sense let user lower number raising number help state transition complex need break pthreadattrt pthreadattrinit pthreadattrsetscope pthreadscopesystem pthreadattrsetdetachstate pthreadcreatedetached tsysconf scnprocessorsonln pthreadcreate id void run void busy pthreadmutexlock tm pthreadcondwait tc tm pthreadmutexunlock tm thread perthread initialisation allocate keventepoll fd void run int id setaffinity id pthreadmutexlock tm ifdef linux worker id else worker id qkqueue endif pthreadmutexunlock tm pthreadcondsignal tc setting processor affinity something must done inprocess platform system administrator able provide input cpusett c cpuzero c cpuset id c pthreadsetaffinitynp pthreadself sizeof c c apple osx nt support pthreadsetaffinitynp directly need easy implement extern int threadpolicyset threadt thread threadpolicyflavort flavor threadpolicyt policyinfo machmsgtypenumbert count threadaffinitypolicydatat ap threadextendedpolicydatat ep eptimesharefalse threadpolicyset machthreadself threadextendedpolicy threadpolicyt ep threadextendedpolicycount threadpolicyset machthreadself threadextendedpolicy threadpolicyt ap threadextendedpolicycount creating listening socket increase number file descriptor handle number connection want handle per thread getrlimit rlimitnofile r rrlimcurn rrlimcurn p setrlimit rlimitnofile r oops setrlimit disabling lingering important otherwise run file descriptor ssocket sinsinfamilyafinet sockstream ipprototcp setsockopt solsocket solinger lf sizeof lf listen n client speaks first http enable deferred accepts linux ifdef linux setsockopt soltcp tcpdeferaccept sizeof endif acceptloop point waiting epollkevent since loop accept connection faccept qpick ifdef linux struct epollevent ev evdatafdf eveventsepollinepollrdhupepollerrepollet epollctl q epollctladd f ev else struct kevent ev evset ev f evfiltread nm null kevent q endif socket option enabled handing next step consider enabling timeout sorcvtimeo socket instead tracking timer application struct timeval tv setsockopt f solsocket sorcvtimeo tv sizeof tv fcntl f fsetfl ononblock hasan alayli observes linux use combine accept fcntl scheduling task usually done rotating thread int pick void static int c c return worker c q however workload benefit analysis choosing worker based probability input belong one type another actually improve mean throughput bias introduced pick routine experiment benchmark requestloop task input begin epollwait kevent step ifdef linux struct epollevent e epollwait q e event epollrdhupepollhup close e datafd else handle e datafd else struct kevent e kevent e flag eveof close e ident else handle e ident endif file descriptor used single request single state array input buffer file descriptor simplify lot algorithm handle fd read input use write sendfile necessary however one syscall needed schedule task worker performs operation
408,Lobsters,scaling,Scaling and architecture,Development and Deployment of Massively Multiplayer Games,http://ithare.com/contents-of-development-and-deployment-of-massively-multiplayer-games-from-social-games-to-mmofps-with-stock-exchanges-in-between/,development deployment massively multiplayer game,bug hare post k indie book wip table content part arch architecture obsolete beta available online business requirement game entity interaction cheating non authoritative server protocol rtt input lag mitigation protocol world state reducing traffic protocol pointtopoint communication nonblocking rpcs protocol idl encoding mapping backward compatibility diy v reuse search balance clientside graphic clientside programming language clientside debugging distributed system deterministic logic finite state machine clientside client architecture diagram thread game loop unity v v photon v diy scalability mogs scaling stateful object serverside nave webbased classical deployment architecture serverside frontend server clientside random load balancing serverside eternal windowsvslinux debate serverside asynchronous processing finite state machinesactors plain event processing future oo lambda call pyramid serverside programming language precoding checklist thing everybody hate everybody need source control coding guideline vol current available sale amazon preface motivation behind book acknowledgement book book book byos bring salt recommended reading chapter game design document mog perspective passionate game todolater doublecheck size crash course firsttime game developer three allimportant gdd rule limitedlifespan v undefinedlifespan game clientdriven v serverdriven development workflow matchmaking social aspect mog technical issue affecting marketing monetization gdd requirement list additional mogspecific team running cost breakdown common gdd pitfall throw multiplayer free game entity interaction chapter cheating non authoritative server popular enough find reason cheat big fat hairy difference ecommerce dealing cheater authoritative client pretty much hopeless cheater deterministic lockstep game rule violation wideopen information leak authoritative server cheaterproof go bottom line yes going authoritative server chapter communication communication rtt input lag mitigate game world state reducing traffic mog architecting scalability broadcasted message pointtopoint communication nonblocking rpcs idl encoding mapping backward compatibility vol ii current beta available backer early draft indiegogo leanpub chapter diy v reuse search balance business perspective diy added value enginecentric approach absolute dependency aka vendor lockin reuse everything sight approach integration nightmare diy everything risk neverending story responsible reuse approach search balance summary chapter reactorfest architecture got event loop got event loop got event loop terminology reactor eventdriven program game loop adhoc final state machine use oo rpc framework game loop game programming classic event loop generalization game loop two allimportant improvement classical eventdriven programming mostlynonblocking processing determinism nonblocking processing determinism divide et impera split reactor reactor exception validatecalculatemodify pattern scaling reactor reactorfest architecture summary chapter appendix va cspecific example comment chapter chapter clientside architecture graphic generic architecture reactorfest clientside architecture programming language game client bottom line chapter chapter clientdriven development unity ue lumberyard party network library clientdriven v serverdriven development workflow popular party game engine vol iii current beta available backer early draft indiegogo leanpub chapter scalability inmemory state multiplayer game scalability load balancing chapter serverside mog architecture chapter fault tolerance failure mode effect chapter precoding checklist thing everybody hate everybody need source control coding guideline source control continuous integration library licensing development process issue tracking system coding guideline continued part prog ramming stats current version vol iiii part prog programming vol iv current beta available online chapter thing keep mind writing crossplatform coding security chapter network programming chapter marshalling encoding encoding requirement chapter basic security logins password war clone ipbased non identification identifying pc mac identifying mobile device vol v current beta available online vol vi current beta available online chapter random number generation psychological aspect rng chapter payment processing credit card two acrosstheboard caveat indirect processing chapter testing debugging logging personal take testing production crash part depl deployment vol vii beta progress vol viii planned chapter bot fighting chapter player abuse chapter protecting internal abuse vol ix planned chapter multicurrency chapter optimization scaling chapter deployment take chapter security take chapter theme modding tool conclusion want book posted site betatesting scope whether writing engine choosing existing one need book focused massively multiplayer game applies lanbased game genre social game mmofps stock exchange topic gameplayaimonetization large fit therefore sketchy physic prerequisite cd included byos bring salt poll edit beta chapter business requirement published beta chapter shown page toc published acknowledgement gordeev animation graphic,author bug hare follow job title sarcastic architecthobbies thinking aloud arguing manager annoying hr calling spade spade keeping tongue cheek table content also scope section upcoming book currently betatested detail beta testing please refer post content published beta testing subject change book published update book grew soooo large decided split volume page instead three mean plan print content planned original new volivoliii content planned original new vol ivvolvi content intended original vol new vol viivolix backed book kickstarter indiegogo please see relevant update k indie book wip development deployment multiplayer online game ddmog short consists part arch itecture prog ramming depl oyment part planned volume current status table content part arch architecture obsolete beta available online chapter business requirement chapter b game entity interaction chapter cheating non authoritative server chapter protocol rtt input lag mitigation chapter b protocol world state reducing traffic chapter c protocol pointtopoint communication nonblocking rpcs chapter protocol idl encoding mapping backward compatibilitychapter diy v reuse search balance chapter clientside graphic chapter b clientside programming language chapter c clientside debugging distributed system deterministic logic finite state machine chapter clientside client architecture diagram thread game loop chapter unity v v photon v diy chapter scalability mogs chapter scaling stateful object chapter serverside nave webbased classical deployment architecture chapter b serverside frontend server clientside random load balancing chapter c serverside eternal windowsvslinux debate chapter serverside asynchronous processing finite state machinesactors plain event processing future oo lambda call pyramid chapter e serverside programming language chapter precoding checklist thing everybody hate everybody need source control coding guideline vol current available sale amazon preface motivation behind book acknowledgement comment helped shape book book book stateful distributed system general genre social game mmofps stock exchange stock exchange game even worse betting game topic gameplayaimonetizationphysics large fit therefore sketchy focused internetbased game aiming millionsofsimultaneousplayers part apply lanbased game game engine diy v reuse v book prerequisite captain obvious hat cd included byos bring salt recommended reading programming general game programming really networkrelated network programming gamerelated game network programming c new c experienced c potentially needing upgrade security chapter game design document mog perspective passionate game todolater doublecheck size crash course firsttime game developer gdd subject change seven day week overgeneric fallacy project stakeholder focus testing playtesting marketing monetization stakeholder availability summary project stakeholder typical nonmog team structure timetomarket mvp planning importance holding horse three allimportant gdd rule limitedlifespan v undefinedlifespan game clientdriven v serverdriven development workflow serverdriven workflow clientdriven workflow dealing clientdriven workflow matchmaking social aspect mog technical issue affecting marketing monetization gdd requirement list additional mogspecific team network team backend team aka server team network team backend team must firstclass citizen running cost breakdown common gdd pitfall throw multiplayer free game entity interaction game entity dealing interaction game entity get entity interaction diagram example entity interaction social farming farminglike game casino multiplayer game stock exchange sport betting auction site large virtual world game mmotbsmmortsmmorpgmmofps team competition esports entity interaction diagram starting point architect game chapter cheating non authoritative server popular enough find reason cheat big fat hairy difference ecommerce dealing cheater attack really big advantage home turf published attack higher impact home turf advantage regained lowimpact highimpact attack legal stuff problem banning cheat game rule violation information leak reflex augmentation abuse disconnect handling grinding bot multiple player account classical attack db attack stealing source code password phishing keyloggerstrojansbackdoors another player device ddos mog attack type summary authoritative client pretty much hopeless cheater code signing work hostile environment web trust might work theory several big buts mogs lead doomsday scenario crosscheck latency latency consensus actually majority vote even latency homomorphic encryption even start fly authoritative client mog summary deterministic lockstep game rule violation wideopen information leak authoritative server cheaterproof go authoritative server scalability imperfect workable example calculation summary authoritative server ideal thing workable bottom line yes going authoritative server think positive maybe still every bit count multilayer protection chapter communication communication rtt input lag mitigate data flow diagram take input lag worst nightmare mog developer input lag user expectation input lag much left mog accounting packet loss jitter internet packetbased packet lost cutting overhead clientside serverside buffering receipt tcp tcp buffering tcp retransmits delay rtt input lag taking bit back data flow diagram take fastpaced game specific rtt lan rtt v internet rtt cdns geo server distribution rtt player back input lag data flow diagram take clientside prediction interpolation clientside interpolation clientside extrapolation aka dead reckoning running wall server reconciliation clientside prediction clientside prediction dealing discrepancy take diagram lag compensation potential cheating v player happiness server rewind subtracting client rtt serverside lag compensation inherently open cheating otoh player happiness much important many option one need game world state reducing traffic serverside publishable clientside game world state clientside state serverside state publishable state keep nonsim game summary publishable state delivery update interest management compression interest management traffic optimization preventing cheating interest management way prevent information leak cheating frustumbased interest management compression minimizing data compression delta compression two flavour delta compression delta compression generalization arbitrary tree delta compression arbitrary tree collecting update fly dead reckoning compression dead reckoning compression variation classical compression combining different compression mechanism law diminishing return double lossless compression adaptive traffic management adaptive traffic management tcp adaptive traffic management udp traffic optimization recommendation traffic realtime strategy mog architecting scalability obvious one separate npcai splitting area seamless world overlap serverside uncertainty broadcasted message pointtopoint communication nonblocking rpcs rpcs implementing nonblocking rpcs void v nonvoid nonblocking rpcs clienttoserver servertoclient pointtopoint communication input macroscopic client action servertoclient servertoserver communication time synchronization synchronization via server rewind value date synchronization without rewind cmslbts seamless handling transient disconnect option separate callercallee handling option two guaranteed delivery stream going interdb async transfer transactional integrity serverside entity addressing using message queue servertoserver communication mqs transactional integrity us mq serverside brokered v brokerless serverside tcp often win udp idl encoding mapping backward compatibility idl development flow idl encoding mapping example idl example mapping mapping existing class example encoding backward compatibility implementing idl specific encoding vol ii current beta available backer early draft indiegogo leanpub chapter diy v reuse search balance business perspective diy added value enginecentric approach absolute dependency aka vendor lockin implication vendor lockin enginecentric approach pretty much inevitable indie mmorpgmmofps enginecentric approach still need understand work enginecentric approach temporary dependency reuse everything sight approach integration nightmare diy everything risk neverending story responsible reuse approach search balance responsible reuse example responsible reuse temporary dependency summary chapter reactorfest architecture got event loop got event loop got event loop terminology reactor eventdriven program game loop adhoc final state machine use oo rpc framework game loop game programming classic event loop generalization game loop react react reactor stay away thread sync game logic eventdriven system gui erlang nodejs java reactor separating infrastructure code logic code advantage reactor reactor game engine two allimportant improvement classical eventdriven programming mostlynonblocking processing determinism nonblocking processing block block question mostlynonblocking reactor case processing call progress required logic level case processing logic level call progress blocking nonblocking mostlynonblocking implementing nonblocking processing game timer nonblocking state publishing pointtopoint communication nonblocking stuff handling nonblocking return reactor take nave approach plain message work plain ugly take voidonly rpcs tiny bit better still really ugly take oostyle le errorprone still way much boilerplate l exception cascading exception handler take lambda continuation callback pyramid continuation cascaded exception handling lambda style take basic future difference std future boost future folly future etc take summary take code builder take enter c preprocessor offloading offloading caveat keeping portion large offload unless proven necessary another offloading caveat flow control take fiberscoroutines boost coroutines boost context goroutines beware thread sync take asyncawait net j proposal surprise different take functionally equivalent close performancewise similarity nodejs handling nonblocking return different programming language serializing reactor state serializable lambda closure c much discussion one thing tl dr nonblocking communication reactor determinism distributed system debugging nightmare nondeterministic test pointless holy grail post mortem portability platformindependent logic nothing moving bit around stronger platformindependent determinism deterministic logic benefit replaybased regression testing patch fuzz testing deterministic logic user replay implementing deterministic logic deterministic logic mode implementing inputslog recordablereplayable infrastructureeventhandler implementing deterministic logic dealing nondeterminism due system call dealing system call original nondeterministic code dealing system call wrapping dealing system call pure logic dealing system call input parameter reactor data member dealing system call tlsbased compromise dealing system call pool ondemand data dealing system call ondemand data via exception dealing system call system function speaking implementing deterministic logic source nondeterminism access nonconst globals tl pointer thread implementing deterministic logic nonissues prng loggingtracing caching isolation perimeter implementing deterministic logic crossplatform determinism achieving crossplatform determinism implementing deterministic logic summary type determinism v deterministic goody relation deterministic reactor deterministic finite automaton deterministic finite state machine nothing new tl dr determinism section divide et impera split hare hair reactor composition reactorwithinreactor state pattern getting rid big ugly switch common data member potentially expensive allocation hierarchical state stackofstates reactor exception validatecalculatemodify pattern validatecalculatemodify pattern exception modification stage safe including cpu exception raii equivalent different programming language posting message calling rpcs etc within validatecalculate exception safety modify stage validatecalculatemodifysimulate validatecalculatemodify summary scaling reactor splitting offloading reactorwithmirroredstate limited relief reactorwithextractors reactorfest architecture reactor factory reactor programming language relation reactorfest system relation actor concurrency relation erlang concurrency akka actor nodejs reactor microservices close cousin physical server vm docker reactor spectrum tradeoff isolation flexibility summary chapter appendix va cspecific example comment chapter avoiding expensive allocation c enforcing constness validatecalculatemodify chapter clientside architecture graphic developer game designer artist using game engine pure graphic engine vendor lockin type graphic game rudimentary graphic game graphic prerendered game graphic generic architecture logictographics api dual graphic including graphic module relationship game logic module game logic module graphic game logic module clientside prediction simulation game logic module game loop game logic module keeping crossplatform animation rendering module communication module sound module relation mvc difference classical singleplayer game interaction example world singleplayer v mog mmofps interaction example shooting mmorpg interaction example ragdoll ui interaction example reactorfest clientside architecture reactorfest client architecture reactor specific animation rendering reactor game loop communication reactor blockingnonblocking socket reactor reactor latency variation code base different platform scaling reactorfest architecture client parallelizing clientside reactor summary reactorfest architecture clientside programming language game client one language programmer another game designer mmorpgmmofps etc word cuda opencl different language provide different protection bot writer resilience reverse engineering different programming language compiled language language compile bytecode interpreted language compilerswithunusualtargets summary language availability game clientside platform garbage collection stoptheworld problem gc gc consistency clientside serverside language sprinkle usual consideration c default game programming language c crossplatform library big fat browser problem linetoline translation code base linetoline translation practical clientonserver trick bottom line chapter chapter clientdriven development unity ue lumberyard network library clientdriven v serverdriven development workflow serverdriven development workflow clientdriven development flow implementing clientdriven workflow singleplayer prototype subsequent conversion important clarification development workflow v data flow popular game engine unity eventdriven programmingreactors builtin communication hlapi state synchronization rpcs aka remote action hlapi summary builtin communication llapi communication unity photon server photon server sdk photon cloud pun communication unity smartfoxserver communication unity ulink communication unity darkrift communication unity lowerlevel library unity summary option using client logic server hlapi probably llapi later option b export unity standalone server option one better unreal engine eventdriven programmingreactors ue mog ue communication close unity hlapi ue communication lowerlevel cc library reliable udp library event handling library socket wrapper library summary option b amazon lumberyard comparison table summary vol iii current beta available backer early draft indiegogo leanpub chapter scalability inmemory state multiplayer game bug rule thumb multiplayer game exception stock exchange applicable singleplayer game inmemory state natural fit bug rule thumb data consistency inmemory state summary scalability load balancing scaling way jose scaling scaling stateful object myth statelessonly scalability kinda stateless scalability stateful scalability load balancing load balancing stateful object two flavor load balancing chapter serverside mog architecture chapter fault tolerance failure mode effect communication failure game server failure containment game world server failure server fault tolerance king dead long live king cluster ha ft faulttolerant server damn expensive faulttolerant vms virtual lockstep available anymore checkpointbased fault tolerance latency even latency diy faulttolerance reactorfest architecture diy virtual lockstep diy reactorbased redundancy diy fault tolerance connection ip diy fault tolerance use diy fault tolerance case almostdeterminism db server failure number nothing else adding fault tolerance make number worse exception stock exchange implementing fault tolerance db fault tolerance failure detection chapter precoding checklist thing everybody hate everybody need source control coding guideline source control git git unmergeable file git library git flow protecting code reducing chance firewall antiviruses mitigating impact compromise continuous integration library licensing development process issue tracking system issue tracking bypassing allowed coding guideline naming convention project peculiarity persubproject guideline enforcement static analysis tool continued part prog ramming stats current version vol iiii word page content added compared beta half content beta rewritten subjective overall content beta new compared beta part prog programming vol iv current beta available online chapter thing keep mind writing crossplatform error handling writing debugging asserts assert postmortem analysis text logging eventdriven recordingreplay coding security trust client sanitize sanitize fieldlevel sanitization interfield sanitization developforsecurity best practice coding translation implicit resource pseudolocalization fake language userentered string font currencynumberdate formatting collating sequence testing part development process unit testing tdd regression testing continuous integration replaybased regression testing simulation testing simulating player simulating network problem bottom line testing chapter network programming chapter marshalling encoding encoding requirement existing encoding xml json google protocol buffer plain c structure endianness endianness day almost exclusively littleendian endiannessagnostic code endianness cc alignment flatbuffers zerocopy protocol notyetexisting encoding bitoriented stream designing bitstream protocol huffman huffmanlike coding optimizing huffman speedwise using huffman coding bitstream bitstreams extensibility comparison different encoding string encoding string mapping string encoding chapter basic security logins password backend logins v player logins player logins social logins player internal id visible id login id visible id censorship avatar censorship diy player logins diy player logins registration form diy player logins clientweb logins diy player logins remember diy logins player backend diy logins weak password diy logins rate limit lockout diy logins forced password change diy logins password hashing diy logins password recovery backend logins access permission player logins loginless spectator war clone ipbased non identification identifying pc mac false negative false positive identification identifying device hidden cryptoid mac address good identifier unbans identifying pc wmi system fingerprinting identifying apple mac virtual machine identifying mobile device identification io identifying io device accessing user social data io identification android identifying browser notreallytechnical identification social identification email paymentbased identification putting together nothing reliable use everything get hand said play fire everybody make small mistake log everything bit realworld interpeople relationship autobans todo key management todo heresy security obscurity necessarily bad vol v current beta available online vol vi current beta available online chapter random number generation psychological aspect rng inherent difficulty finding rng fault three rng spectacular failure two big fat problem crypto rngs numerous problem realworld gamecritical rng qualifies good rng testing bit stream marsaglia diehard nist testing beyond bit stream chi square obtaining random bit stream prngs hardware rngs prngs linear congruential generator mersenne twister reversibility potentially mortal sin exception potential case noncryptosecure rngs noncryptosecure rngs blumblumshub poor man crypto prng aesctr determinism prngs true rngs physicsbased hardware rngs ongoing reseeding fortuna devurandom recommendation bit stream bit stream real world bit stream random number range shuffling chisquare testing dealing perception problem yet another realworld horror story chapter payment processing credit card card present subscription microtransactions merchant account big headache overtheinternet processing merchantacquiring bank issuing bank life cycle credit card transaction every merchant nightmare chargebacks chargeback mechanism heavily skewed favour consumer honest mistake penalty chargebacks reason behind even perfectly honest chargebacks todo secure verified visa etc refund instead chargebacks fraud prevention chargebacks v collateral damage two acrosstheboard caveat data flow payment processing direct processing recovery unknown transaction state main caveat direct processing trusting merchant pci ds complying pci ds architecting pci ds indirect processing clientcentric processing logging logging sensitive information pci ds reconciliation concluding chapter chapter testing debugging logging personal take testing automated regression testing good common wisdom aka popular misperceptions unit testing code coverage tdd atdd bdd automated testing actor scriptdriven testing replaybased regression testing simulation testing summary personal recommendation testing production crash implementing postfactum debugging option text logging option deterministic actor postfactum debugging getting log client backdoor whatsoever part depl deployment vol vii beta progress vol viii planned chapter bot fighting security obscurity difference programming language passive method sensitive text window control serverside statistic clientside detection captcha others chapter player abuse rule enforceable v nonenforceable attack brute force ddos appspecific disconnect abuse payment abuse multiple account identifying computeruser promotion promotion abuse chat abuse item traded secondary market ebay etc chapter protecting internal abuse active v reactive attack vector needtoknowneedtodo logging everything role hierarchy limiteddepth tree vol ix planned chapter multicurrency phraselevel translation translation engine separating translator developer dealing layout multicurrency multicurrency payment casino stock exchange multicurrency account margin margin currencyexchange abuse chapter optimization scaling otherthandb optimization scaling db optimization scaling physical db layout async replica aggregate scaling federated database split database chapter deployment take money object curse disguise cluster sans ddos protection reserve datacenters chapter security take twofactor authentication big problem remote admin laptop byod internal people general hardening security perimeter id chapter theme modding tool intentionalunintentional abuse advantage explicit support tool dlls v process conclusion good development deployment practice make game successful bad one easily kill interrupt beta book ask question want book posted site betatesting start beginning chapter vote skip preliminary part start right coding part b chapter ix vote nt care business requirement skip chapter start chapter ii vote forget wo nt read anyway vote total voter loading edit poll closed back author author book bug hare warren bunnylore known columnist overload journal issn significant contribution software development blog itharecom bug rabbit mother tongue lapine needed somebody translate book human language course book highly technical translate technical detail highest possible fidelity needed translator substantial development experience translator book translated lapine language sergey ignatchenko software architect since known writing industry journal since article appearing cuj overload c report secure magazine knowledge lapine quite extensive routinely translating column bug writes overload sergey software architecting career leading quite project including coarchitect stock exchange software used stock exchange several country sole original architect major gaming site hundred million user transaction per day processing hundred million dollar per year kind paid hobby also inventing thing author coauthor dozen patent unfortunately owned respective employer illustrator illustration book made sergey gordeev gagltdeu professional animator award various animation festival known director animated mr bean series scope whenever open book first time naturally two question book book first let see whether book want use game engine whether writing engine choosing existing one need book one thing never blindly believing engine perfect fit specific gamein many case may tempting use existing usually game engine rather develop one quite often may really good idea however one thing never developing anythingmorecomplicatedthantwoplayertictactoe blindly believing engine perfect fit specific game even game engine used dozen highly successful game guarantee work specific requirement unless course making clone one game instead assuming engine everything exactly want try understand relevant implication engineyou reabouttochoose see limitation peculiarity affect badly road book useful compare existing engine describe explain principle necessary build technically successful multiplayer game mean able make informed decision game engine maybe decide write part complement existing engine going choose existing game engine need understand game engine work choose game engine suitable need even grow million player going write game engine certainly need understand game engine work mean whatever developing massively multiplayer game need book matter discus pro contra argument diy reusing stuff chapter todo focused massively multiplayer game applies lanbased game write lanbased game book still applies make sure read appendixnow let try answer question book title suggests particular book massively multiplayer game working internet mean singleplayer game certainly focus however lanbased multiplayer game player book relevant certain extent certainly build lanbased game along line described book work wayoptimizedformassivelymultiplayerenvironments may involve quite overkill turn may lead unnecessary delay releasing game write lanbased game book still applies make sure read appendix applicability lanbased game discussion potential simplification may apply beta notice point appendix tentative guaranteed whenever saying massively multiplayer game mean game able reach really massive number following advice book including part million player perfectly doable personally architected site hundred thousand simultaneous player handled hundred million user transaction per purpose user transaction interaction user server cause serverside change change visible least player genre social game mmofps stock exchange come game genre book aim cover long multiple player involved game vary term requirement imposed engine requirement still interrelated moreover requirement represent moreorless continuous spectrum breaking given point would quite arbitrary let take look latency tolerance aka acceptable input lag one important requirement multiplayer game engine perspective social game usually facebookphone typical latency tolerance usually minute ok mmo turnbased strategy typical latency tolerance second multiplayer casino game bingo blackjack poker typical latency tolerance singledigit second stock exchange sport betting typical latency tolerance fast possible though important fair requirement massively multiplayer online realtime strategy mmortss typical latency tolerance low tripledigit millisecond massively multiplayer online roleplaying game mmorpgs typical latency tolerance millisecond massively multiplayer online firstperson shooter mmofpss typical latency tolerance high doubledigit millisecond see whole spectrum latency requirement minute social game high doubledigit millisecond mmofps pretty much everything however see soon take latency account difference genre become much le pronounced multiplayer engine perspective typical value provided mileage may vary battery included also note speaking endtoend latencyinput lag controller button pressed player seeing result screen lot different factor eat millisecond allocation even chance look data see discussion chapter todo topic gameplayaimonetization answer question want try answer question whateveryouwanttodo game development deployment huge task important realize exactly want cover book ambitious regard aim cover aspect game development deployment one though allimportant exception book try answer question game game look make money game allimportant business question need answer starting development know exactly want game played want look ai applicable work going monetize question completely scope hand soon answer question book got covered discus pretty much everything need release game keep running overall architecture deployment postdeployment issue word answer question want try answer question whateveryouwanttodo much detail fit one single book large fit therefore sketchy physic aiming answer question related whateveryouwanttodo issue large fit one single book result issue admittedly sketchy book chapter graphic sound discus physic engine please expect detailed topic book dedicated engine design included sufficient choose engine often work physic engine however want write engine need go beyond book provide necessary pointer within appropriate chapter course vision change development go point clear vision want achieve note business question monetization covered technical question payment method covered extent prerequisite game project first programming project likely difficulty understanding bookthis book targeted towards least somewhat experienced developer word develop first program book ide screenshots copypaste example game project first programming project likely difficulty understanding example going explain function database etc also expect know source control necessary encapsulation good thing tm hand book rely indepth knowledge specific area need network guru know every tiny detail rfc heart neither need handon experience shaders andor cuda even le expect c guru capable writing boostlike template honest moment last one beyond capability course graphic experience may helpful game knowledge network basic socket hurt case whenever discussing issue go beyond thing every developer know anyway try provide pointer read specific stuff happen idea feel free read book even case complain much turn difficult cd included book going associated cd first attempt provide cdwithsamplecode almost inevitably end platformspecific code one main point book platform depends need providing heavily platformspecific code illustrate point kind oxymoron second mentioned book intended bookwhereyoucancopypasteyourveryfirstgamefrom reason behind approach internet age easy find specific information write first program tutorial abundant found necessary elsewhere hand view architectural concept practical observation advantage tcp udp vice versa depending particular game exposing multithreading game logic level harmful relatively difficult find book intends cover byos bring salt practical world especially game development every thisthing way statement exists counterexampleone last thing would like mention proceed practical matter one single sentence book book matter taken absolute truth practical world especially game development every thisthing way statement exists counterexample illustrating sometimes thisthing even done different manner every advice applicability limit advice within book know certain gamerelated scenario limit likely exceeded advice become inapplicable try mention however extremely difficult predict usage scenario huge industry game development prepared advice book book inapplicable game without warning therefore take everything read elsewhere good pinch salt salt included book need bring one practical term every decision make based advice book ask advice really apply specific case first actually first second ninth depending poll chapter book coming stay tuned edit beta chapter business requirement published beta chapter shown page toc published acknowledgementcartoons sergey gordeev gordeev animation graphic prague
409,Lobsters,scaling,Scaling and architecture,Why Must Systems Be Operated?,http://brooker.co.za/blog/2016/01/03/correlation.html,must system operated,must system operated patterson et al footnote hafner rao understanding latent sector error protect analysis latent sector error disk drive,must system operated latent failure safety margin system mirrored classic way increasing storage durability also classic example system robust independent failure fragile dependent failure patterson et al paper popularized mirroring even covered problem mentioned make assumption disk manufacturer make failure exponential independent earthquake power surge situation array disk might fail independently striped raid three possible state state failure state one failure state two failure system move first second state second third state failure happens return second state first repair third state data lost returning becomes exercise disaster recovery like restoring backup classic markov model look like failure rate  repair rate model clearly display naive thinking assumes failure rate disk double failure rate single experienced system operator know true practice second disk failure seems likely happen soon first happens three reason failure cause failure like patterson earthquake power surge affect drive time roof falling server move raid state state pretty quickly operator mistake also common maybe dominant source kind failure failure triggered first failure first drive fails trigger failure second drive raid second drive going put high load system attempt get back two good copy extra load increase probability second drive failing latent failure case start system believing system operator believing system stage one failure occurs quickly learns second good copy nt third case latent failure may interesting system designer great example fact often nt know far failure simple raid case storage system latent failure belief first state actually second state problem nt mean isolated raid another good example problem system load balancer webservers behind load balancer run health check server sends load server belief healthy system like mirrored raid susceptible outage caused failure cause flood earthquake etc failure triggered first failure overload latent failure last two vastly common first server fail onebyone time system stay either dy overload last server fails loadbalancer raid case black box monitoring system sufficient black box monitoring including external monitor canary tell system side externally visible failure boundary system many kind system including nearly every kind includes redundancy move towards boundary multiple failure without crossing blackbox monitoring miss internal state transition catching significantly improve actual realworld durability availability system presented way seems obvious however think something worth paying real attention hear complex system kind tend build want build failuretolerant system property simple system nt simple system like teacup either working nt reason invest maintenance beyond occasional cleaning failure happens complex system different need constantly maintained allow achieve optimum safety characteristic requires deep understanding behavior system involves complexity often missed planning management activity planning allocating resource maintenance activity done without knowledge worse considering external failure rate bound underallocate resource real problem nt mean maintenance must done human possible necessary scale automate many task needed keep system far failure boundary got realize automation part system conclusion apply footnote also known raid despite nearly decade working computer storage brain refuse store bit raid raid mirroring striping whole lot hafner rao good place start complete picture raid reliability storage system common cause kind issue latent sector error understanding latent sector error protect good place start theory analysis latent sector error disk drive present possibly dated data frequency system includes operator human automated
410,Lobsters,scaling,Scaling and architecture,The Environmental Toll of a Netflix Binge,http://www.theatlantic.com/technology/archive/2015/12/there-are-no-clean-clouds/420744/,environmental toll netflix binge,incident one study,underwriting cost wind farm laudable address one kind environmental impact focused one particular set metric nt address thing like water usage pollution backup generator microsoft nowinfamous incident ran full tilt quincy washington data center supply chain rare earth mineral used hardware toxic material involved production hardware tech industry really good erasing connection mundane concern like energy electricity maintenance ensmenger noted highlighting silicon valley superfund site alone result industrialhardware productionthe impact data computation something really galvanizes public partly impact typically happens remove everyday life average amount power charge phone laptop negligible amount power required stream video use app either device invokes service data center distributed across globe us energy perform various process travel network device one study weirdly enough sponsored american coal association purpose enthuse great coal technology estimated smartphone streaming hour video weekly basis us power annually new refrigeratorboth cook cantrell argued energy used computational process renewable energy consumed process big deal shaming consumer netflix binge exactly mobilize base end day company lot agency make technology choice could lessen environmental impact still seems weird engineer building platform people use every day basic comprehension different online activity different energy impact individual online activity energy impact beyond laptop battery lifewhich say interest ensmenger research largely spurred introducing topic undergraduate student informationethics course student particularly enthusiastic issue privacy surveillance extremely interested environmental issue part local indiana home three superfund site related hardware production without question popular engaging part course teach ensmenger saidcontrary million thinkpiece lede bemoaning public nt understand care physical reality cloud admittedly guilty writing appetite ensmenger put much way nutritious food want better sustainable technology part future question need become part curriculum taught people build
411,Lobsters,scaling,Scaling and architecture,"How I built ghit.me, hit count badges for github, using just nginx, syslog-ng, and redis",https://benwilber.github.io/nginx/syslog-ng/redis/github/hit/counter/2015/12/25/how-i-built-ghit-me.html,built ghitme hit count badge github using nginx syslogng redis,ghitme nginx syslogng redis nginx setmiscmodule syslogng,read blog know find special pleasure solving problem without writing little code goal ghitme simple hit counter badge github repos goal able put simple hit counter badge github repo readmemd file common useful person view repo github badge fetched backend server display current hit count increment hit count thing build start nginx syslogng redis nginx http logformat badge datestr escapedrepo location badgesvg expires setformattedgmttime datestr setescapeuri escapedrepo argrepo accesslog syslog tagbadge severityinfo badge alias varwwwghitmebadges escapedreposvg set custom log format called badge simple csv date repo name eg benwilber used setformattedgmttime setescapeuri function setmiscmodule important always escape value plan use structured format badge request logging syslog nice csv format syslogng parser pbadge csvparser column badgedate badgerepo flag escapedoublechar stripwhitespace delimiters quotepairs destination dbadgeredis redis command sadd repos badgerepo redis command sadd repos badgedate badgerepo redis command incr repo badgerepo redis command incr repo badgedate badgerepo filter fbadge facility level info program badge log source ssys filter fbadge parser pbadge destination dbadgeredis set syslogng filter parse message redis operation case adding repo set repos time counter repos date daily counter increment hit counter repo one total one day use really simple cronjob aggregate counter repo write new svg badge image usrbinenv python ospath import join pathjoin import locale import redis pretty number localesetlocale localelcall enus badgedir varwwwghitmebadges badgetemplate svg xmlns http width height lineargradient id b stop offset stopcolor bbb stopopacity stop offset stopopacity lineargradient mask id rect width height rx fill fff mask g mask url path fill path fill path fill url b g g fill fff textanchor middle fontfamily dejavu sans verdana geneva sansserif fontsize text x fill fillopacity ghitme text text x ghitme text text x fill fillopacity count text text x count text g svg def repokey repo redis key storing count repo return repo format repo def writebadge repo count write updated count repo svg path svg format pathjoin badgedir repo svg badgetemplateformat countlocaleformat count groupingtrue open path wb fd fdwrite svg def main r redisredis repos rsmembers repos repokeys map repokey repos repo count zip repos rmget repokeys writebadge repo int count name main main
412,Lobsters,scaling,Scaling and architecture,A brief look at research findings on the adoption and use patterns of containers and Docker.,https://deis.com/blog/2015/containers-future-docker,brief look research finding adoption use pattern container docker,common us azure kubernetes service ak,common us azure kubernetes service ak migrate existing application cloud build complex application us machine learning take advantage agility offered microservices architecture
413,Lobsters,scaling,Scaling and architecture,The Troubleshooting Tales: issues scaling Postgres connections,https://gocardless.com/blog/the-troubleshooting-tales-issues-scaling-postgres-connections/,troubleshooting tale issue scaling postgres connection,situation investigating problem resolution,making change postgres setup started noticing occasional error coming deep within activerecord rail orm post detail process went determine cause issue fix itthe situationfirst important understand change made postgres setup postgres connection relatively slow establish particularly using ssl properlytuned server use significant amount memory amount memory used limit number connection feasibly open single server slow establishment encourages client maintain longlived connection due constraint recently hit limit connection server could handle preventing u spinning application server get around problem common advice use connection pooling software pgbouncer share small number postgres connection larger number client application connectionswhen first deployed pgbouncer running session pooling mode assigns dedicated postgres server connection connected client however setup large number idle client connected pgbouncer maintain equal number expensive idle connection postgres server combat alternative mode transaction pooling us postgres server connection duration transaction downside transaction pooling use sessionlevel feature eg prepared statement sessionlevel advisory lock combing apps remove usage sessionlevel feature enabled transaction poolingshortly making switch started seeing relatively infrequent exception coming deep within activerecord nomethoderror undefined method field nil nilclass also noticed instance exception appeared correlated insert query violated unique constraintsinvestigating problemsome initial digging indicated executing certain query asyncexec method ruby postgres driver returning nil rather pg result activerecord expecting get better sense could causing set finding way reliably reproduce exceptionwe set test database cluster matched production setup see image wrote script used ruby postgres driver issue lot uniqueconstraintviolating query parallel using one connection per thread dice nt see exception next tried introducing generic connection pooling library started sharing connection thread worked expected finally swapped connection pooling library activerecord immediately able reproduce exceptiongiven switched transactionpooling mode became curious whether wrapping insert transaction would change anything tried issuing begin followed constraintviolating insert commit sent database server one time error persisted however wrapped one string begin insert commit error suddenly stopped occurring something seem right wrapping single statement transaction effect tried running select insert found effect error went away fix actually generalised command string included multiple statement peculiarto shed light issue happened activerecord library turned tcpdump get complete view going quickly noticed using activerecord see load extra query sent wire query seemed changing sessionlevel setting sessionlevel setting play nice transaction pooling reason transaction pooling work sessionlevel setting transaction query transaction may sent different postgres connection modifying sessionlevel setting change setting random connection necessarily affect successive query may sent different connection entirelylooking setting onebyone seemed pretty ensuring right timezone set making sure standardconformingstrings used etc spotted query setting clientminmessages panic clientminmessages determines message reported back client usually set notice postgres considers unique constraint violation error panic reported ruby postgres driver issuing query expecting either normal result error however error disabled getting nothing back causing return nil finally found issue resolutionbut activerecord disabling error turn postgres standardconformingstrings readonly attempt set would result error activerecord enabled setting available want show error available readonly solution set clientminmessages panic try set standardconformingstrings reset clientminmessagesback original value fortunately rail since dropped support postgres fix easy simply remove query modify clientminmessages assume standardconformingstrings nt readonly patch change present rail since though setting appear innocuous could still cause issue combined transaction pooling instance could end different connection different timezones weve brought rubyonrailscore list dec still unresolved issue
414,Lobsters,scaling,Scaling and architecture,The search for a faster CRC32,https://blog.fastmail.com/2015/12/03/the-search-for-a-faster-crc32/,search faster,fastmail advent calendar server optimised disk throughput perf algorithm added checksum cyrus twoskip contender zlib cloudflare version zlib among thing fallback implementation inside cyrus stephan brumme collection intel zlib google crcutil library intel cpu instruction test method laptop result input buffer get byte replacement build system change pile test end,third post fastmail advent calendar stay tuned another post tomorrow month ago happened looking graph cpu load backend mail server noticed time server gotten busier may remember server optimised disk throughput cpu since time cpu waiting io complete still case noticed cpu active used year ago enough present kind problem enough make u wonder assistance linux perf utility found cpu time spent one many cyrus process function called function computes checksum using common algorithm arbitrary chunk data idea store data checksum separately later read data recompute checksum compare original different know either data checksum corrupted take appropriate action year added checksum cyrus particularly data storage engine known twoskip pdf saved u point became obvious calculate billion checksum add lot cpu time started looking alternative implementation even small gain translate big win run billion time contender default cyrus used widelyavailable zlib library case one shipped debian never reason change spent time researching alternative came following list debian stock debian machine zlib zlib upstream always perform roughly debian one wanted include specifically could compile optimisation implementation tested cloudflare cloudflare version zlib reimplement function among thing using modern cpu instruction mostly pclmulqdq actually took linux kernel cyrus fallback implementation inside cyrus used zlib available simple algorithm slow tablebased slicebyn stephan brumme collection increase n add table unrolls loop relying modern cpu ability parallelise work strategicallyplaced instruction force cpu bring next byte cache math kernel kernel implement load crypto stuff partly internal use eg crypto filesystems partly expose crypto device userspace appealing pclmulqdq implementation one cloudflare us well tablebased one would handy deployment small number machine cpu support pclmulqdq also noncontenders intel zlib unfortunately hooked new code compression code rather cloudflare changing stock implementation call get regular zlib version easily lifted implemented kind memcpywithchecksum apparently compression engine need google crcutil library wanted try something based zlib ca nt actually follow code well enough figure lift important bit anything using intel cpu instruction us different input algorithm known polynomial apparently robust used network filesystems sort thing give different result standard polynomial typically used compression test method anyone following along home code used test available github actual test method probably variable true science would allow nt take long put together looking indication trend anyway ran mostlyidle laptop ac power weird cpu scaling similar shenanigan way nt make effort stop test preempted time reported wallclock time laptop haswell ram grunt imap server particularly important test looking relative performance mention case running getting wildly different number large test round data buffer nt anything like cyrus good test make sure everything working properly really interesting test one small buffer buffer checksum small minimum byte twoskip header average perhaps byte target case implementation except debian compliled gcc marchsandybridge mtuneintel nicest easy set optimisation get laptop roughly match production hardware result column header show input buffer size number round result wallclock time second lower better x x x x x x x learn zlib debian mostly match tell u recompiling debian package optimisation nt going help much cloudflare amazing input buffer get byte point stop using optimised implementation fall back regular zlib implementation sure explanatory comment could find showstopper us shame cyrus obviously rubbish much say pretty much expect getting faster time cpu work parallel impressed consistent large particularly small buffer buffer get small small thing degenerate nt fully understand suspicion whatever optimisation compiler done checksum loop something wait memory often slowing everything nt pursue intuitively make sense much could done anyway seems like waste time small buffer make sense waiting next byte use pointless though nt expect quite much difference large buffer made particular difference maybe make lot difference checksumming enormous data set kernel oh wanted like every operation consists write read socket zerocopy still two context switch per round show larger buffer cost probably acceptable remember code cloudflare used small buffer lose everything switch number interesting like cloudflare version kernel switch alternative implementation kernel case giving good result apart context switching overhead based data pretty sure best immediate plan cyrus use chosen size input buffer absolute fastest still much faster zlib fairly easy maintain assembly consider trying extend kernelcloudflare code good assembly implementation small buffer assembly nt good license nt friendly gpl code ca nt go cyrus old server nt pclmulqdq instruction need fallback anyway considering switching different algorithm better job small buffer licensefriendly assembler implementation us intel cpu support special codepath buffer smaller byte may well prove fastest produce different result would render stored checksum invalid recalculate big job need lot testing first make sure much better actually worth effort replacement time write replacement function cyrus actually simple bringing single file main entry point selecting one based size buffer read whole cyrus source interesting bit apart function entry point function static prev const void data sizet length length return prev data length return prev data length running test version get original zlib comparison x x x x x x x byte slower original understandable know fall zlib us internally since minimum buffer size byte never problem ever became one add well next get optimisation setting right compile cyrus optimisation full debugging make really easy work crash dump lately looked enabling compiler optimisation specific hot function via gcc attribute convenient tried adding pragma gcc optimize top cover whole file thing nt really cover whole file make default function mean crossfunction optimisation ie inlining nt happen see symbol table dump pragma gcc optimize difference extra function call negligible test would bothered done nothing build system change solution meanwhile bron added pile test make sure new stuff actually returned checksum value phew end rerunning profiling busy server morning look like cpu spent calculating checksum perhaps gain huge faster experience way make really fast service make lot tiny performance improvement across board another one change contribute great experience user
415,Lobsters,scaling,Scaling and architecture,State of Mercurial at Mozilla,https://selenic.com/pipermail/mercurial-devel/2015-November/075761.html,state mercurial mozilla,state mercurial mozilla gregoryszorc gmailcom http information mercurialdevel mailing list,state mercurial mozilla gregory szorc gregoryszorc gmailcom sun nov cst mozilla long time user mercurial munich sprint september gave overview state mercurial mozilla year passed like something similar since nt london sprint last month post long rambling tl dr mercurial currently capable awesome mozilla achieving nirvana difficult requires heavy customization reliance potentially unsupported party tool extension overall say state mercurial mozilla improved substantially past year encountering numerous scaling pain client server year ago due size firefox repository improvement tag branch cache ability seed clone pregenerated bundle various performance enhancement around revsets phase discovery etc done wonder scaling hgmozillaorg making interaction pleasant core improvement facebook hgwatchman extension substantially improved clientside experience hg status command blackbox extension proved invaluable tracking performance issue steady stream bug fix performance improvement new feature core bundled extension resulted people much positive opinion mercurial year ago mercurial moving positive direction several front nt want take away still number area improvement like describe worth noting mercurial use mozilla related firefox development revolves around mozillacentral repository currently changesets manifest filelogs file revision file tip manifest comprising mb checked targz working copy mb gzip bundle mb wire transfer size ondisk file size mb month ago mozilla conducted survey engineer developer workflow matter comprehensive section version control people left feedback mercurialrelated question much information present derived response following perceive significant concern mercurial mozilla particular order commitpatch workflow bookmark v branch v mq v topic v nameless head v default setting insufficient requires much effort configure optimal usage performance day day operation status blame diff rebase histedit cloningpullingpushing large repository lack usable shallow clone narrow clone ability resume partial clone window support especially performance lack popularity gitgithub many topic related example commit workflow strongly coupled extension enabled different workflow different performance impact aforementioned survey asked open ended question describe biggest complaint mercurial response summarized commitpatch workflow performance many specific answer referenced git lot people believe git faster better etc like focus bit workflow issue aforementioned survey asked workflow people practiced mq importexport fromto git bookmark evolve changeset evolution nameless head branch subsequent question asked specific thought mq use regularly resolving conflict annoying tolerate using despite deficiency used previously stopped ca nt live without love mq silly avoided somewhere mercurial user using mq reckon lot usage historical mozilla using mercurial bookmark history rewriting facility existed mq game town know mq result bad experience especially large repository like firefox hg qpop hg pull thousand changesets pleasant lack sane merge conflict resolution horrendous yet number user enjoy mental simplicity mq point assuming someone sticking core supported workflow bookmark branch nameless head hard time firefox development without obsolescence marker enabled hg rebase hg histedit hg pull requires stripping repo reapplying data applying rewritten changesets worse mq similar performance issue least mq pop everything pulling avoid expensive strip people enjoy freedom power core supported workflow without performance issue stripping history rewriting need install experimental evolve extension course extension introduces completely new complicated workflow people must learn experimental label making people even uncomfortable lot user easiest choice mq git would absolutely love way turn obsolescence marker changeset hiding without introducing new workflow evolve would provide stripfree history rewriting without added cognitive load know inhibit extension something like pierreyves insists nt appropriate general deployment many performance complaint stem nonevolve user still area performance concern nt bundled mercurial number user nt amazing hgwatchman extension installed includes entire window developer base although bundling hgwatchman experimental feature window development environment soon speaking window number performance pain point written previously io issue writingclosing thousand file hg update still us single process window nt get optimization platform know python startup overhead hurt latency hg command think lack snappiness running command contributes perception mercurial slow er git chg course largely make issue go away also briefly mention blameannotate number user frequently perform blame part investigating change made course several year even decade number performance usability issue blame make workflow harder could see blameplan wiki idea cloning problematic number user community volunteer across world want contribute firefox mozilla project cloning gb data slow unreliable network connection painful work around seeding clone pregenerated bundle file helping put bundle cdn people use curl wget etc resume downloads transparent hg clone via cdn nt work would really nice mercurial could incremental clone lose data clonepull aborted due network hiccup even better would native support shallow narrow clone remotefilelog appears nonstarter developer requires manual garbage collection mechanism client probably suitable automation managed machine however narrow clone still work thing nt terrific think right trajectory making cloning pulling large repository pleasant another major concern user face configuring mercurial configuration option extension feel mercurial awesome however take herculean task configure hgrc line something like extension enabled ok developer extension setting curated year constant tinkering setting know follow developer list party project like facebook repository bitbucket new user nt stand chance consider reasonable experience box especially care performance moving fast understand need kiss first time vcs user hg bare bone box step required mercurial powergit user well defined difficult achieve much take pride simplicity cli getting modern expected configuration complex user version control knowledge especially git think mercurial nt sufficient feature complicated configure nt work contributing configuration problem many extension party marked experimental lot people uneasy using something feel may break next upgrade reasonable concern example facebook hgexperimental repository contains ton useful generic mostly stable extension right name say experimental facebook luxury controlling dev environment dropping backwards compatibility aggressively mozilla try support mercurial version past year many good idea improvement land party extension however using risky support mess underscore point enough summary believe significant gain made mercurial core tool mercurial open source project ecosystem mercurial mozilla feel solution remaining significant pain either available development however solution exist difficult employ achieving optimal mercurial distributionconfiguration difficult painfully obvious open source realm strong control machine environment client configuration could request single thing improve state mercurial mozilla would make achieving optimal client configuration easier could done moving extension core pager color come mind aggressively bundling successful party tool extension like chg hgwatchman evolve remotefilelog smartlog etc even marked experimental subject change tree would make support situation much better providing mechanism server advertise recommended extension setting experimented previously achievable providing configuration wizard prompt help people obtain configuration want could write several paragraph containing gob detail think hit major point wanted hit hope others find useful hopefully go without saying want help committed helping like close reiterating mercurial moving forward rapidly past year general sentiment towards mercurial mozilla improved time keep great work gregory next part html attachment scrubbed url http information mercurialdevel mailing list
416,Lobsters,scaling,Scaling and architecture,True Tales of Engineering Scaling,https://medium.com/baremetrics-founders-journey/true-tales-of-startup-scaling-153c3ee3664e,true tale engineering scaling,trail good intension stripe pgbouncer pgbouncer building importantly software constant trade forward movement present stability,building software sorta like brighteyed bushytailed camp counselor first kid dropped parent one two time kid special request handled one ton luggage stored another vegan another want archery nothing elsethat finethen busload kid start getting dropped camping free luggage dietary constraint planning issue funby end poor counselor toastsoftware camp counselor much commonthe trail good intensionswrite app rail use postgres everything great add simple webhook stripe rail controller main app kicking start calculating background use sidekiq still great customer sign sends event within hour periodnow simple webhook causing main dashboard slowpull webhook little microservice running month without data stripe month work great customer sign event single daynow import segment day movement forward lost time account sign need worry crank wait high memory worker heroku quite literally cost arm leg kidney first bornwe bootstrapped business afford spend time move aws totally reasonableeeeek pulling data local testing becomes nearly impossible slows development sighworker issue solvedoh pg connection limit issue hardly worth fire pgbouncer machinefantastic work finally move pgbouncer machine connection issue resolvedoh meantime query db server killing cpuin case pull high traffic table separate dbwell hot dog connection limit issue new db repeat lesson learned previously oh someone signed plan perplan worker dyingin meantime marketing material main app work great initiallyuntil marketing actually work people go look itwhich kill main dashboard response time paying customerswhich mean quickly split marketing site app separate piecesand known beginning sure could assumed also never launched product first place spending eternity optimizing scale havebuilding importantly shipping software constant trade forward movement present stabilitysilicon valley filled graf startup never actually shipped anything bent fixing problem shipping better dying ever leaving runway yes aware clunky metaphor least shipped
417,Lobsters,scaling,Scaling and architecture,Schema-Agnostic Indexing with Azure DocumentDB,http://www.vldb.org/pvldb/vol8/p1668-shukla.pdf,schemaagnostic indexing azure documentdb,,obj stream c ge h q m r endstream endobj obj endobj obj endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj stream endstream endobj obj stream bcc endstream endobj obj typecatalogstructtreeroot rlang enus page r endobj obj endobj obj parent rcontents r r r typepagetabssresources procset pdf text imageb imagec imagei font mediabox structparents endobj obj stream
418,Lobsters,scaling,Scaling and architecture,Spinnaker: Global Continuous Delivery by Netflix,http://spinnaker.io/,spinnaker global continuous delivery netflix,rolebased access control,rolebased access control restrict access project account hooking internal authentication system using oauth saml ldap cert google group azure group github organization
419,Lobsters,scaling,Scaling and architecture,How Instagram Solved Its Justin Bieber Problem,http://www.wired.com/2015/11/how-instagram-solved-its-justin-bieber-problem/,instagram solved justin bieber problem,beating bieber bug one facebook massive computer data center advanced facility planet blog post published morning instagram everywhere tool called pgq memcached software,hui ding took job instagram three year ago photohappy social network growing one world popular online serviceshe soon noticed every often company cofounder mike krieger normally jovial character would suddenly turn serious intense would hunker computer keyboard mutter got ta fix meant instagram slowed crawl user across globe cause always justin bieberor almost always sometimes kim kardashian single celebrity count could destroy entire infrastructure would go bieber would post photo many beliebers would like instagram computer could nt keep way quickly pushing new stuff million user big internet outfit like instagram operate called memory cache inside computer data center packing popular online content superspeedy memory system hundred even thousand computer server delivering data memory far faster delivering good ol database sitting good ol hard disk bieber pic would receive many like cache could nt hold service fought retrieve like database would grind halt instagram would lock thought wow single celebrity count could destroy entire infrastructure ding saysas krieger still remembers bieber digital id database underpinned bieber account often source latest problem still know heart krieger say many early scaling issue hitting thing never hit got good knowing sighting id beating bieber bugbut day last summer two year acquired facebook instagram moved online operation one facebook massive computer data center recent month company expanded two facebook facility company expanded juggling million photo video day million people modified underlying software avoid bieber bug many glitch commonly plague enormous online servicesin tallying like instance instagram us call denormalized counter mean nt try keep running like count memory nt try tally number querying individual account people posted like since account sit database run hard disk would take long instead company keep like count photo single database cell access single cell needed one disk access ten microsecond say instagram engineer lisa guo always going wherever need instagram bieber fix help provide roadmap business expand online operation usersthis one small tweak vast online infrastructure alongside many tweak help provide roadmap business expand online operation user every online startup option expanding multiple facebook data advanced facility time go many encounter similar growing pain benefit many trick tradeas instagram detail blog post published morning company moved multiple facebook data center keep growth already vast community people instagram service expands beyond million user need computer server moving multiple data center also mean company better respond disaster one data center go another pick slackinstagram everywhereat time setup creates new challenge among thing memory cache one data center wo nt necessarily match memory cache another user comment photo account resides database machine oregon instance comment show oregon cache cache running facebook data center north carolina user look photo north carolina facility wo nt see commentto get around particular problem instagram turned tool called pgq dovetailing company postgresql database ensures cache one region nt date system visit database latest information stored visiting database sits hard disk take longer trick like denormalized counter helpthat may seem like lot wrap head around ensuring quickly see stuff instagram want see enormously complicated task involving memory cache based memcached software multiple database postgresql cassandra also myriad web server message broker point piece place across multiple data center disparate part world nt worry instagram grinding halt neither mike krieger least worry le frequentweb service always vulnerable natural disaster mention justin bieber whoever else inherits selfie crown bieber nt seem likely go away anytime soon least way minimizing bieber problem
420,Lobsters,scaling,Scaling and architecture,The Road to 2 Million Websocket Connections in Phoenix,http://www.phoenixframework.org/blog/the-road-to-2-million-websocket-connections,road million websocket connection phoenix,road million websocket connection phoenix started run benchmark server chrismccordphoenixchatapplication gazlerphoenixchatexample rackspace gb io rackspace onmetal io client tsung documentation first connection first connection first real benchmark http commit observing change observer http commit need machine know ets type gabiz voicelayer pull request need even machine mobileoverlord live help commit,road million websocket connection phoenix posted november gary rennie paying attention twitter recently likely seen increasing number regarding number simultaneous connection phoenix web framework handle post document technique used perform benchmark started couple week ago trying benchmark number connection managed get connection local machine convinced number posted irc see anyone benchmarked phoenix channel turned member core team found number provided suspiciously low beginning journey run benchmark server benchmark number simultaneous web socket open time first thing required phoenix application accept socket test used slightly modified version chrismccordphoenixchatapplication available gazlerphoenixchatexample key difference afterjoin hook broadcast user joined channel removed measuring concurrent connection want limit number message sent future benchmark test performed rackspace gb io machine ram core rackspace kindly let u use server benchmark free charge also let u use onmetal io ram showed core htop one additional change may want make remove checkorigin confprodexs mean application connected regardless ip addresshostname used start server git clone run mixenvprod mix depsget mixenvprod mix depscompile mixenvprod mix phoenixserver validate working visiting client running client used tsung tsung opensource distributed load testing tool make easy stress test websockets well many protocol way tsung work distributing using host name example first machine called assigned ip etchosts machine also etchosts important run client different machine phoenix application benchmark result true representation running machine tsung configured using xml file read particular value documentation config file used however number lowered reflect number client bigger test used client start connection second maximum connection connection open websocket join room lobby topic sleep second used large sleep time wanted keep connection open see responsive application client connected would stop test manually instead closing websockets config type disconnect xml version doctype tsung system tsung loglevel debug version client client host cpu usecontrollervm false maxusers client host cpu usecontrollervm false maxusers client host cpu usecontrollervm false maxusers client server server host serveripaddress port type tcp server load arrivalphase phase duration unit second user maxnumber arrivalrate unit second arrivalphase load option option name portsrange min max option session session name websocket probability type tswebsocket request websocket type connect path socketwebsocket websocket request request subst true websocket type message topic room lobby event phxjoin payload user tsuserserver getuniqueid ref websocket request var incr thinktime value session session tsung first connection tsung provides web interface port used monitor status test chart really interested test similtaneous user first time ran tsung machine tsung phoenix chat application running locally tsung would often crash happens see web interface mean chart show unimpressive connection first connection set machine remotely attempted benchmarking time getting connection least tsung crash reason systemwide resource limit reached verify ran ulimit n returned would explain could get connection point onwards following configuration used configuration took u way million connection sysctl w sysctl w ulimit n sysctl w sysctl w sysctl w sysctl w sysctl w first real benchmark talking tsung irc chris mccord creator phoenix contacted let know rackspace set instance u use benchmark got work setting server following config file http running dedicated one machine phoenix two running tsung first real benchmark ended connection image two line chart line top labeled user line bottom labeled connected user increase based arrival rate test used arrival rate user per second soon result jos valim case commit first improvement big one got connection observing change first improvement realized going blind way could observe happening luckily use erlang ship observer used remotely used following technique http open remote observer chris able use observer order process size mailbox timer process message mailbox due phoenix heartbeat every second ensure client still connected luckily cowboy already take care commit result looked like actually killed pubsub supervisor using observer image explains drop end second performance gain result concurrent connection using tsung machine need machine two problem image one reach full number client timing two actually generate connection per tsung client technically per ip address chris good enough really see limit unless could generate load stage rackspace given u box actually another machine could use using powerful machine tsung client limited connection may seem like waste better machine idling chris set another box u another possible connection ran benchmark got connected client big problem actually connect machine work probably hardware issue decided try running phoenix machine instead surely would issue reaching connection limit right wrong result almost identical chris thought pretty good chris tweeted result called night know ets type achieving fairly easy performance gain sure could performance gain magnitude wrong aware time colleague gabi zuniga gabiz voicelayer looking issue weekend commit gave u best performance gain far see diff pull request also provide convenience local etsnew local bag namedtable public local etsnew local duplicatebag namedtable public additional character made chart look like increase number concurrent connection also allowed u increase arrival rate made subsequent test much faster difference bag duplicatebag duplicatebag allow multiple entry key since socket connect one pid using duplicate bag cause issue u maxed around connection point box memory ready really test larger box need even machine justin schneck mobileoverlord informed u irc company live help would set additional server rackspace u use additional server precise set machine set threshold tsung million connection new milestone easily achieved machine time justin finished setting box convinced million connection possible unfortunately case new bottleneck started appearing million connection connection good enough right wrong time hit subscriber started getting regular timeouts asking subscribe single pubsub server also notice large increase broadcast time taking broadcast subscriber justin interested internet useful thing wanted see could optimize broadcast subscriber since see real use case level idea shard broadcast chunking subscriber parellizing broadcast work trialed idea reduced broadcast time back still pesky subscribe timeouts limit single pubsub server single ets table chris started work pool pubsub server realized could combine justin broadcast sharding pool pubsub server ets table sharded subscriber pid pool pubsub server managing ets table per shard let u reach subscriber without timeouts maintain broadcast change commit refined merging master million connection time thought optimization made another idea pitched leading huge improvement performance million figure pleased however quite max machine yet made effort toward reducing memory usage socket handler addition benchmark performing particular set benchmark set exclusively around number simultaneous open socket chat room million user awesome especially message broadcast quickly typical use case though future benchmarking idea one channel x user sending message x channel user sending message running phoenix application across multiple node simulation sending random number message user arriving leaving randomly behave like real chat room improvement discovered benchmark test made available upcoming release phoenix keep eye information future benchmark test phoenix continue push boundary modern web
421,Lobsters,scaling,Scaling and architecture,Five Years of Building Instagram,https://medium.com/backchannel/war-stories-3696d00207ff#.4ymc4ugeh,five year building instagram,war story five year building instagram milestone million user month file qcon velocity engineering blog takeaway milestone launching android file lesson learned infrastructure evolved feel native takeaway milestone virginia storm file instagramcom wale takeaway milestone instagration file nstagration neti takeaway milestone trend instagram file built new infrastructure takeaway,war storiesfive year building instagramin night launched instagram cofounder kevin bet many people would download app first day wild kevin guessed especially optimistic moment went big guessed next day realist believe hit nosenow birthday instagram million user world upload million photo video day looking back balanced simplicity craft original product last year revamped search discovery launched brandnew take instagram direct continued release creative tool like layoutwhile team thankfully grown evolved last year stayed committed mantra simple thing first keeping core continue scale next five year look biggest milestone building instagram past five year good bad surprising hope takeaway help build grow team companiesmilestone million user monthsfile biggest challengethe first month launch pretty much blur server alert page norm rather exception exploding user first day continued grow rapidly hit millionthere motivation stronger people actually wanting use product went high gear make sure could support growing demand started running single server la le computing power macbook pro called hosting provider asking another server given first day growth quoted fourday turnaround hour rushed given unpredictable growth looked decided move onto amazon web service cloudgiven neither u deep infrastructure experience soak much knowledge could great conference video qcon velocity article facebook netflix twitter others open culture sharing technical insight one best thing industry main motivator behind engineering blogtakeaway mantra simple thing first took shape first week month since two u determine fastest simplest fix time faced new challenge tried futureproof everything might paralyzed inaction determining important problem solve choosing simplest solution able support exponential growthmilestone launching androidfile anticipated launchfor first couple year instagram kevin would get single question every single time onstage android app coming started iosonly first wanted able iterate product quickly two engineer entered though time expand multiple platform typical instagram style android app built three month three engineer two learned android complete project along philip joined u building gowalla android app lead instagram mobile effort daypart role time became professional ebay shopper since wanted test app many device possible including something called ascend ii touch often unpack new phone arrival office load workinprogress app stand amazed well app worked breadth android device posed challenge u especially built instagram video product pretty amazing launch wide variety device minimal customization requiredover million new people joined instagram first hour launch incredible response time wrote lesson learned infrastructure time android app evolved feel native platform today one fastest highestrated android appstakeaway starting single platform allowed u focus iterate quickly without implement everything twice often say fewer thing better inside instagram came time expand multiple platform built small team combining deep android expertise talented engineer new platform time building fullfledged android team allowed u adapt app closely platformmilestone virginia stormsfile worst outagei portland quick three day weekend getaway phone buzzed instagramcom quick check online showed beyond instagram netflix others experienced issue well ran back hotel brought laptop saw dreaded message amazon web service status page power event useast huge storm blown virginia almost half instance lost power next hour would brutal rebuilding almost entire infrastructure silver lining generated meme image time whole backend team consisted first engineer shayne rick started instagram le month prior user data lost outage exposed much work left automating infrastructurethis outage kick butt needed move repeatable server provisioning process next year moved provisioning away fragile shell script towards full chef system substantially lowered bar new team member work infrastructurewe also moved away relying amazon elastic block storage database backup instead adopting wale postgres wal shipping replication also kicked reliability initiative recently yielded crossdata center effort gotten instagram running geographically distributed data centerstakeaway scriptable infrastructure requires upfront work pay huge dividend bringing new engineer onto infra team well helping disasterrecovery scenario also glad hired engineer right stuff faced unimaginably bad scenario shayne rick rolled sleeve started bringing u back one issue time markwatneystylemilestone instagrationfile ambitious engineering projectoctober user million million million million people using instagram every month billion photo stored team growing small thrilled continued growth instagram communityas time went kept finding new integration wanted facebook existing backend system example site integrity system would critical helping u fighting spam integration would difficult amazon web service longer waited harder would migrate evergrowing everpricier infrastructureit clear migrate facebook infrastructure want disrupt service moved million people billion photo began instagration like refer swapping car part going small team eight instagram facebook engineer worked first build common network move instagram amazon virtual private cloud vpc using tool built inhouse called neti meticulously migrated system tooling including building ig commandline tool bridged pattern developer familiar aws new fb datacenter environment end result huge migration minimal disruptionstakeaway reinvent wheel moving facebook server able give infrastructure faster efficient home well take advantage facebook tool like spam fighting etc able stay small take advantage facebook resource experience move much quicklymilestone trend instagramfile next big betearlier year revamped search explore expanded ability easily find interesting moment instagram happen world introduced trending hashtags place built new infrastructure support identifying ranking presenting best content instagramour first take trending back popular page available instagram launch algorithm pretty simple effectively number like photo decayed age photo hour worked great community smaller time realized needed nuanced approachgiven larger community worked personalizing explore bringing infinitely scrollable page photo video tailored person within month user interacting content rate unpersonalized explore year brought back intention original popular page glimpse gestalt instagram trending product ranking machine learning expert since joined team able adapt wellknown trending algorithm nuance instagram communitytakeaway simple thing first mean solution work forever learned open evolving product spinning purposebuilt team like datagram team adapt rapidly scaling communitythe last five year wild ride many u nice pause reflect occasion birthday sure community continues grow product continues evolve shortage thing talk looking back year medium post next five year
423,Lobsters,scaling,Scaling and architecture,checkedthreads: bug-free shared memory parallelism (2013),http://yosefk.com/blog/checkedthreads-bug-free-shared-memory-parallelism.html,checkedthreads bugfree shared memory parallelism,john carmack checkedthreads automatically finding every race condition valgrind thorough fast mutable shared memory forkjoin parallelism tbb ppl parallelfor parallelinvoke openmp pragma omp parallel ctxfor event reordering memory access monitoring implemented checkedthreads strict forkjoin parallelism forkjoin code verifiable raw thread nt many order take reorder every two instruction could ever run parallel two parallel parallel poset dimension reordering every pair independent instruction trivial forkjoin code hard raw thread bug missed bug nt pinpointed pinpointed std functionhandler minvoke ctvalgrindforloop ctvalgrindfor ctfor main never miss bug could ever happen input data always pinpointed shown past lock n chase memory access monitoring pinpoint data race race condition great discussion pure forkjoin code race condition data race deterministically reproduced pinpointed feature checkedthreads guaranteed bug detection integration framework dynamic load balancing api free easily portable small simple custom scheduler downloading building installing using checkedthreads build instruction precompiled binary api environment variable runtime verification yossikreinin gmailcom conclusion automatically deterministically reproduce pinpoint every bug flexibility correctness,deeply frightened issue raised concurrency thinking hard enough john carmack post introduces checkedthreads free framework parallelizing c c code automatically finding every race condition could potentially manifest given program input come valgrindbased checker thorough verification eventreordering scheduler fast verification code checkedthreads fresh nt used production yet however tool using approach successfully used year automotive safety software containing million line code written many dozen developer nice use case checkedthreads complex serial program want parallelize checkedthreads able run test suite sure parallelism bug sure say memory leak parallelization introduce bug valgrind checker pinpoint quickly fix follows explain race detection work checkedthreads briefly discus feature get started thread errorprone accustomed see root problem mutable shared memory commonly proposed alternative thread avoid mutable shared memory example pure functional code get rid mutable part processbased parallelism get rid shared part process pure fp virtue believe mutable shared memory make thread bugprone keep mutable shared memory eliminate bug moreover see perfectly possible eliminate shared memory keep bug root problem believe need find right interface thread lock great lowlevel primitive bad interface use directly source code sense thread unlike goto goto horrible interface human programmer fine machine instruction underlying higherlevel interface ranging loop function call exception coroutines higherlevel interface top thread one interface forkjoin parallelism parallel loop function call loop function call notably similar popular interface top goto forkjoin starting parallel loop logically fork thread per iteration thread joined back loop end example forkjoin interface include tbbsppl parallelfor parallelinvoke openmp pragma omp parallel checkedthreads provides something similar well ctxfor objssize int process objs parallel forkjoin parallelism wellknown appreciated automation synchronization load balancing forkjoin interface help program correctness compared raw thread lock see forkjoin help looking two method verify parallel code event reordering memory access monitoring method implemented checkedthreads together guarantee freedom parallelism bug discus method effective forkjoin code comparatively ineffective raw thread since parallel function call implemented using parallel loop explicitly mention parallel loop discus pure forkjoin program program forking joining synchronization mechanism code inside parallel loop access stuff written whoever spawned loop code loop access stuff written loop ca nt access shared data using semaphore atomic counter lockfree container etc access flagged bug synchronize thread must fork join lastly assume ca nt proceed loop spawned completes ca nt wait anything loop spawned obvious property code spelled using kind parallel forkjoin interface recent paper call property strict forkjoin parallelism thus assuming purity strictness sound restrictive separate topic assumption spelled forkjoin code verifiable raw thread nt first let consider event reordering cheap effective verification method suppose thread writes address x thread b concurrently read x b might see write might depending timing bug cheap way find bug nt run program production thread scheduler order event depends timing instead use debugging scheduler purposefully deterministically reorder event make schedule thing one run write precedes b read another run b read come first compare result two run differ bug many order take reorder every two instruction could ever run parallel actually requirement weak find bug plug hole later usually enough find bug forkjoin program need two order run loop n run backwards n illustration consider example pseudocode nested parallel loop parallel foo parallel bar j baz ordering constraint code block run parallel make following dag two schedule n one backwards one pick two instruction could possibly run parallel see reordered two schedule happens use raw thread lock ordering constraint nt look like simple forkjoin dag say ordering constraint form sort partial order set serial code block whose instruction fully ordered n code block upper bound number order might need n every code segment schedule run early possible schedule run late possible n schedule reordering every two independent instruction n lot order n rather large better forkjoin case le schedule reorder thing perhaps finding lower bound number order nphard problem standard name finding poset dimension heuristic analyzing partial order attempt produce fewer n order take minute run n thousand experience upshot reordering every pair independent instruction trivial forkjoin code hard raw thread let consider improvement upon simple event reordering memory access monitoring based something like program instrumentation compiler pas plain event reordering two main drawback bug missed consider update accumulator whether run suma suma sum reach value whereas parallel run may finergrained reordering trying every way interleave instruction could run parallel reordering every pair independent instruction would catch bug obviously infeasible bug nt pinpointed reordering give evidence bug demonstrating result differ different schedule find bug improve upon plain reordering let intercept memory access record id thread last write every location location owner logically loop index could run separate thread ideally map index separate thread id practice might want id small use low bit index location accessed someone whose id different owner id bug pinpointed need print current call stack checkedthreads error thread accessed owned std functionhandler minvoke ctvalgrindforloop ctvalgrindfor ctfor main tad complicated nested loop much complicated detail nt worth discussing point work regardless whether effect result reproduced serial run pinpoint bug involving accumulator shared temporary buffer whatnot even though result look fine serial run note still rely event reordering catch bug address written first read read first written ok reordering scheduler guaranteeing one run address actually written first read scheduler access monitoring never miss bug could ever happen input data always pinpointed nt work nearly well raw thread semaphore first problem need reordering scheduler many schedule cover discussed shown past example helgrind memory access monitoring valgrind tool debugging pthread application miss bug bug masked order thing happen run another problem raw thread lock preventing memory access monitoring pinpointing bug reproduce specifically two thread access location use lock synchronize access ca nt flag access bug right however well may bug certainly data race unsynchronized access memory race condition bug due event ordering example consider supposedly deterministic simulation interactive game though screenshot game lock n chase simulation agent run around maze picking coin whoever made first pick coin race literally suppose agent simulated thread might simulated speed agent maze depends amount cpu time thread used compared others result depend timing bug thread per maze region would better strategy thread per agent race condition timing affect result however long agent lock coin pick data race access shared memory synchronized memory access monitoring pinpoint data race race condition simulate maze notice memory keep coin representation accessed concurrently without locking print offending call stack ca nt simulate maze notice different event ordering different agent pick properly locked coin print offending call stack simply single offending call stack fact result end differing john regehr great discussion difference race condition data race check example bank account account balance locked nicely bank still nt properly transfer money account thing matter context pure forkjoin code race condition data race general case raw thread longer true get way pinpointing bug note using something like go channel even erlang process nt solve maze race problem sense still ca nt pinpoint bug instead locking coin might agent process goroutines whatnot one several process keeping coin agent would send request pick coin process make first nt deterministic single place code bug one example eliminating shared memory keeping bug opposed forkjoin code ability keep shared memory automatically eliminating bug say erlangstyle lightweight process nt good idea great certain context believe parallelizing computational code somewhat different problem handling concurrent event forkjoin parallelism close right thing computational code summarize entire discussion verification forkjoin code every bug could possibly manifest given input deterministically reproduced pinpointed conversely raw thread lock computationally hard deterministically reproduce bug furthermore even reproducible bug always automatically pinpointed also notable raw thread get worry deadlock pure forkjoin code simply never deadlock analysis mostly nt rigorous welcome correct feature checkedthreads noteworthy feature checkedthreads discussed short summary guaranteed bug detection discussed integration framework already use openmp tbb configure checkedthreads use scheduler thread pool instead fighting machine framework dynamic load balancing work get done soon thread available load balancing automatically take account variability different task well whatever unexpected load cpu might handling running code api available free want freebsd license easily portable theory small simple moment custom scheduler easy add though recreational activity necessity hope downloading building installing using checkedthreads recommend read build instruction possibly download precompiled binary build instruction useful particular know getting binary archive source code version archived git githubcom yosefkcheckedthreadsgit keep latest source code actually using checkedthreads recommend read rather short documentation api environment variable runtime verification run issue checkedthreads drop line yossikreinin gmailcom conclusion quote piece john carmack large enough codebase class error syntactically legal probably exists word correct design almost nonexistent property correctness either demonstrated automatically absent nobody make simple error discussed toy example analogous error necessarily creep large parallel program checkedthreads show one significant family parallel imperative program forkjoin code possible automatically deterministically reproduce pinpoint every bug hope convincing example think general truth namely parallel imperative program nt deeply frightening serial imperative program ought scary nest computed gotos need right higherlevel interface top raw thread lock generally appears necessary tradeoff flexibility correctness lot widely known approach parallelism maximize one expense raw thread many higherlevel framework flexible hard know code correct pure functional code statically guaranteed determinism side effect severe restriction checkedthreads attempt offer different balance determinism guaranteed testing instead statically imperative language program albeit le synchronization option available raw thread hope like nt hesitate email need sort support particular gladly port thing platformslanguages people interested
424,Lobsters,scaling,Scaling and architecture,"Real-world benchmarking of cloud storage providers: Amazon S3, Google Cloud Storage, and Azure Blob Storage",http://lg.io/2015/10/25/real-world-benchmarking-of-s3-azure-google-cloud-storage.html,realworld benchmarking cloud storage provider amazon google cloud storage azure blob storage,really mind moving dropbox google cloud storage azure storage provider fancy new imac equivalentlysexy macbook monkeybrains mimosa testing testing methodology large file small file many instruction methodology mounting filesystems gcsfuse panic transmit app transmit disk patch new signedonly kext change methodology b using sync sdks gsutil awscli blobxferpy result conclusion,really mind moving dropbox google cloud storage azure storage provider recently work using fancy new imac yet home outandabout use equivalentlysexy macbook stuff computer though happens one way faster bigger screen slower though portable much practical lifestyle need access file typically solve problem use dropbox similar service one thing strongly dislike specifically dropbox really want sync everything every computer seems extremely wasteful might want storage overhead syncing personal photo onto work computer ideally file could uploaded service downloaded demand understand dropbox selective sync want able mount stuff onto raspberry pi example got curious though major cloud service actually focused performance symmetric connection home work thx monkeybrains mimosa pic figured optimize lowlocalstorage requirement yet ok highbandwidth usage testing amazon aws google cloud storage microsoft window azure blob storage would wanted also test backblaze thingy invited beta program yet testing methodology three service want test realworld thing large file created file using mkfile small file found git repo machine copied git directory many instruction git directory synced removed file cloud provider also point used consumer side run laptop service storage datacenter closest san francisco ca methodology mounting filesystems google cloud storage gcsfuse gcsfuse limitbytespersec limitopspersec debuggcs lgstorage cloud time cp time cp r projmyrepogit cloudmyrepogit time rm rf cloudmyrepogit amazon aws transmit app aws used panic transmit app mount disk along transmit disk patch would tried use mount via fuse since running el capitan laptop new signedonly kext change completely broken ability use used cp rm command google cloud storage microsoft window azure blob storage tool exist mount disk mac methodology b using sync sdks using sync sdks although easier bring downside syncing filesfolders contrary whole premise exercise said could useful comparison google cloud storage gsutil time gsutil rsync r desktop g lgstorage time gsutil rsync r projmyrepogit g lgstoragemyrepogit time gsutil rsync r empty g lgstoragemyrepogit amazon aws awscli time aws sync storageclass reducedredundancy delete desktop lgstorage time aws sync storageclass reducedredundancy delete projmyrepogit lgstoragemyrepogit time aws sync storageclass reducedredundancy delete empty lgstoragemyrepogit microsoft window azure blob storage blobxferpy time blobxfer storageaccountkey lgfilestorage lgfilestorage time blobxfer storageaccountkey lgfilestorage lgfilestorage projmyrepogit unfortunately blobxferpy provides way delete file blob storage created issue result variety thing note result request service super slow debugging noticed average time get metadata file exists mounting lot statlike request done slowing thing dramatically weird latency high think miliseconds using kind disk mounting bad idea fwiw expected expect bad guess make sense though storage provider idea file going access parallelize anything using sync sdk method usually launch ton parallel request since know file folder need transferred azure tooling nonwindows extremely poor right literally one guy fred building tooling u nonwindows folk immaturity actually available run many test including file deletion azure also form mounting azure storage local filesystem feel like could really hurting microsoft adoption amongst people business use real operating system automation using service sync sdks surprisingly quite fast even footing unlike disk mounting method azure fastest average quite noticeable actually conclusion result good originally set yes great possiblyfasterthandropbox solution want sync everything everywhere disk mounting method though possible still extremely slow make almost impractical trying perhaps tool someone build something us sync sdk method transfer file something else directory listing appears user touching file folder say git repo tool would pull whole repo locally parallel maybe accessing file folder download whole thing think space sort smart caching tool also autodeletes infrequently used file depending free disk space specified quota way stored computer minimal still access everything everywhere assuming good internet connection data collected comparing three provider azure seems fastest hardest use google quite fast better tooling aws also quite fast probably best tooling even io apps browse bucket overall fml know please email trivex gmailcom idea
425,Lobsters,scaling,Scaling and architecture,Startup: Thrift Protocol and Transport,https://docs.google.com/presentation/d/1WfdcV6Wfgeu9cLcgdWHpRmvUulWGCHumd-S8dTTx0Jc/present,startup thrift protocol transport,,javascript nt enabled browser file ca nt opened enable reload
426,Lobsters,scaling,Scaling and architecture,Optimizing Sidekiq,http://www.mikeperham.com/2015/10/14/optimizing-sidekiq,optimizing sidekiq,optimizing sidekiq six time faster get baseline load testing script second bare thread second asynchronous status second parallel fetch toxiproxy second bonus memory latency le garbage drawback must conclusion sidekiq pro sidekiq enterprise github pull request,optimizing sidekiq sidekiq reputation much faster competition always room improvement recently rewrote internals made six time faster quite since touched sidekiq core design intentional last year sidekiq stabilized become reliable infrastructure ruby developer trust building application stop wondering though would happen change tweak recently decided embark experiment hard would remove celluloid could convert sidekiq use bare thread like celluloid much easier make concurrent programming also sidekiq largest dependency celluloid change sidekiq must accomodate change get baseline first thing write load testing script execute noop job could judge whether change effective script creates job redis boot sidekiq worker thread print current state every second queue drained script ran second mri bare thread baseline established spent day porting sidekiq core use nothing plain old thread easy day stable system improvement impressive load testing script ran second every abstraction cost benefit celluloid allows reason build concurrent system much quicker small runtime cost asynchronous status rewritten core stable test passing ran rubyprof load testing script see low hanging fruit profiler showed processor thread spending time sending job status data redis sidekiq processor thread execute job concurrently thread called redis start finish job get precise status cost two network round trip optimize changed processor thread update global status structure memory changed process heartbeat contact redis every second update status part heartbeat sidekiq processing jobssec save round trip result load testing script ran second parallel fetch last major change made noticed mri using cpu jruby using script execution four core laptop using hunch sidekiq always used single fetcher thread retrieve job redis one time test theory introduced latency redis network connection using shopify nifty toxiproxy gadget immediately script execution time shot five minute processor thread starving waiting single thread deliver job one time slow network refactored thing move fetch code processor thread processor thread call redis block waiting job appear along async status change make sidekiq much resilient redis latency fetch happening parallel script ran second even latency jruby us cpu process jobssec bonus memory latency also ran script gc disabled optimization sidekiq executed job using memory optimization sidekiq executed number job memory word optimization result le garbage measured job execution latency time required client one process create job push redis sidekiq pick execute worker latency dropped versionlatencygarbage created processing jobstime process jobsthroughput jobsec jobssec data collected mri running mbp w drawback trade offs consider change redis connection use previously single fetcher thread would block redis processor thread block redis meaning must redis connection concurrency setting sidekiq default connection pool sizing concurrency work great job status busy tab web ui realtime page render may delayed second celluloid longer required sidekiq application us need pull initialize conclusion keep mind talking overhead executing noop job overhead dwarfed application job execution time expect see radical speedup application job said dramatic lowering job overhead still nice win sidekiq user especially lot fast job effort become sidekiq coming later fall made possible sale sidekiq pro sidekiq enterprise rely sidekiq every day please upgrade support work see github pull request gory detail
427,Lobsters,scaling,Scaling and architecture,Making the Case for Building Scalable Stateful Services in the Modern Era,http://highscalability.com/blog/2015/10/12/making-the-case-for-building-scalable-stateful-services-in-t.html,making case building scalable stateful service modern era,data shipping paradigm wasteful repeatedly pull resource load balanced service data locality low latency data intensive application function shipping paradigm highly available stronger consistency model sticky connection give client easier model reason open persistent http connection backpressure must implemented implement routing cluster decouple memory lifetime process lifetime cautious read paper,long time stateless service royal road scalability nearly every treatise scalability declares statelessness best practice approved method building scalable system stateless architecture easy scale horizontally requires simple roundrobin load balancing love perhaps increased latency roundtrips database maybe complexity caching layer required hide database latency problem even troublesome consistency issue stateful service preserving identity shipping function data instead shipping data function better approach often hear much build stateful service fact search little way systematic approach building stateful service wikipedia even entry stateful service caitie mccaffrey tech lead observability twitter fixing refreshing talk gave strange loop conference building scalable stateful service slide refreshing never quite heard building stateful service way caitie talk building recognize idea sticky session data shipping paradigm function shipping paradigm data locality cap cluster membership gossip protocol consistent hashing dht weave around theme building stateful service compelling way highlight talk caitie tie whole talk together around discussion experience developing halo using microsoft orleans top azure orleans get enough coverage based inherently stateful distributed virtual actor model highly available gossip protocol used cluster membership two tier system consistent hashing plus distributed hash table used work distribution approach orleans rebalance cluster node fails capacity addedcontracted node becomes hot result halo able run stateful orleans cluster production cpu utilization across cluster orleans nt example system covered facebook scuba uber ringpop also analyzed using caitie stateful architecture framework also interesting section facebook cleverly implement fast database restarts large inmemory database decoupling memory lifetime process lifetime let jump learn build stateful service stateless service wasteful stateless service worked well storing canonical source truth database horizontally scaling adding new stateless service instance needed effective problem application state hitting limit one database cut anymore response sharding relational database using nosql database give strong consistency cause part database abstraction leak service data shipping paradigm client make service request service talk database database reply data service computation reply sent client data disappears service next request load balanced different machine whole process happens wasteful repeatedly pull resource load balanced service application involve chatty client operating session period time example game ordering product application updating information stateful service easier program caveat stateful service magic stateless service still incredibly effective horizontal scalability requirement yet stateful service offer lot benefit data locality idea request shipped machine holding data need operate benefit low latency hit database every single request database need accessed data fall memory number network access reduced data intensive application client need operate bunch data accessible response returned quickly function shipping paradigm client make request start session database accessed one time get data data move service request handled data left service next time client make request request routed machine operate data already memory avoided extra trip database reduces latency even database request handled statefulness lead highly available stronger consistency model cap world different level consistency operate available others partition cp system chose consistency availability ap system chose availability consistency want highly available system ap get write read monotonic read monotonic write definition sticky connection data single user single machine stronger consistency guarantee like read writes pipelined random access memory werner vogel whether readyourwrite session monotonic consistency achieved depends general stickiness client server executes distributed protocol server every time relatively easy guarantee readyourwrites monotonic read make slightly harder manage load balancing faulttolerance simple solution using session sticky make explicit provides exposure level client reason sticky connection give client easier model reason instead worrying data pulled lot different machine worry concurrency client talking server easier think big help programming distributed system building sticky connection client make request cluster server request always routed machine simplest way open persistent http connection tcp connection easy implement connection mean always talking machine problem connection break stickiness gone next request load balanced another server problem load balancing implicit assumption sticky session last amount time generating amount load generally case easy overwhelm single server many connection dog pile onto server connection last long time lot work done connection backpressure must implemented server break connection overwhelmed cause client reconnect hopefully le burdened server also load balance based amount resource free spread work equitably something little smarter implement routing cluster client talk server cluster route client server containing correct data two capability required cluster membership static cluster membership simplest approach ok start dumbest thing possible see meet need simplest approach config file contains address node cluster config file distributed machine easy operationally painful great choice service highly available fault tolerant machine fails must replaced configuration updated difficult expand cluster add machine entire cluster must restarted work rebalanced correctly across cluster dynamic cluster membership add capacity compensate failure node added cluster immediately start accepting load capacity also reduced removing node two main way handle cluster membership gossip protocol consensus system gossip protocol emphasise availability gossip protocol spread knowledge group sending message message talk talk alive dead machine figure data collect world view cluster stable state machine cluster converge world view alive dead case network failure network partition capacity added deleted different machine cluster different worldviews cluster tradeoff high availability coordination necessary every machine make decision based worldview code able handle uncertainty getting routed different node failure consensus system emphasise consistency node cluster exact worldview consensus system control ownership everyone cluster configuration change node update worldview based consensus system hold true cluster membership problem consensus system available node route work know cluster problem slower coordination added system need highly availability consensus system avoided unless really necessary work distribution work distribution refers work moved throughout cluster three type work distribution system random placement consistent hashing distributed hash table random placement sound dumb effective work scenario lot data query operating large amount data distributed around cluster write go machine capacity read requires querying every single machine cluster get data back sticky connection stateful service good way build inmemory index cache consistent hashing deterministic placement deterministic placement request node based hash perhaps session id user id depending workload partitioned node get mapped cluster request mapped ring walk right find node going execute request database like cassandra often use approach deterministic placement problem hotspot lot request end hashed node node overwhelmed traffic node slow perhaps disk going bad overwhelmed even normal amount request consistent hashing allow work moved way cooling hotspot allocate enough space cluster run enough headroom tolerate deterministic placement fact work moved additional capacity additional cost must absorbed even normal case distributed hash table dht non deterministic placement hash used lookup distributed hash table locate work sent dht hold reference node cluster nondeterministic nothing forcing work go particular node easy remap client go node node becomes unavailable becomes hot three example stateful service real world facebook scuba random fanouts writes scuba fast scalable distributed inmemory database used code regression analysis bug reporting revenue performance debugging info fast always available thought us static cluster membership though explicitly stated paper scuba us random fanouts writes read every single machine queried result returned composed machine running query result returned user real world machine unavailable query run best effort availability node available query return result data available along statistic percentage data processed user decide result meet high enough threshold quality sticky connection used data inmemory lookup fast uber ringpop gossip protocol consistent hashing ringpop nodejs library implementing applicationlayer sharding info uber concept trip start trip user order car requires rider information location information data updated trip throughout ride payment must processed end trip would inefficient update load balanced different stateless server every time data would constantly persisted database pulled back introduces lot latency extra load database design implement routing logic request user directed single machine swim gossip protocol used maintain cluster membership ap cluster membership protocol guaranteed always correct availability chosen correctness important user always order car consistent hashing used route work throughout cluster hot node problem remedy add capacity even node utilized microsoft orleans gossip protocol consistent hashing distributed hash table orleans runtime programming model building distributed system based actor model info orleans came microsoft research extreme computing group presenter caitie mccaffrey worked group productize orleans process shipping halo halo service rebuilt primarily orleans actor core unit computation actor communicate using asynchronous message throughout cluster actor receives message one following send one message update internal state create new actor cluster based actor model bunch state machine running actor model inherently stateful persisting state request halo would deploy cluster machine orleans took care rest request would go machine cluster cluster would look actor lived cluster route message actor hundred thousand actor run single machine cluster gossip protocol used cluster membership order highly available since orleans open sourced zookeeper implementation created slower work distribution orleans us combination consistent hashing distributed hash table request actor sent cluster orleans runtime calculate consistent hash actor id hash map machine distributed hash table id distributed hash table actor know machine contains actor specified id consulting dht request routed appropriate machine consistent hashing used find actor dht deterministic operation location dht cluster change since amount data dht small actor dhts evenly distributed hotspot big concern actor evenly distributed balanced orleans ability automatically rebalance cluster entry dht updated point new machine session dy machine failed new machined must assigned actor evicted memory nobody talking machine get hot orleans move actor different machine capacity hot machine fail rebalancing feature orleans core reason orleans cluster could run production cpu utilization across cluster able move work around nondeterministic fashion mean use capacity box approach may work database work great pulling state service go wrong unbounded data structure stateful system unbounded data structure good way die example unbounded queue unbounded inmemory structure something think often stateless service recover kind transient failure stateful service service get lot request explicit bound must put data structure may keep growing may run memory garbage collector may stop world node may look dead would also add lock held long time data structure grow client friend client want machine die implicit assumption run memory client send reasonable amount data code must protect client memory management data persisted memory lifetime session perhaps minute hour even day memory move longest lived generation garbage collection memory generally expensive collect especially reference spanning generation aware memory work stateful service need actually know garbage collector running could write everything unmanaged code like c deal garbage collector problem orleans run net clr garbage collected environment run cross generational garbage collection problem handled tuning garbage collector also handled realizing lot unnecessary state persisted careful persisting associated cost reloading state typically stateless service database must queried request latency must tuned account round trip database use cache make faster stateful service variety different time state reloaded first connection recovering crash deploying new code first connection generally expensive connection data node data must pulled database request handled could take long time want careful load startup want high latency hit request happens land first connection testing make use percentile average latency look good round trip database every time first connection latency may spike want miss first connection client time access database slow continue trying load database know client retry next access client fast data likely memory stateless service make kind optimization halo example game started first connection would sometimes timeout perhaps user lot game state connection loaded kept pulling data gamebox retried request would succeed latency user noticeable especially given pretty animation user watching recovering crash deploying new code facebook scuba product restart problem inmemory database storing lot data persisted hard disk crash deploy would take machine currently running spin new machine read everything disk took hour per machine slas facebook slow rolling update cluster took hour crash happen often slow restart big concern would like code deploys happen frequently iterate faster try new idea le risky deployment facebook made key observation decouple memory lifetime process lifetime especially stateful service facebook wanted deploy new code knew safe shutdown memory corrupted would stop taking request copy data currently running process shared memory shutdown old process bring new process data shared memory would copied back process memory space request would restart process take minute cluster restart order two hour instead hour possible deploy new code frequently previously possible twitter considering implementing strategy stateful index whole machine restarted talk manhattan database really slow compared memory wrapping lot work thought go cluster membership work distribution one right answer lean available side prefers gossip protocol work distribution depends work load bunch successful stateful system running real world proven make work scale super scary though new ground cautious new territory done stateful service go different changed assumption making make sure make assumption explicit read paper reinvent protocol actually new territory people working since talk come database literature problem already solved cherrypick care based application implement whole paper implement piece paper want related article
428,Lobsters,scaling,Scaling and architecture,CQRS revisited,https://lostechies.com/gabrielschenker/2015/04/07/cqrs-revisited/,cqrs revisited,introduction writes lesson learned reading data writing data classical ntier application event sourcing cqrs summary,introduction opinion command query responsibility segregation cqrs one useful architectural pattern used context complex line business application lob martin fowler writes heart simple notion use different model update information model use read information observation lot today application unnecessarily complex slow since use cqrs high level typical application read write operation handled domain model backend often domain model anemic consists mainly entity pure data container dtos service manipulate entity sometimes repository pattern used abstract data store domain perspective case might see repository interface look similar see interface method query data method change data problem reading querying data fundamentally different concern changing data keep two type operation separate various reason reading data usually happens much frequently writing data many application frequency reading versus writing easily even reading data usually want retrieve quite bit data list record record associated data record eg customer list order shipping billing address etc read operation change data rather provide glimpse snapshot current state observer repeating read operation always return result hand write operation typically affect small set data often one single data record affected write operation even property overall data record changed write operation change state system thus side effect world look different write operation read operation need fast user easily frustrated query take half second user much tolerant come write operation know something important happens system thus tolerate longer response time lesson learned looking point mentioned learn quite bit better job application reading data since read operation need fast make sure data accessed way need least amount db query possible specific context complex business logic need executed retrieving data business logic executed application try modify data aggregated data calculatedaggregated fly querying data precalculated happen whenever data changed affect aggregated value rather rare compared read operation stated read operation never change data read operation need sideeffect free writing data discussed write operation typically affect limited set data let say operation add product xyz shopping cart ecommerce application operation add item shopping cart object consisting product number quantity shopping cart identified shopping cart id thus command send backend might look like see although operation important one limited set data sent backend processing note name command give context operation whilst payload command contains minimal amount information needed successfully fulfill requested operation since clearly distinguishing read write operation command return data maybe status message telling sender whether operation succeeded possibly error message displayed user command failed since business rule violated implementation new world read write operation separated high level diagram application look like classical ntier application let assume lob application using rdbms sql server oracle mysql etc store data net application framework could use eg nhibernate entity framework orm write operation whilst using bare bone adonet query use database view simplify data access reading data view often provide good mean present data denormalized fashion ideal query use approach continue using repository pattern resulting customer repository might look much simpler get method retrieve customer record entity want change save method add new instance type customer completeness also added delete method remove existing customer data store although usually tend avoid imho data never physically deleted data totally different discussion event sourcing cqrs using event sourcing e another architectural pattern come following high level diagram event generated domain result command change state written event store query hand get data read model separate event store eventual consistent write model event store process running background asynchronously dispatch event flowing event store observer build read model particular case read model could document database like mongo db raven db andor file based full index like elastic search solr summary cqrs one preferred architectural pattern come complex lob application unfortunately lot existing lob application follow rather straight forward pattern lead lot unnecessary complexity execution path command query tightly coupled thus individually tweaked tuned cqrs solves problem quite nicely allows u build best possible system given context cqrs limited modern type application using event sourcing andor ddd applied application read writes data
429,Lobsters,scaling,Scaling and architecture,Why Intel added cache partitioning,http://danluu.com/intel-cat/,intel added cache partitioning,intel added cache partitioning typical server utilization google demonstrated utilization xkcd estimated google owns million machine per machine per year ferdman et al kanev et al isca paper looked workload google lo et al heracles paper isca year explores great detail core cpuset network qdisc lwn series cgroups neil brown good place start power intel running average power limit percore dynamic voltage frequency scaling cache cache allocation technology april whitepaper call cache allocation technology cat attack recover rsa key across vms via llc interference dram bandwidth putting together standard model datacenter operating cost observation core waiting mean use bandwidth broadwell v miss also miss llc go main memory liked post probably also like talk dick site various performance profiling related topic post intel new clwb pcommit instruction post new cpu feature,intel added cache partitioning typical server utilization google demonstrated utilization without impacting latency slas xkcd estimated google owns million machine estimate amortized total cost per machine per year billion per year number like even small improvement large impact nt small improvement possible get better utilization hardware low end typical utilization number come service variable demand fixed machine allocation say machine dedicated jenkins machine might busy devs active might also utilization dynamic allocation switching machine work needed get typical latencysensitive service somewhere range better across wide variety latencysensitive workload tight slas need way schedule low priority work machine without affecting latency high priority work obvious possible high low priority workload need monopolize shared resource like lastlevel cache llc memory bandwidth disk bandwidth network bandwidth luck exception specialized service rare max disk network cache memory turn ferdman et al looked back found typical server workload nt benefit llc despite modern server chip much larger cache graph scaleout workload thing like distributed keyvalue store mapreducelike computation web search web serving etc specint mcf traditional workstation benchmark server old school server benchmark like specweb tpc see going llc small effect typical datacenter workload significant effect traditional workstation benchmark datacenter workload operate large data set often impossible fit dataset ram single machine let alone cache making larger llc particularly useful result confirmed kanev et al isca paper looked workload google also showed memory bandwidth utilization average quite low might think low bandwidth utilization workload compute bound nt many memory access however author looked core found lot time spent stalled waiting cachememory row google workload running typical workload core spend somewhere time blocked cachememory curious low cache hit rate lot time stalled cachememory low bandwidth utilization suggestive workload spending lot time waiting memory access kind dependency prevent executed independently llcs highend server chip even though need get performance ile utilization bandwidth seems like waste resource lot resource sitting idle used effectively good news since get low utilization shared resource chip able schedule multiple task one machine without degrading performance great happens schedule multiple task one machine lo et al heracles paper isca year explores great detail goal heracles get better utilization machine colocating multiple task machine figure show three latency sensitive lc workload strict slas websearch query serving service google search mlcluster realtime text clustering memkeyval keyvalue store analogous memcached value latency percent maximum allowed sla column indicate load service row indicate different type interference llc dram network exactly sound like custom task designed compete resource hyperthread mean interfering task spinloop running hyperthread core running hyperthread nt even considered since o context switch expensive cpu power task designed use lot power induce thermal throttling brain deep learning interference task run container low priority lot going figure immediately see best effort task like schedule ca nt coexist lc task container priority used brain row red even low utilization leftmost column latency way sla latency also clear different lc task different profile handle different type interference example websearch mlcluster neither network compute intensive handle network power interference well however since memkeyval network compute intensive ca nt handle either network power interference paper go lot detail infer detail table find one interesting part paper going skip recommend reading paper interested kind thing simplifying assumption author make type interference basically independent mean independent mechanism isolate lc task much individual type resource sufficient prevent overall interference set cap type resource usage stay cap however assumption nt exactly true example author show figure relates llc cache size number core allocated lc task vertical axis max load lc task handle violating sla allocated specific llc number core see possible trade cache v core mean actually go resource cap one dimension maintain sla using le another resource general case might also able trade resource however assumption deal resource independently reduces complex optimization problem something relatively straightforward let look type shared resource interference heracles allocates resource prevent slaviolating interference core pinning lc task different core sufficient prevent samecore context switching interference hyperthreading interference heracles used cpuset cpuset allows limit process child run limited set cpu network local machine heracles used qdisc enforce quota cpuset qdisc quotapartitioning mechanism lwn series cgroups neil brown good place start cgroups used lot widely used software docker kubernetes mesos etc probably worth learning even nt care particular application power heracles us intel running average power limit estimate power feature sandy bridge newer processor us onchip monitoring hardware estimate power usage fairly precisely percore dynamic voltage frequency scaling used limit power usage specific core keep going budget cache previous isolation mechanism around one new broadwell chip released problem task need llc lc task need llc single large allocation task scribble llc shared wiping cached data lc need intel cache allocation technology cat allows llc limit core access different part cache since often want pin performance sensitive task core anyway allows u divide cache pertask basis intel april whitepaper call cache allocation technology cat simple benchmark comparing cat v nocat example measure latency respond pcie interrupt another application heavy cputomemory traffic cat condition min max avg cat cat cat average latency latency without cat tail latency nt improve much also substantial improvement interesting interesting question effective real workload see put mechanism together another use cat going discus prevent timing attack like attack recover rsa key across vms via llc interference dram bandwidth broadwell newer intel chip memory bandwidth monitoring control mechanism work around heracles drop number core allocated task interfering lc task using much bandwidth coarse grained monitoring control inefficient number way detailed paper still work despite inefficiency however percore bandwidth limiting would give better result le effort putting together graph show effective utilization lc websearch task scheduled enough slack sla websearch nt violated barroom conversation folk company baseline red already look pretty good utilization peak time hour trough utilization heracles worst case utilization average amazing note effective utilization greater since measured throughput lc task single machine load plus throughput task single machine load example one task need dram bandwidth network bandwidth task need opposite two task would able colocate machine achieve effective utilization real world might get average utilization system like heracles recalling operating cost estimate billion large company company already quitegood average utilization using standard model datacenter operating cost expect throughput per dollar million free compute talking smaller company way becoming large company spend range million million year compute often utilization range using total cost model expect get increase compute per dollar million million year free compute depending size observation paper looked lot interesting gem going go jumped arm atom server known long time datacenter machine spend approximately half time stalled waiting memory addition average number instruction per clock server chip able execute real workload quite low top row horizontal bar internal google workload bottom row green dot workstation benchmark spec standard benchmark suite see google workload lucky average instruction per clock also previously saw workload cause core stalled memory least half time despite spending time waiting memory averaging something like half instruction per clock cycle highend server chip much better atom arm chip real workload reddi et al tocs sound bit paradoxical chip waiting memory need highperformance chip tiny arm chip wait effectively fact might even better waiting since core waiting mean use bandwidth turn server also spend lot time exploiting instructionlevel parallelism executing multiple instruction time graph many execution unit busy time almost third time spent execution unit busy long stall waiting memory highend chip able get computation done start waiting next stall earlier something else curious server workload much higher instruction cache miss rate traditional workstation workload code data prioritization technology top row horizontal bar internal google workload bottom row green dot workstation benchmark spec standard suite benchmark suite author attribute increase instruction miss two factor first normal deploy large binary overwhelm instruction cache second instruction compete much larger data stream space cache cause lot instruction get evicted order address problem intel introduced call code data prioritization technology cdp extension cat allows core separately limit subset llc instruction data occupy since targeted lastlevel cache nt directly address graph show cache miss rate however cost cache miss hit llc something like broadwell v miss also miss llc go main memory substantial difference kanev et al propose going step split icachedcache hierarchy nt exactly radical idea cache already split everything else guess intel major chip vendor simulation result showing nt improve performance per dollar know maybe see split cache soon spec general observation spec basically irrelevant benchmark somewhat dated workstation benchmark simply completely inappropriate benchmark server office machine gaming machine dumb terminal laptop mobile device market spec designed getting smaller every year spec nt even really representative market least decade yet among chip folk still widely used benchmark around search query look like google query come wide fanout set rpcs issued set machine first row machine also set rpcs second row rpcs third row fourth row shown graph much going look like noise one quite normal type workload datacenter nothing spec look like lot fun tidbit paper recommend reading thought anything post interesting liked post probably also like talk dick site various performance profiling related topic post intel new clwb pcommit instruction post new cpu feature thanks leah hanson david kanter joe wilder nico erfurth jason davy commentscorrections
430,Lobsters,scaling,Scaling and architecture,"AtlasDB, a transactional layer for Distributed Key-Value Stores, is now open source",https://github.com/palantir/atlasdb,atlasdb transactional layer distributed keyvalue store open source,join github today sign,dismiss join github today github home million developer working together host review code manage project build software together sign
431,Lobsters,scaling,Scaling and architecture,Container Camp London 2015 (Playlist),https://www.youtube.com/playlist?list=PLcHZXHMeDzxUrNpD2Tms-zrZn9etw6JcQ,container camp london playlist,,
432,Lobsters,scaling,Scaling and architecture,Mercury: Hybrid Centralized and Distributed Scheduling in Large Shared Clusters,http://research.microsoft.com/pubs/238833/mercury-tr.pdf,mercury hybrid centralized distributed scheduling large shared cluster,,datacenterscale computing analytics workload increasingly common high operational cost force heterogeneous application share cluster resource achieving economy scale scheduling large diverse workload inherently hard existing approach tackle two alternative way centralized solution offer strict secure enforcement scheduling invariant eg fairness capacity heterogeneous application distributed solution offer scalable efficient scheduling homogeneous application argue solution complementary advocate blended approach concretely propose mercury hybrid resource management framework support full spectrum scheduling centralized distributed mercury expose programmatic interface allows application tradeoff scheduling overhead execution guarantee framework harness flexibility opportunistically utilizing resource improve task throughput experimental result show gain productionderived workload benefit translated appropriate application operator policy job throughput job latency improvement implemented currently opensourcing mercury extension apache hadoop yarn
433,Lobsters,scaling,Scaling and architecture,"Ready-Made Java Containers, Part 2",https://deis.com/blog/2015/ready-made-java-containers-2,readymade java container part,common us azure kubernetes service ak,common us azure kubernetes service ak migrate existing application cloud build complex application us machine learning take advantage agility offered microservices architecture
436,Lobsters,scaling,Scaling and architecture,Provisioning and Capacity Planning,http://www.slideshare.net/brianbrazil/provisioning-and-capacity-planning-workshop-dogpatch-labs-september-2015,provisioning capacity planning,user agreement privacy policy privacy policy user agreement,slideshare us cooky improve functionality performance provide relevant advertising continue browsing site agree use cooky website see user agreement privacy policy slideshare us cooky improve functionality performance provide relevant advertising continue browsing site agree use cooky website see privacy policy user agreement detail
437,Lobsters,scaling,Scaling and architecture,"Let a 1,000 flowers bloom. Then rip 999 of them out by the roots.",http://www.gigamonkeys.com/flowers/,let flower bloom rip root,let flower bloom rip root gave talk scaling software let thousand flower bloom jack dorsey claimed tweet alex roetter garden overrun monorepi chris fry nandini ramani think engineering effectiveness build model weeding garden,let flower bloom rip root september past eleven month tech lead engineering effectiveness group engineering effectiveness name elsewhere might called developer tool developer productivity engineering infrastructure developer efficiency provide tool process allow rest twitter engineer job focusing thing used twitter engineer super twitter specific build tool continuous integration system source control recently gave talk scale conference team like engineering effectiveness scale work essay extended dance remix talk tl dr however industry mostly know consequently massively underinvest making engineering orgs actually effective every software company start single line code written first developer grows twitter facebook company happens industry part know scale software know build abstraction modularize code manage large code base deploy software handle demand million even billion user also know scale organization putting necessary management structure allow thousand people work together le efficiently hand argue really yet good handle scale area exists intersection engineering human place group like engineering effectiveness work worse often seems even understand importance scaling go first line code thousand engineer working million line code rate bit twitter history would suggest scaling software let thousand flower bloom first line written interim ceo jack dorsey later claimed tweet originally wanted write python c ocaml soon hired rail core contributor went ruby rail started simple rail app grew eventually probably largest monolithic rail app planet known monorail however twitter ruby long twitter acquired fiveperson search company whose technology stack java based time nine twitter engineer handful ops people adding five new engineer came code base way working immediately increased diversity engineering style practice within twitter let thousand flower bloom also year first scala written twitter main product twittercom rail app search using java folk experimenting scala let thousand flower bloom someone started repo called science time creating new repo new project one big deal however repo grew become main repository java code included search code ad code associated data science code need operate data structure defined ad code platform group moving toward scala partly scala could made look like ruby java could ad team going direction alex roetter svp engineering engineer personally led effort convert scala code ad team already written java also around time couple engineer started trying make science googlestyle monorepo homegrown build system pant let thousand flower bloom finally arrive year quadrennial soccer world cup excitement cup high twitter bit high fact le every time someone scored goal tweet per second would spike knock site gooooaaaaallll fail whale gooooaaaaallll fail whale consequently aftermath world cup decided really needed move monorail onto jvm based service oriented architecture around time marius eriksen principal engineer others working new scala rpc library finagle soon became standard around new service written effort began earnest started teaching ruby developer scala could write service replace monorail ad side data scientist started work scala dsl writing map reduce job became scalding soon three kind scala written twitter scala written people wished ruby scala written people wished java scala written people wished haskell let thousand flower bloom yet despite diversity perhaps made real progress reliability scaling software fast forward four year next world cup totally different story handled massive tweet volume basically without million tweet shellacking brazil semifinal peak tweet per minute shortly fifth goal lot thing obviously happened four year discussed including lot specific work runup world cup point figure scale software garden overrun around time celebrating success handling world cup traffic thing coming head internal tool front science repo grown one two monorepi monorepo formed scala service spun offthemonorail effort started separate repos eventually consolidated single repo birdcage order make easier finagle developer upgrade library client together might ask scala developer decided needed monorepo move code science make another monorepo good question tooling issue build system science exgoogle engineer inspired blaze build system time birdcage made pant know build scala pant view inherited google repository organized odds scala repos built scala build organized also perhaps nerd posturing fan java fan scala real issue job team called developer productivity somewhat involved charter people say really work necessary get scala code science make another monorepo force java scala partisan find way get along unfortunately finagle also really taken eventually code science started taking dependency finagle code birdcage took dependency library science update one side published consumed side time consuming process one hop project multiple back forth hop two monorepi situation could visualized suggested eponymous monorepi twitter account somewhere along way someone decided would easier convert birdcage use pant since learned build scala deal mavenstyle layout however point prior pant open sourced throw wall fashion picked engineer company square foursquare moved forward meantime enough people whose job take care thing science still original internally developed version fact evolved independently open source version however time wanted move birdcage onto pant open source version moved ahead one birdcage folk chose make matter insufficient version pant worked entirely reliably consequently different team evolved cargoculted strategy making work well enough get team included running script washpants aggressively wiped state pant cached run speed build also started using thrift data interchange format agree thrift compiler use whether published thrift idl jar already compiled artifact definitely people opinion bunch engineer nobody position make time work make stick let thousand flower bloom indeed say mess arrived twitter april work ab experimentation system full conversion birdcage pant fact completed year half later late svp engineering chris fry declared would move code monorepo basically meant merging science birdcage may said definitely monorepo july gone twitter couple week announcement new head engineering alex roetter last seen excising scala code ad code base reaffirmed goal date came went monorepo month became head engineering roetter decided enough enough hired new vp bos nandini ramani head new organization engineering effectiveness would subsume old developer productivity group team order expand scope work could support rest twitter engineering recruited ee couple month since reinvigorated renamed engineering effectiveness team finishing project like monorepo consolidation also thinking take actually provide worldclass tool support twitter engineer happened could twitter scale software well yet much trouble software built help u job think engineering effectiveness think big part problem good thinking make engineer effective software especially backend software measure goodness number query per second handle number incident experience amount hardware buy run thing easy measure even fairly easy tie financial implication business effectiveness hand hard measure even really know make people productive thus talk engineer though thing even study lead notion engineer pointed strongly notion office agree think possible affect productivity least possible harm twitter ee motto speed three thing trying affect across twitter engineering unlike famous triple fast cheap good believe pick two fact feed building thing right let go faster building faster give time experiment find way right thing everybody enjoys building good stuff lot engineering effectiveness actually affect quality speed joy one place start simple time saving assume certain amount time engineer spend every day waiting thing put productive give people effectiveness boost eliminating time assuming standard standard save everyone five minute day get speed gain obviously save everyone five minute day every day working something everyone us time easy depends obviously much time currently induced tool process also possible save time removing thing periodically eat larger amount time extra hour spent every couple week debugging problem due confusing error message log equivalent five minute day thus total time year another dramatic way influence effectiveness help stay flow state assume heard psychological state flow aka everything clicking concentrate solve hard problem etc generally thought take fifteen minute get flow instant lose interrupted modern open plan office inimical flow essay another day present concern whether tool provide engineer helping keep flow fact thing breaking flow even using simple analysis time saving went one way save people fifteen minute day break flow one le time per day flow conducive tool probably provide way boost saving fifteen minute since flow powerful state losing flow costly flow also obviously name game come increasing developer joy bad flowbreaking interruption something much worse sapping effectiveness know dune fear mind killer fear manifest context software development would say tech debt tech debt mind killer tech debt lack quality slows u make u miserable break flow sap live financial debt small amount taken eye wide open good thing also like financial debt compound time little bit technical debt taken ten hundred engineer left book killing thousand engineer engineering effectiveness team often intimate relationship tech debt often pile tooling people whose job work tool tend something hacked together well enough get something done good news cleared accumulated tech debt tool team well positioned help team tackle tech debt lead really massive gain effectiveness nt gotten quite yet one dream ee twitter establish standing cruft slaying army would experienced senior folk chop dive section code base make code people working better another area engineering effectiveness team help coordinate engineer push good way thing stamp bad way whether code review test code write design doc anything else however expect ee team get involved question better make sure strong senior engineer part team making decision recommendation affect lot engineer finally psychological aspect providing good tool engineer believe really impact overall effectiveness one hand good tool pleasure work basis alone provide good tool reason many company provide awesome food employee make coming work every day much pleasure good tool play another important role tool use software spend day writing software bad tool corrosive psychological effect suggesting maybe actually know write good software intellectually may know different group working internal tool main feature product tool use get way obviously poorly engineered hard doubt overall competence build model take given thing increase quality software engineering org produce speed produce joy engineer experience work increasing real way ultimately benefit company lack better term call simple model total effectiveness engineering org e total effectiveness org eng total number engineer ee number engineer devoted engineering effectiveness style team b boost first ee engineer give remaining effectiveness represents additional ee engineer scale total productivity boost one ee engineer would add boost b le one seems likely new ee engineer smaller effect previous one obviously simple model like model wrong may useful assuming total number engineer le given two interesting parameter model scaling factor boost b frankly great idea set pretty sure le one would mean total boost would grow square root number ee engineer ie double total boost would square number ee engineer seems bit steep fall say reasonable guess b think way discussed ee folk help boost effectiveness engineer simple time saving helping keep engineer flow removing tech debt encouraging supporting good practice increasing joy providing excellent tool seems like across board effectiveness boost reasonable perhaps even conservative look graph total effectiveness depending many engineer devote effectiveness work assuming b say seem fairly conservative value later play model put value first plot see engineering org ten people fact worth devote engineer tooling maximum ten engineer worth get devote zero engineer engineering effectiveness small engineering org individual engineer probably automate thing bugging making simple tradeoff quickly time spend automation paid time saved automation worth dedicating anyone look closely chart however see look like straight line actually slightly curved devote one engineer ee work lose work gain back bit engineer making nine effective problem nine engineer benefit add enough make lost work ee engineer one hundred engineer parameter curve start bend noticeably enough engineer effectiveness gain make cost couple ee engineer model suggests devote two engineer ee bring total productivity engineer worth free engineer worth work however get thousand engineer small gain per engineer start add even though additional ee engineer adding le le effectiveness boost parameter right thousand person engineering org devote quarter engineering effectiveness yielding total effectiveness equivalent engineer price assume model still work engineering org grows another order magnitude engineer want third engineer ee style work giving total effectiveness equivalent engineer ee admit hard time imagining ee org bigger twitter also trouble imagining person engineering team note even step engineer optimal number ee engineer grew much faster total number engineer payoff disproportionately large important underinvest supporting engineer weeding garden one caveat however order engineering effectiveness engineer able boost effectiveness across engineering thing need standardized saw big investment engineering effectiveness work start pay work lot engineer may work person engineering org tool process working used hundred one hundred relevant number tearing flower root come engineering org get certain size benefit obtain investing making engineer slightly productive start swamp slight gain one team might get thing slightly different way thousand flower phase people planted kind exotic blossom lovely even well adapted local microclimate need able decide one going first class nurtured member garden one weed get delicate balance get right everyone would love make decision thing bring consistency long choose current preferred way thing hand thing sufficiently chaotic lot people happy someone come make decision thing actually make decision work need engineer good technical judgment technical chop get ahead chaos dig accumulated tech debt probably piled tool good news time deliver real improvement quality speed joy engineer earn little trust make decision goal pick set tool process support support heck invest probably think need focus relentlessly making tool process support awesome get senior engineer involved early make good choice engineering org get buy make reality get behind twitter may need invest even get garden back shape behind way really work well get flower blooming people team need find way get work done get point flower tend awesome people use real reason overwhelmed trying get existing stuff working time treat team going way chance learn something product offering quite meet need word garden tidy well tended pretty new volunteer sprout freak afraid going overrun garden watch grow look like might valuable contribution garden start nurture like rest flower middle twitter engineering effectiveness team year old still operating somewhat weed whacker mode lot opportunity dramatically improve effectiveness twitter engineer simply making existing tool better maybe year sophisticated model theory get everything sorted scale back ee style team take nearly many people tend wellmaintained garden whip one shape think path twitter took grew one ten hundred thousand developer different path company taken take future perhaps learn u consequence investing enough soon enough supporting engineer meantime wish quality speed joy work pleasant time tending garden correction earlier version essay said pant picked extwitter engineer company fact one company became major user pant extwitter employee also mistakenly identified world cup match germany brazil final semifinal
438,Lobsters,scaling,Scaling and architecture,"Dick Sites - ""Data Center Computers: Modern Challenges in CPU Design""",http://www.youtube.com/watch?v=QBu2Ae8-8LM,dick site data center computer modern challenge cpu design,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature dick site data center computer modern challenge cpu design youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature dick site data center computer modern challenge cpu design youtube
439,Lobsters,scaling,Scaling and architecture,First new cache-coherence mechanism in 30 years,http://news.mit.edu/2015/first-new-cache-coherence-mechanism-30-years-0910,first new cachecoherence mechanism year,time travel unexplored potential,modern multicore chip every core processor small memory cache store frequently used data chip also larger shared cache core accessif one core try update data shared cache core working data need know shared cache keep directory core copy datathat directory take significant chunk memory chip might percent shared cache percentage increase core count envisioned chip even core need efficient way maintaining cache coherenceat international conference parallel architecture compilation technique october mit researcher unveil first fundamentally new approach cache coherence three decade whereas existing technique directory memory allotment increase direct proportion number core new approach increase according logarithm number coresin chip mean new technique would require onethird much memory predecessor intel set release highperformance chip near future hypothetical advantage chip space saving rise percent chip percentwhen multiple core simply reading data stored location problem conflict arise one core need update shared data directory system chip look core working data sends message invalidating locally stored copy directory guarantee write happens stale copy data exist say xiangyao yu mit graduate student electrical engineering computer science first author new paper write happens read previous version happen write ordered previous read physicaltime order time travelwhat yu thesis advisor srini devadas edwin sibley webster professor mit department electrical engineering computer science realized physicaltime order distributed computation really matter long logicaltime order preserved core keep working away piece data core b since overwritten provided rest system treat core work preceded core b sthe ingenuity yu devadas approach finding simple efficient mean enforcing global logicaltime ordering assign time stamp operation make sure operation follow time stamp order yu sayswith yu devadas system core counter data item memory associated counter program launch counter set zero core read piece data take lease meaning increment data item counter say long core internal counter exceed copy data valid particular number matter much matter relative value core need overwrite data however take ownership core continue working locally stored copy data want extend lease coordinate data item owner core writing increment internal counter value higher last value data item countersay instance core read data setting internal counter incrementing data counter core e need overwrite data take ownership set internal counter internal counter designates operating later logical time core way back ahead idea leaping forward time give system name tardis timetraveling spaceship british science fiction hero dr whonow core try take new lease data find owned core e sends message core e writes data back shared cache core read incrementing internal counter higherunexplored potentialin addition saving space memory tardis also eliminates need broadcast invalidation message core sharing data item massively multicore chip yu say could lead performance improvement well see performance gain experiment yu say may depend benchmark industrystandard program yu devadas tested tardis highly optimized maybe already removed bottleneck yu say people looked sort lease idea say christopher hughes principal engineer intel lab least knowledge tend use physical time would give lease somebody say ok yes use data say cycle guarantee nobody else going touch amount time kind capping performance somebody else immediately afterward want change data got wait cycle whereas problem advance clock something knowledge never done key idea really neat hughes say however chip designer conservative nature almost massproduced commercial system based directorybased protocol say mess easy make mistake changing implementation part advantage scheme conceptually somewhat simpler current directorybased scheme add another thing guy done propose idea separate paper actually proving correctness important folk field
440,Lobsters,scaling,Scaling and architecture,Sharding Pinterest: How we scaled our MySQL fleet,https://engineering.pinterest.com/blog/sharding-pinterest-how-we-scaled-our-mysql-fleet,sharding pinterest scaled mysql fleet,win learn,win learn pinterest growing learning experimentation believe spirit exploration inside pinterest wall learn success epic failure
441,Lobsters,scaling,Scaling and architecture,Dynamic log formats in nginx,https://benwilber.github.io/nginx/syslog/logging/2015/08/26/dynamic-log-formats-in-nginx.html,dynamic log format nginx,wrote openrestyorg httpsetmiscmodule dynamic log format,time ago wrote way implement simple realtime pixel tracking using nginx redis syslogng somewhat novel approach log request csv format produce log line foobar easy enough parse know two field timestamp query parameter python split first instance comma confident accurately capture field regardless whether args also contained comma ie urlencoded event foobar bazthis comma timestamp args eventsplit print timestamp args foobar bazthis comma however break try use csvreader import csv stringio import stringio sio stringio event row csvreader sio print row foobar bazthis comma solution would quote args logging foobar bazthis comma csvreader work expected foobar bazthis comma happens args contains comma quote character foobar baz quoted predict problem going cause foobar bazthis quoted could fight day real solution always urlencode query parameter however always possible control client making request server want force urlencoding log folk openrestyorg developed whole slew neat nginx module complement primary goal bringing lua luajit nginx cool project encourage check familiar among useful module developed httpsetmiscmodule provides number utility encodedecodehashunhash variable use nginx use setescapeuri directive make sure variable want log always properly urlencoded thus avoiding problem might encounter parsing later let try logformat pixel msec escapedargs location pixelgif setescapeuri escapedargs args accesslog varlognginxpixelgifaccesslog pixel expires emptygif urlencoded args assigned result escapedargs used logformat event logged much safer way parse foo log processor need aware need urldecode args used args already urlencoded need done dynamic log format want conditionally set variable get logged format logformat pixel msec escapedargs requesttime extramsg added two additional field extramsg use add whatever extra info want log time builtin requesttime long request took process case emptygif pretty much always location pixelgif setescapeuri escapedargs args accesslog varlognginxpixelgifaccesslog pixel expires emptygif nginx emerg unknown extramsg variable uh oh problem explicitly set extramsg via set extramsg src nginx resolve variable compiles config default empty string might except solution use map map status extramsg default map cool underutilized structure nginx ever faced using request check see use map instead case going use map little differently intended usecases actually care status need pick variable know resolve set extramsg variable matter status extramsg variable always default hyphen world access log mean unset contrived thing like location pixelgif setescapeuri escapedargs args accesslog varlognginxpixelgifaccesslog pixel expires emptygif httpxfoo setescapeuri extramsg httpxfoo curl h xfoo bar baz http examplecompixelgif foobar baz quoted devnull foo bar easy parse header timestamp args requesttime foo sio stringio event row csvreader sio print zip header row timestamp args foo requesttime foo bar
442,Lobsters,scaling,Scaling and architecture,Building a Distributed Full-Text Index for the Web (2001),http://www10.org/cdrom/papers/275/,building distributed fulltext index web,building distributed fulltext index web abstract keywords distributed indexing text retrieval inverted file pipelining embedded database introduction testbed architecture pipelined indexer design theoretical analysis experimental result managing inverted file embedded database system rationale implementation pre index term location experimental result collecting global statistic termlevel global statistic design statistician statistic gathering strategy experiment related work conclusion footnote reference vitae sergey melnik sriram raghavan beverly yang hector garciamolina,building distributed fulltext index web sergey melnik sriram raghavan beverly yang hector garciamolina computer science department stanford university stanford ca usa melnik rsram byang hector dbstanfordedu copyright held authorowner may hong kong acm abstract identify crucial design issue building distributed inverted index large collection web page introduce novel pipelining technique structuring core indexbuilding system substantially reduces index construction time also propose storage scheme creating managing inverted file using embedded database system suggest compare different strategy collecting global statistic distributed inverted index finally present performance result experiment testbed distributed indexing system implemented keywords distributed indexing text retrieval inverted file pipelining embedded database introduction various access method developed support efficient search retrieval text document collection example include suffix array inverted file inverted index signature file inverted file traditionally index structure choice web commercial search engine use custom network architecture highperformance hardware achieve subsecond query response time using inverted index inverted index collection web page consists set inverted list one occurring word index term inverted list term sorted list location term appears collection location consists page identifier position term within page necessary track term occurrence within page location include page identifier optionally number occurrence within page given index term w corresponding location l refer pair w l posting w conceptually building inverted index involves processing page extract posting sorting posting first index term location finally writing sorted posting collection inverted list disk collection small indexing rare activity optimizing indexbuilding critical optimizing runtime query processing retrieval however webscale index index build time also becomes critical factor two reason scale growth rate web large growing rapidly traditional build scheme become unmanageable requiring huge resource taking day complete becoming vulnerable system failure measure comparison million page gb webbase repository represents estimated size publicly indexable web january already larger large collection benchmark large ir system rate change since content web change extremely rapidly need periodically crawl web update inverted index index either updated incrementally periodically rebuilt every crawl approach key challenge handle large wholescale change commonly observed successive crawl web efficiency simplicity commercial web search system employ rebuilding approach case critical build index rapidly quickly provide access new data study evaluate index building context special challenge imposed web implemented testbed system operates cluster node workstation built testbed encountered several challenging problem typically encountered working smaller collection paper report issue experiment conducted optimize build time massive collection particular propose technique constructing software pipeline indexing node enhance performance intranode parallelism section argue use embedded database system berkeley database storing inverted file number important advantage propose appropriate format inverted file make optimal use feature database system section distributed system building inverted index need address issue collecting global statistic eg inverse document frequency idf examine different strategy collecting statistic distributed collection section issue wherever appropriate present experiment performance study compare alternative emphasize focus paper actual process building inverted index using index process search query result address issue ranking function relevance feedback distributed query processing also wish clarify focus paper presenting comprehensive performance featurelist comparison testbed indexing system existing system indexing web nonweb collection rather use experience testbed identify key performance issue building webscale index propose generic technique applicable distributed inverted index system testbed architecture testbed system building inverted index operates distributed sharednothing architecture consisting collection node connected local area network identify three type node system figure distributorsthese node store collection web page indexed page gathered web crawler stored repository distributed across disk node indexersthese node execute core index building engine query server node store portion final inverted index associated lexicon lexicon list term corresponding portion index associated statistic depending organization index file query server may involved answering search query figure testbed architecture note many traditional information retrieval ir system employ architecture building inverted index system page document indexed placed disk directly attached machine build index however architecture provides significant benefit context web search service note web search service must perform three resource intensive task crawling indexing querying simultaneously even existing index used answer search query newer index based recent crawl must constructed parallel crawler must begin fresh crawl architecture clearly separate three activity executing separate bank machine thus improving performance ensures page indexed made available querying quickly possible thereby maximizing index freshness overview indexing process inverted index built two stage first stage distributor node run distributor process disseminates collection web page indexer indexer receives mutually disjoint subset page associated identifier indexer parse extract posting page sort posting memory flush intermediate structure disk second stage intermediate structure merged together create one inverted file associated lexicon inverted file lexicon pair generated merging subset sorted run inverted file lexicon pair transferred one query server depending degree replication paper simplicity assume indexer build one pair distributed inverted index organization distributed environment two basic strategy distributing inverted index collection query server one strategy partition document collection query server responsible disjoint subset document collection called local inverted file option partition based index term query server store inverted list subset index term collection called global inverted file performance study described indicate local inverted file organization us system resource effectively provides good query throughput case hence testbed employ local inverted file organization testbed environment indexing testbed us large repository web page provided webbase project test corpus performance experiment storage manager webbase system receives page web crawler populates distributor node indexer query server single processor pc mhz processor mb main memory equipped multiple ide disk distributor node dualprocessor machine scsi disk housing repository machine interconnected mbps ethernet lan pipelined indexer design core indexing system indexbuilder process executes indexer input indexbuilder process sequence web page associated output indexbuilder set sorted run sorted run contains posting extracted subset page received indexbuilder figure logical phase process generating sorted run logically split three phase illustrated figure refer phase loading processing flushing loading phase number page read input stream processing phase involves two step first page parsed remove html tagging tokenized individual term stored set posting memory buffer second step posting sorted inplace first term location flushing phase sorted posting memory buffer saved disk sorted run three phase executed repeatedly entire input stream page consumed loading processing flushing tend use disjoint set system resource processing obviously cpuintensive whereas flushing primarily exerts secondary storage loading done directly network tape separate disk therefore indexing performance improved executing three phase concurrently since execution order loading processing flushing fixed three phase together form software pipeline figure multithreaded execution figure illustrates benefit pipelined parallelism index construction figure show portion indexing process us three concurrent thread operates three reusable memory buffer generates six sorted run disk key issue pipelining design execution schedule different indexing phase result minimal overall running time also called makespan scheduling literature problem differs typical job scheduling problem vary size incoming job ie every loading phase choose number page load rest section describe make effective use flexibility first derive certain simplifying assumption characteristic optimal indexing pipeline schedule determine theoretical speedup achievable pipelining next describe experiment illustrate observed performance gain differ theoretical prediction theoretical analysis let u consider indexer node one resource type single cpu single disk single network connection receive page design pipeline shown figure minimize index construction time first notice executing concurrent phase kind two disk flush futile since one resource type consider indexbuilder us n execution pipeline process entire collection page generate n sorted run execution pipeline refer sequence three phase loading processing flushing transform set page sorted run let bi n buffer size used n execution sum bn btotal fixed given amount text input represents total size posting extracted page aim come way choosing bi value minimize overall running time loading flushing take time linear size buffer processing time linear component representing time removing html tokenizing linearlogarithmic component representing sorting time let li bi fi bi pi bi bi log bi represent duration loading flushing processing phase ith execution large n overall indexing time determined scarcest resource cpu figure approximated tp max ln pn fn shown see tp minimized n pipeline execution use buffer size b b bn btotaln let l b f b p b b log b duration loading processing flushing phase respectively must choose value b maximizes speedup gained pipelining calculate speedup follows pipelined execution take time tp n max l p f figure us buffer size b comparison sequential execution using single buffer size take time t l p f l f p log thus node single resource type maximal theoretical speedup achieve pipelining simplification whereas log max typical value refer table therefore ignore concentrate choosing value b maximizes maximum value reached l p f ie three phase equal duration guarantee l f since requires however maximize choosing p max l f min l f max l f example figure ratio phase l p f thus setting could improve changing ratio general setting b b log b max b b obtain expression represents size posting buffer must used maximize pipeline speedup indexer single resource type use buffer size le one specified equation loading flushing depending relative magnitude bottleneck processing phase forced periodically wait phase complete analogous effect take place buffer size greater one prescribed equation generalize equation handle indexer multiple cpu disk experimental result study impact pipelining technique indexing performance conducted number experiment testbed using single indexer supplied stream web page distributor table measured constant first ran indexbuilder process measurement mode recorded execution time various phase determined value table using value constant equation evaluate b mb therefore optimal total size posting buffer predicted theoretical analysis mb figure optimal buffer size impact buffer size performance figure illustrates performance indexbuilder process varies size buffer highlight importance analytical result aid choosing right buffer size optimal total buffer size based actual experiment turned mb even though predicted optimum size differs slightly observed optimum difference running time two size le minute million page collection buffer size le loading proved bottleneck processing flushing phase wait periodically loading phase complete however buffer size increased beyond processing phase dominated execution time larger larger buffer posting sorted figure performance gain pipelining performance gain pipelining figure show pipelining impact time taken process generate sorted run variety input size note small collection page performance gain pipelining though noticeable substantial small collection require pipeline execution overall time dominated time required startup load buffer shutdown flush buffer one reason pipelined index building received prior attention system dealt smaller collection however collection size increase gain becomes significant collection million page pipelining completes almost hour earlier purely sequential implementation experiment showed general large collection sequential indexbuilder slower pipelined indexbuilder note observed speedup lower speedup predicted theoretical analysis described previous section analysis based ideal pipeline loading processing flushing interfere way practice however network disk operation use processor cycle access main memory hence two concurrently running phase even different type slow note given total buffer size pipelined execution generate sorted run approximately time smaller generated sequential indexer consequently time many sorted run need merged second stage indexing however experiment described show even large collection size potential increase merging time offset time gained first stage pipelining expect long enough main memory merge time allocate buffer sorted run performance substantially affected managing inverted file embedded database system building inverted index massive webscale collection choice efficient storage format particularly important traditionally two approach storing managing inverted file either using custom implementation leveraging existing relational object data management system advantage custom implementation enables effective optimization tuned specific operation inverted file eg caching frequently used inverted list compressing rarely used inverted list using expensive method may take longer decompress leveraging existing data management system allow finegrained control implementation reduces development time complexity however challenge lie designing scheme storing inverted file make optimal use storage structure provided data management system storage scheme must space efficient must ensure basic lookup operation inverted file ie retrieving inverted list given index term efficiently implemented using native access method data management system section present compare different storage scheme managing large inverted file embedded database system test scheme used freely available embedded database system called berkeley db widely deployed many commercial application embedded database library toolkit provides database support application welldefined programming api unlike traditional database system designed accessed application embedded database linked compiletime runtime application act persistent storage manager provide devicesensitive file allocation database access method btrees hash index optimized caching optional support transaction locking recovery also advantage much smaller footprint compared fullfledged clientserver database system following briefly sketch capability berkeley db propose btree based inverted file storage scheme called mixedlist scheme qualitatively quantitatively compare mixedlist scheme two scheme storing inverted list berkeley db database rationale implementation berkeley db provides programming library managing key value pair arbitrary binary data length offer four access method including btrees linear hashing berkeley db support transaction locking recovery turned desired efficiency chose use btree access method since efficiently support prefix search eg retrieve inverted list term beginning pre higher reference locality hashbased index standard organization btree based inverted file involves storing index term btree along pointer inverted list stored separately organization though easy implement using berkeley db fully utilize capability database system since berkeley db efficiently handle arbitrary sized key value efficient store index term inverted list within database enables u leverage berkeley db sophisticated caching scheme retrieving large inverted list minimum number disk operation figure mixed list storage scheme storage scheme challenge design efficient scheme organizing inverted list within btree structure considered three scheme qualitative comparison storage scheme summarized table full list key index term value complete inverted list term single payload posting index term location pair separate value either empty may contain additional information posting mixed list key posting ie index term location however value contains number successive posting sorted order even referring different index term posting value field compressed every value field number posting chosen length field approximately note scheme inverted list given index term may spread across multiple key value pair figure illustrates mixedlist storage scheme simplicity example assume additional information maintained along posting however actual implementation allocated payload field store extra postinglevel information top half figure depicts inverted list four successive index term bottom half show stored key value pair using mixedlist scheme example second key value pair figure store set posting etc first posting stored key remaining posting stored value indicated figure posting value compressed using prefix compression index term representing successive location identifier term numerical difference example posting represented sequence entry empty field indicates length common prefix word posting indicates posting refer word difference location similarly posting represented sequence entry ch length common prefix cat catch ch remaining suffix catch location scheme index size zig zag join hot update single payload full list mixed list table comparison storage scheme index size crucial factor determining index size number internal page function height btree number overflow page berkeley db us handle large value field single payload scheme every posting corresponds new key resulting rapid growth number internal page database large collection database size becomes prohibitive even though berkeley db employ prefix compression key also query time many performanceimpeding disk access needed situation significantly better full list scheme database key created every distinct term value field well compressed however many term occur time collection whereas others may occur almost every page accommodate large variation size value field many overflow page created database comparison mixed list scheme length value field approximately constant limit number overflow page moreover total number key hence number internal page reduced choosing larger size value field however since value field contain posting different index term compressed well full list zigzag join ability selectively retrieve portion inverted list useful processing conjunctive search query inverted file example consider query green catchfly term green occurs web million document whereas catchfly produce couple dozen hit zigzag join inverted list green catchfly allows u answer query without reading complete inverted list green single payload scheme provides best support zigzag join posting retrieved individually full list scheme entire list must retrieved compute join whereas mixed list scheme access specific portion inverted list available example figure retrieve location cat starting read portion list location skippedlist random invertedlist structure also provide selective access portion inverted list dividing inverted list block containing fixed number posting however scheme assume custom inverted file implementation built top existing data management system hot update hot update refers ability modify index query time useful small change need made index two successive index rebuilds example web search service often allow user organization register home page service addition immediately accommodated index using hot update facility without defer till index next rebuilt three scheme concurrency control mechanism database used support hot update maintaining consistency however crucial performance factor length inverted list must read modified written back achieve update since limit length value field hot update faster mixed list full list single payload scheme provides best update performance individual posting accessed modified notice three scheme significantly benefit fact posting first sorted inserted inserting key btree random order negatively affect pagefill factor expensive tree reorganization needed berkeley db optimized sorted insertion high performance nearone pagefill factor achieved initial index construction phase table show mixedlist scheme provides best balance small index size support efficient zigzag join following section present quantitative comparison storage retrieval efficiency three storage scheme discussed section experimental result experimental data presented section obtained building inverted index collection million web page collection contains million distinct term total million posting figure varying value field size figure illustrates choice storage scheme affect size inverted file show variation index size value field size using mixedlist scheme dotted line represents index size database stored using fulllist scheme note since value field size applicable fulllist scheme graph horizontal line single payload scheme viewed extreme case mixed scheme value field empty figure show small large value field adverse impact index size mixed list scheme small value field require large number internal database page potentially taller btree index accommodate posting hand large value field cause berkeley db allocate large number overflow page turn lead larger index indicated figure value field size byte provided best balance two effect fulllist scheme result moderate number overflow page internal database page however still requires around storage space optimal mixedlist inverted file examined storage scheme time write inverted file disk roughly proportional size file number page million input size gb index size gb index size age table mixedlist scheme index size table show index size using mixedlist scheme varies size input collection index size listed table include sum size inverted file associated lexicon number table generated using mixedlists optimal value field size byte derived figure table show mixedlist storage scheme scale well large collection size index consistently size input html text compare favorably size reported track also used crawled web page best reported index size approximately size input html index size also comparable recently reported size nonweb document collection using compressed inverted file note exact index size dependent type amount information maintained along posting eg information handle proximity query however believe payload field used implementation accommodate postinglevel information normally stored inverted index figure time retrieve inverted list figure illustrates effect value field size inverted list retrieval time dotted horizontal line represents retrieval time using fixedlist scheme figure produced generating uniformly distributed query term measuring time required retrieve entire inverted list query term warmingup period allowed measurement fill database file system cache optimal retrieval performance mixedlist scheme achieved value field size byte notice figure value field size byte result maximum storage well maximum retrieval efficiency mixedlist scheme figure also indicates fixedlist mixedlist optimal value field size scheme provide comparable retrieval performance note figure measure raw inverted list retrieval performance different storage scheme true query processing performance affected factor caching inverted list use query processing technique zigzag join distribution query term collecting global statistic textbased retrieval system use kind collectionwide information increase effectiveness retrieval one popular example inverse document frequency idf statistic used ranking function idf term inverse number document collection contain term query server idf value local collection ranking would skewed favor page query server return result order offer effective ranking user query server must know global idf value depending particular global statistic ranking function nature collection may may necessary statistic computed accurately case might suffice estimate global statistic local value individual query server however section analyze problem gathering accurate collectionwide information minimum overhead case required present two technique capable gathering different type collectionwide information though focus problem collecting termlevel global statistic idf value termlevel refers fact gathered statistic describes single term higher level entity page document design author suggest computing global statistic query time would require extra round communication among query server exchange local statistic communication adversely impact query processing performance especially large collection spread many server since query response time critical advocate precomputing storing statistic query server index creation approach based using dedicated server known statistician computing statistic dedicated statistician allows computation done parallel indexing activity also minimizes number conversation among server since indexer exchange statistical data one statistician local information sent statistician various stage index creation statistician return global statistic indexer merging phase indexer store global statistic local lexicon lexicon consists entry form term termid localstatistics globalstatistics term stored lexicon term occurring associated inverted file section order avoid extra disk io local information sent statistician already memory identified two phase occurs flushing sorted run written disk merging sorted run merged form inverted list lexicon sending information two phase lead two different strategy various tradeoff discussed next section note sending information statistician phase without additional io huge fraction statistic collection eliminated sending information statistician optimized summarizing posting identified phase posting occur least partially sorted order meaning multiple posting term pas memory group group condensed term local aggregated information pair sent statistician example indexer hold page contain term cat instead sending individual posting statistician indexer count posting pas memory group send summary cat statistician statistician receives local count indexer aggregate value produce global document frequency cat technique greatly reduces network overhead collecting statistic phase statistician load memory usage parallelism merging fl flushing table comparing strategy statistic gathering strategy describe compare two strategy mentioned sending information statistician table summarizes characteristic column titled parallelism refers degree parallelism possible within strategy figure strategy strategy sending local information merging summary term aggregated inverted list created memory sent statistician statistician receives parallel sorted stream term localaggregateinformation value indexer merges stream term aggregating subaggregates term produce global statistic statistic sent back indexer sorted term order approach entirely stream based require inmemory ondisk data structure statistician indexer store intermediate result however using stream mean progress indexer synchronized statistician turn cause indexer synchronized result slowest indexer group becomes bottleneck holding back progress faster indexer figure illustrates strategy collecting document frequency statistic term note bottom lexicon include statistic rat term present local collection figure fl strategy fl strategy sending local information flushing sorted run flushed disk posting summarized summary sent statistician since sorted run accessed sequentially processing statistician receives stream summary globally unsorted order compute statistic unsorted stream statistician keep inmemory hash table term related statistic update statistic summary term received end processing phase statistician sort statistic memory sends back indexer figure illustrates fl strategy collecting document frequency statistic figure overhead statistic collection experiment demonstrate performance scalability collection strategy ran indexbuilder merging process testbed using hardware configuration consisting four indexer experimented four different collection size page respectively result shown figure see relative overhead defined time full index creation statistic collection time full index creation statistic collection strategy general experiment show fl strategy outperforming although seem converge collection size becomes large furthermore collection size grows relative overhead strategy decrease comparison strategy first glance might expected outperform fl since statistician receives many summary stream fl one indexer performs comparison aggregation fl however mentioned earlier merging progress synchronized among server hence good portion computation done statistician done parallel merging activity indexer fl hand indexer simply writes summary network continues work statistician asynchronously process information network buffer parallel however work done parallel since statistician consumes summary slower rate indexer writes network network buffer generally hold summary sorted run hence still nontrivial waiting indexer flushing summary sent statistician enhancing parallelism strategy synchronization occurs indexer creates lexicon entry summary term sends summary statistician wait global statistic returned lexicon entry completed reduce effect synchronization merging process instead write lexicon entry lexicon buffer separate process wait global statistic include entry way first process need block waiting process operate parallel figure varying lexicon buffer size figure show effect lexicon buffer size merging performance collection million page lexicon entry created faster global statistic returned indexer slowest lexicon buffer often becomes full occurs process creating lexicon entry must block current state change larger lexicon buffer reduce possibility saturation expect see initial increase size result large performance gain lexicon buffer size becomes large however performance slowly deteriorates due memory contention although entire buffer need present memory one time lexicon buffer accessed cyclically therefore lru replacement fast rate lexicon entry created cause buffer page cycle rapidly memory swapping nonbuffer page sublinear growth overhead constant decrease fl relative overhead figure explained fact number distinct term page collection sublinear function collection size overhead incurred gathering statistic grows linearly number term collection cost index creation grows linearlogarithmically size collection result overhead statistic collection display sublinear growth respect index creation time prediction consistent experimental result however decreasing relative overhead fl subject constraint hashtable fit memory considering collection billion page would require hash table roughly gb size constraint may become problem large collection memory gb completely unreasonable simple alternative using commodity hardware would run several statistician parallel partition term alphabetically statistician way statistician collect sort moderately sized set global statistic yet implemented option system related work motivated web recent interest designing scalable technique speed inverted index construction using distributed architecture ribeironeto et al describe three technique efficiently build inverted index using distributed architecture however focus building global partitioning index term rather local partitioning collection inverted file furthermore address issue global statistic collection optimization indexing process individual node technique structuring core index engine pipeline much common pipelined query execution strategy employed relational database system chakrabarti et al present variety algorithm resource scheduling application scheduling pipeline stage prior work using relational objectoriented data store manage process inverted file brown et al describe architecture performance information retrieval system us persistent object store manage inverted file result show using offtheshelf data management facility improves performance information retrieval system primarily due intelligent caching devicesensitive file allocation experienced similar performance improvement reason employing embedded database system storage format differs greatly utilize btree storage system object store reference discus question maintain global statistic distributed text index technique deal challenge arise incremental update wished explore strategy gathering statistic index construction great deal work done several issue relevant invertedindex based information retrieval discussed paper issue include index compression incremental update distributed query performance conclusion paper addressed problem efficiently constructing inverted index large collection web page proposed new pipelining technique speed index construction showed choose right buffer size maximize performance demonstrated large collection size pipelining technique speed index construction several hour proposed compared different scheme storing managing inverted file using embedded database system showed intelligent scheme packing inverted list storage structure database provide performance storage efficiency comparable tailored inverted file implementation finally identified key characteristic method efficiently collecting global statistic distributed inverted index proposed two method compared analyzed tradeoff thereof future intend extend testbed incorporate distributed query processing explore algorithm caching strategy efficiently executing query also intend experiment indexing querying larger collection integration textindexing system index link structure web footnote url normally replaced numeric identifier compactness rate page loaded memory network average ratio size page total size posting generated page storing indexing term key single location value viable option location given term guaranteed sorted order one posting generated occurrence term page billion page contain roughly million distinct term term using byte storage result hashtable gb reference eric w brown james p callan w bruce croft fast incremental indexing fulltext information retrieval proc intl conf large database pp september eric w brown james p callan w bruce croft j eliot b moss supporting fulltext information retrieval persistent object store intl conf extending database technology pp march chakrabarti muthukrishnan resource scheduling parallel database scientific application acm symposium parallel algorithm architecture pp june junghoo cho hector garciamolina evolution web implication incremental crawler september appear intl conf large database c faloutsos christodoulakis signature file access method document analytical performance evaluation acm transaction office information system october h garciamolina j ullman j widom database system implementation prenticehall gorssman j r driscoll structuring text within relation system proc intl conf database expert system application pp september hawking n craswell overview large collection track proc seventh text retrieval conf pp november http jun hirai sriram raghavan hector garciamolina andreas paepcke webbase repository web page proc intl world wide web conf pp may b jeong e omiecinski inverted file partitioning scheme multiple disk system ieee transaction parallel distributed system february steve lawrence c lee giles accessibility information web nature udi manber gene myers suffix array new method online string search proc acmsiam symposium discrete algorithm pp patrick martin ian macleod brent nordin design distributed full text retrieval system proc acm conf research development information retrieval pp september sergey melnik sriram raghavan beverly yang hector garciamolina building distributed fulltext index web number stanford digital library project computer science department stanford university july available mike burrow personal communication moffat bell situ generation compressed inverted file journal american society information science moffat j zobel selfindexing inverted file fast text retrieval acm transaction information system october anh ngocvo alistair moffat compressed inverted file reduced decoding overhead proc intl conf research development information retrieval pp august olson k bostic seltzer berkeley db proc summer usenix technical conf june berthier ribeironeto edleno moura marden neubert nivio ziviani efficient distributed algorithm build inverted file proc acm conf research development information retrieval pp august b ribeironeto r barbosa query performance tightly coupled distributed digital library proc acm conf digital library pp june g salton information retrieval data structure algorithm addisonwesley reading massachussetts anthony tomasic hector garciamolina performance inverted index sharednothing distributed text document information retrieval system proc intl conf parallel distributed information system pp january anthony tomasic hector garciamolina query processing inverted index sharednothing document information retrieval system vldb journal anthony tomasic hector garciamolina kurt shoens incremental update inverted list text document retrieval proc acm sigmod intl conf management data pp may charles l viles maintaining state distributed information retrieval system southeast conf acm pp charles l viles james c french dissemination collection wide information distributed information retrieval system proc intl acm conf research development information retrieval pp july inktomi webmap http wwwinktomicomwebmap h witten moffat c bell managing gigabyte compressing indexing document image morgan kauffman publishing san francisco j zobel moffat r sacksdavis efficient indexing technique fulltext database system intl conf large database pp august vitae sergey melnik currently research scholar computer science department stanford university california research interest include interoperability web database digital library distributed system knowledge representation received master m computer science leipzig university germany currently participates graduate research program sriram raghavan currently phd student computer science department stanford university stanford california received bachelor technology btech degree computer science engg indian institute technology chennai india master m computer science stanford university research interest include information management web largescale searching indexing database ir system integration web query processing beverly yang currently phd student computer science department stanford university california received master m computer science stanford university research interest include information management web peertopeer system hector garciamolina leonard bosack sandra lerner professor department computer science electrical engineering stanford university stanford california chairman computer science department since january august december director computer system laboratory stanford faculty computer science department princeton university princeton new jersey research interest include distributed computing system database system received b electrical engineering instituto tecnologico de monterrey mexico stanford university stanford california received m electrical engineering phd computer science garciamolina fellow acm received acm sigmod innovation award member president information technology advisory committee pitac
443,Lobsters,scaling,Scaling and architecture,Logarithmic scaling with the fastest JSON validator,http://simongrondin.name/?p=383,logarithmic scaling fastest json validator,,something backend engineer love manage solve problem log n way week number request per second go exponentially number server needed handle go linearly reach bandwidth bottleneck amazing first let recap performance issue application needed lot big expensive computeoptimized instance handle fairly low traffic standard instance burning cpu like crazy codebase built performance mind find single inefficiency mean time flame graph look like load testing code profiling mode time spent function recorded displayed graph found cpu time spent json schema validation already using fastest validator around called ismyjsonvalid validate complicated json object byte contain date ip reference custom format etc object took validate figured time bring big c gun since nodejs run engine built c call c code transparently vice versa time validate object dropped including back forth nodejsc bridge since done outside even loop could even detach run computation asynchronously thread pool success ala code vastly complicated involves manually editing cmakelisttxt file c library prior building involves ugly elf hack make usable work o x devs company use thread pool even help u since code already fully multiprocess distributed behind load balancer available core already used mess realized ismyjsonvalid imjv benchmarked huge object already fastest json validator language due performance race going javascript json validation right batched object together result object object object object object object object object object object awesome request serve already go asynchronous flow validation module register callback put object array every second entire array validated error separated per object callback called concurrently without error depending result object tried c crazy magic imjv us leverage jit compiler matched c validator object always took matter many batched together say bad idea lock event loop relatively long period time u extreme case irrelevant codebase serf data back client act data sink process data need cheaply possible technique every performance problem solved brilliantly entry posted algorithm javascript
444,Lobsters,scaling,Scaling and architecture,Scaling Ruby Apps to 1000 Requests per Minute,http://nateberkopec.com/2015/07/29/scaling-ruby-apps-to-1000-rpm.html,scaling ruby apps request per minute,scaling ruby apps request per minute beginner guide scaling increase throughput speed going use heroku example many custom devops setup work quite similarly request get routed app server life request see digitalocean tutorial see digitalocean tutorial connecting server importance server choice webrick rail default thin work unicorn phusion passenger puma threaded puma clustered mean slow client protection slow response protection puma clustered mode phusion passenger mean scale app instance scale application based response time alone dyno count must obey law law checking math april presentation given sdforum silicon valley shopify scale rail youtube scaling rail presentation present form law scaling rail presentation present form law distributional form law distributional form law envato posted rail scale checklist step scaling ruby apps rpm,scaling ruby apps request per minute beginner guide summary scaling resource ruby apps written company hundred request per second scaling rest u minute scaling intimidating topic blog post internet resource around scaling ruby apps scaling ruby ten thousand request per minute twitter shopify scale interesting good know ceiling much ruby achieve useful majority u apps bigger server le server scaling think problem people comfortable writing big huge think problem people comfortable writing big huge thus scaling resource ruby application developer completely inappropriate need technique twitter used scale requestssecond requestssecond reading reqsec app scaled app get requestsminute going appropriate getting app requestsminute requestsminute megascale unique set problem database io especially becomes issue app tends scale horizontally across process machine database scale vertically adding cpu ram combine make scaling tough topic rail application developer scale scale since limiting discussion rpm le discus scaling db datastores like memcache redis using highperformance message queue like rabbitmq kafka distributing object also going tell get faster response time post although help scale also cover devops anything beyond application server unicorn puma etc first although seems shocking admit spent entire professional career deploying application heroku work small startup le requestsminute scale time sole developer one handful small team small scale like think payoff immense yes pay perhaps even server bill developer hour save screwing chefansibledockerdevops flavor week pay big time work small startup le requestsminute scale time sole developer one handful small team small scale like think payoff immense yes pay perhaps even server bill developer hour save screwing chefansibledockerdevops flavor week pay big time experience share scaling custom setup docker chef whathaveyou nonheroku platform second running le requestsminute devops workflow really need specialized much material post apply ruby apps regardless devops setup consultant gotten see quite rail application overscaled wasting money dyno slider many service aws make scaling simple also make easy scale even need many rail developer think scaling dynos upping instance size make application faster yes scaling dynos heroku never make application faster unless app request queued waiting time explained even px dynos make performance consistent faster changing instance type aws though example may change performance characteristic app instance yes scaling dynos heroku never make application faster unless app request queued waiting time explained even px dynos make performance consistent faster changing instance type aws though example may change performance characteristic app instance see application slow first reflex scale dynos instance size indeed heroku support usually encourage spend money solve problem time though help problem site still slow glossary post host refers single host machine virtualized physical heroku dyno sometimes people call server post want differentiate host machine application server run machine single host may run many app server like unicorn puma heroku single host run single app server app server many app instance may separate process like unicorn thread puma running jruby multithreaded purpose post multithreaded web server single app instance mri like puma app instance thread executed time thus typical heroku setup might hostdyno app server puma master process app instance puma clustered worker scaling increase throughput speed scaling host speed response time request spending time waiting served application request waiting served scaling waste money order learn scale ruby apps correctly requestsminute going need learn considerable amount application server http routing actually work going use heroku example many custom devops setup work quite similarly ever wondered exactly request get queued routed server well find request get routed app server one important decision make scaling ruby web application application server choose ruby scaling post thus date ruby application server world changed dramatically last year whirlwind change happened last year however understand advantage disadvantage application server choice going learn request even get routed application server first place understandably lot developer understand exactly request routed queued simple gist rail devs already understand heroku router load balance unicorn puma think routing changed bamboo cedar rapgenius got pretty screwed back day think request queueing incorrectly use unicorn wait guess heroku say use puma know request queue somewhere really know documentation http routing good start quite explain whole picture example immediately obvious heroku recommends unicorn puma application server also really lay exactly request get queue important follow request start finish life request request come yourappherokuappcom first place stop load balancer load job make sure load router evenly distributed much decide router request go load balancer pass request whichever router think best heroku publicly discussed load balancer work load balancer make decision heroku router undisclosed number heroku router safely assume number pretty large job find dynos pas request dyno spending locating dynos router attempt connect random dyno app yes random one rapgenius got tripped year ago back heroku best unclear worst misleading router chose dyno route heroku chosen random dyno wait five second dyno accept request open connection request waiting placed request queue however router request queue since heroku told u many router could huge number router queue given time application heroku start throwing away request request queue get large also try quarantine dynos responding individual router basis every router heroku individually quarantine bad dynos basically custom setup utilize nginx see digitalocean tutorial sometimes nginx play role load balancer reverseproxy setup behavior duplicated using custom nginx setup though may want choose aggressive setting nginx actually actively send healthcheck request upstream application server check alive custom nginx setup tend request queue however basically custom setup utilize nginx see digitalocean tutorial sometimes nginx play role load balancer reverseproxy setup behavior duplicated using custom nginx setup though may want choose aggressive setting nginx actually actively send healthcheck request upstream application server check alive custom nginx setup tend request queue however two critical detail heroku user router wait second successful connection dyno waiting request wait router request queue connecting server importance server choice router custom setup people say router say custom setup people say router say attempting connect server critical stage understand happens differs greatly depending choice web server happens next depending server choice webrick rail default webrick singlethread singleprocess web server keep connection open downloaded entirety request router router move next request webrick server take request run application code send back response router time host busy accept connection router router attempt connect host request processed router wait second heroku host ready router attempt open connection dynos wait problem webrick exaggerated slow request uploads someone trying upload hd video cat modem luck webrick going sit wait request downloads anything meantime got mobile user phone bad webrick going sit accept request wait request slowly painfully complete webrick deal well slow client request slow application response thin thin eventdriven singleprocess web server way run multiple thins single host however must listen different socket rather single socket like unicorn make setup herokuincompatible way run multiple thins single host however must listen different socket rather single socket like unicorn make setup herokuincompatible thin us eventmachine hood process sometimes called evented io work unlike nodejs give several benefit theory thin open connection router start accepting part request catch though suddenly request slows data stop coming socket thin go something else provides thin protection slow client matter slow client thin go receive connection router meantime request fully downloaded thin pas request application fact thin even write large request like uploads temporary file disk thin multithreaded multiprocess thread run one time mri actually running application host becomes unavailable negative consequence outlined webrick section unless get fancy use eventmachine thin accept request waiting io application code finish example application code post payment service credit card authorization thin accept new request waiting io operation complete default essentially need modify application code send event back eventmachine reactor loop tell thin waiting io go something work thin deal slow client request deal slow application response application io without whole lot custom coding unicorn unicorn singlethreaded multiprocess web server unicorn spawn number app instance process sit listen single unix socket coordinated connection request come host go master process instead directly unicorn socket worker process waiting listening special sauce ruby web server know use unix domain socket sort interference worker process listening socket processing request accepts request socket wait socket request fully downloaded setting alarm bell yet stop listening socket go process request done processing request sending response listens socket unicorn vulnerable slow client use nginx custom setup buffer request unicorn eliminating slowclient issue exactly passenger use nginx custom setup buffer request unicorn eliminating slowclient issue exactly passenger way webrick downloading request socket unicorn worker accept new connection worker becomes unavailable essentially serve many slow request unicorn worker unicorn worker slow request take download fourth request sit wait request processed method sometimes called multiprocess blocking io way unicorn deal slow application response free worker still accept connection another worker process working many slow client request notice socketbased model form intelligent routing available application instance accept request socket phusion passenger passenger us hybrid model io us multiprocess workerbased structure like unicorn however also includes buffering reverse proxy important bit like running nginx front worker addition pay passenger enterprise run multiple app thread worker like puma see phusion passenger builtin reverse proxy customized nginx instance written c ruby important walk request passenger instead socket router connects nginx directly pass request nginx specially optimized build whole lot fancy technique make extremely efficient serving ruby web application download entire request forwarding next step protecting worker slow uploads slow client completed downloading request nginx forward request helperagent process determines worker process handle request passenger deal slow application response helperagent route request unused worker process slow client run instance nginx buffer puma threaded puma default mode operation multithreaded singleprocess server application connects host connects eventmachinelike reactor thread take care downloading request asynchronously wait slow client send entire request like thin request downloaded reactor spawn new thread communicates application code thread process request specify maximum number application thread running given time configuration puma multithreaded multiprocess thread run one time mri ruby special puma however unlike thin modify application code gain benefit threading puma automatically yield control back process application thread wait io example application waiting http response payment provider puma still accept request reactor thread even complete request different application thread puma deliver big performance increase waiting io operation like database network request actually running application host becomes unavailable processing negative consequence outlined webrick section puma threadedonly mode deal slow client request deal slow cpubound application response puma clustered puma mode combine multithreaded model multiprocess model clustered mode router connect essentially reactor part puma example master reactor downloads buffer incoming request pass available puma worker sitting unix socket similar unicorn clustered mode puma deal slow request thanks separate master process whose responsibility download request pas slow application response thanks spawning multiple worker mean paying attention far realized scalable ruby web application need slow client protection form request buffering slow response protection form kind concurrency either multithreading multiprocessforking preferably leaf puma clustered mode phusion passenger scalable solution ruby application heroku running mric ruby running setup unicorn nginx becomes viable option web server make varying claim get caught web server handle request per minute meaning take le actually handle request puma faster unicorn great really help much rail application take average turn around request biggest difference ruby application server speed varying io model characteristic discussed think puma clustered mode phusion passenger really serious choice scaling ruby application io model deal well slow client slow application many difference feature phusion offer enterprise support passenger really know one right full feature comparison mean seen explanation really single fact application may interacting hundred place request might load balancer unlikely load balancer tuned fast load balancer queue heroku router remember router queue separate router queue using multiprocess server like unicorn puma phusion passenger queueing otherwise inside host queue per host heck new relic know report queue time well rapgenius got burned rapgenius got burned hard discovered intelligent fact completely random essentially heroku transitioning bamboo cedar stack also changed load balancerrouter infrastructure everyone bamboo cedar stack bamboo stack apps like rapgenius suddenly getting random routing instead intelligent routing intelligent routing mean something better random usually intelligent routing involves actively pinging upstream application server see available accept new request decrease wait time router intelligent routing mean something better random usually intelligent routing involves actively pinging upstream application server see available accept new request decrease wait time router even worse infrastructure still reported stats intelligent routing single request queue onequeueperrouter heroku would report queue time back new relic form http header new relic displayed queue however header reporting time particular request spent router queue router could extremely low regardless load host imagine heroku connects master socket pass request onto socket request spends socket waiting application worker pick previously would unnoticed router queue time reported imagine heroku connects master socket pass request onto socket request spends socket waiting application worker pick previously would unnoticed router queue time reported nowadays new relic report queue time based http header reported heroku called requeststart header mark time heroku accepted request load balancer new relic subtracts time application worker started processing request requeststart get queue time requeststart exactly pm application start processing request new relic report queue time nice take account time spent level time load balancer time heroku router time spent queueing host whether master process worker socket otherwise course setting correct header nginxapache instance get accurate request queueing time custom setup course setting correct header nginxapache instance get accurate request queueing time custom setup scale app instance scale application based response time alone application may slowing due increased time request queue may request queue empty scaling host wasting money check time spent request queue scaling applies worker host scale based depth job queue job waiting processed scaling worker host pointless effect worker dynos web dynos exactly incoming job request need process scaled based number job waiting processing newrelic provides time spent request queue although gem help measure spending lot time average server response time request queue benefit scaling extremely marginal dyno count must obey law usually see application overscaled developer understand many request server process per second sense many requestsminute equal many dynos already explained practical way determine measuring responding change request queueing time also theoretical tool use law wikipedia explanation bit obtuse formulation adapted slightly first definition mentioned application instance atomic unit setup job process single request independently send back client using webrick application instance entire webrick process using puma threaded mode define entire puma process application instance using mri using jruby thread count application instance using unicorn puma clustered passenger application instance process really multithreaded puma process mri count app instance since work waiting io simplicity say one really multithreaded puma process mri count app instance since work waiting io simplicity say one math typical rail app prototypical setup unicorn say unicorn process fork unicorn worker singleserver app actually application instance app getting request per second average server response time need app instance service load using available server capacity theoretical maximum capacity change unknown example app theoretical maximum throughput request per second pretty impressive theory never reality unfortunately law true long run meaning thing like wide varying distribution server response time request take second process others second wide distribution arrival time make equation inaccurate good think whether might overscaled addition think caveat mean scaling maximize actual throughput request close median possible app predictable response time scalable app fact may obtain accurate result law instead using average server response time use percentile response time good slowest response server response time variable unpredictable decrease percentile response time aggressively push work background process like sidekiq delayedjob addition think caveat mean scaling maximize actual throughput request close median possible app predictable response time scalable app fact may obtain accurate result law instead using average server response time use percentile response time good slowest response server response time variable unpredictable decrease percentile response time aggressively push work background process like sidekiq delayedjob recall scaling host directly increase server response time increase number server available work request queue average number request waiting queue le server working capacity benefit scaling host marginal ie maximum benefit obtained always least request queue probably good reason scale point reached especially slow server response time aware rapidly decreasing marginal return setting host count try math law scaling host according law le maximum capacity might scaling prematurely alternatively mentioned spending large amount time perrequest request queue measured newrelic good indication time scale host checking math april presentation given sdforum silicon valley twitter engineer scaling twitter time twitter still fully rail app presentation engineer gave following number requestssecond application instance mongrel average server response time theoretical instance required appeared running twitter running maximum utilization seems like recipe disaster twitter lot scaling issue time may unable scale application instance still stuck single database server yup bottleneck elsewhere system solved instance recent example big ruby shopify engineer john duff gave presentation shopify scale rail youtube scaling rail presentation present form law scaling rail presentation present form law claimed shopify receives requestssecond average response time run application server total application instance nginx unicorn theoretical required instance count application instance using wasting theoretically capacity application instance block way like reading data socket receive request law fail hold count puma thread application instance mri another cause cpu memory utilization application server maxing cpu memory worker work full capacity blocking application instance anything stop application instance operating time cause major deviation distributional form law help inaccuracy unless math phd probably reach distributional form law help inaccuracy unless math phd probably reach finally envato posted rail scale number math mean envato theoretically requires app instance serve load running theoretical maximum good ratio checklist step scaling ruby apps rpm hopefully post given tool need scale requestsperminute reminder need remember choose multiprocess web server slow client protection smart routingpooling currently choice puma clustered mode unicorn nginx frontend phusion passenger scaling dynos increase throughput application speed app slow scaling first reflex hostdyno count must obey law queue time important queue time low scaling host pointless realize three lever increasing application instance decreasing response time decreasing response time variability scalable application requires fewer instance fast response time low response time variability share
447,Lobsters,scaling,Scaling and architecture,"Why you should never, ever, ever use MongoDB",http://cryto.net/~joepie91/blog/2015/07/19/why-you-should-never-ever-ever-use-mongodb/,never ever ever use mongodb,two year knex bookshelf sequelize waterline never use database everybody else research advantage drawback particular database,mongodb evil loses data source fact long time ignored error default assumed every single write succeeded matter system led losing data silently due mongodb limitation slow even advertised usecases claim contrary completely lacking evidence source force poor habit implicit schema nearly usecases source locking issue source atrociously poor response time security issue took two year patch insecure default configuration would expose data anybody asked without authentication source acidcompliant source nightmare scale maintain nt even exclusive offering jsonbased storage postgresql better document store like couchdb around long time source realistically nothing good bunch stuff outright bad bulletpoints fact opinion go verify case want actually relational database postgresql good option case use query builder orm make working easier nodejs option knex query builder bookshelf sequelize waterline orms project involves user account kind relationship two record use relational database document store data relational find using mongoose also using relational database library like mongoose try poorly emulate schemaful relational database using document store might well use relational database directly even need document store case nt many better option available mongodb list document store database note list ranked popularity say nothing quality database never use database everybody else research advantage drawback particular database popularity largely subject hype right marketing team hard popularize inferior solution nt good prototyping either end locking database never reasonably make production prototype considered viable still going rewrite everything using different database nearly every development ecosystem migration rollback mechanism provide comparable ease use prototyping without drawback rewrite code production valid usecases mongodb technically inferior option offer exclusive feature actually work developer failing ensure data integrity security arguably two important aspect database
448,Lobsters,scaling,Scaling and architecture,Running a host-local Docker Registry,http://0x74696d.com/posts/host-local-docker-registry/,running hostlocal docker registry,running hostlocal docker registry bridget peter jeff,running hostlocal docker registry one option running private docker registry run docker registry daemon started using docker approach beginning time docker registry python app running gunicorn new version written go nice feature one use backing store almost make registry proxy ran pretty serious problem early term scalability scale bunch queue worker take additional load application could overload instance docker registry running pretty trivially nothing else network io limitation could probably gotten away scaling box lot would leave scary spof system make moot reliability instead registry daemon hostlocal service backed mean container running registry host cname like dockerlocalexamplecom point service start docker pull tag docker registry backend retrieves image layer iam role given production node readonly access bucket never push use jenkins build ship container colleague bridget peter talked depth year chefconf one team alumnus jeff talked grubhub implementation similar setup dockercon couple week ago jenkins server hostlocal docker registry setup iam permission write bucket sole node allowed write avoid possibility data race backing store need multiple writer want shard repository write namespace setup network io throughput start limited host requires new image whatever give u let u scale node fast aws give u without worrying registry point failure
450,Lobsters,scaling,Scaling and architecture,Aeron: Do we really need another messaging system?,http://highscalability.com/blog/2014/11/17/aeron-do-we-really-need-another-messaging-system.html,aeron really need another messaging system,kafka replicated persistent log message implosion effect human suck estimatio building distributed system hard defensive code feature code building distributed system rewarding design monitoring debugging start leaving performance table configuration part java really suck nice part java average mean absolutely nothing,really need another messaging system might promise move million message second small microsecond latency machine consistent response time large number client using innovative design promise aeron celtic god battle chair though tell search engine new highperformance open source message transport library team todd montgomery multicast reliable protocol expert richard warburton expert compiler optimization martin thompson pasty faced performance gangster claim aeron already beating best product throughput latency match best commercial product percentile aeron push small byte message million message second difficult case talk martin gave aeron strangeloop aeron opensource highperformance messaging give gloss talk well integrating source information listed end article martin team enviable position client required product like aeron willing finance development also making open source go git aeron github note early day aeron still heavy optimization phase world changed therefore endpoint need scale never martin say need new messaging system multieverything world multicore multisocket multicloud multibillion user computing communication happening time huge number consumer regularly pound channel read publisher cause lock contention queueing effect cause throughput drop latency spike needed new messaging library make new world move microservices heightens need move world micro service need low predictable latency communication otherwise coherence component usl come rain fire brimstone design aeron goal keep thing pure focused benchmarking done far suggests step forward throughput latency quite unique choose throughput latency highend messaging transport distinct choice algorithm employed aeron give maximum throughput minimising latency saturation many messaging product swiss army knife aeron scalpel say martin good way understand aeron full featured messaging product way may used like kafka aeron persist message support guaranteed delivery clustering support topic aeron know client crashed able sync back history initialize new client history best way place aeron mental matrix might message oriented replacement tcp higher level service written top todd montgomery expands idea aeron iso layer protocol provides number thing messaging system ca nt also nt provide several thing messaging system make sense let explain slightly wrt typical messaging system kafka one way think aeron fit tcp option reliable multicast delivery however little limited aeron also design number possible us go well beyond tcp thing consider todd continues detail please keep reading article see subject core aeron replicated persistent log message conscious design process message waitfree zerocopy along entire path publication reception mean latency good predictable sum aeron nutshell created experienced team using solid design principle sharpened many previous project backed technique everyone tool chest every aspect well thought clean simple highly performant highly concurrent simplicity indistinguishable cleverness lot cleverness going aeron let see please note lot going talk martin thompson best capture idea get feeling well fit together martin great job conveying sense wholeness talk well worth watching miscellaneous todd montgomery continued one way think aeron fit tcp option reliable multicast delivery however little limited aeron also design number possible us go well beyond tcp thing consider individual identification time persisted stream data full record boundary channel sessionid channelid offset length tuples heart log buffer strategy allows nonvolatile storage stream arbitrary playback fall quite interesting lead reconnect persistence data stream truly mechanical sympathetic way ca nt stress enough open fact also open true location transparency within transport protocol ie local subscriber read directly publisher log buffer driver transparently sends data offbox subscriber thing identified far think also apply unique way handle proactivereactive forward error correction carouseling arbitrary replay etc aeron nt topic individual noncontended stream messaging system provide topic space blessing curse keeping stream space aeron deliberately bounded implementationwise unbounded designwise aeron allows topic space built top allows host system leverage without locking implementation wasting resource use case needed reliable unicast design familiar firewall traversable design mimic tcp reliable multicast design allows pubsub semantics infinitely configurable flow control strategy open kind use case possibility beyond normal messaging streaming etc transmission medium changed message tcp anymore multicast happening infiniband taking high performance space pci express builtin memory model possible transfer byte bus bus machine lot high performance space going need communicate process socket machine machine built core effectively become datacenters box intel new haswell cpu core per socket two hyperthreads per core possible thread running time machine need way break work communicate efficiently written pure java exploit version newly introduced lambda expression peertopeer brokered solution one reason lowlatency us udp shm ipc infiniband soon secret building high performance system simplicity complexity kill performance drive towards clean simple design provides flow control pluggable hook providing alternative algorithm support clustering archived message though future project might add functionality reliable multicast delivery aeron essentially replicates log buffer one machine another buffer persistent functional sense stored record mutated persisted disk offer reliable guaranteed delivery subscriber dy time message delivery retried protocol could layered top aeron archive published message subscriber could recover base functionality provide transactional guarantee offer done return either rejected placed shared memory log buffer sent driver driver best send flow control allows aeron try best get subscriber best effort delivery guarantee much like tcp slight bit better handle certain glitch connectivity tcp ca nt nt guarantee test capture full latency histogram trying case like multiple thread concurrently publishing contention occurs compare bunch different messaging factor well example ipcunicastmulticast different message size etc basic operation publisher send message channel subscriber read within channel stream subscriber channel keep message ordered stream independent block loss detected dealt minimal impact latency throughput flow control backpressure used overwhelm client congestion avoidance control deal packet transit congested network considered optional service high performance space congestion occurs service needed scenario latency need low possible congestion control algorithm slow example slowstart problem tcp tcp trying avoid swamping network traffic stream multiplexed demultiplexed channel dealing large message dealing fragmentation avoiding headofline blocking framework library composable design layer built top design three thing system architecture data structure protocol interaction get architecture right nice clean data structure important day le emphasis good protocol work fundamental distributed agent communicating cooperating architecture publisher publish subscriber subscriber two way communication two machine need register publisher subscriber single pair publisher subscriber one way communication communication happens medium like udp udp multicast infiniband pcie rdma sender responsible sending data across medium receiver receive data medium sender receiver independent agent running thread thing modern processor really good thing cache hot instruction data conductor independent agent work sending byte b includes housekeeping admin like user setup event new publication new subscription telling system going sender receiver kept really simple clean ship data fast possible two point medium using optimal mean data structure shared process communication using message passing conductor communicate nonblocking structure using message allows use clean single threaded code go really fast avoided concurrency lock using queue clean separation concern mean option part located independent agent thread running code internal data structure shared need concurrent need lock perform incredibly fast conductor split required service sender receiver needed service client since communication using shared memory queue part conductor split separate process medium driver process client process medium driver could kernel fpga data structure data structure java far much complexity much indirection mean low performance built map ipc ring buffer ipc broadcast buffer itc queue dynamic array log buffer ring buffer used communicate conductor client driver also need send event driver multiple client without slowing driver regardless number client broadcast used event number data structure change fly like number subscriber given subscription number publisher publication need dynamic nonblocking log buffer used move message publication subscription persistent log message stored along header persistent log structure greatness log structure log mapped disk life memory using memory mapped file log live file file available across process file mapped memory avoids make system call go disk immutable data structure safe read without lock grow time adding message described protocol state machine lock involved adding message first tail moved forward atomically another thread adding message exactly time space add message message copied buffer signal message completed single word operation written field message header message header completely written log send message publisher requires writing contiguous byte header message nothing formulated fly efficient send message lot client log single file grows time single file design common lot problem increase vm pressure page churn page fault would happening time page fault expensive linus torvalds page fault cost page fault getting faster process getting faster gap enormous though always true page fault always something avoid alternative single file approach keep three buffer clean dirty active active buffer writing dirty persistent history clean next buffer become active cleaning occur background conductor latency penalty writing buffer hot cache page caching going another thread could archive dirty buffer another data store message could kept forever waitfree implemented let say two writer trying write message x log time two different publisher two different thread possibly different process writing driver since driver run process shared across multiple process win race increment tail x increment tail tail run past end buffer use space allocated padding record put end end buffer buffer always contiguous complete x responsible rotating next buffer writes message beginning buffer new tail work cause delay thread happened completely independently blocking operation process take interrupt none thread become blocked sending message header contains version information flag message type frame length term offset offset buffer replicated side session id stream id term id encoded message everything needed header written directly network frame length byte written message sent interesting implication header design every byte stream unique identifier across time composite key streamid sessionid termid termoffset feature used later receiver tell sender region log dropped sender resend point log replicated using protocol message sender receiver sender want send message sends setup message receiver receiver send status message back sender start sending data receiver receiver sends status message telling sender x amount space left keep sending message nt acked naks instead modern multicast protocol nak instead ack order minimise implosion effect receiver telling sender much space left requires fewer message ack based approach give mechanism implement flow control back pressure sender sends last message sent heartbeat receiver approach also handle case last message dropped remember udp used receiver know message dropped sends sender nak region term gone missing point good remember replicated log receiver assume entire log available small window size receiver copy data structure sender header message written log two counter completed high water mark let say message written message come message ordering problem completed counter point end message high water mark point end message hole message completed point contiguous stream message message arrives completed moved forward point end message approach need keep skip list missing message gap history extra data structure complicated slow thing cause concurrency issue cache miss log buffer completely linear memory accessing requires striding forward memory really hardware friendly pointer chasing really bad performance cause page fault representation also compact scream memory design result going back basic thinking solve problem loss reordering keeping history memory concurrency friendly result persistent data structure faster existing approach persistent data structure data structure always preserve previous version modified idea functional side track lot learn discipline work together stupid functional ignore functional technique conductor always looking gap completed high water mark send nak sender resend part log simple easy debug know message consumed make use counter point location byte stream publisher sender receiver subscriber keep position counter byte stream make easy monitor apply flow control apply congestion control counter made available another memory mapped file available separate monitoring application protocol hard need watch selfsimilar behaviour scale example multicast world something go wrong get something called selfsimilar behaviour pattern start happening resonance get set randomization must injected system nak must sent random point future prevents subscriber hammering source lesson learned human suck estimation even great experience team estimation estimation something human good building distributed system hard people want build distributed system qualified build one least appreciation hard job say hey let make distributed concurrent system make single threaded system first work rest later defensive code feature code distributed system thing go wrong massive scale failure need handled mean code riddled exception handler aware lot failure case still lot defensive code building distributed system rewarding beautiful thing see whole network machine adjust injected fault get corrected design monitoring debugging start make everything visible beginning one nice thing lockfree approach actually observe state another thread make easier debug leaving performance table configuration separation programmer system admins antipattern developer talk people root access machine talk people network access mean system never configured right lead lot packet loss loss throughput buffer size strongly related need workout bridge gap know parameter fix know o network parameter tune part java really suck unsigned type nio system riddled lock difficult work offheap get required performance inside sandbox model string encoding require copying buffer three time people grownup let u map unmap memory mapped file use socket like grownup garbage collection issue around hashmaps painful ing byte flag promotes byte int assign result back byte nice part java tooling java world worst language world best tool chain ides gradle hdrhistogram java lambda method handle quite nice bytecode instrumentation really useful debugging unsafe java nice example counter incremented lockfree scale well across cpu memory fence make possible build broadcast buffer optimiser great garbage collection nice certain class algorithm average mean absolutely nothing pay attention percentile future feature complete heavy profiling optimization thing looking good already beating best product throughput push small byte message million message second difficult case latency looking superb best commercial product percentile work beyond percentile partially algorithm also nio selector lock c port coming next infiniband ipc related article
451,Lobsters,scaling,Scaling and architecture,"Infrastructure at Scale: Apache Kafka, Twitter Storm & Elastic Search at Loggly",http://www.youtube.com/watch?v=LpNbjXFPyZ0,infrastructure scale apache kafka twitter storm elastic search loggly,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature infrastructure scale apache kafka twitter storm elastic search aws invent youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature infrastructure scale apache kafka twitter storm elastic search aws invent youtube
452,Lobsters,scaling,Scaling and architecture,Fighting spam with Haskell,https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/,fighting spam haskell,inhouse fxl language haskell sigma work scale haskell purely functional strongly typed automatically batch overlap data fetch push code change production minute performance support interactive development haskell automatic batching concurrency haxl framework earlier blog post paper haxl github applicative donotation hotswapping compiled code garbage collector get involved haskell fit performance change way ghc manages heap blog post resource limit enabling interactive development package build system hackage yakshaving exercise stackage find bug ghc bug bug else protect graph spam fighting scale,one weapon fight spam malware abuse facebook system called sigma job proactively identify malicious action facebook spam phishing attack posting link malware etc bad content detected sigma removed automatically show news feed recently completed twoyearlong major redesign sigma involved replacing inhouse fxl language previously used program sigma haskell haskellpowered sigma run production serving one million request per second haskell common choice large production system like sigma post explain thinking led decision also wanted share experience lesson learned along way made several improvement ghc haskell compiler fed back upstream able achieve better performance haskell compared previous implementation sigma work sigma rule engine mean run set rule called policy every interaction facebook posting status update clicking like result sigma evaluating set policy specific type interaction policy make possible u identify block malicious interaction affect people facebook policy continuously deployed time source code repository code running sigma allowing u move quickly deploy policy response new abuse also mean safety language write policy important allow code checked repository unless typecorrect louis brandy facebook site integrity team discus scalable spam fighting antiabuse structure facebook instagram scale talk haskell original language designed writing policy fxl ideal expressing growing scale complexity facebook policy lacked certain abstraction facility userdefined data type module implementation based interpreter slower wanted wanted performance expressivity fully fledged programming language thus decided migrate existing language rather try improve fxl following feature top list choosing replacement purely functional strongly typed ensures policy inadvertently interact crash sigma easy test isolation strong type help eliminate many bug putting policy production automatically batch overlap data fetch policy typically fetch data system facebook want employ concurrency wherever possible efficiency want concurrency implicit engineer writing policy concentrate fighting spam worry concurrency implicit concurrency also prevents code cluttered efficiencyrelated detail would obscure functionality make code harder understand modify push code change production minute enables u deploy new updated policy quickly performance fxl slower performance meant writing anything performancecritical c putting sigma number drawback particularly time required roll change support interactive development developer working policy want able experiment test code interactively see result immediately haskell measure quite well purely functional strongly typed language mature optimizing compiler interactive environment ghci also abstraction facility would need rich set library available backed active developer community left u two feature list address automatic batching concurrency hotswapping compiled code automatic batching concurrency haxl framework existing concurrency abstraction haskell explicit meaning user need say thing happen concurrently datafetching considered purely functional operation wanted programming model system exploit whatever concurrency available without programmer use explicit concurrency construct developed haxl framework address issue haxl enables multiple datafetching operation automatically batched executed concurrently discussed haxl earlier blog post published paper haxl icfp conference haxl open source available github addition haxl framework needed help haskell compiler form applicative donotation allows programmer write sequence statement compiler automatically rearranges exploit concurrency also designed implemented applicative donotation ghc hotswapping compiled code every time someone check new code repository policy want code running every machine sigma fleet quickly possible haskell compiled language involves compiling code distributing new compiled code machine running sigma want update compiled rule running sigma process fly serving request changing code running program tricky problem general subject great deal research academic community case fortunately problem simpler request sigma shortlived need switch running request new code serve new request new code let existing request finish discard old code careful ensure change code associated persistent state sigma loading unloading code currently us ghc builtin runtime linker although principle could use system dynamic linker unload old version code garbage collector get involved garbage collector detects old code longer used running request know safe unload running process haskell fit haskell sandwiched two layer c sigma top use c thrift server principle haskell act thrift server c thrift server mature performant also support feature furthermore work seamlessly haskell layer call haskell c reason made sense use c server layer lowest layer existing c client code talking internal service rather rewrite code haskell would duplicate functionality create additional maintenance burden wrapped c client haxl data source using haskell foreign function interface ffi could use haskell haskell ffi designed call c rather c calling c requires intermediate c layer case able avoid intermediate c layer using compiletime tool demangles c function name called directly haskell performance perhaps biggest question run fast enough request sigma result user performing action facebook sending message messenger sigma must respond action take place wanted serve request fast enough avoid interruption user experience graph show relative throughput performance fxl haskell common type request served sigma request account approximately percent sigma typical workload haskell performs much three time faster fxl certain request typical workload mix measured percent percent improvement overall throughput meaning serve percent percent traffic hardware believe additional improvement possible performance analysis tuning optimizing ghc runtime workload achieving level performance required lot hard work profiling haskell code identifying resolving performance bottleneck specific thing implemented automatic memoization toplevel computation using sourcetosource translator particularly beneficial usecase multiple policy refer shared value want compute note perrequest memoization rather global memoization lazy evaluation already provides made change way ghc manages heap reduce frequency garbage collection multicore machine ghc default heap setting frugal also use larger allocation area size least mb per core fetching remote data usually involves marshaling data structure across chaskell boundary whole data structure required better marshal piece needed better still fetch whole thing although possible remote service implement appropriate api uncovered nasty performance bug aeson haskell json parsing library bryan sullivan author aeson wrote nice blog post fixed turn thing facebook scale oneinamillion corner case tend crop time resource limit latencysensitive service want single request using lot resource slowing request machine case resource include everything machine shared running request cpu memory network bandwidth request us lot resource normally bug want fix happen time time often result condition occurs production encountered testing perhaps innocuous operation provided unexpectedly large input data pathological performance algorithm certain rare input example happens want sigma terminate affected request error subsequently result bug fixed continue without impact performance request served make possible implemented allocation limit ghc place bound amount memory thread allocate terminated terminating computation safely hard problem general haskell provides safe way abort computation form asynchronous exception asynchronous exception allow u write code ignoring potential summary termination still nice guarantee need event limit hit including safe releasing resource closing network connection forth following graph illustrates well allocation limit work practice track maximum live memory across various group machine sigma fleet enabled one request resourceintensive outlier saw large spike maximum live memory disappeared enabled allocation limit enabling interactive development facebook engineer develop policy interactively testing code real data go enable workflow haskell needed ghci environment work full stack including making request backend service command line make work make build system link c dependency code shared library ghci could load also customized ghci front end implement command streamline desired workflow result interactive environment developer load code source second work fast turnaround time full set apis available test real production data source ghci easy customize could already made several improvement contributed upstream hope make improvement future package build system addition ghc make use lot opensource haskell library code haskell packaging build system cabal opensource package hosted hackage problem setup pace change hackage fast often breakage combination package work well together system version dependency cabal relies much package author getting right hard ensure tool support could found using package directly hackage together facebook internal build tool meant adding updating existing package sometimes led yakshaving exercise involving cascade update package often element trial error find right version combination result experience switched stackage source package stackage provides set package version known work together freeing u problem find set trial error find bug ghc yes notably fixed bug ghc garbage collector causing sigma process crash every hour bug gone undetected ghc several year fixed bug ghc handling finalizers occasionally caused crash process shutdown following fix seen crash either haskell runtime haskell code across whole fleet else facebook using haskell scale fight spam type abuse found reliable performant practice using haxl framework engineer working spam fighting focus functionality rather performance system exploit available concurrency automatically information spam fighting facebook check protect graph page watch video recent spam fighting scale event
453,Lobsters,scaling,Scaling and architecture,Docker Really Is the Future,http://blog.circleci.com/it-really-is-the-future/,docker really future,future haterade future merging world real problem solved constant change shitshow going frontend development sign free drop u line free day trial circleci enterprise,last week wrote future piece satirized container ecosystem lightly mocking docker google coreos bunch technology lot docker enthusiast enjoying butt joke also much loved shared lot people yelling told easy see people might think container ecosystem bullshit exactly way satirized exactly clear first glance docker containerization like virtualization quite got dockerfile kinda like chef combined something called layer involves weird filesystem something solves similar problem aws heroku vmware vagrant case slightly different way particularly clear first also really clear got competing version tool funny name like machine swarm flannel weave etcd rkt kubernetes compose flocker somehow linked microservices new shiny seem like fantastically stupid idea considering hard keep single service running first place got weird culty vibe dozen startup big corp competing get might somehow someday relate money definitely kind koolaid drunk really unreasonable look whole docker container thing conclude bullshit except actually future build application haterade many folk reacted future felt accurate satirical questioned hype around whole container thing docker container ecosystem hereafter taking bunch staple application developer world virtualization serviceoriented architecture operating system redelivering different goal benefit raise hackle large portion developer community curmudgeon hate anything new software industry contrary might expect absolutely filled people hate progress sort people would walk sistine chapel michelangelo done declare already perfectly good picture god prefer ceiling white fresco cool anyway time software industry make decision like high school teenager obsessively check cool clique maybe look around instagram facebook follow blindly led around technology form clique conformity even going far carve identity around technological niche fit even cover laptop gang hate complain thing strange different world drop docker new way almost everything throw away old rule operating system deployment ops packaging firewall paases everything else developer instantly love sometimes valid reason problem solves sometimes shiny toy allows cool kid get developer hate pure hype say came see everyone talking reason tribal rational reaction docker necessarily based technology hater really reacting solution important complex problem mostly problem one might noticed spent time scaling big system intuitively deeply understand meant important lot choice made docker related tool going seem weird scary merging world docker merge point two discipline web application distributed system past decade web community largely pretending build web application knowing code write html javascript rail site add form handler maybe api done enough launch product gain traction customer revenue change world meanwhile last two decade distributed system people rather boring shit experimented complex protocol like corba soap learned deal issue like cap theorem clock synchronization impossible two general problem appear largely theoretical problem solution rather uninteresting anyone simply trying take knowledge code use ship web application something interesting happened web application got large enough started need scale enough people arrived internet web apps could longer sit single vps scale vertically started scale started seeing bug application bug interesting name like problem distributed system folk working quite time problem whose solution difficult many case theoretically impossible early year scalability crisis heroku happened heroku made really easy scale infrastructure horizontally allowing u pretend really making simple web apps bought industry maybe year pretending selfdelusion hit limit selfdelusion come find trying build scalability early rearchitecting broken thing scale learning downside monolithic architecture using single database keep working u come phrase like immutable architecture v microservices whole set best worst practice try make easier point shift docker come try solve lot problem instead telling u pretend problem scaling exist keep thing basically way like heroku docker tell u distributed system fundamentally along need accept start work within model instead dealing simple thing like web framework database operating system presented tool like swarm weave kubernetes etcd tool pretend everything simple actually require u step game solve problem understand deeply problem solving upside gain ability build scalable architecture long pretend abstract away need know network partition deal choose ap cp system build architecture actually scale duress real network machine sometimes electrical storm virginia sometimes thing get set fire sometimes shark bite undersea cable sometimes latency delivery failure machine die abstraction leak everything need resilient reliable need acknowledge thing need think part developing application need shiny mythical best practice people like amazon netflix google put year sweat blood industry experience working shit telling u build system real scale real problem solved exactly docker solving u everything build web application extremely fragile docker forcing sanity deploying machine ops part devops separately application dev part even two different team administering part application stack ludicrous application relies machine o well code thinking separately make sense container unify o app within toolkit running serviceoriented architecture aws heroku iaa paases lack real tool managing serviceoriented architecture kubernetes swarm manage orchestrate service used entire operating system deploy application security footprint entail rather absolute minimal thing could deploy container allow expose minimal application port need even small single static binary fiddling machine went live either using tool redeploying application machine multiple time since container scaled orchestration framework immutable image started running machine never reused removing potential point failure using language framework largely designed single application single machine equivalent route serviceoriented architecture really existed kubernetes compose allow specify topology cross service deploying heavyweight virtualized server size aws provides say want cpu wasting virtualization overhead well using resource application need container deployed much smaller requirement better job sharing deploying application service using multiuser operating system unix built dozen user running simultaneously sharing binary database filesystems service complete mismatch build web service container hold simple binary instead entire os result lot le think application service constant change industry move quickly deify new exciting technology wait technology mature docker moving incredible pace meaning come close stabilizing maturing multiple option container runtimes image format orchestration tool host os different level utility scope traction community support looking around rest industry thing get stable become old boring example many protocol die got rest built rest ajax json corps soap corba using lesson learned building two major technology transition course year yet still got level tooling restbased apis soap decade ago soap particular yet fully die thing happening frontend indeed lot folk compared parody docker ecosystem shitshow going frontend development thing going programming language since escaped java decade ago consistently problem good solved developer continually come new solution docker ecosystem ton problem solved expect docker mature yet still many edge case weirdness going hit try decision weird may actually plain wrong look back year hence best practice still tried failed retried refailed get right going take number year figure stuff settle mean container bullshit ignore always faced choice staying still technology know taking bit leap trying new thing learning lesson adapting iterating improving industry around u looking future want move fast break thing sign free modern continuous integration delivery platform software team love use need awesome cicd behind firewall drop u line sign free day trial circleci enterprise
454,Lobsters,scaling,Scaling and architecture,It's The Future,http://blog.circleci.com/its-the-future/,future,docker support sign free drop u line free day trial circleci enterprise followup,hey bos said talk hear know lot web apps yeah distributed system guy back containercamp gluecon going dockercon next week really excited way industry moving making everything simpler reliable future cool building simple web app moment normal crud app using rail going deploy heroku still way go oh old school heroku dead noone us anymore need use docker future oh ok docker new way containerization like lxc also packaging format distribution platform tool make distributed system really easy containeri lxe lxc like chroot steroid cheroot ok look docker containerization future like virtualization faster cheaper oh like vagrant vagrant dead everything going containerized future ok need know anything virtualization still need virtualization container provide full security story yet want run anything multitenant environment need make sure escape sandbox ok getting little lost back thing like virtualization called container use heroku well heroku support docker told dead want run container coreos ok cool host o use docker hell even need docker use rkt rocket rkt right rocket called rkt totally different alternative containerization format bundled together docker composable good course good composability future ok use know think anyone us sigh saying something coreos yeah host o use docker host o host o run container run container yeah got ta something run container set like instance put coreos run docker daemon deploy docker image part container look take app write dockerfile turn image locally push docker host ah like heroku heroku told heroku dead run cloud using docker yeah real easy look gifee gify infrastructure everyone take shelf tool stack using container infrastructure google use thing think going around month ok someone else hosting stuff really want host stuff well amazon ec got ta write xml shit something openstack ew ew ew look really want host stuff really easy set kubernetes cluster need cluster kubernetes cluster manage deployment service one service mean app right got ta least service one app service whatever one look microservices future everything take monolithic app split like service one job seems excessive way make sure reliable authentication service go authentication service going use gem used time great use gem put project put restful api service use api gracefully handle failure stuff put container continuously deliver shit ok got dozen unmanageable service yeah saying kubernetes orchestrate service orchestrate yeah got service reliable need multiple copy kubernetes make sure enough distributed across multiple host fleet always available need fleet yeah reliability kubernetes manages know kubernetes work cause google built run etcd etcd implementation raft ok raft like paxos christ deep fucking rabbit hole going want launch app sigh fuck ok deep breath jesus ok paxos paxos like really old distributed consensus protocol noone understands us great thanks telling raft since noone understands paxos guy oh know work coreos anyway diego built raft phd thesis cause paxos hard wicked smart dude wrote etcd implementation aphyr said shit aphyr aphyr guy wrote know distributed system bdsm guy say bdsm yeah bdsm san francisco distributed system bdsm uh ok wrote katy perry song wrote set blog post every database fails cap cap cap theorem say consistency availability partition tolerance ok db fail cap even mean mean shit like mongo thought mongo web scale one else ok etcd yeah etcd distributed keyvalue store oh like redis nothing like redis etcd distributed redis loses half writes network partition ok distributed keyvalue store useful kubernetes set standard cluster using etcd message bus combine service provide pretty resilient orchestration system node one app many machine gon na need well going service course need redundant copy load balancer etcd cluster database kubernetes cluster like maybe running container wtf big deal container really efficient able distribute across like machine amazing one way put able simply deploy app sure mean storage still open question docker kubernetes networking take bit work basically see ok think getting great thanks explaining problem let repeat back see got right sure need split simple crud app microservices apis call apis handle failure resiliently put docker container launch fleet machine docker host running coreos using small kubernetes cluster running etcd figure networking storage continuously deliver multiple redundant copy microservice fleet yes glorious going back heroku want use docker continuously deliver shit check docker support want move fast break thing sign free modern continuous integration delivery platform software team love use need awesome cicd behind firewall drop u line sign free day trial circleci enterprise followup
455,Lobsters,scaling,Scaling and architecture,Scalability! But at what COST?,http://blog.acolyer.org/2015/06/05/scalability-but-at-what-cost/,scalability cost,scalability cost felixcuadrado frank mcsherry blog post paper graphx paper blog post,scalability cost mcsherry et al thanks felix cuadrado felixcuadrado pointing paper via twitter scalability highly prized yet misleading metric studied isolation mcsherry et al study cost distributed system configuration outperforms single thread cost system hardware platform number core required platform outperforms competent single threaded implementation cost weighs system scalability head introduced system indicates actual performance gain system without rewarding system bring substantial parallelizable overhead relatively short paper many highly quotable thoughtprovoking passage strongly encourage click link top read full thing frank mcsherry blog post paper also great cost turn damning metric many published system unbounded cost never outperform best single threaded application others order magnitude slower even using hundred core second computer shown know use first one paul barham take look graph comparing system b showing scale add core better system answered based purely scalability probably said system better answer presented one graph know yet consider second graph two system system b outperforms every scale know one rather deploying one scale worst fact graph b base system b performance optimization applied remove parallelizable overhead contrary common wisdom effective scaling evidence solid system building system scale arbitrarily well sufficient lack care hope shed light issue future research directed toward distributed system whose scalability come advance system design rather poor baseline low expectation taking exact datasets task used graphx paper mcsherry et al implement evaluate single threaded version compare result graphx paper conveniently gonzalez et al evaluated latest version several graphprocessing system plotted result table paper running iteration pagerank twitter dataset number core used system shown parenthesis name graphlab manages get close singlethreaded system using core v one core used singlethreaded system later paper mcsherry et al show using hilbert curve order edge get singlethreaded implementation perform even better rambased pagerank completing significantly better even graphlab core time comparing published scaling information powergraph graphlab graphx naiad single threaded system yield following cost conclusion curve would say naiad cost core pageranking twitter rv graph although presented part scaling data graphlab report measurement core achieves cost core graphx intersect corresponding singlethreaded measurement would say unbounded cost happens comparison done connected component task twitter graph based table paper published work scaling information graph connectivity given absolute performance label propagation scalable system relative singlethreaded unionfind optimistic scaling data would lead bounded cost scalable connected component implementation based label propagation fit well within programming model presented problem properly choosing good algorithm lie heart computer science label propagation algorithm used graph connectivity good algorithm fit within think like vertex computational model whose implementation scale well unfortunately case many others appealing scaling property largely due algorithm suboptimality label propagation simply work better algorithm scalable programming model leading u suboptimal path achieve scalable parallelism big data system restrict program model parallelism evident model may align intent programmer efficient parallel implementation problem hand mapreduce intention ally precludes memoryresident state interest scalability leading substantial overhead algorithm would benefit pregel think like vertex model requires graph computation cast iterated local computation graph vertex function state neighbor capture limited subset efficient graph algorithm neither design wrong choice important distinguish scalability efficient use resource mcsherry et al leave u three recommendation relating evaluation system design algorithm choice evaluating system important consider cost explain whether high cost intrinsic proposed system highlight avoidable inefficiency thereby lead performance improvement system learn system achieved performance scale several example performant scalable system galois ligra sharedmemory system significantly outperform distributed peer run single machine naiad introduces new general purpose dataflow model outperforms even specialized system understanding system right improve important rehashing existing idea new domain compared poorest prior work make sure using appropriate algorithm numerous example scalable algorithm computational model one need look back parallel computing research decade past boruvka algorithm nearly ninety year old parallelizes cleanly solves general problem label propagation bulk synchronous parallel model surprisingly general related work section would believe algorithm model richly detailed analyzed many case already implemented blog post frank mcsherry offer additional takeaway including going use big data system see faster laptop going build big data system others see faster laptop
456,Lobsters,scaling,Scaling and architecture,Flying faster with Twitter Heron,https://blog.twitter.com/2015/flying-faster-with-twitter-heron,flying faster twitter heron,storm opensourced heron sigmod rationale approach highlight heron shelf scheduler handling spike congestion easy debugging compatibility storm scalability latency heron performance heron twitter next research paper heron acknowledgement sanjeev kulkarni maosong fu nikunj bhagat sailesh mittal vikas r kedigehalli staneja zhilan zweiger christopher kellogg mengdieh michael barry storm community reference twitter heron streaming scale storm twitter,process billion event twitter every day might guess analyzing event real time present massive challenge main system analysis storm distributed stream computation system opensourced scale diversity twitter data increased requirement evolved designed new system heron realtime analytics platform fully apicompatible storm introduced yesterday sigmod rationale approach realtime streaming system demand certain systemic quality analyze data large scale among thing need process billion event per minute subsecond latency predictable behavior scale failure scenario high data accuracy resiliency temporary traffic spike pipeline congestion easy debug simple deploy shared infrastructure meet need considered several option including extending storm using alternative open source system developing brand new one several requirement demanded changing core architecture storm extending would required longer development cycle open source streaming processing framework perfectly fit need respect scale throughput latency system compatible storm api adapting new api would require rewriting several topology modifying higher level abstraction leading lengthy migration process decided build new system meet requirement backwardcompatible storm api highlight heron developing heron main goal increase performance predictability improve developer productivity ease manageability made strategic decision architect various component system operate twitter scale overall architecture heron shown figure figure user employ storm api create submit topology scheduler scheduler run topology job consisting several container one container run topology master responsible managing topology remaining container run stream manager responsible data routing metric manager collect report various metric number process called heron instance run userdefined spoutbolt code container allocated scheduled scheduler based resource availability across node cluster metadata topology physical plan execution detail kept zookeeper figure heron architecture figure topology architecture specifically heron includes feature shelf scheduler abstracting scheduling component made easy deploy shared infrastructure running various scheduling framework like mesos yarn custom environment handling spike congestion heron back pressure mechanism dynamically adjusts rate data flow topology execution without compromising data accuracy particularly useful traffic spike pipeline congestion easy debugging every task run processlevel isolation make easy understand behavior performance profile furthermore sophisticated ui heron topology shown figure enables quick efficient troubleshooting issue figure heron ui showing logical plan physical plan status topology compatibility storm heron provides full backward compatibility storm preserve investment system code change required run existing storm topology heron allowing easy migration scalability latency heron able handle largescale topology high throughput low latency requirement furthermore system handle large number topology heron performancewe compared performance heron production version storm forked open source version october using word count topology topology count distinct word stream generated set word figure throughput acks enabled figure latency acks enabled shown figure topology throughput increase linearly storm heron however heron throughput higher storm experiment similarly endtoend latency shown figure increase far gradually heron storm heron latency lower latency beyond run topology scale hundred machine many handle source generate million event per second without issue also heron numerous topology aggregate data every second able achieve subsecond latency case heron able achieve le resource consumption storm heron twitter twitter heron used primary streaming system running hundred development production topology since heron efficient term resource usage migrating topology seen overall reduction hardware causing significant improvement infrastructure efficiency next would like collaborate share lesson learned storm community well realtime stream processing system community order develop program first step towards sharing research paper heron sigmod paper find detail motivation designing heron feature performance using twitter acknowledgement heron would possible without work sanjeev kulkarni maosong fu nikunj bhagat sailesh mittal vikas r kedigehalli siddarth taneja staneja zhilan zweiger christopher kellogg mengdie hu mengdieh michael barry would also like thank storm community teaching u numerous lesson moving state distributed realtime processing system forward reference twitter heron streaming scale proceeding acm sigmod conference melbourne australia june storm twitter proceeding acm sigmod conference snowbird utah june
457,Lobsters,scaling,Scaling and architecture,Where are the self-tuning systems?,https://00f.net/2015/06/01/self-tuning-systems/,selftuning system,,computer science gone long way machine learning transformed thing looked like scifi couple year ago reality however major thing totally suck point still virtually nonexistent selftuning system every single system remains full knob human waste tremendous amount time tweaking breaking thing way usually solved turning knob introduces sideeffects require knob turning rince repeat want run server grain salt brave people running unikernels library driver kernel utterly bloated specifically tuned use case always suboptimal web server could way faster scalable reliable filesystem one use set mount option possibly adjusting couple sysctl value maximum number file descriptor peruser limit tunable value piece hardware especially network adapter one might end system might able run desired application normal workload without break horribly system parameter initially high low application come insanely long set knob one still adjust maximum number connection timeouts size various memory buffer number thread process bunch knob run basic web server jvm still need messing around gc setting get acceptable performancememory usage everything come many knob whose number keep increasing version version service unresponsive knob right position even though hardware would totally able keep workload simple old lamp stack come many knob one possibly know actually exact implication turning two time eventually people end copyingpasting setting found random web site add adjustment according intuition usually basis keep moving random slider thing go wrong whole construction appears stable enough course knob adjusted scientific way usually running synthetic benchmark unfortunately held true benchmark unlikely hold true forever reaching holy grail stable unexpected traffic hit server configuration whole thing likely remain suboptimal set knob position happens work partly accident point time remains far optimal service could always way faster accept connection use way le memory selftuning system mostly exist every single piece software still relies magic number found empirically pulled thin air developer user possibly manually adjusted later order get closer acceptable securityreliabilityperformance balance collecting system application network metric longsolved problem accessing knob unified way remains unsolved engineeringonly problem systemd bound tackle point database network stack virtual memory manager partly selftuning long time partly cluster resource managersschedulers pretty smart still rely much parameter whose value chosen human even academic research selftuning system scarce old given worth effort reasonable default ought enough everyone general solution global optimization problem unrealistic remains perplexing every single piece software instead converging towards knobless software
458,Lobsters,scaling,Scaling and architecture,Google systems guru explains why containers are the future of computing,https://medium.com/s-c-a-l-e/google-systems-guru-explains-why-containers-are-the-future-of-computing-87922af2cf95,google system guru explains container future computing,developer datacenters developer perspective advantage deploying application type system curious see evolution distributed system past couple decade given seen hugely popular technology hadoop nosql emerge also getting back idea shared resource management sound like see development revolution infrastructure revolution timing result much cloud computing empowered developer expect better experience tool company began small cloudbased startup like pinterest airbnb running scale problem company like google year ago limited flexibility reason app engine early paas offering take like people expected would end result mass adoption container consumer perspective service like google facebook handle huge number user without going,developer datacentersfrom developer perspective advantage deploying application type system lot advantage role kubernetes really play little bit longerterm view applicationthe initial value container really run laptop deploy thing cloud great thing docker particularly great job kubernetes answer question run fleet container controlled way upgrade controlled way send traffic scale service term number container included running increase capacity load go upthese kind operational thing really think important contribution kubernetesi curious see evolution distributed system past couple decade given seen hugely popular technology hadoop nosql emerge also getting back idea shared resource managementi think people virtual machine baseline resource worked around constraint terrible constraint especially small service although small constraint service terrible limitation interfere thing like utilization way capacity planningi think bigger issue developer really want think detail o security patching right instance size thing really ought able handle spend time focusing actual meat applicationthat would say actual revolution middle ofit sound like see development revolution infrastructure revolutionthey go hand hand main goal actually get developer thinking application collection service really think resource quite directly certainly manage directly deal installs patch thing like feel like going end least big change ability use server cloud timing result much cloud computing empowered developer expect better experience tool company began small cloudbased startup like pinterest airbnb running scale problem company like google year ago think thing going like look snapchat actually run google app engine problem actually solved entire existence app engine worry o patch machine boundary fact snapchat say actually operation people use google operation remarkable statement given sizethat kind like developer app engine flexible enough thing people want whereas container model essentially give flexibility virtual machine lot usability something like app engine much automate approach great developersis limited flexibility reason app engine early paas offering take like people expected would think took niche well suited app engine took website heroku took thing salesforce engine yard quite good want ruby rail none fully generalwe tried make app engine general time managed vms really feel like correct generalization app engine fact container issue whether add goodness app engine container world time think trivial general thing need gois end result mass adoption container consumer perspective service like google facebook handle huge number user without going think end result essentially higher velocity entire industry consumer mean choice service interesting thing arriving every day many nosql project used cap theorem justify make decision make sometimes correctly sometimes incorrectly
459,Lobsters,scaling,Scaling and architecture,Why we switched to Cassandra,https://victorops.com/blog/cassandra/,switched cassandra,cassandra high availability scalability selfhealing multidatacenter replication important part multidatacenter failover capability victorops one major factor decision go cassandra large community datastax,due nature business high availability extremely important victorops something take seriously know customer rely service always process deliver alert notification one key component critical functioning availability saas service datastore victorops historically used mysql high availability percona xtradb cluster operational analytical us mysql mature reliable relational database performed well planned early move horizontally scalable datastore order meet scalability high availability requirement including multidatacenter failover capability last fall began evaluate datastore alternative could help improve scalability relational nosql deciding use cassandra evaluating option decided cassandra best option help deliver extreme high availability reliability requirement cassandra strength influenced decision include high availability cassandra distributed database node equivalent ie master node client connect available node data replicated configurable number node failure number node depending replication factor result loss data cap theorem perspective consistency availability partition tolerance cassandra design provides tunable consistency readwrite request level allows increase availability expense consistency make sense scalability cassandra shown linearly scalable since node add processing power well data capacity possible scale incrementally large data volume high throughput simply adding new node cluster selfhealing cassandra eventually consistent data model node repair feature ensure consistency cluster automatically maintained time also make easy recover failed node increase decrease size cluster needed even place version upgrade case multidatacenter replication cassandra node replication eventual consistency feature core functioning distributed system feature designed outset improved battle tested throughout lifetime considered highly reliable feature therefore easily extended cluster contain node different geographical location due eventual consistency model includes support true activeactive cluster fact cassandra reputation robust reliable multidatacenter replication datastore industry important part multidatacenter failover capability victorops one major factor decision go cassandra large community cassandra apache project large active community including influential company like netflix addition datastax continues drive development continual improvement cassandra core well operational component also provide support subscription cassandra many advantage including described different datastores cassandra relational database interface retrieve data cql similar sql underlying data storage access model different result performance operational characteristic cassandra dependent application data model therefore important understand data accessed design data model perform well common query application us one data model cassandra performs particularly well log structured time series data type model data represents series measurement event happen time rather set update existing data item cassandra allows storing immutable event contiguously disk ordered clustering key often insertion time therefore efficient return set item based clustering key using serial rather random disk io many part victorop data model naturally map log structured approach example incident lifecycle comprised set event cause state incident change eg critical alert creation incident paging escalation acknowledgement recovery etc victorops surface notion main timeline well incident timeline obviously choice datastore important decision major affect scalability reliability availability maintainability extensibility saas service cassandra requires awareness underlying data access pattern operational characteristic designing system feel benefit provides term availability linear scalability seamless reliable multidatacenter replication great fit business requirement scale meet need future
460,Lobsters,scaling,Scaling and architecture,This Team Used Apache Cassandra... You Won't Believe What Happened Next!,http://blog.parsely.com/post/1928/cass/,team used apache cassandra wo nt believe happened next,team used apache believe happened next industry shocker cql sql use compact believe happened next compactly revealed separator ugly hack appear evil cassandra counter still hate u even check row size wide narrow right dress break internet database per hour future traveler dragon really grokked lucene olap system pythonic analytics elasticsearch truly ap highly available partitiontolerant data store postscript living cassandra clause breaking cql clause time series model advanced time series cassandra basic time series cassandra tyler hobbes basic rule cassandra data modeling cql python driver interested python devops andor distributed system work parselycom like pizza like content analytics data newsletter delivers join u,team used apache believe happened next know old saying seems good true probably technologist probably apply saying database vendor claim pretty regularly summer parsely team finally kicked tire apache cassandra say finally year using distributed database technology realtime analytics problem tired hearing technologist even one team saying problem domain obviously fit cassandra figured least entertain notion cassandra highlyavailable linearlyscalable data store supposedly battletested facebook apple netflix scale supposedly key netflix horizontal elastic scalability aws cloud supposedly built let ops staff sleep easily night handling relentless write volume hundred thousand per second ease grace supposedly unassailable reliability model let pull plug node without intervention thanks cluster autohealing kicker supposedly optimized parsely data set analytics data especially time series data lot supposedly learned course next month really really surprised u think surprise learned cassandra though cool technology panacea realtime analytics time series problem also learned technology loaded trap require deep knowledge cassandra internals work around also find technology much violate principle least surprise indeed almost every cassandra feature surprising behavior rest article explores surprise use fun internet news style headline followed short explanation problem hope help team use cassandra confidence fewer battle scar preview headline industry shocker cql sql team studied cassandra project introduced cql cassandra query language line cassandra release would wise study cassandra history diving cql cql nothing sql relationship lead surprise see cassandra data model centered around cassandra call column family column family contains row identified row key row key need fetch data row row one column name value timestamp value also called cell cassandra data model flexibility come following fact column name defined perrow row wide hundred thousand even million column column sorted range ordered column selected efficiently using slice cassandra data partitioning scheme come ensuring sharding replication occurs row key basis range row key establish cluster shard row automatically replicated among cluster node notice mentioned cql yet cql try hide every detail described even though knowledge critical running cassandra cluster cql cassandra column family renamed table row key become primary key table syntax look like restricted subset sql select insert create table offer facade cassandra facility none underlying sql machination fact row wide narrow fall design cql schema use resultant table column family discus wide v skinny row later get core warning cql sql way javascript java javascript designed look somewhat javalike java trendy wellunderstood technologist javascript designed javascript java fact javascript respect superficial set syntax similarity different java tried cql designed look somewhat sqllike sql trendy wellunderstood technologist today relationship sql respect superficial completely different beast hope warning help definitely help interpret following statement apache cassandra github readme cassandra query language cql close relative sql false statement ignore like distant cousin later find actually adopted parent way read line readme developer excited cql actually make cassandra easier use something important community unifies client library around single way interacting database like sql many sql database good thing mean cql anything like sql learn let keep going use compact believe happened next ok cheated reused headline format blog post resist fit well compact storage ask well along cassandra cql cassandra project introduced capability built directly cql schema modeling capability tried codify pattern community specifically way storing map list set data inside cassandra row cassandra row already sort like ordered map column key map storing mapslistssets cassandra row like storing mapslistssets inside ordered map admirable goal since provide data modeling flexibility cassandra call cql collection like siren singing beautiful hymn sunkissed shoreline seem attractive cql collection required enforced structure onto row stored column family structure allows mixture map field list field set field exist within single logical table remember table column family hood cassandra know key column name value structure built smartly inserting marker column embedding concatenated key name column name appropriately imagine python rather storing dict dict needed store subdict key inside parent dict key quot embedding quot unstructured data point quot quot quot x quot quot quot quot quot quot x quot quot quot v quot encoding quot unstructured data point quot quot quot dict quot quot x quot quot quot quot quot quot dict quot quot x quot quot quot cql collection really embed structure cell much encode structure column cost structure data storage disk fair cassandra operates assumption disk cheap simple example parsely attempted use cql map store analytics data saw data size overhead v using simpler storage format cassandra old storage format called compact storage ah name come compact small lightweight put another way cassandra cql new default storage format compact large heavyweight whole reason adopting cassandra first place huge datasets terabyte upon terabyte raw analytics event data suffering data overhead storage even compression trivial matter actually hoping cassandra could store thing compactly raw data happening due overhead cql collection cql row format team enjoys practicality purity would scrapped hope using new shiny stuck appropriatelynamed compact storage though cassandra documentation full warning compact storage backwardscompatibility cql restricts use advanced feature found cost feature simply make sense large data scale team use compact storage result got storage compact revealed separator ugly hack appear speaking compact v noncompact storage type able use cassandra cql cql map store unstructured data cassandra pull decided pick extremely simple separatedvalue storage format every analytics event go column named analytics event identifier timestamp lovingly labeled scheme xsv separator chose use unicode escape character example storing visit specific url twitter might store value like named http named http way read xsv value imagine unpacked python look like gt gt gt url urlref xsvparse line gt gt gt url quot http quot gt gt gt urlref quot http quot word key separated value stored cassandra instead stored serializationdeserialization code seems like terribly ugly hack actually really value show different color cassandra cql shell cqlsh easy split serialize require quoting value like traditional csv would every byte count course need worry showing raw data escape quite rare downside schema stored row selfdescribing might cql map storage saving tremendous live since use consistent order grow schema time simply appending new value right side serialization form amazing thing client performance xsv value much much better cql collection equivalent even better schema tried actually modeled every field directly cql schema client would spend time decoding structure row rather decoding actual value case know thinking use something like avro protocol buffer considered felt would make cassandra data storage bit inscrutable able meaningfully use cql shell thus eliminating one main benefit cql willing throw cql cql collection entire idea usable cql shell also calculation found xsv performed similarly data set anyway since value string binary serialization approach actually help much store large gob unstructured data cassandra recommend compact storage serialized value xsv format storageefficient cpu ioefficient go figure smoke official way modeling unstructured data cassandra mean team advice ignore advice using compact storage doc ignore fancy new cql stuff pretty weird unlike marketed feature scheme actually work evil cassandra counter still hate u even oh dreading section got burned badly cassandra counter shudder even think sixorso week development time lost trying make counter work use case watch lot cassandra presentation online hear different thing counter different people cassandra oldtimer say thing like counter evil counter hack counter barely work counter dangerous never ever use get idea good mood sometimes ask question counter cassandra irc channel lucky longtime developer laugh room sometimes call brave soul cassandra thing hopeful counter received several sprint development feature starting better understood first bunch fix made counter reliable whole new counter implementation improved performance especially contention reason counter controversial weird cassandra community seem odds cassandra philosophy cassandra meant data store built principle immutability idempotence incrementing counter nonidempotent operation seems require inherently mutable data store like redis mongodb cassandra work around implementing counter increment readthenwrite operation similar another cassandra trap feature lightweight transaction lwt implementation clever work performance plain cassandra rowcolumn insert optimized even live cassandra different performance characteristic counter discover oddity example column family use counter live underlying storage different counter really deleted unless delete definitive even seem weird cluster bug surface counter deletes related counter deletable also learn cassandra timetolive ttl feature data expiration simply work counter finally find opting counter threw baby bathwater yes got convenient durable data structure storing count lost benefit idempotent data store matter analytics use case although realize quite much mattered well say cassandra counter trap run even many cassandra counter problem fixed still find trap since need counter also decided roll back stable cassandra line still support cql production longer maintained parallel stable branch check row size wide narrow right remember mentioned cassandra underlying data model row key determines system distributes data turn important data model challenge cassandra user controlling row size way fashion magazine ebb flow right waist size men woman cassandra ebb flow question right row size short answer frustrating want row neither wide narrow right frustrating rule thumb go cassandra theoretically support row billion column quite proud fact ask anyone community tell storing billion column one row bad idea many right million hundred million demanded answer get consistent one data set settled roughly also suggested average row many fewer column typically thousand data set seems strike right balance read performance compactionmemory pressure cluster determining right row size data iterative process require testing think hard schema design integration test case decided partition data event type every tracked url partitioned data hour data single url single day stored row key seemed work well row something awful happened dress break internet database silly internet meme dress went around many customer site large news information publisher result many customer url receiving upwards million unique visitor per hour single url meant partitioning scheme cassandra would get wide row nowhere near billion column sure definitely ten hundred million put compaction pressure cluster also led write contention unfortunately come natural partitioning scheme would handle rare case instead introduce synthetic partition key another system track hot url lightweight way reach certain threshold systematically partition partition using split factor million event might spread among row key ensuring hold column data keeping row size right introduced operational complexity necessary make cassandra work use case future traveler dragon wellseasoned technologist friend mine surprised walked issue cassandra said honestly expected adopting data store scale would require learn internals point adopt elasticsearch really grokked lucene time series analytics data funny thing parsely use cassandra original adoption driven desire utilize time series analytics capability turn without capability group filter aggregate core function olap system cassandra simply could play role hoped counter would give u basic aggregation holding cumulative sum dice instead ended using data staging area data sits index lucenebased time series system discussed little bit elastic blog article pythonic analytics elasticsearch come understand strength limitation work well role providing timeordered durable idempotent distributed data store something cassandra handle adopt cassandra large data use case recommend heed advice learn cql actually avoid cql collection use compact storage adopt custom serialization format use counter stay moststable manage row size carefully watch partitioning hotspot guideline mind likely end better experience good true probably indeed overall cassandra powerful tool one truly ap highly available partitiontolerant data store powerful data distribution cluster scaleout model main fault overmarketing bug feature trying hard make quirky feature appealing mass market dumbing experienced distributed system practitioner adopt cassandra comfort knowing scale existing deployment caution come knowing largescale data management silver bullet postscript living cassandra asked didier deshommes longtime parsely backend engineer tip newcomer tip included postscript article though article discussed many trap hit cassandra many resource online model data cassandra way serve positive instruction find modeling data cassandra involves essentially one trick several variation wide row although several cassandra howto data modeling tutorial usually keep going back handful link want refresh memory funny thing link many come around time cql introduced even clause writing data easy cassandra often forget read efficiently best guide know knowing get away breaking cql clause nice side effect also inform structure writes take advantage rule time series model cassandra onetrick pony sometimes need little creativity fitting problem feeling stuck worried might run wide row trick go back advanced time series cassandra give idea build cassandra modeling technique developed basic time series cassandra go back older article often introduce wellworn idea timebased rollups tyler hobbes tyler hobbes author advanced time series post also recently put basic rule cassandra data modeling great place get started article point newer cassandra user right away tyler primary author cassandra cql python driver also much learn via public slide avoid trap embrace trick good luck interested python devops andor distributed system want work company dealing petabytescale data realtime analytics cloud scaling challenge parsely always looking great talent small nimble python backend engineering devops team write email work parselycom cassandra subject line related like pizza like content analytics data newsletter delivers join u
461,Lobsters,scaling,Scaling and architecture,Distributed Read-Write Mutex in Go,https://gist.github.com/jonhoo/05774c1e47dbe4d57169,distributed readwrite mutex go,reload reload,instantly share code note snippet distributed rwmutex go perform action time signed another tab window reload refresh session signed another tab window reload refresh session
462,Lobsters,scaling,Scaling and architecture,Building HTTP/2 services with gRPC,http://lwn.net/Articles/636129/,building service grpc,page representational state transfer grpc recently approved blog post account version available python tutorial detailed description stream authentication method log,please consider subscribing lwnsubscriptions lifeblood lwnnet appreciate content would like see subscription help ensure lwn continues thrive please visit page join keep lwn net nathan willismarch google recently released new remote procedure call rpc framework designed supplant traditional representational state transfer rest development web application called grpc faq defines grpc remote procedure call new framework take advantage several feature recently approved standard project claim result better performance greater flexibility rest apis implementation client serverside variety programming language basic idea behind grpc quite similar used restful service make many web application today method server application accessible client welldefined set http request server answer call http response necessary error code grpc dispenses http relying strictly thus project claim gaining considerable performance grpc announced february blog post post described grpc enabling highly performant scalable apis microservices said google starting expose grpc endpoint service also said company involved creation grpc naming mobilepayment processor square one example square posted account explains background post highlight key feature enabled building top bidirectional streaming multiplexed connection flow control http header compression improvement course would available rpc framework using grpc built top google protocol buffer updated version aka conjunction grpc release largely strippeddown version multiple field removed including several required required default value extension mechanism replaced simple standard type hold also add standard type time date dynamic data plus binding ruby javanano androidcentric java library optimized lowresource system grpc us protocol buffer serialize deserialize overthewire message employing technique leverage binary wire format allowing efficient rpc traffic http ascii encoding grpc also us interface definition language idl specify structure content web service rpc message take get started application developer writes schema new web service defining request possible response service use protocol buffer compiler protoc used generate class stub code client server schema supported language launch time available language python c c objectivec php ruby go java generic android flavor javascript tailored use nodejs quickstart guide available language c objectivec php guide absent moment tutorial begin basic service definition syntax greeting service definition service greeter sends greeting rpc sayhello hellorequest return helloreply request message containing user name message hellorequest string name response message containing greeting message helloreply string message one side point worth noting reason explained project python tutorial indeed specify syntax rather simple hello world example used syntax identical visible comparing java go tutorial generated code leave artifact behind portion module name output language protoc compiler specified commandline switch using pythonout example generates module stub function called sayhello client greeter class server includes stub function respond sayhello message well simple serve function wait incoming message using abstract idl argument go developer quickly generate client code variety different platform desktop mobileoriented grpc also make easy include version number messageformat definition client server support interoperability across version project detailed description grpc message sent frame advantage http come play example whenever endpoint client server sends message choose keep stream open used bidirectional communication remainder session grpc us stream id internal identifier rpc call mapping rpc call associated stream come free multiplexing several open stream one connection another way enables efficient use available bandwidth developer determine many active stream make sense service grpc also piggyback error code allows application pick error condition one endpoint stream throttling connection little additional effort finally grpc designed support pluggable authentication method tl greater oauth supported far grpc head naturally remains seen certainly straightforward enough framework warrant closer examination doubt many developer interested seeing take advantage promised improvement http said still expect see wide array new framework announced next year claiming leverage new protocol range enhanced feature grpc may one first time tell many developer outside google partner find good fit log post comment
463,Lobsters,scaling,Scaling and architecture,DocStore: Document Database for MySQL at Facebook,https://www.percona.com/live/mysql-conference-2015/sites/default/files/slides/Facebook%20DocStore%20Percona%202015.pdf,docstore document database mysql facebook,wrap attendee rave review thank making percona live mysql conference expo one best year save date next year april tutorial schedule monday april session schedule tuesday april percona opening keynote polyglot persistence facebook keynote panel next disruptive technology come cloud big data woz mind materialized view mysql using flexviews fast master failover without data loss navigate many instrument mysql mysql hosted cloud innodb new mysql backup scale accelerating mysql flash persistent memory lightweight openstack benchmarking service rally docker mysql openstack deep dive introduction database service emphasis openstack using trove case study webscale openstack midonet mysql performance scalability benchmark tip boost galera cluster next generation monitoring moving beyond nagios learned migrating mysql onpremises amazon rds database innodb journey core iii undrop innodb yahoo experience percona cluster deploying openstack cloud scale time warner cable closer look ceph performance trove real world adventure running production workload upstream code leveraging openstack cinder peak application performance practical performance tuning using digested sql log running galera cluster kubernetes qa pro test monitor visualise mysql performance jenkins managing mysql performance percona cloud tool prepared statement pdo php docstore document database mysql facebook pseudo gtid easy replication management practical mysql optimization replication clustering openstack trove mongodb profiler deep dive openstack mimedia scalable consumer medium innodb defragmentation evolution percona replication manager prm database defense depth improving performance better index workbench dba meet mariadb multithreaded replication mysql lifecycle data augmenting mysql big data nosql option designing highly resilient network infrastructure openstack cloud want contribute openstack show service leveraging openstack paas service cloud foundry apps using openstack build next generation cloud towards one million sql query per second mariadb innodb new future plan improving database design practical database theory utilizing ansible manage highly available mysql environment sharding horizontal vertical scaling almost instant monitoring deadly mysql performance sin running mysql aws identifying major source variance traditional dbms case predictable database rdo community scaling mysql cloud vitess kubernetes mariadb connect storage engine simplify heterogeneous data access weapon slaying monolithic monster managing multidc mysql installation chef sqoop getting hadoop orchestrating mysql pxc right application introduction high availability option mysql everything mysql bug mysql galera cluster percona xtradb cluster mariadb galera cluster data security emerging legal trend webscalesql meetinghacking mariadb bof test merge improve mysql branch maintenance rocksdb storage engine mysql performance dbhangops real life session schedule wednesday april celebrating mysql cloud done data lately innodb change buffering making pagerduty reliable using xtradb cluster better devops mysql docker mysql dba tool mysql facebook messenger using hadoop together mysql data analysis backup sure work moving workload effectively hybrid cloud deployment mysql indexing best practice mysql backup strategy tool recovery scenario exploring data innodb explorer online schema change maximizing uptime managing mysql puppet scaling mysql amazon web service java mysql connector connection pool feature optimization gtid replication implementation troubleshooting driving mysql big data scale practically address trillion row designing effective schema innodb mysql replication setup benefit limitation shard big transaction galera cluster schema change multiple time day ok high performance mysql choice amazon web service beyond rds pivot table analytics pure sql using mysql audit plugins elasticsearch elk binlog server bookingcom managing scale consolidation sql injection prevent introduction high availability option mysql advanced mysql query tuning sharding mysql via cellular mitosis okta mysql automation facebook scale transparent sharding application open source database virtualization engine dve assembling perfect mysql toolbox mysql security essential faster logical backup restores using mydumper yahoo mysql performance analyzer becoming dba minute day important thing know sysadmins developer stay ahead mysql operational problem percona toolkit pmp plugins character set collation basic create useful mysql bug report linux internals mysql dba highavailability using mysql fabric mysql ecosystem github mysql cluster performance tuning talk lsm database facebook securing database systemd container service mysql twitter realtime event notification using flexcdc practical mysql troubleshooting performance optimization percona cloud tool introduction tokudb storage engine large data set writing high performance sql statement percona server advantage wild supposition mysql kafka idea tool manage mysql range partition date column building high availability solution work running percona xtradb cluster microsoft azure future mysql quality assurance introducing pquery mysql bug birthday end faster replication slave creation indexing json document efficient mysql query json data peril compatibility scripting mysql installation across version working home fun fact scare session schedule thursday april percona take smart data management next product service building database asaservice world analyze tune mysql query better performance mysql cluster ndb introduction overview xdb shared mysql hosting facebook scale encrypting mysql data google writing application code mysql high availability insert cassandra prod usecase practical performance schema simplified high availability w multi source replication mysql query tuning deploying mysql ha ansible vagrant systematic approach performance adding indirection enhances functionality story proxy devops met gaming need encrypting data mysql go bookingcom evolution mysql system design rapid fulltext indexing elasticsearch mysql row based replication incremental logical backup replicate everything tungsten replicator using mysql java highly efficient backup percona xtrabackup overview mysql connector php running mysql aws analyze executable statement new way optimizer troubleshooting mariadb webscalesql think know disk well think incremental backup available xtrabackup percona server enabling eventdriven analytics custom tungsten replicator filter rabbitmq extending mysql sphinx search replication enhancement mysql extending mysql highvelocity timeseries data deadly mysql performance sin mysql openstack using mysql asynchronous call increase performance user experience hardening mysql mysql security basic understanding innodb lock deadlock bootstrapping database single command elastic provisioning win solid state storage open source mysql database need know optimize performance xtradb key performance algorithm advanced database operation simplified percona toolkit json support mysql iop belong u pinteresting case study mysql performance optimization tokumx avoiding common pitfall mongodb pxc right application mysql innodb fundamental configuration,wrap attendee rave review thank making percona live mysql conference expo one best year save date next year april percona live mysql conference expo premier event rich diverse mysql ecosystem place open source community well business thrive mysql marketplace attendee include dba sysadmins developer architect ctos ceo vendor around worldmysql world popular open source database powering massively popular web application facebook youtube twitter mysql continues evolve rapidly new release related software delivery method us enabling reach market power application every daytutorials schedule monday april schedule tuesday april opening keynotepeter zaitsevpolyglot persistence facebookharrison fiskkeynote panel next disruptive technology come cloud big data kenneth hui kenny gorman michael coburn peter zaitsev ritesh chhajer robert hodges thomas hazelwoz mind steve wozniakmaterialized view mysql using flexviewsjustin swanhartfast master failover without data lossyoshinori matsunobunavigate many instrument mysql grinch tusamysql hosted cloudcolin charlesupgrading mysql scaletom krouper jonah berquistinnodb newsunny bainsmysql backup scalealexey kopytovaccelerating mysql flash persistent memorynisha talagalalightweight openstack benchmarking service rally dockerswapnil kulkarnimysql openstack deep divepeter borosan introduction database service emphasis openstack using trovematt griffin amrith kumarcase study webscale openstack midonetadam johnson shacolby jacksonmysql performance scalability benchmarksdimitri tip boost galera clusterfrederic descampsnext generation monitoring moving beyond nagiosjenni snyder josh snyderwhat learned migrating mysql onpremises amazon rdsmichael coburnthe database jeremy tinleyinnodb journey core iiidavi arnaut jeremy coleundrop innodbaleksandr kuzminskyyahoo experience percona clustertrey raymond yashada jadhavdeploying openstack cloud scale time warner cablematthew fischer clayton oneilla closer look ceph performancezoltan arnold nagy mark koronditrove real world adventure running production workload upstream codenikhil manchanda andrew conradleveraging openstack cinder peak application performancesteven walchek chris merz amrith kumarpractical performance tuning using digested sql logsbob burgessrunning galera cluster kubernetespatrick galbraithqa pro test monitor visualise mysql performance jenkinsramesh sivaramanmanaging mysql performance percona cloud toolsdaniel nichterprepared statement pdo phpchris shumakedocstore document database mysql facebookpeng tian tian xiapseudo gtid easy replication managementshlomi noachpractical mysql optimizationpeter zaitsevreplication clustering openstack troveamrith kumar george lorchmongodb profiler deep divekenny gormanopenstack mimedia scalable consumer mediamichael yooninnodb defragmentationrongrong zhongthe evolution percona replication manager prm yves trudeaudatabase defense depthgeoffrey andersonimproving performance better indexesronald bradfordworkbench dbaligaya turmellemeet mariadb golubchik michael wideniusmultithreaded replication mysql combaudonthe lifecycle data augmenting mysql big data nosql optionsdavid murphyballroom hdesigning highly resilient network infrastructure openstack cloudspere monclusso want contribute openstack george lorchshow service leveraging openstack paas service cloud foundry apps andrew conrad vipul sabhaya nikhil manchandausing openstack build next generation cloudkenneth huitowards one million sql query per secondstewart smithmariadb innodb new future plansmichael wideniusimproving database design practical database theorydavid berubeutilizing ansible manage highly available mysql environmentmiklos szel alkin tezuysalsharding horizontal vertical scalingtom christ almost instant monitoringdaniel guzman deadly mysql performance sinsmartin arrietarunning mysql awsmichael coburnidentifying major source variance traditional dbms case predictable databasesbarzan mozafarirdo communityalvaro lopez ortegamonasca monitoring service scale roland hochmuth deklan dieterly craig bryantscaling mysql cloud vitess kubernetesanthony yehmariadb connect storage engine simplify heterogeneous data accessserge frezefondweapons slaying monolithic monsterjohn williamsmanaging multidc mysql installation cheftatyana arenburgsqoop getting hadoopabraham elmahrekorchestrating mysqlivan zorattiis pxc right application stephane combaudon peter borosan introduction high availability option mysqlben mildreneverything mysql bugssveta smirnova valerii kravchukmysql galera cluster percona xtradb cluster mariadb galera clusterjay janssendata security emerging legal trend keith moulsdalewebscalesql meetinghackingsteaphan greenemariadb bofcolin charlestest merge improve mysql branch maintenancelaurynas biveinisrocksdb storage engine mysqlyoshinori matsunobuperformancerick james dbhangops real lifegeoffrey andersonsessions schedule wednesday april power collaborationsteaphan sallnercelebrating mysqltomas ulinwhat cloud done data lately robert hodgesinnodb change bufferingdavi arnautmaking pagerduty reliable using xtradb clusterdoug barthbetter devops mysql dockersunny gleasonmore mysql dba toolsryan lowe randy wiggintonmysql facebook messengerdomas mituzasusing hadoop together mysql data analysisalexander rubinyour backup sure work matthew boehmmoving workload effectively hybrid cloud deploymentsmc brownmysql indexing best practicespeter zaitsevmysql backup strategy tool recovery scenariosroman vynar akshay suryawanshiexploring data innodb explorerjeremy coleonline schema change maximizing uptimedavid turner ben blackmanaging mysql puppetjaakko pesonenscaling mysql amazon web servicesmark filipijava mysql connector connection pool feature optimizationkenny grypmysql hbase ecosystem realtime big data overviewstom komenda lukas putnagtid replication implementation troubleshootingabdelmawla ghariebdriving mysql big data scale practically address trillion rowsthomas hazeldesigning effective schema innodbyves trudeaumysql replication setup benefit limitationsstephane combaudon rick golba shard itmaggie zhou keyur govandebig transaction galera clusterseppo jaakolaschema change multiple time day ok jenni snyderhigh performance mysql choice amazon web service beyond rdsandrew shiehpivot table analytics pure sqlgiuseppe maxiausing mysql audit plugins elasticsearch elkjeremy glick andrew moorebinlog server bookingcomjeanfranois gagnmanaging scale consolidationchris merzsql injection prevent justin swanhartan introduction high availability option mysqlben mildrenadvanced mysql query tuningalexander rubinsharding mysql via cellular mitosis oktawill guntymysql automation facebook scaleshlomo priymaktransparent sharding application open source database virtualization engine dve amrith kumar peter borosassembling perfect mysql toolboxike walkermysql security essentialsronald bradfordfaster logical backup restores using mydumpermaximiliano bubenickyahoo mysql performance analyzerxiang rao ashwin nellorebecoming dba minute day important thing know sysadmins developersmartin arrietastay ahead mysql operational problem percona toolkit pmp pluginsdaniel guzman burgos fernando laudares camargoscharacter set collation basicsjustin swanharthow create useful mysql bug reportvalerii kravchuklinux internals mysql dbasryan lowe randy wiggintonhighavailability using mysql fabricmats kindahlthe mysql ecosystem githubsam lambertmysql cluster performance tuning talkjohan andersson alex yulsm database facebookyoshinori matsunobusecuring database systemd container servicesraghavendra prabhumysql twitter sun inaam rana anton kuraevrealtime event notification using flexcdcjustin swanhartpractical mysql troubleshooting performance optimization percona cloud toolsdaniel nichterintroduction tokudb storage engine large data setspeter zaitsevwriting high performance sql statementstim sharppercona server advantagemichael coburn christopher jeffusa wild supposition mysql kafka vishnu raoideas tool manage mysql range partition date columnmichael wangbuilding high availability solution worksjeanfranois gagnrunning percona xtradb cluster microsoft azurepaige liuthe future mysql quality assurance introducing pqueryroel van de paar ramesh sivaramanmysql bug birthday endsveta faster replication slave creationchris merzindexing json document efficient mysql query json datajustin swanhartthe peril compatibility scripting mysql installation across versionsgiuseppe maxiaworking home fun fact scaresraghavendra prabhu session schedule thursday april take smart data management next product servicesrob youngbuilding database asaservice worldamrith kumarhow analyze tune mysql query better performanceystein grvlenmysql cluster ndb introduction overviewmatthew boehmxdb shared mysql hosting facebook scaleevan eliasencrypting mysql data googlejeremy cole jonas orelandwriting application code mysql high availabilityjay jansseninsert cassandra prod usecase susanne lehmannpractical performance schemabill karwinsimplified high availability w multi source replicationgerardo narvajamysql query tuning smirnova alexander rubindeploying mysql ha ansible vagrantdaniel guzman burgos robert barabasa systematic approach performanceryan lowe john cesarioadding indirection enhances functionality story proxymark riddoch massimiliano pintohow devops met gaming needssean chighizola peter garbesencrypting data mysql gobaron schwartzbookingcom evolution mysql system designnicolai plumrapid fulltext indexing elasticsearch mysqlsunny gleasonrow based replication incremental logical backupsantosh bandareplicate everything tungsten replicatorgiuseppe maxiausing mysql javadavid bennetthighly efficient backup percona xtrabackupnilnandan joshi valerii kravchukoverview mysql connector phpjustin swanhartrunning mysql awsmichael coburnanalyze executable statement new way optimizer troubleshooting mariadb petruniawebscalesqlsteaphan greenethink know disk well think aurimas mikalauskasincremental backup available xtrabackup percona servervladislav lesin george lorchenabling eventdriven analytics custom tungsten replicator filter rabbitmqscott wimer dj hansonextending mysql sphinx searchvladimir fedorkovreplication enhancement mysql soaresextending mysql highvelocity timeseries datamoshe deadly mysql performance sinsmartin arrietamysql openstackpeter borosusing mysql asynchronous call increase performance user experiencealexander rubinhardening mysql mysql security basicsdavid busbyunderstanding innodb lock deadlocksvalerii kravchuk nilnandan joshibootstrapping database single command elastic provisioning winjosh snydersolid state storage open source mysql database need know optimize performancepeter zaitsevxtradb key performance algorithmslaurynas biveinis alexey stroganovadvanced database operation simplified percona toolkitmatt griffin vinoth kanna rsjson support mysql potemkin manyi luall iop belong u pinteresting case study mysql performance optimizationernie souhradatokumx avoiding common pitfall mongodbjon tobinis pxc right application stephane combaudon peter borosmysql innodb fundamental configurationjustin swanhart michael coburn
464,Lobsters,scaling,Scaling and architecture,"CAP, Availability, High-Availability and Big Data databases in a world of partitions",http://blog.thislongrun.com/2015/04/cap-availability-high-availability-and_16.html,cap availability highavailability big data database world partition,,would give consistency purchase little temporary availability deserve neither consistency availability b franklin quoted memory post part cap theorem series may want start post acid v cap database background never really exposed cap theorem post discussing trap availability consistency definition cap also used introduction know cap looked formal definition post previous one using cap categorize distributed system summarizes result extends big data distributed system compared distributed system big data system add another dimension definition data fit single node see implication science fiction author often ahead time case looking terminology cap theorem availability use definition proof cap theorem gilbert lynch consistent atomic linearizable consistency must exist total order operation operation look completed single instant equivalent requiring request distributed shared memory act executing single node responding operation one time seen cap consistency different acid consistency partition network allowed lose arbitrarily many message sent one node another network partitioned message sent node one component partition node another component lost seen node failure packet loss partition available distributed system continuously available every request received nonfailing node system must result response use capavailable distinguish definition definition availability cap defines availability clearly common definition availability find elsewhere availability time system available divided total time two issue definition cap context defines availability without defining available mean cap look system system partitioned something considered common definition system considered whole however possible define highavailability partial availability using terminology comparable one used cap next highavailability every request received subset nonfailing node system must result response usually highavailability understood whatever state node failing nonfailing least subset node answer request partialavailability request received subset nonfailing node system must result response nonavailability request fail looking cap category let recapitulate cap category cp consistent capavailable ap capavailable consistent ca consistent capavailable partitiontolerant specifies operating range saw overlap ca cp real issue ca consistent capavailable system neither consistent capavailable notation overline come ensemble theory cap availability applied distributed system let match cap real system network partition table mean probable case mean happen le probable case system description cap category availability cap high partial non one webserver connected one sql database ca one webserver connected one eventually consistent database ca twophase commit multiple sql database ca consensus chubby zookeeper cp typical cp big data system cp typical eventually consistent big data system ca ideal eventually consistent small data system ap read like first row mean one webserver connected one sql database ca system available partition typical cp big data system cp system usually highlyavailable partition partition may lead partial nonavailability let look row one one cacp system looked previous post single webserver single database system claim belong two category choose saying cp good marketing system work partition tagged partitiontolerant great partition enduser discovers really meant cp saying ca highlight network partition real issue system special attention given network seems best category software engineering point view single webserver eventually consistent database nothing much add said previous post option ca system never consistent available partition multiple database twophase commit ca best choice twophase commit give consistency also key property acid atomicity twophase commit givesup acid atomicity transaction progress network partition started choose consistencyoveravailability others case distributed consensus nothing add said previous post typical example cp system cp big data store one multiple type availability single reason cp big data store highlyavailable partition every partition partition lead least partial availability le intuitive data big fit possible partition depending network partition find one partition copy data highavailability none partition copy data application using big data store cope situation system whole nonavailable none partition copy data application cope missing data partial availability application dependent application manage case explicitly able handle situation put number let look real implementation logic math use optimistic calculation reallife case actual availability would worse math hdfs hdfs hadoop distributed file system hdfs cp big data system system split two partition one continue serve query characteristic data divided block block replicated synchronously node copy go rack copy another configurable value mentioned commonly used let imagine rack tb data replication whole cluster million hdfs block mb network partition rack level two partition one rack another rack partition first rack continue serve request data construction block written rack also copied another rack however client rack send request rack system whole highlyavailable capavailable note even rack data client rack would still able work locally cp system two partition running parallel cap network partition rack partition highlyavailable partition need least one copy block probability least one copy data rack calculate let simplify say every block replicated different rack probability copy partition first copy bad rack second copy bad rack third copy bad rack give u single block ie single block probability least one copy main partition around good million block probability least one copy block million practically zero cap categorization point view change anything still cp cp allows u fail request node system highlyavailable anymore reallife issue yes much data copy everywhere need pick fight win support loss node rack nothing else impact taking topology account rack require extra level configuration make loadbalancing complicated really avoid want tolerant racklevel partition single rackloss cluster take account topology calculation show would missing data first rack still much topology must taken account possible partition want support eventually consistent big data store guess result similar cp big data store eventual consistency topology taken account partition data even topology taken account network partition lead split one partition enough data another partition without enough data partition system highlyavailable cp store capavailable second partition handle request replication data center configured partition data center one system capavailable math looking rackloss scenario would eventually consistent store better hdfs word probability least one copy data partitioned rack single block probability least one copy partition around rack contains total data word eventually consistent would allow client application run partitioned rack access data available difference strongly consistent weakly consistent client application reasonably run last rack system belong notavailable category cap application eventually consistent ap system impossible eventually consistent big data system ap also difficult eventual consistent system belong category ap system must data partition continue support writes network partition partition ap system rare exist possible configuration service naming service judging documentation netflix eureka seems close reallife ap system servicediscovery component data size remains small fit node also localwritesthenresolveconflict conflict service registration likely easy merge service quite reliable couchdb generic database also belongs category disclaimer used system however even technically possible build eventually consistent ap system application may security constraint make disconnected operation difficult requirement time revoke access formulated minute business application today access right authentication application typical example application willing choose available consistency also limited time revoke access requirement conclusion tl dr cap categorization used marketing little cap theorem actually system describe cp partitiontolerant system claiming ap available quick takeaway cap used distributed system deployment specified taken account example choosing ca make sense lan wan availability limited capavailability many reallife application target highavailability partialavailability ca used distributed system want communicate network partition severe issue documentation issue actually thing eventually consistent ap big data store big data store available partition network topology taken account eventually consistent save configuring rack replication accordingly many point raised already mentioned eric brewer paper presentation already went detail partially available data using concept harvest also daniel abadi already pointed main problem cap focus everyone consistencyavailability tradeoff resulting perception reason nosql system give consistency get availability far case actually application gave consistency nt get much availability end thanks nick dimiduk comment earlier version post error mine reference eric brewer podc keynote july towards robust distributed system gilbert lynch brewer conjecture feasibility consistent available partitiontolerant web service acm sigact news eric brewer cap twelve year later rule changed perspective cap theorem seth gilbert nancy lynch daniel j abadi consistency tradeoff modern distributed database system design
465,Lobsters,scaling,Scaling and architecture,Blade: A Data Center Garbage Collector,http://arxiv.org/pdf/1504.02578v1.pdf,blade data center garbage collector,,obj type xobject subtype form bbox filter flatedecode formtype length ptexfilename etcdlatencyonofflogpdf ptexinfodict r ptexpagenumber resource colorspace srgb r extgstate font r r procset pdf text stream j m f f l c ndl j e z  w p kc w  f n l c xc x endobj obj alternate devicergb filter flatedecode length n stream l e cbh j qhf g ph endobj obj filter flatedecode length stream x li p bxx b wb b yl w g h n ur eh  ch f az r pd endobj obj type objstm filter flatedecode first length n stream xz l ijs h q  g x ub  endobj obj filter flatedecode length stream x  ir x q goi t aap c w q mj g e q  endobj obj filter flatedecode length stream x  efu uv ek y ift v p  aqoq q x
466,Lobsters,scaling,Scaling and architecture,Large-scale cluster management at Google with Borg,http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/43438.pdf,largescale cluster management google borg,,obj endobj obj filterflatedecodeid index info rlength rsize stream b rl v endstream endobj startxref eof obj stream f e e g endstream endobj obj endobj obj endobj obj stream h 
467,Lobsters,scaling,Scaling and architecture,Systems at Facebook Scale (Applicative Conference 2015),http://www.youtube.com/watch?v=dlixGkelP9U&list=PLn0nrSd4xjjZoaFwsTnmS1UFj3ob7gf7s&index=2,system facebook scale applicative conference,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature keynote system facebook scale youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature keynote system facebook scale youtube
468,Lobsters,scaling,Scaling and architecture,True Zero Downtime HAProxy Reloads,http://engineeringblog.yelp.com/2015/04/true-zero-downtime-haproxy-reloads.html,true zero downtime haproxy reloads,haproxy cornerstone reliable website haproxy service oriented architecture smartstack haproxy reloads drop traffic mi handle drop syn packet rfc faithfully hardcodes making haproxy reloads drop traffic lartc howto tc iptables netlink set queuing discipline mark syn packet toggle plug reloading design tradeoff huptime experimental setup basic configuration reload haproxy experiment result drop syns let tcp rest experiment result use graceful restart method experiment result conclusion acknowledgement back blog,since migrated robust solution us nginx haproxy together achieve goal see solution haproxy cornerstone reliable website one primary goal infrastructure team yelp get close zero downtime possible mean user make request wwwyelpcom want ensure get response get response fast possible one way yelp using excellent haproxy load balancer use everywhere external load balancing internal load balance move service oriented architecture find running haproxy every machine yelp part smartstack love flexibility smartstack give u developing soa flexibility come cost service service backends added permanently removed haproxy reload across entire infrastructure reloads cause reliability problem haproxy top notch dropping traffic running drop traffic reloads haproxy reloads drop traffic version haproxy support zero downtime restarts reloads configuration instead support fast reloads new haproxy instance start attempt use soreuseport bind port old haproxy listening sends signal old haproxy instance shut technique close zero downtime modern linux kernel brief period time process bound port critical time possible traffic get dropped due way linux kernel mi handle multiple accepting process particular issue lie potential new connection result rst haproxy issue syn packet get put old socket queue right call close result rst connection various workarounds issue example willy tarreau primary maintainer haproxy suggested user drop syn packet duration haproxy restart tcp automatically recovers unfortunately rfc dictate initial syn timeout linux kernel faithfully hardcodes dropping syns mean connection attempt establish haproxy reload encounter extra second latency exact latency depends tcp implementation client mobile device retry fast many device retry given number haproxy reloads level traffic yelp becomes barrier reliability service making haproxy reloads drop traffic avoid latency built solution proposed willy solution actually work well dropping traffic extra second latency problem better solution u would delay syn packet reload done would impose latency haproxy reload new connection turned linux queueing discipline qdiscs queueing discipline manipulate network packet handled within linux kernel specifically control packet enqueued dequeued provides ability rate limit prioritize otherwise order outgoing network traffic information qdiscs highly recommend reading lartc howto well relevant man page light bedtime reading linux kernel source code one sres josh snyder discovered relatively undocumented qdisc available since linux plug queueing discipline using plug qdisc able implement zero downtime haproxy reloads following standard linux technology tc linux traffic control allows u set queueing discipline route traffic based filter newer linux distribution also libnlutils provide interface newer qdiscs plug qdisc iptables linux tool packet filtering nat allows u mark incoming syn packet smartstack client connect loopback interface make request haproxy fortunately turn incoming traffic outgoing traffic mean set queuing discipline loopback interface look something like figure figure queueing discipline set classful implementation standard pfifofast queueing discipline using prio qdisc fourth lane plug qdisc capability queue packet without dequeuing command flush packet capability combination iptables rule allows u redirect syn packet plug reload haproxy unplug reload handle eg label allow u connect qdiscs together send packet particular qdiscs using filter information consult lartc howto referenced programmed functionality script call qdisctool tool allows infrastructure haproxy reload plug traffic restart haproxy release plug delivering delayed syn packet invocation look something like qdisctool protect normal haproxy reload command easily reproduce technique standard userspace utility modern linux distribution ubuntu trusty setup nlqdiscadd linux kernel manipulate plug via netlink manually set queuing discipline graceful haproxy reloads must first set queueing discipline described using tc nlqdiscadd note every command must run root set queuing discipline tc qdisc add dev lo root handle prio band tc qdisc add dev lo parent handle pfifo limit tc qdisc add dev lo parent handle pfifo limit tc qdisc add dev lo parent handle pfifo limit create plug qdisc meg buffer nlqdiscadd devlo plug limit release plug nlqdiscadd devlo update plug releaseindefinite set filter packet marked directed plug tc filter add dev lo protocol ip parent prio handle fw classid mark syn packet want syn packet routed plug lane accomplish iptables use link local address redirect traffic want reload client always option making request wish avoid plug note assumes set link local connection iptables mangle output p tcp syn j mark setmark toggle plug reloading everything set need gracefully reload haproxy buffer syns reload reload release syns reload cause connection attempt establish restart observe latency equal amount time take haproxy restart nlqdiscadd devlo update plug buffer service haproxy reload nlqdiscadd devlo update plug releaseindefinite production observe technique add latency incoming connection restart drop request design tradeoff design benefit drawback largest drawback work outgoing link incoming traffic way queueing discipline work linux namely shape outgoing traffic incoming traffic one must redirect intermediary interface shape outgoing traffic intermediary working integrating solution similar external load balancer yet production furthermore qdiscs could also probably tuned efficiently example could insert plug qdisc first prio lane adjust priomap accordingly ensure syns always get processed packet could tune buffer size pfifoplug qdiscs believe work interface loopback plug lane would moved first lane ensure syn deliverability reason decided go solution something like huptime hacking file descriptor passing haproxy dancing multiple local instance haproxy deemed qdisc solution lowest risk huptime ruled quickly unable get function machine due old libc version uncertain ldpreload mechanism would even work something complicated haproxy one engineer implement proof concept file descriptor patch hackathon complexity patch potential large fork caused u abandon approach turn file descriptor passing properly really hard three option seriously considered running multiple haproxy instance machine using either nat nginx another haproxy instance switch traffic ultimately decided number unknown implementation level maintenance would required infrastructure solution maintain basically zero infrastructure trust linux kernel haproxy handle heavy lifting trust appears well placed month running production observed issue experimental setup demonstrate solution really work fire nginx http backend haproxy sitting front generate traffic apache benchmark see happens restart haproxy evaluate different solution way test carried freshly provisioned aws machine running ubuntu trusty linux kernel haproxy compiled locally nginx started locally default configuration except listens port serf simple reply instead default html compiled haproxy started locally basic configuration single backend port corresponding frontend port reload haproxy experiment restart haproxy option initiate fast reload process pretty unrealistic test restarting haproxy every illustrates point experiment restart haproxy every haproxy f tmphaproxycfg p tmphaproxypid sf cat tmphaproxypid sleep done result ab c n benchmarking patient aprsocketrecv connection reset peer total request completed ab r c n benchmarking patient complete request failed request longest request request failed bad well goal zero drop syns let tcp rest try method drop syns method seems completely break high restart rate end exponentially backing connection get reliable result could restart haproxy every second experiment restart haproxy every second sudo iptables input p tcp dport syn j drop sleep haproxy f tmphaproxycfg p tmphaproxypid sf cat tmphaproxypid sudo iptables input p tcp dport syn j drop sleep done result ab c n benchmarking patient complete request failed request longest request figure iptables experiment result expected drop request incur additional one second latency request timing plotted figure see clear bimodal distribution request hit restart take full second complete le one percent test request observe high latency still enough problem use graceful restart method experiment restart haproxy option use queueing strategy delay incoming syns sure getting lucky one million request process test restarted haproxy time experiment invalid request result invalid request figure tc experiment result success restarting haproxy basically effect traffic causing minor delay seen figure note method heavily dependent long haproxy take load configuration running reduced configuration result deceivingly fast production environment observe penalty haproxy restarts conclusion technique appears work quite well achieve goal providing rocksolid service infrastructure developer build delaying syn packet coming haproxy load balancer run machine able minimally impact traffic haproxy reloads allows u add remove change service backends within soa without fear significantly impacting user traffic acknowledgement thanks josh snyder john billing evan krall excellent design implementation discussion back blog
469,Lobsters,scaling,Scaling and architecture,Nameko for Microservices,http://lucumr.pocoo.org/2015/4/8/microservices-with-nameko/,nameko microservices,nameko microservices onefinestay nameko freeing mind enter nameko web dependency injection concurrency parallelism take python soa,nameko microservices written wednesday april december tech guy onefinestay invited london general improvement nameko library collaboration came together nameko pretty similar generally like build certain infrastructure experience similar system improvement hit release version nameko figured might good idea give feedback like sort architecture freeing mind right want build web service python many tool pick live specific part stack common tool web framework typically provide whatever glue necessary connect code incoming http request come client however need application instance often periodic task need execute case framework often helping also way instance might built code assumption access http request object want run cronjob request object unavailable addition crons often also wish execute something result request client without blocking request instance imagine admin panel trigger expensive data conversion task actually want current request finish conversion task keep working background data set converted obviously many existing solution celery come mind however typically separated rest stack system treat process free mind make microservices interesting away http request handler direct relationship message queue worker task cronjobs instead coherent system component talk well defined point part system especially useful python traditionally support parallel execution bad abysmal enter nameko nameko implementation idea similar architecture structure code fireteam based distributing work process amqp amqp though nameko abstract away allows write transport staying true amqp pattern nameko handful thing build complex system idea build individual service emit event service subscribe directly invoke via rpc communication service happens amqp nt need manually deal connectivity addition message exchange service also use lifecycle management find useful resource dependency injection sound like mouthful actually simple service class add special attribute resolved runtime lifetime value resolved customized instance becomes possible attach property class provide access database connection lifetime database connection automatically managed look practice something like namekorpc import rpc class helloworldservice object name helloworld rpc def hello self name return hello format name defines basic service provides one method invoked via rpc either another service process run nameko also invoke long connect amqp server experiment service nameko provides shell helper launch interactive python shell n object provides rpc access nrpchelloworldhello namejohn uhello john amqp server running rpchelloworldhello contact helloworld service resolve hello method upon calling method message dispatched via amqp broker picked nameko process shell block wait result come back useful example happens service want collaborate activity instance quite common one service want respond change another service performs update state achieved event namekoevents import eventdispatcher eventhandler namekorpc import rpc class servicea object name servicea dispatch eventdispatcher rpc def emitanevent self selfdispatch myeventtype payload class serviceb object name serviceb eventhandler servicea myeventtype def handleanevent self payload print service b received payload default behavior one service instance service type pick event however nameko also route event every single instance every single service useful inprocess cache invalidation instance web nameko good internal communication however us werkzeug provide bridge outside world allows accept http request ingest task service world import json namekowebhandlers import http werkzeugwrappers import response class httpserviceservice object name helloworld http get get int value def getmethod self request value return response jsondumps value value mimetypeapplicationjson endpoint function invoke part system via rpc method functionality generally also extends websocket world even though part currently quite experimental instance possible listen event forward websocket connection dependency injection one really neat design concept nameko use dependency injection find resource good example sqlalchemy bridge attache sqlalchemy database session service dependency injection descriptor hook lifecycle management automatically manage database resource namekosqlalchemy import session import sqlalchemy sa sqlalchemyextdeclarative import declarativebase base declarativebase class user base tablename user id sacolumn sainteger primarykeytrue username sacolumn sastring class myservice object name myservice session session base rpc def getusername self userid user selfsessionquery user get userid user none return userusername implementation session dependency provider ridiculously simple whole functionality could implemented like weakref import weakkeydictionary namekoextensions import dependencyprovider sqlalchemy import createengine sqlalchemyorm import sessionmaker class session dependencyprovider def init self declarativebase selfdeclarativebase declarativebase selfsessions weakkeydictionary def getdependency self workerctx dburi selfcontainerconfig databaseurl engine createengine dburi sessioncls sessionmaker bindengine selfsessions workerctx session sessioncls return session def workerteardown self workerctx sess selfsessionspop workerctx none sess none sessclose actual implementation tiny bit complicated basically bit extra code support different database url different service declarative base overall concept however dependency needed connection database established worker shuts session closed concurrency parallelism make nameko interesting scale really well use amqp eventlet first nameko start service container us eventlet patch python interpreter support green concurrency allows service container become quite concurrent multiple thing useful service wait another service thread python disappointing story however largely eliminates possibility true parallelism becomes necessary start multiple instance service scale thanks use amqp however becomes transparent process long service need store local state becomes trivial run many service container necessary take nameko stand right principle building platform small service probably best open source solution problem python world far bit disappointing python async story diverging different python version framework eventlet gevent far cleanest practical implementation intent purpose eventlet base nameko probably best currently get async io fear though nameko also run nt tried sort service setup yet might want give nameko try entry tagged python soa
470,Lobsters,scaling,Scaling and architecture,Notes from Facebook's Developer Infrastructure at Scale F8 Talk,http://gregoryszorc.com/blog/2015/03/28/notes-from-facebook%27s-developer-infrastructure-at-scale-f8-talk/,note facebook developer infrastructure scale talk,available online encourage glimpse list talk watch whatever relevant big code developer infrastructure facebook scale nt want human waiting computer want computer waiting human post monorepos quote google excited work facebook mercurial glad collaborating facebook mercurial development facebook moved serverside rebasing push developing xcode facebook exercise frustration facebook measure everything tool mercurial operation time xcode time build time data tell tool workflow need worked nt want developer waiting computer build test diffs goal get feedback developer minute run fewer test get back developer quicker thing le likely break run test take longer give feedback whatever scale engineering organization developer efficiency key thing infrastructure team striving facebook top engineer working developer infrastructure mozillians infra work charge head count infra work watch video,time facebook talk technical matter tend listen track record demonstrating engineering leadership several space unlike many company talk facebook often give others access idea via source code healthy open source project rare see company operating frontier computing field provide much insight inner working gain much riding cotails following lead instead clinging cargo culting past facebook developer conference past week talk available online encourage glimpse list talk watch whatever relevant really little bit everyone particular interest big code developer infrastructure facebook scale talk highly relevant job role developer productivity engineer mozilla note talk follow nt want human waiting computer want computer waiting human common theme talk facebook subversion moved git deployed bridge people worked git distributed workflow pushed subversion hood new platform time server code io android one git repo per platformproject git repos initially code sharing problem time code sharing repos lot code copying confusion owns facebook mere week away completing migration consolidate big three repos mercurial monorepo see also post monorepos reason easier code sharing easier largescale change rewrite universe unified set tooling facebook employee run source control command per day commits per week vcs tool need fast prevent distraction context switching slow people facebook implemented sparse checkout shallow history mercurial necessary scale distributed version control large repos quote google excited work facebook mercurial glad collaborating facebook mercurial development well guess cat finally bag google working mercurial kind open secret month guess official pushpullrebase bottleneck rebase push someone beat pull rebase try get worse commit rate increase people needle legwork facebook moved serverside rebasing push mostly eliminate pain point part stillexperimental feature mercurial hopefully lose experimental flag soon starting speaker change move away version control ides nt scale facebook scale developing xcode facebook exercise frustration average minute open facebook io xcode minute average index peg cpu make responsive xcode crash per day across facebook io developer facebook measure everything tool mercurial operation time xcode time build time data tell tool workflow need worked facebook belief ides worth pain make people productive facebook want support editor ides since people want use whatever comfortable react native changed thing supported developing multiple platform single ide support people launched several editor tool react native development people needed window development experience acceptable built ide set plugins top atom fork like hackable weby nature atom demo showing io development look nice objectivec javascript simulator integration version control one window connect remote server transparently save deploy change also get realtime compilation error hint remote server demo hack sure others langs supported beefy central server eg gecko development would fun experiment starting presentation shift continuous integration number one goal ci facebook developer efficiency nt want developer waiting computer build test diffs goal ci highsignal feedback nt want developer chasing failure nt fault waste time must provide rapid feedback developer nt want wait provide frequent feedback developer know soon possible something think refers local feedback sandcastle ci system diff lifecycle discussion basic test lint run locally understanding talking facebookers local often mean facebook server local laptop machine developer fingertip often dumb terminal appear use code coverage determine test run going run test unless diff might actually broken run flaky test le often run slow test le often goal get feedback developer minute run fewer test get back developer quicker thing le likely break run test take longer give feedback also want feedback quickly reviewer see result review time use web driver heavily love crossplatform nature web driver addition test result performance size metric reported ship button diff landcastle handle landing diff ok facebook land diff without using landcastle read developer nt push directly master repo landcastle land something run test issue found task filed task push blocking code wo nt ship user push blocking issue resolved tweet confirm backouts fairly aggressively valid resolution push blocking task backout fixing forward fine well branch cut occurs cherry pick onto release branch addition diffbased testing continuous testing run much comprehensive time restriction continuous run master release candidate branch auto bisect pin regression sandcastle process test result per second year machine work per day thousand machine data center started buildbot single master hit scaling limit single thread single master master could push work worker fast enough sandcastle distributed queue worker pull job distributed queue highsignal feedback critical flaky failure erode developer confidence need developer trust sandcastle extremely careful separating infra failure failure developer nt see infra failure infra failure reported sandcastle team bot look flaky test stress test individual test run test parallel goal developer nt see flaky test fault button developer use report bad signal whatever scale engineering organization developer efficiency key thing infrastructure team striving facebook top engineer working developer infrastructure preach excellent talk mozillians infra work charge head count infra work watch video update utc clarified bit response new info tweeted added link monorepos blog post
471,Lobsters,scaling,Scaling and architecture,The Mystery Machine: End-to-end Performance Analysis of Large-scale Internet Services,https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-chow.pdf,mystery machine endtoend performance analysis largescale internet service,,obj stream
472,Lobsters,scaling,Scaling and architecture,Scaling MySQL in the cloud with Vitess and Kubernetes,http://googlecloudplatform.blogspot.com/2015/03/scaling-MySQL-in-the-cloud-with-Vitess-and-Kubernetes.html,scaling mysql cloud vitess kubernetes,learn,demonstrate proficiency design build manage solution google cloud platform learn
473,Lobsters,scaling,Scaling and architecture,Fault Tolerance 101,http://www.infoq.com/presentations/fault-tolerance-101-erlang,fault tolerance,infoq homepage presentation fault tolerance summary bio conference related sponsored content,infoq homepage presentation fault tolerance fault tolerance summary joe armstrong discus fault tolerance relates scalability concurrency erlang help build faulttolerant system multicore cluster bio joe armstrong principal inventor erlang coined term concurrency oriented programming ericsson developed erlang chief architect erlangotp system formed bluetail developed product erlang obtain phd royal institute technology stockholm author book software concurrent world conference build stuff conference software development conference created developer team leader software architect technical project manager goal bring world class speaker share innovation latest development new trend direction software development world baltic recorded may related sponsored content
474,Lobsters,scaling,Scaling and architecture,Linux Profiling at Netflix,http://www.brendangregg.com/blog/2015-02-27/linux-profiling-at-netflix.html,linux profiling netflix,scale livestream front row slideshare svg thread perfevents page perf wiki straughan harsh karmarkar,southern california linux expo scale last weekend gave talk linux profiling netflix using perfevents aka perf covered get cpu profiling work including tricky target like java nodejs tour perfevents capability quick complete cpu profiling something everyone able turn tricky however due various gotchas covered talk including fixing stack symbol also included new map show event perf instrument room talk full people could nt get fortunately recorded two camera room livestream one front row video slide talk also slideshare material talk shared first time covering really get cpu profiling work java particular importance netflix since application environment java linux perfevents excellent profiler since work asynchronously jvm safety point sampling issue inspect full stack jvm internals system library java code kernel matter cpu issue perfevents find highlight talk mixedmode cpu flame graph java slide included embedded svg click zoom showing cpu usage java context amazing useful recently found issue using flame graph cpu time mostly spent jvm compiler issue invisible java profilers focus execution java code covered talk getting full java stack profile work tricky currently involves patching openjdk fix frame pointer patch filed although hope becomes option openjdk oraclejdk eg xx moreframepointer also started thread hotspot compiler dev mailing list interested java profiling still recommend watching talk java make interesting use case getting system profiling work issue may encountered runtimes talk included crash course perfevents many oneliners tour capability included oneliners counting event profiling static tracing dynamic tracing example sample cpu stack trace specified pid hertz second perf record f p pid g sleep sample cpu stack trace entire system hertz second perf record f ag sleep sample cpu stack trace every level data cache miss perf record e c ag sleep sample cpu stack trace every last level cache miss second perf record e llcloadmisses c ag sleep sample oncpu kernel instruction second perf record e cycle k sleep perf see perfevents page full list oneliners perf wiki documentation linux source toolsperfdocumentation scale always really good conference great time meeting people listing others working thanks volunteer run scale well straughan editing video harsh karmarkar apcera running camera
475,Lobsters,scaling,Scaling and architecture,"Introducing gRPC, a new open source HTTP/2 RPC Framework",http://googledevelopers.blogspot.com/2015/02/introducing-grpc-new-open-source-http2.html,introducing grpc new open source rpc framework,grpc commitment header compression faq protocol buffer proto github repository documentation mailing list irc grpc channel freenode stackoverflow grpc square,today open sourcing grpc brand new framework handling remote procedure call bsd licensed based recently finalized standard enables easy creation highly performant scalable apis microservices many popular programming language platform internally google starting use grpc expose public service grpc endpoint part long term commitment year google developed underlying system technology support largest ecosystem microservices world server make ten billion call per second within global datacenters scale nanosecond matter efficiency scalability reliability core building google apis grpc based many year experience building distributed system new framework want bring developer community modern bandwidth cpu efficient low latency way create massively distributed system span data center well power mobile apps realtime communication iot device apis building standard brings many capability bidirectional streaming flow control header compression multiplexing request single tcp connection feature save battery life data usage mobile speeding service web application running cloud developer write responsive realtime application scale easily make web efficient read feature benefit faq alongside grpc releasing new version protocol buffer high performance open source binary serialization protocol allows easy definition service automatic generation client library proto add new feature easier use compared previous version add support language provides canonical mapping proto json project support c c java go nodejs python ruby library objectivec php c development start contributing please fork github repository start submitting pull request also sure check documentation join u mailing list visit irc grpc channel freenode tag stackoverflow question grpc tag google working closely square organization grpc project excited potential technology improve web look forward developing project open help direction contribution community post mugur marculescu product manager
477,Lobsters,scaling,Scaling and architecture,Writing Custom Plugins for PHPs MySQLnd,https://blog.engineyard.com/2015/writing-custom-plugins-for-php-mysqlnd,writing custom plugins php mysqlnd,previously written custom routing mysqlndms custom cache handler mysqlndqc instalation setup mysqlnduh proxy registering proxy creating proxy tying together earlier article series many feature mysqlnd plugins conclusion mysqlnduhconnection mysqlnduhpreparedstatement,previously written custom hook specific mysqlnd plugins like custom routing mysqlndms custom cache handler mysqlndqc time looking mysqlnduh user handler allows u write entire plugin mysqlnduh least stable mysqlnd plugins available alpha likely need compile source instalation setup first grab source code install svn co http svnphpnetrepositorypeclmysqlnduhtrunk mysqlnduh cd mysqlnduh phpize configure enablemysqlnduh make sudo make install add following phpini similar note must load mysqlnduh extension mysqlnd mysqlnduh proxy mysqlnduh plugin considered applicationlevel proxy application code mysql server two type proxy connection proxy prepared statement proxy implement must extend mysqlnduhconnection mysqlnduhpreparedstatement respectively code important understand class mysqlnd driver written c exposed directly php userland mean fail call parent method may cause php crash leak memory also number method mapped directly userland functionality eg mysqli pdomysql extension apis instead called internal underlying implementation registering proxy mysqlnduh extension expose two function register proxy mysqlnduhsetconnectionproxy connection proxy extending mysqlnduhconnection mysqlnduhsetstatementproxy statement proxy extending mysqlnduhpreparedstatement accepts instance proxy class mean must careful maintaining object state shared globally creating proxy create proxy simply extend appropriate class example want create proxy simply record every time mysql connection made create connection proxy extending mysqlnduhconnection like class connectionrecorderproxy extends mysqlnduhconnection public function connect connection host user password database port socket mysqlflags time time fileputcontents tmpconnectionlog connection host port port user user date r time phpeol return parent connect connection host user password database port socket mysqlflags override connect method add line log return result parent connect note may register one proxy type connection statement given time would fairly trivial write single proxy would allow register callback method however beyond scope article tying together saw earlier article series many feature mysqlnd plugins controlled using sql hint mysqlndms readwrite routing query caching hint available constant injected inside sql comment however injecting hint either printfsprintf concatenation inelegant messy instead write simple proxy automatically transform query replace string constant name value need add connection proxy catch query issued mysqli query pdo query statement proxy catch prepared query created mysqli prepare pdo prepare might look like utility class replacing sql hint class mysqlndpluginquerytransformer static public transform sql list constant replace constant mysqlndmsmasterswitch mysqlndmsslaveswitch mysqlndmslastusedswitch mysqlndqcenableswitch mysqlndqcdisableswitch mysqlndqcttlswitch replace hint foreach constant constant sql strreplace constant constant constant sql handle mysqlndqcttlswitch would double sql strreplace mysqlndqcttlswitch mysqlndqcttlswitch sql return sql connection proxy class class mysqlndpluginconnectionproxy extends mysqlnduhconnection public function query connection query transform query query mysqlndpluginquerytransformer transform query call return parent function transformed query return parent query connection query statement proxy class class mysqlndpluginstatementproxy extends mysqlnduhpreparedstatement public function prepare connection query transform query query mysqlndpluginquerytransformer transform query call return parent function transformed query return parent prepare connection query set proxy mysqlnduhsetconnectionproxy new mysqlndpluginconnectionproxy mysqlnduhsetstatementproxy new mysqlndpluginstatementproxy work first creating single simple class mysqlndpluginquerytransformer perform replacement first iterate constant string replace string value constant using constant function additionally mysqlndqcttlswitch constant contains equal sign left hint like select user readability want also allow following select user must check ensure remove extraneous sign create proxy class override mysqlnduhconnection query mysqlnduhpreparedstatement prepare method pas sql query mysqlndpluginquerytransformer transform method call parent new sql finally register proxy query transparently transformed change existing code issue query using one constant literally like mysqlndmsmasterswitch select user whether prepared statement literal mysqlndmsmasterswitch replaced automatically automatically becomes msmaster select user conclusion mysqlnduh powerful also simple expose one thing nothing unless tell given hook almost part connection query process extension unlimited monitoring statistic backwards compatibility layer name potentially possible however given extension alpha state recommended production use moment mean useless however thing like able detect report bad query dev eg issued nonexistant renamed table handy next step read php manual mysqlnduhconnection mysqlnduhpreparedstatement class give idea hook purpose p used mysqlnduh idea using love hear idea throw u comment
478,Lobsters,scaling,Scaling and architecture,Very High Performance C Extensions For JRuby + Truffle,http://www.chrisseaton.com/rubytruffle/cext/,high performance c extension jruby truffle,click redirected,click redirected
479,Lobsters,scaling,Scaling and architecture,Incremental Stream Processing using Computational Conflict-free Replicated Data Types,http://asc.di.fct.unl.pt/~nmp/pubs/clouddp-2013.pdf,incremental stream processing using computational conflictfree replicated data type,,obj length filter flatedecode stream cq b zv w
481,Lobsters,scaling,Scaling and architecture,bsdtalk249: Interview with Scott Long of Netflix about their FreeBSD-based local caching appliances,http://bsdtalk.blogspot.com/2014/12/bsdtalk249-netflix-update-with-scott.html,interview scott long netflix freebsdbased local caching appliance,,netflix update scott long meetbsd california file info ogg link http
482,Lobsters,scaling,Scaling and architecture,Advanced MySQL Scaling with PHP's MySQLnd,https://blog.engineyard.com/2014/advanced-read-write-splitting-with-phps-mysqlnd,advanced mysql scaling php mysqlnd,part one series multiple configuration note failover note transaction filter replication lag eventual consistency dependant faster network session consistency note partitioning sharding custom routing conclusion cache support distributed transaction mysql fabric support,part one series took first look mysqlndms mysqlnd plugin readwrite splitting article look advanced usage multiple configuration keen eyed reader part one may noticed mysqlndms configuration toplevel key named example appname contains configuration allows specify multiple configuration one configuration file appspecifickey master slave specify configuration use use key hostname connect using mysqli mysqli new mysqli appname user password dbname using pdo pdo new pdo mysql hostappname dbnamedbname user password note need even one configuration otherwise mysqlndms configuration used failover mysqlndms plugin allows automatic failover connection fail trying connect server happens transparently issuing first query disabled default issue state eg sql user variable however easy set setting failoverstrategy configuration option option three possible value disabled never failover automatically default master always failover master loopbeforemaster slave request attempted first loop slave trying master appname master slave failover strategy loopbeforemaster addition setting strategy additionally set two setting rememberfailed setting tell plugin remember failed server remainder session default false recommended maxretries number retries attempted server considering failed server tried per iteration list removed failing n time default mean unlimited server never removed conflict rememberfailed configuration option appname master slave failover strategy loopbeforemaster rememberfailed true maxretries note transaction discussed failover implicitly disabled transaction one pitfall splitting query load balancing multiple read server transaction could end different query hitting different server particular issue call insertupdatedelete want select modified data set completing transaction solve either use mysqlndmsmasterswitch mysqlndmslastusedswitch sql hint using mysqli pdo let plugin automatically track transaction latter must either turn autocommit start transaction end hook transaction method first need setup plugin configuration enable sticky transaction use autocommit mysqli extension mysqli autocommit false disable autocommit implicitly start transaction mysqli query begin query mysqli query commit rollback mysqli autocommit true enable autocommit pdo extension pdo setattribute pdo attrautocommit false disable autocommit implicitly start transaction pdo exec begin query pdo exec commit rollback pdo setattribute pdo attrautocommit true disable autocommit plugin also track begin transaction commit rollback method extension send request master call pdo begintransaction mysqli begintransaction pdo commit pdo rollback mysqli commit mysqli rollback end transaction filter mysqlndms support filter chain determining server ultimately used execute query two type filter multi filter return list master slave server one ultimately picked single filter return single server specify multiple multi filter multiple single filter one passed result previous filter filter executed order defined configuration file must end single filter typically one load balancing filter random randomonce roundrobin user replication lag first filter look quality service qos multi filter one primary reason use mysqlndmslastusedswitch sql hint send read master write occurred affected replication lag otherwise use slave distribute load also possible achieve transparently using masteronwrite configuration setting appname master slave masteronwrite however would mean remaining query request go master necessary query unaffected write triggered switch newer version mysqlndms tried solve qos filter feature allows granular come handling replication lag writethenread scenario three type service level eventual consistency default mastersslaves considered read query additionally automatically filter slave lag n second behind master session consistency similar however additionally automatically include server already replicated data using global transaction id gtids strong consistency used using multimaster synchronous mysql setup example mysql cluster configure mysqlndms either add filter config appname master slave filter qualityofservice typeofconsistency option value random also set programmatically using mysqlndmssetqos connection mysqlndmsqosconsistency type mysqlndmsqosoption option value eventual consistency simplest eventual consistency transparently check secondsbehindmaster value slave le equal accepted value considered read candidate however value increment whole second mean still caught replication lag issue also dependant faster network eventual consistency tell mysqlndms consider slave within n second master configure plugin either use config appname master slave filter qualityofservice eventualconsistency age mysqlndmssetqos connection mysqlndmsqosconsistencyeventual mysqlndmsqosoptionage set plugin consider node le equal one second behind master session consistency session consistency ensure use server definitely sync master slower provides robust solution eventual consistency session consistency achives using global transaction id gtids mysql mysql support serverside gtids plugin use automatically preferred solution using older version mysql gtids enabled need implement clientside gtids discussed manual gtids may available mysql plugin fallback master configure plugin use appname master slave filter qualityofservice sessionconsistency globaltransactionidinjection select select globalgtidexecuted fetchlastgtid select globalgtidexecuted trxid dual checkforgtid select gtidsubset gtid globalgtidexecuted trxid dual reporterrors true case add new section configuration globaltransactionidinjection tell plugin query use check gtid server note query work mysql plugin check consider server selects insync master type query still use master add additional section configuration function setting masteronwrite earlier possible set additional configuration programmatically however enable masteronwrite behavior using mysqlndmssetqos mysqli mysqlndmsqosconsistencysession partitioning sharding due fact hard scale writes multiple machine eg multimaster replication traditional mysql ie mysql cluster fairly common use partitioning sharding enable scale across multiple master writing different data set one example user profile data might master shopping cart transaction might master b partitioning sharding done scale writes however sometimes replication topology choose replicate subset master database specific slave allows scale hardware effectively around specific data set mysqlndms plugin allows setup multiple master using mysqlndmsmultimaster ini setting partitioning using nodegroups multi filter unlike lot feature mysqlndms partitioning entirely transparent process partition must determined either writing query editor programmatically running query require enable multimaster support add following phpini setup node group add following json configuration appname master host db dbname host db dbname slave host db dbname host db dbname filter nodegroups groupa master slave groupb master slave add two node group groupa groupb using slave one master route query specific node group adding sql hint containing name beginning query groupa select user chain sql hint like groupb mslastused select transaction prepend sql hint mentioned editor writing code programmatically constructing query custom routing final feature cover custom routing allows write logic determining server query routed done using user usermulti filter filter must define callback must return either single server list master slave server respectively callback must function static object method call type callable closure however wrapper around object code feature able programmatically detect edge case might otherwise missed default mechanism first let setup configuration appname master slave filter user selectdbserver next define callback function function selectdbserver connection sql master slave lastused intransaction always switch master last used inarray master lastused return lastused always use last used connection transaction intransaction true return lastused check sql hint strpos sql mysqlndmsmasterswitch false force master return arrayrand master use arrayrand case multiple master elseif strpos sql mysqlndmsslaveswitch false force slave return arrayrand slave elseif strpos sql mysqlndmslastusedswitch false force last used return lastused check edgecases allow query start select select go slave stripos sql select stripos sql select select remember already transaction lastused used pcrematch select outfiledumpfile sql return arrayrand master return arrayrand slave send create temporary table lastused stripos sql create temporary table return lastused pick one server end default master return arrayrand master please note example probably used production however illustrates power user filter number check ultimately arrive server used much like masteronwrite flag session consistency qos filter switch master future query master used next ensure respect transaction use last used connection currently transaction check follow sql hint finally get edgecases query starting select query create temporary table query nothing else returned simply default master ensures le likely send write query slave accident anything like within function long return one server alternatively use usermulti filter identical except would return array two value array master slave server respectively conclusion mysqlndms plugin lot advanced feature allow effectively scale application much easily using traditional mysql replication topology still feature covered cache support distributed transaction mysql fabric support next installment series take look mysql innodb memcache interface well easily transparently utilize another mysqlnd plugin mysqlndmemcache p using advanced technique already love hear experience mysqlndms plugin via twitter reddit
483,Lobsters,scaling,Scaling and architecture,PostgreSQL 9.4 Released,http://www.postgresql.org/about/news/1557/,postgresql released,postgresql increase flexibility scalability performance flexibility scalability performance link postgresql web page,postgresql increase flexibility scalability performance posted postgresql global development group december postgresql global development group announces release postgresql latest version world leading open source database system release add many new feature enhance postgresql flexibility scalability performance many different type database user including improvement json support replication index performance flexibility new jsonb data type postgresql user longer choose relational nonrelational data store time jsonb support fast lookup simple expression search query using generalized inverted index gin multiple new support function enable user extract manipulate json data performance match surpasses popular document database jsonb table data easily integrated document data fully integrated database environment jsonb brings postgresql javascript development community allowing json data stored queried natively nodejs serverside javascript framework benefit safety robustness postgresql still storing data schemaless format prefer said matt soldo product manager heroku postgres scalability logical decoding supply new api reading filtering manipulating postgresql replication stream interface foundation new replication tool bidirectional replication support creation multimaster postgresql cluster improvement replication system replication slot timedelayed replica improve management utility replica server main reason behind immediate adoption postgresql production new logical decoding feature said marco favale cartographic production manager navionics ability write custom flexible output plugins allow u transparently collect change selected table replicate change like removing heavier complex manage trigger based replication solution zalando relies stability performance hundred postgresql database server continuously serve million customer around europe said valentine gogichashvili team lead database operation zalando technology excited run timedelayed standby server work box evaluate new bidirectional replication tool soon released performance version also introduces multiple performance improvement allow user get even postgresql server include improvement gin index making smaller faster concurrently updatable materialized view faster uptodate reporting rapidly reload database cache restart using pgprewarm faster parallel writing postgresql transaction log support linux huge page server large memory definitely benefit concurrent refresh materialised view delayed standby server make disaster recovery even robust well usual performance improvement every new release carry added marco favale link postgresql postgresql leading open source database system global community thousand user contributor dozen company organization postgresql project build year engineering starting university california berkeley unmatched pace development today postgresql mature feature set match top proprietary database system exceeds advanced database feature extensibility security stability learn postgresql participate community web page
485,Lobsters,scaling,Scaling and architecture,Graceful Degradation,http://www.solipsys.co.uk/new/GracefulDegradation.html?LB_20141212,graceful degradation,graceful degradation software contact comment,graceful degradation first learned graceful degradation colleague prefaced story saying good people learn mistake best people learn people mistake bit like saying aviation circle good landing one walk away excellent landing use plane digress told learned graceful degradation message clear telling mistake could learn wish people willing make mistake public software industry would certainly benefit could learn everyone mistake story go broad outline forget detail like made comfortable living programming electronic till time programmed assembly language best output could hope debugging see whether opened till difficult work simulator higher level language debugger singlesteppers help really wrote code assembler transferred machine tested see worked sometimes colleague amazing work could get contract time wanted money good worked six month year whatever wanted rest time except christmas one biggest mistake would come back haunt large department store major capital city used till ran program one sophisticated system around till would keep list item sold central computer would interrogate till turn find much sold item would keep track many remained stock would project reordering would necessary reordering automatic staff would alerted stock low item decision could made loved system buffer stock could reduced reducing store overhead making entire store responsive consumer demand except christmas see every christmas system essence would simply stop entire floor till would stop responding refusing accept purchase unpredictable length time suddenly start working apparent reason apparent rhyme nothing could except call colleague get come fix poblem really nothing could either even worked problem problem capacity till would keep list item sold asked would dump list central computer would go back patronising servicing customer requirement asked problem heaviest time year simply much download queue till would fill central computer got back would stop wait download data could restart entire floor till would grind halt waiting download data restart floor would simply stop good busiest shopping time year really good patch run christmas make till run slightly slow still occasional full stopping moment general system would limp along exhibit catastrophic failure entire floor stopping difficulty judging exactly slow make till run also made central computer contact till clever pseudorandom way meant one till stopped floor others would probably still working done well apart faster communication system could deal load placed till could notice queue full start slowing performance system whole would degrade gracefully sale would slow slightly evenly across store come match processing throughput course would better system simply faster overloaded degrading gracefully usually better option simply stopping think next time worry system capacity better halt clear backlog degrade gracefully continue serve customer comment decided longer include comment directly via disqus system instead delighted get email people wish make comment engage discussion comment integrated page appropriate number emailscomments get large handle might return semiautomated system looking increasingly unlikely
487,Lobsters,scaling,Scaling and architecture,Redis Pipelining and Scripting,http://useartisan.com/blog/redis-pipelining-and-scripting/,redis pipelining scripting,improves web presence,besides memorable com domain unique one com name kind extension usually drive traffic com counterpart learn premium com domain valuation watch video improves web presence get noticed online great domain name domain registered web coms reason simple com web traffic happens owning premium com give great benefit including better seo name recognition providing site sense authority
488,Lobsters,scaling,Scaling and architecture,Solving the Mystery of Link Imbalance: A Metastable Failure State at Scale,https://code.facebook.com/posts/1499322996995183/solving-the-mystery-of-link-imbalance-a-metastable-failure-state-at-scale/,solving mystery link imbalance metastable failure state scale,aggregated link clue collaboration collusion unintended consequence custom mysql connection pool simple fix lesson learned,building running system facebook sometimes encounter metastable failure state problem create condition prevent solution gridlocked traffic example car blocking intersection keep traffic moving exit intersection stuck traffic kind failure end external intervention like reduction load complete reboot blog post code caused tricky metastable failure state facebook system one defied explanation two year great example interesting thing happen scale open cooperative engineering culture help u solve hard problem aggregated link packet inside data center traverse several switch get one server another switchtoswitch connection path carry lot traffic link aggregated aggregated link us multiple network cable carry traffic source destination packet go one cable switch need strategy routing packet tcp traffic facebook configures switch select link based hash source ip source port destination ip destination port keep packet tcp stream link avoiding outoforder delivery lot stream routing scheme evenly balance traffic link except reality sometimes active tcp connection would hashed single link link would overloaded drop packet even worse failure state metastable imbalanced link became overloaded rest link would remain uselessly idle traffic drained hash algorithm changed two year tackled problem switch level worked vendor detect imbalance rapidly rotate hash function seed occurred kept problem manageable system grew however autoremediation system stopped working well often would drain imbalanced link problem would move another one clear needed understand root cause clue link imbalance occurred multiple vendor switch router hardware multiple trigger sometimes trigger transient network congestion large bulk transfer sometimes hardware failure sometimes load spike database query confined particular data center suspected metastable state would outlast trigger lot pattern difficult separate cause effect find one robust clue imbalanced link always carried traffic mysql database cache server tao graph store social graph seemed likely tao behavior causing imbalance plausible mechanism hash algorithm source ip destination ip destination port change onset problem source ip port variable factor implies switch fault route predestined switch get syn packet hand tao server fault pick pseudorandom blind even server aware link aggregated server know hash algorithm hash seed choose particular route lot eye looked code involved seemed correct using nonstandard switch setting weird kernel setting code ordinary significant change since year link imbalance bug first surfaced stumped collaboration collusion facebook culture collaboration proved key layer system seemed working correctly would easy team take entrenched position blame instead decided crosslayer problem would require crosslayer investigation started internal facebook group network engineer tao engineer mysql engineer began look beyond layer public abstraction breakthrough came started thinking component system malicious actor colluding via covert channel switch actor knowledge packet routed tao actor choose route somehow switch tao must communicating possibly aid mysql malicious agent inside switch would secrete information counterparty inside tao application covert channel switch make linux tcpip stack latency lightbulb moment congested link cause standing queue delay embeds information packet routing mysql query latency mysql query fast easy look query execution time tell went across congested link even congestion delay millisecond clearly visible applicationlevel timer receiving end collusion must code manages connection access timing information query tao sql connection pool seemed suspect connection removed pool duration single query careful bookkeeping malicious agent inside pool could record timing since timing information give good guess whether particular connection routed congested link could use information selectively close connection even though new connection randomly distributed congested link would left would code malicious agent unintended consequence custom mysql connection pool surprisingly code receiving agent present first version tao implemented autosizing connection pool combining recently used mru reuse policy idle timeout second mean moment many connection open peak number concurrent query previous second assuming query arrival correlated minimizes number unused connection keeping pool hit rate high probably guessed word assuming query arrival uncorrelated facebook collocates many user node edge social graph mean somebody log data cache might suddenly perform database query single database load data start race among query query go congested link lose race reliably even millisecond loss make recently used put back pool effect query burst stack deck putting congested connection top deck individual cache server overload link deckstacking story play simultaneously hundred machine net effect tao system continually probe link database find slow link rapidly shift large amount traffic toward mechanism explains multitude cause robustness effect link imbalance bug create congestion make metastable problem cause link saturate connection pool shift traffic way keep link saturated even original trigger removed effect strong time lag changing switch hash draining outbound queue allow problem reoccur new set connection simple fix fix simple switched least recently used lru connection pool maximum connection age us slightly connection database database easily handle increase wanted completely sure resolved problem took extra step manually triggering time fixed connection pool included command would let u switch old new behavior runtime set policy mru manually caused congested link time watched imbalance disappear within second enabling lru policy tried unsuccessfully trigger bug lru selected case closed would impact facebook new fabric network facebook newest data center altoona iowa us innovative network fabric reduces reliance large switch aggregated link still need careful balance fabric computes hash select among many path packet might take like hash selects among bundle aggregated link path selection stable link imbalance bug would become path imbalance bug fixed lesson learned literal conclusion draw story mru connection pool used connection traverse aggregated link metalevel next time debugging emergent behavior might try thinking component agent colluding via covert channel organizational level investigation great example say nothing facebook somebody else problem thanks engineer helped u manage fix bug including james paussa ernesto ovcharenko mark drayton peter hoose ankur agrawal alexey andreyev billy choe brendan cleary jj crawford rodrigo curado tim eberhard kevin federation han fugal mayuresh gaitonde cj infantino mark marchukov chinmay mehta murat mugan austin myzk gaya nagarajan dmitri petrov marco rizzi rafael rodriguez steve shaw adam simpkins david swafford wendy tobagus thomas tobin tj trask diego veca kaushik veeraraghavan callahan warlick jason wilbanks jimmy williams keith wright
489,Lobsters,scaling,Scaling and architecture,Building Dating Site at HappyPancake,http://abdullin.com/long/happypancake/,building dating site happypancake,,
490,Lobsters,scaling,Scaling and architecture,Operating Apache Samza at Scale,http://engineering.linkedin.com/samza/operating-apache-samza-scale,operating apache samza scale,logcentric system apache kafka kafka isr paxos understandability samza apache samza interesting nearline calculation simple samza application kafka lingo mirroring message apache yarn resource management hudson artifactory binary repository metric metric metric graphite rrd whisper influxdb yaml riemann cabot hardware configuration moving forward,linkedin use logcentric system called apache kafka move ton data around familiar kafka think publishsubscribe system model message log entry however difficult developer spend time thinking problem domain relevant application addition complex fault tolerant semantics required work log kafka isr paxos term understandability samza apache samza framework give developer tool need process message incredibly high rate speed still maintaining fault tolerance allows u perform interesting nearline calculation data without major architectural change samza ability retrieve message kafka process way output result back kafka simple samza application look bit like although samza designed work many type streaming system linkedin primarily depend kafka message stream diving run samza linkedin let start kafka related terminology kafka lingo every day linkedin kafka ingesting half trillion message terabyte might picturing massive cluster magically handle number message without falling really accurate really graph cluster handle specific type message control flow message cluster cluster mirroring message directed graph cluster ability fail data center usually provision cluster topology keep message around data center go completely offline datacenter concept local aggregate kafka cluster local cluster mirrored across data center aggregate cluster one datacenter go shift traffic another without skipping beat horizontal layer topology refer tier tier set cluster across multiple data center perform function example diagram shown local tier aggregate tier local tier set local cluster every data center aggregate tier set aggregate cluster every datacenter also another tier refer offline aggregate tier production aggregate tier mirrored offline aggregate tier batch processing example mapreduce job utilizes hadoop allows u separate production batch processing different datacenters since single local cluster single aggregate cluster within datacenter would result huge cluster reliability risk split local cluster several cluster different category data example application metric message contains sample cpu usage tracking event example advertisement impression event message sent different cluster local aggregate cluster kafka could probably handle single huge cluster every type message throw splitting thing give u better chance elegantly dealing complex failure also another term refer pipeline pipeline path message travel source destination example metric pipeline might work first production application produce message say something like memory used host message sent application production metric local tier message mirrored production metric aggregate tier aggregate metric offline tier finally metric consumed long term storage system designed display pretty graph emit alert employee path producer consumer across datacenters refer pipeline many message share common pipeline nice way quickly describe message going came samza fit another set kafka cluster dedicated samza job mirrored aggregate tier kafka cluster samza associated cluster run apache yarn launching samza job may notice diagram kafka samza share hardware past colocated kafka samza hardware however ran issue samza job would interfere kafka page cache separated onto different hardware future like explore locality samza kafka example task paired partition leader partial isr would share rack work well u moment resource management although modular design samza make simple support almost resource manager future yarn currently option time writing job launch samza job link yarn application master api api samza work yarn request resource container run samza task task fails samza job negotiates yarn find replacement resource get point starting samza job run yarn time linkedin built internal tooling designed simplify automate software building testing utilize write build deploy samza job first developer creates samza project linkedin tooling creates version control repository git subversion depending developer preference every time commit repository automated tooling build project using hudson create samza job package test post commit task finished job package automatically versioned uploaded artifactory binary repository job package available artifactory tool install small set file onto host yarn resourcemanager one file script tell yarn download job package artifactory unpack job working directory launch samza application master metric metric metric job running yarn job nt yet time monitor everything luxury using something call ingraphs accomplish ingraphs similar open source graphite backend based rrd whisper influxdb samza job start linkedin default metric produced kafka consumed ingraphs since samza job easily produce ten thousand metric per second introduced default metric blacklist keep thing bit quieter engineer actually need data holding onto historical metric making easily accessible everything young generation size process callback latency extremely valuable tool nt overlooked able visualize metric useful even important ability create alert take action based metric another application linkedin call autoalerts autoalerts language mostly yaml alerting metric similar functionality alert related feature riemann cabot alert minimum maximum threshold defined example screenshot threshold message per second per task defined minimum threshold job throughput task go value appropriate action taken automatic remediation oncall notification every alert create escalation path alert simply send email thing nt need immediate attention minor hardware failure successfully failed alert nonresponsive load balancer would escalated central network operation center would engage appropriate person oncall service metric monitor vary example threshold set application master heap size heap size fall certain threshold know good chance job trouble also several alert yarn including number unhealthy nodemanagers jvm metric number active resource master regard high availability also graph number job killed yarn failed reason outside samza also monitor disk iop byte available cpu memory network usage local interface rack switch uplink also hardwarespecific alert boolean alert health ssds one particular type alert like emphasize capacity alert threshold set based historical growth lead time order hardware alert go know time plug current capacity growth projection model provides estimate much hardware need order since currently limited memory rapid growth aim keep memory use samza job time word double size cluster every time order hardware platform matures growth slows relative total size cluster expect slowly increase target probably settling around peak traffic still unexpected growth addition metric monitor also graph ton data point without setting threshold many monitoring system handle mysterious subtle problem encountered ability quickly skim thousand data point glance often uncovers root problem least point u general direction need investigate hardware configuration order new hardware kafka broker use standard linkedin kafka configuration kafka broker samza exactly regular broker except relaxed threshold low number underreplicated partition far current scale found job usually memory bound node run samza task primarily look healthy ram core ratio currently aim core hardware thread per core ram per server moving forward expect change likely network next limiting factor job utilize samza keyvalue store typically use pcie based ssd terabyte two available space unlike ssds zookeeper cluster typically nt worry gc pause long overall throughput remains high moving forward although nt many stability issue samza like make entire system robust grow larger one thing improving soon handle job task isolation experimented cgroups namespaces various form yet settle path make u completely happy mostly come disk isolation another area like explore selfservice model developer minimal interaction reliability engineering create new samza application overview operate samza linkedin significant amount success working samza plan use many new task upcoming year
491,Lobsters,scaling,Scaling and architecture,Node.js in Flames,http://techblog.netflix.com/2014/11/nodejs-in-flames.html,nodejs flame,dousing fire perf event excellent article nodejs jstack support nodejs flame graph linux original svg one global,graph plot request latency m region time color corresponds different aws az see latency steadily increase m hour peak around m instance rebooteddousing fireinitially hypothesized might something faulty memory leak request handler causing rising latency tested assertion loadtesting app isolation adding metric measured latency request handler total latency request well increasing nodejs heap size saw request handler latency stayed constant across lifetime process m also saw process heap size stayed fairly constant around gb however overall request latency cpu usage continued rise absolved handler blame pointed problem deeper stacksomething taking additional m service request needed way profile application cpu usage visualize spending time cpu enter cpu flame graph linux perf event rescuefor unfamiliar flame graph best read brendan gregg excellent article explaining quick summary straight article box represents function stack stack frame yaxis show stack depth number frame stack top box show function oncpu everything beneath ancestry function beneath function parent like stack trace shown earlierthe xaxis span sample population show passing time left right graph left right ordering meaning sorted alphabetically width box show total time oncpu part ancestry oncpu based sample count wider box function may slower narrow box function may simply called often call count shown known via sampling sample count exceed elapsed time multiple thread running sampled concurrentlythe color significant picked random warm color called flame graph showing hot oncpu interactive mouse svgs reveal detailspreviously nodejs flame graph used system dtrace using dave pacheco nodejs jstack support however google team recently added perfevents support allows similar stack profiling javascript symbol linux brendan written instruction use new support arrived nodejs version create nodejs flame graph linuxhere original svg flame graph immediately see incredibly high stack application yaxis also see spending quite lot time stack xaxis closer inspection seems stack frame full reference expressjs routerhandle routerhandlenext function expressjs source code reveals couple interesting handler endpoint stored one global arrayexpressjs recursively iterates invokes handler find right route handlera global array ideal data structure use case unclear expressjs chose use constant time data structure like map store handler request requires expensive n look route array order find route handler compounding matter array traversed recursively explains saw tall stack flame graph interestingly expressjs even allows set many identical route handler route unwittingly set request chain like b c c c c e f g h request route c would terminate first occurrence c handler position array however request would terminate position array needle spent time spinning b multiple instance c verified running following vanilla express appvar express require express var app express appget foo function req re ressend hi add second foo route handlerappget foo function req re ressend consolelog stack approuterstack applisten running expressjs app return route handlersstack key regexp handle function query key regexp handle function expressinit key regexp foo handle function route path foo stack object method object key regexp foo handle function route path foo stack object method object notice two identical route handler foo would nice expressjs throw error whenever one route handler chain route point leading hypothesis handler array increasing size time thus leading increase latency handler invoked likely leaking handler somewhere code possibly due duplicate handler issue added additional logging periodically dump route handler array noticed array growing element every hour handler happened identical mirroring example handle function servestatic name servestatic params undefined path undefined key regexp fastslash true route undefined handle function servestatic name servestatic params undefined path undefined key regexp fastslash true route undefined handle function servestatic name servestatic params undefined path undefined key regexp fastslash true route undefined something adding expressjs provided static route handler time hour benchmarking revealed merely iterating handler instance cost m cpu time correlate latency problem seen response latency increase m every hour turned caused periodic function code main purpose refresh route handler external source implemented deleting old handler adding new one array unfortunately also inadvertently adding static route handler path time ran since expressjs allows multiple route handler given identical path duplicate handler added array making matter worse added rest api handler meant invoked service request service fully explains request latency increasing every hour indeed fixed code stopped adding duplicate route handler latency cpu usage increase went away
492,Lobsters,scaling,Scaling and architecture,colossus - I/O and Microservice library,https://github.com/tumblr/colossus,colossus io microservice library,colossus http tumblrgithubiocolossus google group json serialization plaintext license http,colossus colossus lightweight io framework building scala service full documentation found http tumblrgithubiocolossus general discussion q check google group colossus take part techempower web framework benchmark json serialization plaintext test overview find repo colossus framework colossusmetrics highperformance metric library depend colossus colossusexamples simple example run colossustestkit small library containing useful tool testing colossustests unit integration test colossus colossusdocs markdown documentation license copyright tumblr inc licensed apache license version license may use file except compliance license may obtain copy license http unless required applicable law agreed writing software distributed license distributed basis without warranty condition kind either express implied see license specific language governing permission limitation license
493,Lobsters,scaling,Scaling and architecture,WriterReaderPhaser: A story about a new (?) synchronization primitive,http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html,writerreaderphaser story new synchronization primitive,hdrhistogram latencyutils come hdrhistogram jhiccup hdrhistogram writerreaderphaser existing solution care leftright defining writerreaderphaser example use working implementation intervalhistogramrecorderjava doublerecorderjava recorderjava,recently added synchronization primitive mechanism hdrhistogram latencyutils code think generic use common operation specifically waitfree writer updating stuff background analyzer logger need look isolated call writerreaderphaser name intentional get moment code actual line elaborate comment first stray come storytelling writerreaderphaser new think synchronization primitive provides straightforward interface api coordinate waitfree writing shared data structure blocking reading operation data reader view stable ie non changing coherent data set writer continue modify data without waiting reader guaranteed forward progress block reader writer may flight time reader establishes stable view data come sometimes happens build stuff find need behavior thought would common ca nt find existing implementation name description obviously ascribed weak googlefu skill give build thing complicated build oneoff implementation whatever time move life later point find needing thing since already solved problem go back old code let honest copyandpaste first implementation whatever new thing working sometimes little guy right shoulder win guy come back refactor behavior separate class build api generic use point deserve library repo thinking start coupled much yak shaving sometimes guy left shoulder win actually get real work supposed leave decide little guy red white sometimes usually much later realize built actually new even though thought common use case built version simply impatience frustration finding something could use asis may actually first person solve time realization quickly followed someone showing paper piece code year old make go oh right sometimes nt happen sometimes really new hdrhistogram started way nothing line code oneoff jittermeter tool playing needed record latency quickly report accurate percentile many nine found building sort variation jitter meter sharing jhiccup evolved version better name found people included taking code ripping histogram trick inside needed histogram actually useful talking latency recognizing fast histogram good precision accurate fine grained quantile reporting capability actually common use case decided build yak shaving coop github called hdrhistogram first yak hair produced javacolored others recently added color breed hdrhistogram presumably successful example process going distance often nt probably stale repos github star fork represent writerreaderphaser currently halfway precarious process point pretty sure going die class yet library certainly repo yet need find home orggiltenestuff probably need end since short blog entry good home importantly look like may actually new generically useful synchronization primitive accurately nobody shown oh right link paper yet done holding breath writerreaderphaser ever need logging analyzing data actively updated ever wanted without stalling writer recorder way writerreaderphaser talking logging message text line talking data data larger one word memory data hold actual interesting state data keep updated need viewed stable coherent way analysis logging data like frame buffer data like histogram data like usage count data change existing solution sure use channel queue magic ring move data update safely process background copy data use persistent data structure sort immutable trickery expensive order magnitude expensive updating incache state place data thing want look could updated million time per second invariably end sort doublebuffered multi buffered scheme update done active copy analysis done background stable inactive copy double buffered scheme usually involve sort phase flipping point notion copy active change writer update new active copy reader access stable coherent copy used active nt phase flipping usually come way keeping writer blocking sort variation flipping obviously use form mutual exclusion lock protect writes flip writer block blocked flipping operation use readerwriter lock backwards state protected readerwriter lock would notion data set active one one writer write scheme writer take read lock duration active state modification operation reader take write lock flip role active inactive data set much better complete mutual exclusion multiple writer involved since writer longer block writer reader still block writer flip also start asking read mean context good sign problem people write buggier code standing head juggling sure whole bunch scheme people use looking around thus far nt find example nonblocking writer care thing actually wanted doublebuffer histogram histogram fixedfootprint histogram support lossless recording experienced latency later computation precise percentile possible way level purpose histogram often capture analyze latency outlier behavior recording operation allowed cause outlier trying measure latency recording mechanism susceptibility blocking locking would unacceptable latency histogram basically nonblocking data structure ten hundred kilobyte state rapidly mutated critical path writer code wanted log content interval short enough interesting monitoring purpose later time based analysis order log latency information captured needed logging reader somehow gain access stable coherent snapshot latency data recorded prior interval needed way reader flip role active inactive histogram needed without ever blocking writer classic case asymmetric synchronization need fine blocking delaying pausing reader ca nt afford writer ever block otherwise delay execution thread recording come writerreaderphaser best starting point understanding dissect name phaser part main function coordinate phase shift writer reader besides could nt bring call thing lock lock important function phase shift coordination writer remain lockfree case actually remain wait free architecture support atomic increment operation never block lock calling writerreaderphaser lock would like calling atomiclong add lock someone could also construct spinlock around writerreader part reversal commonly used readerwriter readwrite term readerwriter lock asymmetric reverse direction needed enable relatively smooth reader operation causing writer block really cool waitfree leftright martin thompson pointed achieves perfectly smooth reader operation still needed writerreaderphaser work exactly reversed need writer remain nonblocking perfectly smooth reader suffer desired behavior looking writerreaderphaser writer remaining lockfree time ideally remain waitfree time reader coordinate phase flip access inactive data reader flip phase reader still interested inactive data writer modification made inactive data phase flip operation complete long reader interested inactive data reader guaranteed forward progress even presence heavy continuous writer activity even writer activity defining writerreaderphaser high level desired behavior stated let clearly define quality guarantee well implemented writerreaderphaser primitive would provide user relevant rule user must adhere order maintain quality guarantee writerreaderphaser instance provides following operation writercriticalsectionenter writercriticalsectionexit readerlock readerunlock flipphase writerreaderphaser instance used protect actively updated data structure set data structure involving potentially multiple writer potentially multiple reader assumption reader writer act two set data structure active set inactive set writing done perceived active version perceived writer within critical section delineated writercriticalsectionenter writercriticalsectionexit operation reader switch perceived role active inactive data structure holding readerlock switch done execution flipphase reader hold onto readerlock indefinitely reader perform readerlock readerunlock writer remain critical section indefinitely writer perform writercriticalsectionenter writercriticalsectionexit reader perform flipphase operation holding readerlock assumption met writerreaderphaser guarantee inactive data structure modified writer read readerlock protection flipphase operation following progress guarantee provided writer reader adhere stated assumption writer operation writercriticalsectionenter writercriticalsectionexit wait free architecture support waitfree atomic increment operation flipphase operation guaranteed make forward progress blocked writer whose critical section entered prior start reader flipphase operation yet exited critical section readerlock block reader holding readerlock example use imagine simple use case large set rapidly updated counter modified writer reader need gain access stable interval sample counter reporting analysis purpose counter represented volatile array value array reference volatile value cell within volatile long count writer update specific count n set counter writercriticalsectionenter count n use atomic increment multiwriter writercriticalsectionexit reader gain access stable set count collected interval report accumulates long accumulatedcounts readerlock reset intervalcounts long tmp count count intervalcounts intervalcounts tmp flipphase point intervalcounts content stable reportintervalcounts intervalcounts accumulatedcountsadd intervalcounts readerunlock working implementation hood writerreaderphaser implementation achieves quality fairly straightforward way using dual set epoch counter odd set even set coordinate phase flip operation coupled read lock used purely protect reader multireader situation ie prevent one reader flipping phase changing notion active inactive data another reader still operating many implementation mechanism possible one certainly sufficient job hand rather describe logic text easiest list code point entire writerreaderphaser class implemented current hdrhistogram repository spelled java code detailed comment mechanism obviously ported language envrionment provide support atomic increment atomic swap operation api documentation case detail javadoc comment important simple example used practice found hdrhistogram various interval histogram recorder like original probably simplest example intervalhistogramrecorderjava recent replacement doublerecorderjava recorderjava add unrelated complicated logic deal safely avoiding copy cost getintervalhistogram variant yes public domain enjoy apparent etymology term yak shaving read example story attributed
494,Lobsters,scaling,Scaling and architecture,Why your code doesn't scale,http://www.mullie.eu/why-your-code-doesnt-scale/,code nt scale,introduction horizontal vertical vertical scaling horizontal scaling readwriteheavy application storage server application cpu memory example bad worse better storage top website sharding even bother excellent article database replication cache network,building scalable software mean prepared accommodate growth basically thing need consider data grows request handled faster rate come hardware able store data obviously need infrastructure grow need machine probably also needwant introduce additional application help lighten load like cache server load balancer introduction imagine selfsufficient village inhabitant grows population million initial power supply network state art quite fit magnitude order service resident power plant need produce power horizontal vertical vertical scaling mean increase overall capacity increasing capacity machine eg running disk space could add hard disk database server horizontal scaling mean adding machine setup villageanalogy scaling vertically would adding nuclear reactor power plant increase amount power generated scaling horizontally would mean building second power plant vertical scaling easiest rest existing infrastructure still work scaling horizontally mean need additional power circuit new plant new software keep track flow energy plant computer hardware scaling horizontally cluster lower tier machine usually cheaper compared scaling vertically one supercomputer also provides failover one machine dy others take however horizontal scaling often harder software point view data distributed multiple machine readwriteheavy readheavy application usually easier scale readheavy mean plenty request need fetch output data compared store data readheavy application mostly able service request need enough machine handle load enough application server computing andor enough database slave read later writeheavy application need even careful planning likely readproblems well also need able store data somewhere lot continuously growing might outgrow machine application storage server application server one hosting php python code really hard scale code typically change based userinput stuff database run code machine machine b exact thing scaling code easy adding machine put code machine load balancer front evenly distribute request machine able handle much traffic important thing application effective possible actually able handle lot request issue storage completely different well readissues comparable enough copy enough machine write issue require hard thinking exactly store data application cpu application consists lot instruction code request come code start bunch thing processing order finally respond appropriate output amount request server able respond limited capacity hardware want keep machine service request simple possible even machine capable lot way handle request problem cpuintensive application something take increasingly long time compute response time delayed amount request able handle lower eventually able service request cpuintensive work critical response need immediately display result consider deferring work job queue scheduled later execution memory problem memoryintensive application twofold processor may idling access occupied memory thus delaying response time problem cpuintensive application may trying fit memory possible request fail always try limit unknown idea expensive request could much resource need probably good job scaling usually limit memory usage processing data smaller batch known size loading thousand row database process divvy smaller batch say row process next batch ensures never exhaust memory amount row grows large fit never hold row example let say want retrieve random row database bad select tablename order rand limit mostly cpuintensive task mysql iterate row calculate random number per row amount row grows operation take longer complete take machine longer respond process request slower rate actually grows even larger mysql also longer able keep random number memory save temporary table hard disk much slower access data memory certain point start affecting response time drastically worse even worse would fetching row database passing application calling arrayrand similar would scale memory size table outgrows available memory crash better select rand round rand max id tablename select tablename id rand limit regardless amount row billion always get max id database generate random number lower equal get record mysql easily fetch via pk index scale matter many row grow feature never ever harder compute note instead limit ignore potential gap id could generate random id longer exists db query also settle case storage image video often consume plenty storage biggest problem storage usually database long know machine save imagesvideos easily retrieve top website deal much data hardly fit one database distributing data multiple server easier said done data often linked database grows might consider moving problematic table dedicated server data multiple table often need joined one another though quite impossible spread among multiple physical virtual machine sharding instead distributing specific table several machine cause join issue often better finding common shared column decide distribute data could multiple database server full database schema every server hold different data simple example would blogging platform provider sign blog hosted server servicing thousand blog one machine may feasible easily distribute blog across multiple machine long data blog machine thing pretty easy blog could perfectly well another machine data never joined mine complex example could social network lot user lot data per user much one machine data user account setting image message status update could live one machine may stored entirely different machine case data sharded different server based user id considerate distribute data though viewing single person status update requesting data one machine however want see stream status update friend could located different machine thing want let get ahead sharding database incredibly complex really big deal implement maintain moore law may machine growing faster rate need ever likely even bother technical detail sharding jurriaan persyn wrote excellent article sharded netlog database database replication database replication setting master database multiple slave data always written master database turn replicates slave database data read slave database enormous amount read need add slave master database deal writes hopefully relatively sharding likely solution writeheavy application replication readheavy problem cache magic bullet well really incredibly helpful caching storing result expensive operation known result next time like storing result call external api database storing result expensive query computation memcached cm store page database point getting navigation database every single page request change given time simply store static copy cache new page added navigation purgeinvalidate cache next time fetching updated navigation storage cache come many form memcached redis server disk cache temporary cache matter reading result faster executing note introducing caching increase complexity codebase read write data multiple place also need make sure sync data cache get updated invalidated data storage change network network request usually really considered drastically slow application carefully limit production environment database server caching server whatnot likely another network resource want read database connect cache server although probably little connecting server take time try make database cache request possible especially important cache request often thought cheap cache server quick respond ask hundred cached value time spent connecting cache server accumulates possible could attempt request data bulk mean fetching multiple cache key fetching column table know querying particular column function another function b
495,Lobsters,scaling,Scaling and architecture,Aeron: Open-source high-performance messaging,https://www.youtube.com/watch?v=tM4YskS94b0,aeron opensource highperformance messaging,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature aeron opensource highperformance messaging martin thompson youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature aeron opensource highperformance messaging martin thompson youtube
496,Lobsters,scaling,Scaling and architecture,AWS Lambda,http://aws.amazon.com/lambda/,aws lambda,data processing aws step function serverless realtime file processing read case study realtime stream processing read case study machine learning backends serverless web application iot backends mobile backends read case study,data processing use aws lambda execute code response trigger change data shift system state action user lambda directly triggered aws service dynamodb kinesis sn cloudwatch connect existing efs file system orchestrated workflow aws step function allows build variety realtime serverless data processing system realtime file processing use amazon trigger aws lambda process data immediately upload also connect existing amazon efs file system directly enables massively parallel shared access large scale file processing example use lambda thumbnail image transcode video index file process log validate content aggregate filter data realtime seattle time us aws lambda resize image viewing different device desktop computer tablet smartphones read case study realtime stream processing use aws lambda amazon kinesis process realtime streaming data application activity tracking transaction order processing click stream analysis data cleansing metric generation log filtering indexing social medium analysis iot device data telemetry metering localytics process billion data point realtime us lambda process historical live data stored streamed kinesis read case study machine learning use aws lambda preprocess data feeding machine learning model lambda access efs also serve model prediction scale without provision manage infrastructure aible focus delivering powerful ai technology lowest possible operating cost use aws lambda serverless machine learning training prediction serverless run wide variety machine learning workload cost effectively benefiting burst compute resource needed rapid iteration scaling create ai optimal business impact rod butter cto aible backends build serverless backends using aws lambda handle web mobile internet thing iot party api request take advantage lambda consistent performance control multiple memory configuration provisioned concurrency building latencysensitive application scale web application combining aws lambda aws service developer build powerful web application automatically scale run highly available configuration across multiple data center zero administrative effort required scalability backup multidata center redundancy iot backends build serverless backends using aws lambda handle web mobile internet thing iot party api request mobile backends aws lambda make easy create rich personalized app experience build backends using aws lambda amazon api gateway authenticate process api request use aws amplify easily integrate backend io android web react native frontends bustle run serverless backend bustle io app website using aws lambda amazon api gateway serverless architecture allow bustle never deal infrastructure management every engineer focus building new feature innovating read case study
497,Lobsters,scaling,Scaling and architecture,Amazon EC2 Container Service (ECS) - Container Management for the AWS Cloud,http://aws.amazon.com/blogs/aws/cloud-container-management/,amazon container service ec container management aws cloud,container computing introducing amazon amazon container service docker amazon elastic compute cloud benefit easy cluster management high performance flexible scheduling extensible portable resource efficiency aws integration secure amazon virtual private cloud using docker daemon cluster scheduler container task definition task ecsenabled ami ami dockerd docker hub pricing availability click jeff,earlier year wrote container computing enumerated benefit get use basis distributed application platform consistency amp fidelity development efficiency operational efficiency container lighter weight le memory computational overhead virtual machine make easy support application consist hundred thousand small isolated moving part properly containerized application easy scale maintain make efficient use available system resource introducing amazon amazon container service order help realize benefit announcing preview new container management service container service ec short service make easy run number docker container across managed cluster amazon elastic compute cloud instance using powerful apis tool install cluster management software purchase maintain cluster hardware match hardware inventory software need use ec simply launch instance cluster define task start ec built around scalable faulttolerant multitenant base take care detail cluster management behalf way let word cluster scare cluster simply pool compute storage networking resource serf host one containerized application fact cluster even consist single instance general single midsized instance sufficient resource used productively starter cluster container service benefit service help build run scale dockerbased application easy cluster management ec set manages cluster made docker container launch terminates container maintains complete information state cluster scale cluster encompass ten thousand container across multiple availability zone high performance use container application building block start stop manage thousand container second flexible scheduling ec includes builtin scheduler strives spread container across cluster balance availability utilization ec provides access complete state information also build scheduler adapt existing open source scheduler use service apis extensible portable ec run docker daemon would run onpremises easily move onpremises workload aws cloud back resource efficiency containerized application make efficient use resource choose run multiple unrelated container instance order make good use available resource could example decide run mix shortterm image processing job longrunning web service instance aws integration application make use aws feature elastic ip address resource tag virtual private cloud vpc container effect new baselevel building block vein secure task run instance within amazon virtual private cloud task take advantage iam role security group aws security feature container run multitenant environment communicate across defined interface container launched instance control using container service ec designed easy set use launch ecsenabled ami instance automatically checked default cluster want launch different cluster specify modifying configuration file image passing user data launch ecsenable linux ami simply install ec agent docker daemon ec add newly launched instance capacity pool run container directed scheduler word add capacity cluster simply launching additional instance ec agent available open source form apache license install existing linux amis call registercontainerinstances add cluster vocabulary item help get familiar terminology used ec cluster cluster pool instance particular aws region managed ec one cluster contain multiple instance type size reside within one availability zone scheduler scheduler associated cluster scheduler responsible making good use resource cluster assigning container instance way respect placement constraint simultaneously drive much parallelism possible also aiming high availability container container packaged dockerized cool kid like say application component instance cluster serve host one container task definition json file defines task set container field file define image container convey memory cpu requirement also specify port mapping needed container task communicate task task instantiation task definition consisting one container defined work relationship ecsenabled ami amazon machine image ami run ec agent dockerd plan ecsenable amazon linux ami working partner similarly enable amis container service includes set apis simple powerful create describe destroy cluster register instance therein create task definition initiate manage task basic set step follow order run application ec making assumption already dockerized application breaking finegrained component described dockerfile running nicely existing infrastructure plenty good resource online help process many popular application component already dockerized found docker hub use ec public private docker repository acccess ok step create cluster decide use default one account target region create task definition register cluster launch instance register cluster start desired number copy task monitor overall utilization cluster overall throughput application make adjustment desired example launch register additional instance order expand cluster pool available resource container service pricing availability service launch today preview form interested signing click join waiting list extra charge ec usual pay resource use jeff
498,Lobsters,scaling,Scaling and architecture,Post Mortem: The One Where We Accidentally DDOSed Ourselves,https://keen.io/blog/101427090951/post-mortem-the-one-where-we-accidentally-ddosed,post mortem one accidentally ddosed,opportunity knock closet muted kafka source opportunity huge opportunity fix huge retrospective cringe http opportunity fix fix fix opportunity fix fix learned file system monitoring correctly unmuted customer vital source debugging information remember think outside ye olde box added extensive amount monitoring api layer upgraded newer version kafka replication awesome thing using timebased bound spacelimited thing bad process comprehensive rethink retry logic mobile sdks two general problem overall really beefed historical metric monitoring awareness key part service survived take responsibility customer seriously closing,old bos mine phrase used whenever shit hit fan know special sometimes amazing circumstance inevitably happen trying run bunch computer toward common goal fond saying opportunity arise summer keen io series postmortemworthy opportunity threatened everything hold near dear stability platform trust customer sanity team please allow u tell cathartic detail opportunity knock closet morning july working home flight reilly oscon later day time kill looking various chart alert infrastructure noticed muted alert triggered generally mute alert want bugged email phone call temporarily unfortunately alert forgotten happened one dreaded computerdom filesystem nearly full oh shit mean uhm great opportunity quite odd error see kafka distributed log keen write incoming event kafka sort holding tank write cassandra permanent storage kafka configured retain rolling hour people call week data case need refer back significance error longer enough space accommodate week customer data happen source opportunity reaching teammate learned one customer released new mobile game weekend great success awesome downside huge wave inbound data game exceeding shortterm storage needed get worked fast option limited see version kafka using time allow change retention configuration without restarting restart due lack replication kafka thing disk filled without fix stop accepting new event quickly serious pressure came clever workaround wrote short python script changed created time oldest kafka log file date outside rolling window kafka dutifully deleted file u afterward feeling smart clever patted back sighed big breath relief phew immediate disk shortage problem resolved little know first shot scalability war consume platform team next week see bailing water large volume data continue flood rate data coming continued increase looked like term bandwidth used felt like confused awkward moment seal sister oh god happening platform seal opportunity fix beginning plan kafka upgrade would help scalability hit another setback started getting warning writing data fast enough deep queue began form write path completely overwhelmed spent entire day firefighting tuning issue made lot progress eventually started realize might able keep got phone customer put head together turn big important game launch biggest game launch history particular game quite heavily instrumented sending disproportionate amount data even nastier appeared junky duplicate data effort keep incoming flood ensure captured important event identified data type could start dropping like every time app brought foreground background release valve gave u room breathe begin investigation started feel little bit ill usually customer send lot data good thing gaming company run business data store keen io last thing want big game launch decide data keep something ever keen io incident terrifying customer u awful even progress made even though stopped recording event inflow data growing growing previously monthly data volume daily data volume looked like though know time already committed one biggest mistake technical one focused keeping platform alive told customer figure game sending order magnitude data game launch ever time seemed reasonable collecting data lot apps wrong huge retrospective cringe hoped customer would figure app going berserk also began bit snooping around client data kept pouring volume terrifying ever used lot data coming keen io particularly big unexpected wave come slows write time battling point though seeing new dire challenge getting many request threatened stability load balancer load balancer go handle new request customer data spill ether never recovered opportunity go would pretty terrifying one might even warrant ditching whole opportunity joke actually calling problem thankfully mercifully new information surfaced right time received email customer noting inspected http response batch post event see normal acknowledgement write failure expecting thunderclap echoed around office slapped forehead turn previous awesome fix actually causing bad payload retried time amplified little data right blocking event commanded hundred thousand device world repeatedly try sending u increasingly large payload plan better ddos paused second let sink http minute later pushed change correctly acknowledged message dropping effect immediate opportunity fix fix pretty sharp decline bandwidth consumed shipped fix excitement isolated significant problem took u day note despite change still seeing dayoverday increase load balancer handling traffic well hint something still amiss fix opportunity fix fix soldiered dan dug plane train family vacation kevin developed healthy addiction game log file attending wedding michelle family openly questioned working much customer scoured game code clue dug android sdk incoming data pattern finally interesting http response capture sent little http capture almost dismissed finally pointed source neverending stream data well quite opportunity created turn edgecase scenario bug api client malformed event poison pill would cause event batch request written database entire request would fail ie response code event would retried reason game impacted happened generate null event device effectively poisoning turning little warrior selfinflicted ddos deployed fix poison pill right away saw huge improvement immediately relief relief learned added lot safeguard learned lesson series event let look specifically file system monitoring correctly unmuted oops customer vital source debugging information user amazingly smart help essential moreover really important assume customer fault remember think outside ye olde box many clue got came place expecting source think related added extensive amount monitoring api layer detect anomaly failed event problem monitoring tuning alert use last month upgraded newer version kafka replication awesome thing directly related factor early pain running kafka replication lot storage using timebased bound spacelimited thing bad also need spacebased bound thing well fallback prevent filling disk process comprehensive rethink retry logic mobile sdks tricky question hint two general problem heart design choice mobile device sends event server sure made err side retrying sure side deleting event avoiding duplication prior incident assumed former realize nuanced answer mandatory overall really beefed historical metric monitoring awareness key part service good reminder something ever really done thing need considered every deploy rollout automating essential survived amazing learn pressure month prior event made difficult decision take new large customer wanted send u perceived huge data volume scary wonderful opportunity happened choice huge data invited middle upgraded kafka tweaked zillion layer stack added hardware came significantly capacity process thought would take month achieve also survived ddos without data loss end looked back shock mortified mistake also amazed pulled take responsibility customer seriously first time keen io saw firsthand mistake impacted customer real tangible important way incredibly humbling maturing time team continue amazed patience understanding engineer shown working u profound appreciation closing fine admit despite repeated joke situation really opportunity looking list accomplishment showed much gained term knowledge capacity camaraderie really hard see thick pretty plain hindsight wherever old bos mine opportunity apologize scoffing opportunity talk indefatigable optimism also time set desktop wallpaper screenshot desktop hid icon maybe right
499,Lobsters,scaling,Scaling and architecture,How League of Legends Scaled Chat to 70 million Players,http://highscalability.com/blog/2014/10/13/how-league-of-legends-scaled-chat-to-70-million-players-it-t.html,league legend scaled chat million player,thing fail scale surface bug key success understanding system actually strategy make work make visible make devops reduce chatty protocol avoid shared mutable state leverage social graph,would build chat service needed handle million concurrent player million daily player message per second billion event per server per day could generate much traffic game course league legend league legend team based game multiplayer online battle arena moba two team five battle control map achieve objective team succeed communication crucial learned michal ptaszek interesting talk scaling league legend chat million player slide strange loop conference michal gave good example multiplayer team game require good communication player imagine basketball game without ability call play work mean chat crucial chat nice feature michal structure talk interesting way using template expression make work make right make fast making work meant starting xmpp base chat whatsapp followed strategy box get something work scale well user count really jump make right fast like whatsapp league legend found customizing erlang vm adding lot monitoring capability performance optimization remove bottleneck kill performance scale perhaps interesting part chat architecture use riak crdts convergent replicated data type achieve goal shared nothing fueled massively linear horizontal scalability crdts still esoteric may heard yet next cool thing make work different way thinking handling writes let learn league legend built chat system handle million player stats million unique player every month counting service using chat million daily player million concurrent player billion event routed per server per day using percent available cpu ram message per second hundred chat server deployed around world managed people uptime platform chat oneonone group chat chat act presence service also maintains friend list know player online offline playing long playing champion playing rest apis provide chat backend service lol service store talk chat verify friendship example league use chat social graph group new player together play often compete chat must run low stable latency chat work game experience degraded selected xmpp protocol provides messaging presence information maintains contact list performance reason implement new feature diverge core xmpp protocol chose ejabberd server worked well beginning erlang many benefit built concurrency distribution scalability mind support hot code reloading bug patched without stopping service goal share nothing architecture achieve massive linear horizontal scalability also allows better error isolation traceability yet making progress towards goal hundred chat server people manage important server fault tolerant every failure requires human intervention let crash try slowly recover major failure instead restart known state example large backlog pending query database database restarted new query processed realtime queued query rescheduled processing physical server run ejabberd riak riak used database server added horizontally necessary ejabberd run cluster risk server also run cluster riak server use multi datacenter replication export persistent data secondary riak cluster costly etl query like social graph query run secondary cluster without interrupting primary cluster backup also run secondary cluster time necessity focus scalability performance fault tolerance ejabberd rewritten rewrote match requirement example lol friendship bidirectional xmpp allows asymmetrical friendship xmpp friendship creation required message client server hit database new protocol three message removed unnecessary unwanted code optimised protocol wrote lot test make sure nothing broken profile code remove clear bottleneck avoid shared mutable state scale linearly single server well clustered environment multi user chat muc every chat server handle hundred thousand connection every user connection session process every time user wanted update presence send message room event go single process called muc router would relay message relevant group chat clear bottleneck solution parallelize routing lookup group chat room happens user session able use available core every ejabberd server contains copy session table contains mapping user id session sending message requires looking user session cluster message written session table checking session exists checking presence priority check number distributed writes reduced huge win user could login much faster presence update could occur much frequently added functionality get visibility code upgraded production made code could updated multiple server transactional context optimize server debug functionality added erlang vm needed ability see memory used session could better optimize memory usage database scalability designed start started mysql hit multiple performance reliability scalability issue example possible update schema fast enough track change made code chose riak riak distributed fault tolerant keyvalue store truly masterless single point failure even two server dying degrade service lose data spent lot work chat server implement eventual consistency implemented ejabberd crdt library convergent replicated data type take care write conflict try converge object stable state crdt work instead appending new player directly friend list operational log maintained object log entry like add player add player next time object read log consulted conflict resolved log applied order object order matter way friend list consistent state idea value updated place instead long log operation built object operation applied whenever object read riak huge success allowed scale linearly also gave schema flexibility object could changed fly huge mindshift lot work changed tested service built tool around monitoring built realtime counter collected every minute posted monitoring system graphite zabbix nagios counter threshold generate alert crossed problem addressed long player notice service problem example recent client update entered infinite loop broadcasting presence looking graphite immediately clear chat server hammered presence update began new client release implementation feature toggle feature flag ability turn new feature fly without restart service new feature developed surround onoff toggle feature cause problem disabled partial deployment new code enabled certain user percentage user new code activated allows testing potentially dangerous feature far le full load new feature work turned everyone code reloading fly one great feature erlang ability hot load new code fly one case third party client like pidgin well tested turned sending different kind event official client fix could deployed integrated chat server without restart whole chat mean lower downtime player logging log abnormal condition like error warning server also report healthy state give ability look log determine server ok logging user accepting new connection modifying friend list builtin ability enter debug mode selected user session suspicious user experimental user qa testing production server even though session chat server particular session need logged logging includes xml traffic event metric save huge log storage combination feature toggle partial deployment selective logging possible deploy feature production server tested people relevant log collected analyzed without noise user load testing code every night automatic verification system deploys build change load test environment run battery load test server health monitored test metric pulled analyzed confluence page generated metric test result email summary sent summary test result change compared previous build possible track code change impact test finding problem like disaster change like memory consumption improved x percent future migrating data mysql would like make chat available outside game player enjoy friendship without log game would like use social graph make experience better analyze player connection understand impact enjoyment game plan migrate ingame chat outofgame chat server lesson learned thing fail full control even code bug free happens isp router dy player lost time prepared make sure system handle losing half player losing chat server without degrading performance scale surface bug even bug happens billion time mean scale league legend bug occur day even unlikely event happen time key success understanding system actually know system healthy state crash strategy lol strategy horizontal scaling chat service support strategy something different bought nosql riak changed approach leverage crdts make horizontal scaling seamless powerful possible make work start somewhere evolve ejabberd got ground would easier make scratch maybe able evolve system match requirement learned requirement make visible add tracing logging alerting monitoring graph good stuff make devops lol added transaction software update feature flag hot update automated load testing highly configurable log level etc make system easier manage reduce chatty protocol tailor functionality system need system supported bidirectional friendship example general costly protocol avoid shared mutable state common tactic always interesting see shared state cause problem step scale leverage social graph chat service naturally provides social graph information used improve user experience implement novel new feature related article
500,Lobsters,scaling,Scaling and architecture,The LMAX Architecture,http://martinfowler.com/articles/lmax.html,lmax architecture,overall structure business logic processor keeping memory event sourcing tuning performance programming model input output disruptors queue lack mechanical sympathy use architecture cqrs,last year keep hearing free lunch ca nt expect increase individual cpu speed write fast code need explicitly use multiple processor concurrent software good news writing concurrent code hard lock semaphore hard reason hard test meaning spending time worrying satisfying computer solving domain problem various concurrency model actor software transactional memory aim make easier still burden introduces bug complexity fascinated hear talk qcon london march last year lmax lmax new retail financial trading platform business innovation retail platform allowing anyone trade range financial derivative product trading platform like need low latency trade processed quickly market moving rapidly retail platform add complexity lot people result user lot trade need processed quickly given shift multicore thinking kind demanding performance would naturally suggest explicitly concurrent programming model indeed starting point thing got people attention qcon nt ended fact ended business logic platform trade customer market single thread thread process million order per second using commodity hardware processing lot transaction lowlatency none complexity concurrent code resist digging fortunately another difference lmax financial company quite happy talk technological decision lmax production time explore fascinating design overall structure figure lmax architecture three blob top level architecture three part business logic processor input disruptor output disruptors name implies business logic processor handle business logic application indicated singlethreaded java program reacts method call produce output event consequently simple java program nt require platform framework run jvm allows easily run test environment although business logic processor run simple environment testing rather involved choreography get run production setting input message need taken network gateway unmarshaled replicated journaled output message need marshaled network task handled input output disruptors unlike business logic processor concurrent component since involve io operation slow independent designed built especially lmax like overall architecture applicable elsewhere business logic processor keeping memory business logic processor take input message sequentially form method invocation run business logic emits output event operates entirely inmemory database persistent store keeping data inmemory two important benefit firstly fast database provide slow io access transactional behavior execute since processing done sequentially second advantage simplifies programming objectrelational mapping code written using java object model without make compromise mapping database using inmemory structure important consequence happens everything crash even resilient system vulnerable someone pulling power heart dealing event sourcing mean current state business logic processor entirely derivable processing input event long input event stream kept durable store one job input disruptor always recreate current state business logic engine replaying event good way understand think version control system version control system sequence commits time build working copy applying commits vcss complicated business logic processor must support branching business logic processor simple sequence theory always rebuild state business logic processor reprocessing event practice however would take long need spin one version control system lmax make snapshot business logic processor state restore snapshot take snapshot every night period low activity restarting business logic processor fast full restart including restarting jvm loading recent snapshot replaying day worth journal take le minute snapshot make starting new business logic processor faster quickly enough business logic processor crash result lmax keep multiple business logic processor running time input event processed multiple processor one processor output ignored live processor fail system switch another one ability handle failover another benefit using event sourcing event sourcing replica switch processor matter microsecond well taking snapshot every night also restart business logic processor every night replication allows downtime continue process trade event sourcing valuable allows processor run entirely inmemory another considerable advantage diagnostics unexpected behavior occurs team copy sequence event development environment replay allows examine happened much easily possible environment diagnostic capability extends business diagnostics business task risk management require significant computation nt needed processing order example getting list top customer risk profile based current trading position team handle spinning replicate domain model carrying computation wo nt interfere core order processing analysis domain model variant data model keep different data set memory run different machine tuning performance far explained key speed business logic processor everything sequentially inmemory nothing really stupid allows developer write code process tps found concentrating simple element good code could bring tps range need wellfactored code small method essentially allows hotspot better job optimizing cpu efficient caching code running took bit cleverness go another order magnitude several thing lmax team found helpful get one write custom implementation java collection designed cachefriendly careful garbage example using primitive java longs hashmap key specially written array backed map implementation longtoobjecthashmap general found choice data structure often make big difference programmer grab whatever list used last time rather thinking implementation right one context another technique reach top level performance putting attention performance testing long noticed people talk lot technique improve performance one thing really make difference test even good programmer good constructing performance argument end wrong best programmer prefer profilers test case speculation lmax team also found writing test first effective discipline performance test programming model style processing introduce constraint way write organize business logic first tease interaction external service external service call going slow single thread halt entire order processing machine result ca nt make call external service within business logic instead need finish interaction output event wait another input event pick back use simple nonlmax example illustrate imagine making order jelly bean credit card simple retailing system would take order information use credit card validation service check credit card number confirm order within single operation thread processing order would block waiting credit card checked block would nt long user server always run another thread processor waiting lmax architecture would split operation two first operation would capture order information finish outputting event credit card validation requested credit card company business logic processor would carry processing event customer received creditcardvalidated event input event stream processing event would carry confirmation task order working kind eventdriven asynchronous style somewhat unusual although using asynchrony improve responsiveness application familiar technique also help business process resilient explicit thinking different thing happen remote application second feature programming model lie error handling traditional model session database transaction provides helpful error handling capability anything go wrong easy throw away everything happened far interaction session data transient discarded cost irritation user middle something complicated error occurs database side rollback transaction lmax inmemory structure persistent across input event error important leave memory inconsistent state however automated rollback facility consequence lmax team put lot attention ensuring input event fully valid mutation inmemory persistent state found testing key tool flushing kind problem going production input output disruptors although business logic occurs single thread number task done invoke business object method original input processing come wire form message message need unmarshaled form convenient business logic processor use event sourcing relies keeping durable journal input event input message need journaled onto durable store finally architecture relies cluster business logic processor replicate input message across cluster similarly output side output event need marshaled transmission network figure activity done input disruptor using uml activity diagram notation replicator journaler involve io therefore relatively slow central idea business logic processor avoids io also three task relatively independent need done business logic processor work message done order unlike business logic processor trade change market subsequent trade natural fit concurrency handle concurrency lmax team developed special concurrency component call disruptor crude level think disruptor multicast graph queue producer put object sent consumer parallel consumption separate downstream queue look inside see network queue really single data structure ring buffer producer consumer sequence counter indicate slot buffer currently working producerconsumer writes sequence counter read others sequence counter way producer read consumer counter ensure slot want write available without lock counter similarly consumer ensure process message another consumer done watching counter figure input disruptor coordinate one producer four consumer output disruptors similar two sequential consumer marshaling output output event organized several topic message sent receiver interested topic disruptor disruptors described used style one producer multiple consumer nt limitation design disruptor disruptor work multiple producer case still nt need lock benefit disruptor design make easier consumer catch quickly run problem fall behind unmarshaler problem processing slot return receiver slot read data slot one batch catch batch read data disruptor make easier lagging consumer catch quickly thus reducing overall latency described thing one journaler replicator unmarshaler indeed lmax design would allow multiple component run ran two journalers one would take even slot journaler would take odd slot allows concurrency io operation become necessary ring buffer large million slot input buffer million slot output buffer sequence counter long integer increase monotonically even ring slot wrap buffer set size power two compiler efficient modulus operation map sequence counter number slot number like rest system disruptors bounced overnight bounce mainly done wipe memory le chance expensive garbage collection event trading also think good habit regularly restart rehearse emergency journaler job store event durable form replayed anything go wrong lmax use database file system stream event onto disk modern term mechanical disk horribly slow random access fast streaming hence tagline disk new tape earlier mentioned lmax run multiple copy system cluster support rapid failover replicator keep node sync communication lmax us ip multicasting client nt need know ip address master node master node listens directly input event run replicator replicator broadcast input event slave node master node go lack heartbeat noticed another node becomes master start processing input event start replicator node input disruptor thus journal unmarshaling even ip multicasting replication still needed ip message arrive different order different node master node provides deterministic sequence rest processing unmarshaler turn event data wire java object used invoke behavior business logic processor therefore unlike consumer need modify data ring buffer store unmarshaled object rule consumer permitted write ring buffer writable field one parallel consumer allowed write preserve principle single writer figure lmax architecture disruptors expanded disruptor general purpose component used outside lmax system usually financial company secretive system keeping quiet even item nt germane business lmax open overall architecture opensourced disruptor code act make happy allow organization make use disruptor also allow testing concurrency property queue lack mechanical sympathy lmax architecture caught people attention different way approaching high performance system people thinking far talked work nt delved much developed way tale interesting architecture nt appear took long time trying conventional alternative realizing flawed team settled one business system day core architecture relies multiple active session coordinated transactional database lmax team familiar approach confident would nt work lmax assessment founded experience betfair parent company set lmax betfair betting site allows people bet sporting event handle high volume traffic lot contention sport bet tend burst around particular event make work one hottest database installation around many unnatural act order make work based experience knew difficult maintain betfair performance sure kind architecture would work low latency trading site would require result find different approach initial approach follow many saying day get high performance need use explicit concurrency scenario mean allowing order processed multiple thread parallel however often case concurrency difficulty come thread communicate processing order change market condition condition need communicated approach explored early actor model cousin seda actor model relies independent active object thread communicate via queue many people find kind concurrency model much easier deal trying something based locking primitive team built prototype exchange using actor model performance test found processor spent time managing queue real logic application queue access bottleneck pushing performance like start become important take account way modern hardware constructed phrase martin thompson like use mechanical sympathy term come race car driving reflects driver innate feel car able feel get best many programmer confess fall camp nt much mechanical sympathy programming interacts hardware worse many programmer think mechanical sympathy built notion hardware used work many year date one dominant factor modern cpu affect latency cpu interacts memory day going main memory slow operation cputerms cpu multiple level cache significantly faster increase speed want get code data cache one level actor model help think actor object cluster code data natural unit caching actor need communicate queue lmax team observed queue interfere caching explanation run like order put data queue need write queue similarly take data queue need write queue perform removal write contention one client may need write data structure deal write contention queue often us lock lock used cause context switch kernel happens processor involved likely lose data cache conclusion came get best caching behavior need design one core writing memory location multiple reader fine processor often use special highspeed link cache queue fail onewriter principle analysis led lmax team couple conclusion firstly led design disruptor determinedly follows singlewriter constraint secondly led idea exploring singlethreaded business logic approach asking question fast single thread go freed concurrency management essence working single thread ensure one thread running one core cache warm much memory access possible go cache rather main memory mean code working set data need consistently accessed possible also keeping small object code data together allows swapped cache unit simplifying cache management improving performance essential part path lmax architecture use performance testing consideration abandonment actorbased approach came building performance testing prototype similarly much step improving performance various component enabled performance test mechanical sympathy valuable help form hypothesis improvement make guide forward step rather backward one end testing give convincing evidence performance testing style however wellunderstood topic regularly lmax team stress coming meaningful performance test often harder developing production code mechanical sympathy important developing right test testing low level concurrency component meaningless unless take account caching behavior cpu one particular lesson importance writing test null component ensure performance test fast enough really measure real component writing fast test code easier writing fast production code easy get false result test nt fast component trying measure use architecture first glance architecture appears small niche driver led able run lot complex transaction low latency application nt need run million tps thing fascinates application ended design remove much programming complexity plague many software project traditional model concurrent session surrounding transactional database nt free hassle usually nontrivial effort go relationship database objectrelational mapping tool help much pain dealing database nt deal performance tuning enterprise application involves futzing around sql day get main memory server u old guy could get disk space application quite capable putting working set main memory thus eliminating source complexity sluggishness event sourcing provides way solve durability problem inmemory system running everything single thread solves concurrency issue lmax experience suggests long need le million tps enough performance headroom considerable overlap growing interest cqrs event sourced inmemory processor natural choice commandside cqrs system although lmax team currently use cqrs indicates nt go path always tricky question littleknown technique like since profession need time explore boundary starting point however think characteristic encourage architecture one characteristic connected domain processing one transaction always potential change following one processed transaction independent le need coordinate using separate processor running parallel becomes attractive lmax concentrate figuring consequence event change world many site taking existing store information rendering various combination information many eyeball find eg think medium site architectural challenge often center getting cache right another characteristic lmax backend system reasonable consider applicable would something acting interactive mode increasingly web application helping u get used server system react request aspect fit well architecture architecture go system absolute use asynchronous communication resulting change programming model outlined earlier change take getting used team people tend think programming synchronous term used dealing asynchrony yet long true asynchronous communication essential tool responsiveness interesting see wider use asynchronous communication javascript world ajax nodejs encourage people investigate style lmax team found took bit time adjust asynchronous style soon became natural often easier particular error handling much easier deal approach lmax team certainly feel day coordinating transactional database numbered fact write software easily using kind architecture run quickly remove much justification traditional central database part find exciting story much goal concentrate software model complex domain architecture like provides good separation concern allowing people focus domaindriven design keeping much platform complexity well separated close coupling domain object database always irritation approach like suggest way
501,Lobsters,scaling,Scaling and architecture,Binary Search Eliminates Branch Mispredictions,http://www.pvk.ca/Blog/2012/07/03/binary-search-star-eliminates-star-branch-mispredictions/,binary search eliminates branch mispredictions,lower bound people branch mispredictions entropy rwt common symptom conditionals created equal microbenchmarking mispredictions graphic time box plot let throw cache miss wrapping,searching short sorted vector fairly common task lot time data set small enough quicker simpler store sorted vector use alternative asymptotically faster insertion even use complex data structure often make sense specialise leaf pack data single node implement search two basic option linear binary search running bunch test claim decent binary search offer good robust performance nearly always right choice interpolation search could appropriate tad complicated short vector fail hard input arranged wrong data expected cache binary search pretty much always preferable binary search faster short one two cache line long vector good linear search best option data fit couple cache line focus one specific case interest searching short mediumlength sorted vector known size interested microoptimisations work vector unsigned value also assuming looking lower bound need people looked consider case use sizespecialised code also try represent whole distribution runtimes help u take long tail account usually willing take small hit average performance improvement consistency branch mispredictions entropy processor pipelined quite mispredicted conditional branch well known wrecking performance program branch predicted wrong work speculatively done far must thrown effect processor massive execution resource completely wasted moment branch decoded retired order execution improves thing bit end result course thing bad old one exhibited awesome combination preternaturally long pipeline slow conditional move slow shift extra latency instruction used condition flag think convey frustrating microarchitecture could mispredicted branch killed ipc conditional move slightly better portable bittwiddling trick generating mask sign bit sar carry flag via sbb slow believe linus torvalds posted quite rant topic rwt still think tell period left mark many programmer go great length avoid anything look like branch one common symptom notion normal linear search execute faster binary search even vector value reasoning branch linear search trivially predicted comparison always fails except last contrast comparison binary search hard predict lg n one extract one full bit information vector word linear search expected run better conditional extract le information predictor le entropy deal right better execute many nearly useless branch conditionals created equal sometimes really faster perform redundant work avoid mispredicted branch however definitely case linear versus binary search linear search pretty much implemented conditional branch really want abort early found looking clearcut binary search think standard exercise ask help much leave binary search early exact match size known ahead time remaining conditional update upper lower bound actually conditional move fixedlength binary search naturally executed without branch get predictionfriendly short actually expect linear search faster binary search conditional branch allow former leave search loop early linear search hope overtake binary search conditional branch microbenchmarking mispredictions tested hypothesis ghz gcc first linear lower bound search inverse linear search alwaysinline sizet linearsearch unsigned key unsigned vector sizet size unsigned size unsigned v vector v key return return sizespecialised version generated calling constant size argument gcc unrolled loop size lower seems reasonable larger size loop unrolled seems reasonable current special logic accelerate tiny compare conditional jump decrement loop body compiled unrolled linear search cfistartproc cmpl rsi edi jae cmpl rsi edi jae cmpl rsi edi jae cmpl rsi edi sbbq rax rax ret movl eax ret movl eax ret movl eax ret linear search walk data backwards high low address unusual also tested slightly complicated version search data forward forward linear search alwaysinline sizet fwdsearch unsigned key unsigned vector sizet size sizet size unsigned v vector v key return v key return sizespecialisation trick used binary search binary search ceiling alwaysinline unsigned lb unsigned long x x return return sizeof unsigned long builtinclzl alwaysinline sizet binarysearch unsigned key unsigned vector sizet size unsigned low vector unsigned lb size size unsigned mid low size mid key low size return low key low vector assume size power two track lower bracket search upper one always known offset return value check key smaller element vector rather assuming presence sentinel case happen test conditional branch always predicted right tried taking difference pretty much noise order one cycle note size could handled specialcasing first iteration setting midpoint size lb size midpoint becomes lower bound rest search range otherwise sequel performed wider necessary range still correct case number iteration regular binary search could slight loss performance due worse locality access pattern different size six iteration fully unrolled expected body simple cmpleacmov compiled unrolled binary search cfistartproc cmpl rsi edi leaq rsi rax cmovb rsi rax cmpl rax edi leaq rax rdx cmovae rdx rax cmpl rax edi leaq rax rdx cmovae rdx rax cmpl rax edi leaq rax rdx cmovb rax rdx movq rax cmpl rdx edi jae rep ret movq rdx rax subq rsi rax sarq rax ret finally also tested vectorised linear search using sse instruction implementation exploit vectorised comparison perform one test per cache line value fact vector length le mispredicted branch implementation actually broken performs signed rather unsigned comparison issue test performed vectorised search typedef int attribute vectorsize typedef float attribute vectorsize define pcmp key vals key vals sizet unsigned key unsigned vector vector key return key key key key key vals vector int mask pcmp key vals return builtinctz mask define bit unsigned mask pcmp key pcmp key mask bit mask return builtinctz mask sizet unsigned key unsigned vector vector key return key key key key key vals vector vals vals bit branch compiled away preliminary test always correctly predicted end prefetching data used couple instruction later similar trick used generate bitmask comparison finally even loop fully unrolled disassembly show conditional branch around return compiled away last iteration size loop fully unrolled compiled vectorised linear search cfistartproc cmpl edi rsi movq rax jbe rep ret movl edi rsp movd rsp pshufd movdqa pcmpgtd rsi pcmpgtd rsi movmskps eax movmskps edx sall eax orl edx eax xorb al orb al bsfl eax eax cltq ret graphic time first scenario search random key incache vector test search performed overhead benchmarking loop estimated minimal iteration time calling empty function cycle subtracted measured value finally extreme outlier value larger percentile scenario size algorithm removed bit overhead rdtscp exactly lightweight shorter search reveal much except complex current cpu also tried measuring couple search time lost interesting information let order execution mess result graphic overlay jittered scatter plot box plot density point reveals often execution time clock cycle observed box plot give u median first third quartile median robust singlenumber summary mean strange distribution still also included arithmetic mean without outlier thin grey line search algorithm left right forward linear search lin backward linear search inv vectorised forward linear search vec binary search bin size vector reported top general interesting transition happen around length especially forward backward linear search binary search scalar linear search longer element unrolled vectorised search work chunk element unsigneds fit exactly one cache line length binary search may access one two cache line depending data vectorised search start executing conditional branch box plot pretty clear except vector length two four benchmarking infrastructure probably dominates everything binary search always faster linear search single mispredicted branch latter hurt vectorised search always bit quicker scalar version performs nearly identically binary search size execute unpredictable branch distribution runtimes vectorised search exhibit distinct peak quartile might robust usually scatter plot also interesting first raise interesting question exactly going bimodal distribution vector length two four clue current cpu awfully complex stateful system strange interaction would guess combination backtoback call return benchmarking code interfering vector size distribution latency linear search distinctly multimodal search end quickly execute much faster effect batching cache line vectorised search also visible unpredictable branch executed length median time spread obviously increase quite lot also variation forward backward linear search frankly know access pattern impact might noise happens regular query help eliminate branch mispredictions obviously binary search affected expect linear search execute quickly leftmost four plot show happens linear search always find key eight iteration one vectorised search remaining two instead set key last position searched case binary search remains key found quickly first four plot scalar linear search approximately par binary search vectorised search faster hand whole vector must traversed two rightmost plot linear search take thrice much time binary search even vectorised search best comparable binary search comparing top distribution first plot one show mispredicted branch slow linear search cycle however still quicker overall take hit break search early compare median conditional branch speed linear search effect marked vectorised search still slower binary search plot branch always predicted correctly end good approximation fully vectorised search rather eliminating branch vectorised linear search always processing whole vector simpler faster go binary search mispredictions slow linear search traversing data completely even next plot summarises timing search succeed first second iteration scalar linear search almost best environment think linear search short always searching one value found first iteration even always succeed two iteration linear search slightly faster binary search slow run vectorised search shown best light always succeeds first iteration overall comparable linear search tighter spread much like binary search vectorising minimises branch cost additional work plot exercise vectorised search better element found third last iteration linear search binary search affected linear search become appreciably factor two four slower even regular query linear search best slightly faster binary search otherwise significantly slower data expected cached believe fair say binary search always used instead linear search even unrolled vectorised fully version worst case scalar linear search cycle slower decent binary search best case partially vectorised linear search save cycle sound like horrible bargain let throw cache miss data cached linear search win keep hammering query found first iteration data cached ran another set test time hopping around gb worth short vector vector aligned size vectorised search worked naturally also easily think cache line vector chosen picking vector different kb page every potential offset start page shuffling randomly foil prefetching pretty much worstcase scenario tlb miss latency tlb miss varies lot depending part page table still cached could also tried use huge page eliminate tlb miss think realistic huge page widely used tlbs miss tend issue even working set still big enough expect two thing timing order magnitude slower linear search eke speedup skipping access thanks conditional branch first set plot corresponds random search outofcache vector search even shortest vector slow enough somewhat meaningful observe latency two access cache line varies lot cycle distribution strongly multimodal clear comparing quartile always useful sometimes fall nearlyempty zone two peak would expect tiny change affect lot plot length eight similar drop size two four sequel forward linear search scalar vectorised bit faster backward one bit counterintuitive vector size fit one cache line latency cache miss always soon touch cache line useful lie usually good enough approximation real world chip load whole cache line atomically current socket hundred pin dedicated communicating dimms test executed singlecore bandwidth gb exactly bandwidth one module sends data chunk bit one cache line loaded bit time cpu memory controller first load chunk corresponding instruction triggered cache miss rest cache line controller simply seems tuned program load data low high address binary search seems tiny bit quicker search size fare badly vector unsigneds exactly one cache line surprising fewer correctly aligned element binary searched quickly case benefit implicit data dependency binary search key fall latter half vector half read initial midpoint first element latter half unsigneds fit exactly two cache line binary search thus behaves similarly linear search read one cache line half time otherwise two explain solid peak occurrence around cycle though finally data dependency help much vector length half four cache line still two binary search significantly slower variation execution time must still caused branch mispredictions following plot show happens key always found eight iteration scalar linear search one iteration vectorised search binary search also slightly advantaged size key found latter half vector rightmost two plot fix key found last iteration search disadvantage binary search searching minimum value data key found early forward linear search seems tiny bit faster rest even vectorised search trade memory access fewer branch interestingly key always found end linear search affected much speculative execution seems job hiding latency last plot correspond situation key found one two iteration equiprobably linear search one vectorised version last cache line binary search column show latency worse scenario key found iteration linear search three four vectorised search first quarter data binary search good case search seem similarly except vector length binary search really suffers fact comparing plot reveals binary search affected variation query much linear search bad memory access pattern hurt lot mispredicted branch end look like search behave pretty much searching uncached vector spanning two cache line linear search better access pattern asymptotics catch wrapping performance matter linear search preferred binary search minimise branch mispredictions linear search used expected bail quickly situation property useful involve uncached mediumsize vector general stick binary search easy get good consistent performance simple portable implementation case might worth trouble go heroically optimised linear search maintenance may bitch expected working larger potentially uncached datasets instead try exploit cache topic another post friend mine pointed one case fully vectorised linear search without early exit may highperformance option cryptographic code data cached even binary search leak information via number distinct cache line accessed also completely ignored question shared resource memory channel particularly multicore chip happens throughput multiple core executing kind search workload probably leave someone else
502,Lobsters,scaling,Scaling and architecture,Designing successful Web app load tests (best practices from Toms Senart),http://dotgo.sourcegraph.com/post/99636152128/tomas-senart-designing-successful-web-app-load-tests,designing successful web app load test best practice toms senart,evan culver segment,evan culver build developer tool segment customer data platform let product manager software team understand user data evan career spanned many year software stack frontend ui development infrastructure ops past five year focus developer tooling infrastructure worked tenure uber hypergrowth year dev tool team segment charter empower engineer segment tool automate optimize streamline workflow episode explains beyang exactly mean discussing segment use technology aws ecosystem popular opensource secret management tool created chatops various docker kubernetesbased tool useful managing deployment many microservices
503,Lobsters,scaling,Scaling and architecture,Fallacies of Distributed Computing Explained,http://www.rgoarchitects.com/Files/fallacies.pdf,fallacy distributed computing explained,,obj endobj obj endobj obj procset pdftextimagebimagecimagei annots r mediabox content rgroup structparents endobj obj stream v lw w endstream endobj obj endobj obj endobj obj endobj obj endobj obj structparent endobj obj endobj obj endobj obj endobj obj endobj obj procset pdftextimagebimagecimagei annots r mediabox content rgroup structparents endobj obj stream r rx q ho p xv nu   w e vm c zgjo v endstream endobj obj structparent endobj obj endobj obj r endobj obj endobj obj endobj obj endobj obj procset pdftextimagebimagecimagei annots r r r r r r mediabox content rgroup structparents endobj obj stream w q g  sg k q ha ihr u z endstream endobj obj structparent endobj obj endobj obj r endobj obj endobj obj endobj obj endobj obj endobj obj endobj obj structparent endobj obj structparent endobj obj structparent endobj obj structparent endobj obj structparent endobj obj procset pdftextimagebimagecimagei mediabox content rgroup structparents endobj obj stream q b  h pb f f endstream endobj obj procset pdftextimagebimagecimagei annots r mediabox content rgroup structparents endobj obj stream ad  n k j  n q endstream endobj obj structparent endobj obj endobj obj endobj obj procset pdftextimagebimagecimagei annots r r mediabox content rgroup structparents endobj obj stream e  x q jd dqye jj cz c p endstream endobj obj structparent endobj obj structparent endobj obj procset pdftextimagebimagecimagei annots r r mediabox content rgroup structparents endobj obj stream  j hoe
504,Lobsters,scaling,Scaling and architecture,The Tail at Scale: Achieving Rapid Response Times in Large Online Services,https://www.youtube.com/watch?v=C_PxVdQmfpk&list=PL9Jh2HsAWHxKAIZ5n-PkPW3CHg2wI-MWb,tail scale achieving rapid response time large online service,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature
505,Lobsters,scaling,Scaling and architecture,The Surprising Path to a Faster NYTimes.com,https://speakerdeck.com/nytdevs/the-surprising-path-to-a-faster-nytimes-dot-com,surprising path faster nytimescom,velocity new york,current mantra performance thinking tool rule premise simple path faster website fast request interact paint animation script execution tool part solution new york time discovered performance truly understanding product user sum total site following approach lead surprising result new york time underwent major redesign involved rewrite entire technology stack product team bought idea performance goal mandated part product success implemented many community best practice biggest win little surprising first glance counter community best practice frontend software architect eitan koningsburg cover change worked got presented velocity new york
506,Lobsters,scaling,Scaling and architecture,Google: Taming the Long Latency Tail - When More Machines Equals WorseResults,http://highscalability.com/blog/2012/3/12/google-taming-the-long-latency-tail-when-more-machines-equal.html,google taming long latency tail machine equal worse result,luiz andr barroso warehousescale computing entering teenage decade google like request level parallelism easy wimpy core require parallelization hard wimpy cpu scale property warehouse scale computing require reliably low latency warehouse scale computing datacenter computer latency tail distribution latency suddenly majority query slow take greater second high performance equal high tolerance networking example latency tail microstorm teacup suffering long tail effect spdy like per second measurement cut real engineering flash high latency may save world flash bridge gap performance idiosyncrasy increase latency variance disk suck random io flash good random io better random io story disk latency improve paper review warehousescale computing entering teenage decade three age google batch warehouse instant software technique tree distribution response google strategy tree distribution request response fanin level tree manageable response filtering data reduction collocation focus middleware faulttolerance magical study unpredictability show inherent unpredictability system arises magical remote invocation focusing statistical performance indicator percentile endtoend latency latency tied blocking rather queueing moving network server latency disk speed curve headofline blocking short request often served much higher latency much larger request addressing blocking issue related article,likewise current belief case artificial machine large small equally feasible lasting manifest error thus example small obelisk column solid figure certainly laid set without danger breaking large one go piece slightest provocation purely account weight galileo galileo observed thing broke naively scaled interestingly google noticed similar pattern building larger software system using technique used build smaller system luiz andr barroso distinguished engineer google talk fundamental property scaling system fascinating talk warehousescale computing entering teenage decade google found larger scale greater impact latency variability request implemented work done parallel common today service oriented system overall response time dominated long tail distribution parallel operation every response must consistent low latency overall operation response time tragically slow implication high performance equal high tolerance mean entire system must designed exacting standard forcing deeper look latency variability advent interactive realtime computing responsiveness becomes key good average response time nt good enough simply ca nt naively scale technique build larger system reason surprising deep implication design service dominated system google like request level parallelism easy google really like request level parallelism query sent many machine code run machine need parallelized really nice simple answer one query example instant search instantly answer query easy code wimpy core require parallelization hard wimpy cpu cpu slow order really energy efficient going get point going need parallelize piece code harder matter make cpu infinitely energy efficient responsible budget improve energy efficiency cost term engineering time performance going wimpy hard code scale property warehouse scale computing require reliably low latency warehouse scale computing term google invented capture idea datacenter computer warehousescale computing considers computer resource fungible interchangeable location independent individual computer lose identity become part service really important google aggregate performance entire system much work parallel vision put novel performance restriction warehouse whole particularly latency tail luiz illustrates example imagine client making request single web server ninetynine time hundred request returned within acceptable period time one time hundred may say disk slow reason look distribution latency small one tail end large bad really mean one customer get slightly slower response every let change example instead one server server request require response server change everything system responsiveness suddenly majority query slow take greater second bad using component scaling result really unexpected outcome fundamental property scaling system need worry latency tail latency longer event system high performance equal high tolerance scale ignore tail latency networking example latency tail microstorm teacup suffering long tail effect good discussion impact long tail problem trading long tail distribution trading network represent distribution latency experienced trade majority trade experience low latency le millisecond small number experience trade latency upwards latency symptomatic long tail anomaly bandwidth saturating microburst latency could come rcp library dns lookup packet loss microbursts deep queue high task response latency locking garbage collection o stack issue routerswitch overhead transiting multiple hop slow processing code good discussion sensitive tcp packet loss buffering take look spdy like see tricky problem degug correct stackexchange microburst problem devil find per second measurement cut implication real engineering anyone stand web server make decent website building something large completely different kind take real skill incredible attention detail every level every part system must work reliably within bounded tolerance time everyone make work flash high latency may save world might expect flash would enter save world fast luiz explains flash great also depressing time great random read order magnitude faster ram compared disk flash bridge gap ram disk term latency bandwidth especially bandwidth depressing flash really glorified eeprom really write erase whole chunk begin reprogramming erasing incredibly slow also try minimize number era creates kind performance idiosyncrasy disk drive flash read data microsecond read stuck behind erase may wait millisecond increase latency variance particular variance used fast effect real impact latency tail strange way disk better flash scale disk suck random io flash good random io flash win end flash better random io story disk disk keep getting better better term space little faster sequential access stayed constant random access random readssec disk continue get bigger cheaper per gb basis nt matter drive getting big actually use extra capacity effectively software need find way mitigate problem flash latency improve paper review warehousescale computing entering teenage decade andrew wang good summary current latency situation io latency variability right terrible basically durable storage displaying long latency tail random access spinning disk slow flash writes slow highlatency event muck latency potentially fast event network io suffers similar problem using tcp interrupt add order magnitude latency network request making fast network hardware slow software summarize two big idea talk first latency variation latency key performance metric service day today webbased application demand provide good user experience may require reexamination lot fundamental assumption io second increasing utilization resource cluster important efficiency performance standpoint server hardware fungible resource easily shared among different service accordance high tolerance theme luiz ambitious research development agenda squeezing latency entire stack three age google batch warehouse instant really surprised google simply ripped layer run abstraction directly hardware used simpler networking technology etc warehouse computer making custom system software technique nt talk found interesting related toss tree distribution response article ago google strategy tree distribution request response idea create tree node root node talk number parent node parent node talk number leaf node request pushed tree parent hit subset leaf node solution fanin level tree manageable cpu cost processing request response spread across parent reduces cpu network bottleneck response filtering data reduction ideally parent provide level response filtering root see subset response data reduces network cpu needed root collocation parent collocated leaf one rack keep traffic datacenter network focus middleware faulttolerance magical study unpredictability present extensive empirical study unpredictability distributed system ranging simple transport protocol faulttolerant middlewarebased enterprise application show inherent unpredictability system arises magical remote invocation magical suggests emergent behavior middleware system strictly predictable enterprise application could cope inherent unpredictability focusing statistical performance indicator percentile endtoend latency latency tied blocking rather queueing moving network server latency disk speed curve nd faster processor improve server capacity little effect latency experimenting workload various size determine disk access occur mean median latency increase though median unaffected trace root problem headofline blocking within lesystemrelated kernel queue behavior turn cause batching burstiness little impact throughput severely degrades latency examining individual request latency nd blocking give rise phenomenon call service inversion request served unfairly service inversion short request often served much higher latency much larger request also nd phenomenon increase load responsible growth server latency addressing blocking issue apache flash server improve latency order magnitude demonstrate qualitatively different change latency proles resulting server also exhibit higher capacity lower burstiness fair request handling across wide range workload finally result show serverinduced latency tied blocking effect rather queuing related article
507,Lobsters,scaling,Scaling and architecture,Everything I Ever Learned about JVM Performance Tuning @twitter,http://www.infoq.com/presentations/JVM-Performance-Tuning-twitter,everything ever learned jvm performance tuning twitter,infoq homepage presentation everything ever learned jvm performance tuning twitter summary bio conference related sponsored content,infoq homepage presentation everything ever learned jvm performance tuning twitter everything ever learned jvm performance tuning twitter summary attila szegedi discus performance problem encountered twitter running java scala application presenting solve jvm tuning bio attila szegedi software engineer working twitter core system library group author dynalink dynamic linker framework language jvm contributor mozilla rhino javascript runtime jvm well one principal developer freemarker templating language runtime conference qcon practitionerdriven conference designed team lead architect project management program includes two tutorial day led industry expert author three conference day track speaker covering wide variety relevant exciting topic software development today event u similar opportunity learning networking tracking innovation occurring enterprise software development community recorded dec related sponsored content
508,Lobsters,scaling,Scaling and architecture,Scalable A/B Tests at Pinterest,http://engineering.pinterest.com/post/95378137929/scalable-a-b-experiments-at-pinterest,scalable ab test pinterest,win learn,win learn pinterest growing learning experimentation believe spirit exploration inside pinterest wall learn success epic failure
509,Lobsters,scaling,Scaling and architecture,breadis: Transparent sharding/failover-handling proxy for redis and redis sentinel,https://github.com/mediocregopher/breadis,breadis transparent shardingfailoverhandling proxy redis redis sentinel,breadis usage use case,breadis proxy redis cluster automatically handle command redirection failover detection client act like redis server change necessary redis client communicate usage compile go build see commandline option breadis help load commandline option config file breadis config file see example config file breadis example use case breadis act simple proxy entire redis cluster acting exactly normal single redis instance existing redis driver already talk breadis might seem bit silly since could use clusteraware driver application interact redis cluster two downside know interacting redis cluster first place may always case instance dev environment application commmunicates single redis production cluster multiple box something application differentiate instead dev environment could keep using single local redis production application server could breadis listening port meaning application would nt know difference redis cluster handling nt trivial work best persistant process managing may possible stateless request based language eg php executing bunch oneoff script use redis cluster case breadis instance handle hard part application hit instead hard part
510,Lobsters,scaling,Scaling and architecture,Profiling PHP Part 1: Intro to Xhprof & Xhgui,https://blog.engineyard.com/2014/profiling-with-xhprof-xhgui-part-1,profiling php part intro xhprof xhgui,profiling profile code cloning environment engine yard cloud common cause slowdown profiler active v passive profiling xhprof xhgui installing installing xhprof pecl installing xhgui http githubcomperftoolsxhgui alternative configuration note running xhgui running profiler must next next installment indepth look xhgui,profiling profiling measuring relative performance application codelevel profiling capture thing like cpu usage memory usage time number call per function well capturing call graph act profiling impact performance differs benchmarking benchmarking performed externally measure actual performance endusers see profile code first thing determine thinking profiling performance issue need quantify ask big problem fall trap premature optimization may well waste time determine even problem decide goal performance example concurrent user le response time need benchmark see meeting goal one common mistake people make benchmarking development staging environment must perform benchmark production hardware either actual production live copy latter easily achieved cloud see cloning environment engine yard cloud many option benchmarking including ab siege jmeter personally prefer featureset jmeter ab siege much simpler use determined indeed performance issue profile code make fix benchmark see issue gone away benchmark every individual change make many change might find performance negatively impacted idea specific change caused problem call performance lifecycle common cause slowdown number common cause slowdown contrary people might think even highlevel language like php code rarely issue unlikely cpubound problem arise today hardware common cause datastore postgresql mysql oracle mssql mongodb riak cassandra memcache couchdb redis external resource apis filesystems network socket external process bad code profiler two distinct type profilers phpworld active passive active v passive profiling active profilers used development enabled initiated developer active profilers gather information passive profilers impact performance larger way active profilers used production scenario xdebug active profiler inability use active profilers production facebook introduced passive profiler xhprof xhprof built usage production environment minimal impact performance still gathering enough information diagnose performance issue xhprof new relic passive profilers typically additional data gathered xdebug needed general performance problem meaning passive profilers better option alwayson profiling even development xhprof xhgui xhprof created facebook includes basic ui reviewing profile data additionally paul reinheimer created xhgui enhanced ui project reviewing comparing analysing profile data installing installing xhprof xhprof available via pecl installation simple pecl install xhprofbeta pecl command try update configure phpini automatically file pecl try update found using following command pecl configget phpini simply add new configuration line top indicated file may want move appropriate location extension compiled must enable need add following php ini file xhprof extensionxhprofso combine xhgui easily perform profiling review installing xhgui install xhgui must directly git project found github http githubcomperftoolsxhgui xhgui requires php extmongo composer mongodb optional collection required analysis first clone project wherever want live debianbased eg ubuntu others linux machine might varwww mac o x might librarywebserverdocuments cd varwww git clone http githubcomperftoolsxhguigit cd xhgui php installphp last command run composer install dependency check permission xhgui cache directory fails run composer install hand next may need create config easily done using default config found pathtoxhguiconfigconfigdefaultphp running mongodb locally without authentication likely need fault back default multiserver environment however want single remote mongodb server server store need configure appropriately enhance mongodb performance add index running following mongo use xhprof dbresultsensureindex metaserverrequesttime dbresultsensureindex profilemain wt dbresultsensureindex profilemain mu dbresultsensureindex profilemain cpu dbresultsensureindex metaurl alternative configuration wish install mongo production environment difficulty enabling access web server mongo server save profile data disk import local mongodb later analysis change following configphp php savehandler file savehandlerfilename pathtoxhguixhprof uniqid true dat changing savehandler file uncommenting savehandlerfilename setting appropriate value note default save one profile per day ready analyze data import using script included xhgui php pathtoxhguiexternalimportphp pathtofiledat point process running xhgui xhgui php web application setup standard virtualhost using pathtoxhguiwebroot document root alternatively simply use php cliserver like cd pathtoxhgui php webroot make xhgui available via port network interface running profiler run profile need include externalheaderphp script page wish profile easily setting autoprependfile php ini setting done directly global ini file limit single virtual host apache add following virtualhost phpadminvalue autoprependfile pathtoxhguiexternalheaderphp nginx add following server block fastcgiparam phpvalue autoprependfilepathtoxhguiexternalheaderphp using php cliserver php must pas setting via command line flag php dautoprependfilepathtoxhguiexternalheaderphp default profiler run approximately run controlled following code snippet externalheaderphp php rand return want run every request example development simply comment want run run could change php rand return allows profile small portion user request impacting individual user much many user overall want manually control profiling occurs could use something like php isset request isset cookie return else remove trace special variable requesturi server requesturi strreplace array server requesturi setcookie isset request setcookie time return check randomly named getpostcookie variable case set cookie name allow profiling part request example redirects form submission ajax request etc furthermore allows getpost value remove cookie stop profiling next next installment take indepth look xhgui ui reviewing comparing data xhprof
511,Lobsters,scaling,Scaling and architecture,ONS2014 Keynote by Amin Vahdat about networking at Google,https://www.youtube.com/watch?v=n4gOZrUwWmc,keynote amin vahdat networking google,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature keynote amin vahdat google youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature keynote amin vahdat google youtube
512,Lobsters,scaling,Scaling and architecture,"Tumblr: Hashing Your Way to Handling 23,000 Blog Requests perSecond",http://highscalability.com/blog/2014/8/4/tumblr-hashing-your-way-to-handling-23000-blog-requests-per.html,tumblr hashing way handling blog request per second,michael schenck related article,guest post michael schenck sre staff engineer tumblr tumblr blog tumblelog one highly trafficked face internet one convenient aspect tumblelogs highly cacheable nature fantastic high viewspost ratio tumblr network offer user said entirely trivial scale perimeter proxy tier let alone caching tier necessary serving request article describes architecture portion perimeter responsible blog serving one highly trafficked perimeter endpoint stats employee total team responsible tumblr perimeter perimetersre including one manager server le used blog serving functionality blog request per second peak blog cache purge event per second peak million blog billion post platform mapbased partitioning early day needed one active one standby proxy server well varnish node easy manage monitor make highly available however inline user request capacity limit reached next step even ideal deployment need ready wind downtime due popularity outgrowing single proxy node outgrowing single active proxy server pretty common often involves dns something basic roundrobin record might meet need often time worth extra money go healthchecking gslb configuration even one geographic location downside dns nameservers respond ip fairly even rate guarantee lookup used number request user might make request resolved ip single minute bot b might make request minute two ip user get one bot b get two client making request one proxy request rate effect mitigated lower ttls second ttl balance request two proxy first second user go proxy bot b go proxy next resolution might swap ip user send request proxy bot b send request proxy end minute window proxy would handled roughly request downside lower ttls lookup thus higher dns cost dns typically one le expensive thirdparty service outgrowing single varnish node dns buy lot time growing proxy tier scaling varnish little complex even capacity limitation single varnish node revolves around request concurrency simply adding two varnish node roundrobin want reduces cachehit ratio make purging resourcetime intensive nt actually increase working size cache duplicate simplest iteration outgrowing single varnish node static partitioning involves determining unique identifier split space two varnish node tumblelogs hostname blog since dns caseinsensitive worry character alphanumerics az four allowed character two varnish node blog hostnames split first character two cache node evenly distributed partitioning consistent hashing previous example dns roundrobin static partitioning step right direction provide coarse granularity partitioning small enough scale granularity nt necessarily problematic traffic start grow variance becomes significant result reducing variance traffic handled leasthot mosthot node becomes moreandmore important consistent hashing really shine partitioning proxy traffic server environment influence routing table router infront server router user proxy server take advantage equalcost multipath routing ecmp ecmp affords ability treat proxy slice consistent hash ring map requester across slice accomplished informing routing infrastructure multiple path proxy server particular destination ip highly available ip ecmp hash request source order determine proxy receive packet request session typical ecmp implementation offer layer iponly layer ip port hashing option layer mean request particular ip go particular proxy helpful debugging imbalanced large network using single nat ip layer typically provides best distribution debugging particular client becomes challenging number way inform router multiple path suggest using ospf ibgp dynamic route advertisement one need listen highlyavailable ip loopback interface enable internal routing advertise one ip nexthop highly available ip found bird provides lightweight reliable mean performing route advertisement proxy partitioning varnish traffic tumblelogs identified fully qualified domain name fqdn uri path blog always found blog fqdn majority tumblelogs subdomains tumblrcom engineeringtumblrcom however also support user bringing custom domain name considering variety fqdns clear tld least number variation domain name particularly due vast majority tumblrcom subdomains mostsignificant bit appear leftmost position variablelength string understanding problem domain perfect demonstrates hashing function perfect applied test dataset consistenthdr consistent hashing host header best realworld result consistenthdrusedomainonly consistent hashing base domain name ie tumblrcom foonet two camp tumblrcom allothers mapbasedfirstchar mapping host header first character varnish node original static partitioning implementation mapbasedhdr map based host header consistent hashing clear frontrunner mosteven distribution tumblelog fqdns went determine hash function appropriate haproxy us sdbm hashing function default however investigation comparing sdbm crc determined offered even better distribution resulting submitting patch haproxy make hash function configurable see thanks section information comparing static partitioning consistent hashing show change variance request per second varnish node moving bestfit hash function additional consideration node growth either model node growth mean keyspace shift thus cache invalidation consistent hashing model much easier predict percent key invalidated essentially n number cache node prior new node added static partitioning model unless analysis request going node taking keyspace left worst case lessthan equalto total percent key node taking keyspace node failure static partitioning single node failure result key inaccessible unless provide failover option haproxy allow standby node decision make cache node one active one standby key space shared standby node one extreme waste hardware end spectrum standby node shared active node mean two failed node result standby support keyspace active node consistent hashing node failure handled automatically one node removed key shifted resulting key invalidated even rise keyspace per remaining active node purging cache sending purge request single varnish node easy purging multiple varnish node easy instead keep proxy purgers sync easiest send purge request proxy important reject purge attempt nonlocal ip space prevent malicious bulk purging lesson learned know answer question nt asked yet facing scaling challenge nt look pattern already using elsewhere scale simplicity adding complexity overcome scalability challenge may work short run eventually catch know hash function hash function use important deciding hash degrade fail advisable proxy monitor ability reach backends nt stop advertising route advertise nonpreferential route higher path cost ospf way backends become unhealthy still serve error page instead becoming unreachable thanks related article
514,Lobsters,scaling,Scaling and architecture,Impossible Engineering Problems Often Aren't,http://blog.scalyr.com/2014/07/impossible-problems-often-arent/,impossible engineering problem often nt,,exploding data complexity modern stack result explosion event data make sense complexity ingest hundred terabyte day tap data second le second visualize analyze trend improve fix
515,Lobsters,scaling,Scaling and architecture,Introducing Ark: A Consensus Algorithm For TokuMX and MongoDB,http://www.tokutek.com/2014/07/introducing-ark-a-consensus-algorithm-for-tokumx-and-mongodb/,introducing ark consensus algorithm tokumx mongodb,tokumx ark paxos raft tech report design ark aphyr analyzing mongodb jepsen series cap theorem currently case ark implemented branch paper heavy reading simple explanation done shortcoming ark community,time blog post explain great mongodb improvement already shipped tokumx sometimes though fun talk coming soon especially user feedback would really help get feature right next series blog post get geek talk feature developing personally find really exciting arkwhat ark ark implementation consensus algorithm also known election similar paxos raft working handle replica set election failovers tokumx many similarity raft also big differenceshere tech report explains algorithm provides proof correctness please download tell u thinkwhy design ark short fix known problem election protocol used tokumx mongodbas many know mongodb existing election protocol issue kyle kingsbury known aphyr twitter showed basic behavioral problem analyzing mongodb part jepsen series blog post analyzing impact network partition distributed database conclusion arrived mongodb neither ap cp considered context cap theorembecause tokumx inherited mongodb election protocol inherited problem want fix themour main goal modify election protocol make tokumx true cp system face network partition tokumx remain consistent mean ensuring write successfully acknowledged majority write concern never lost face network partition currently case tokumx mongodbadditionally want fix known user experience issue know along issue discovered analyzing code high level goal improve failover behavior tech report linked detail issue see approach fixing themis ark implemented currently yes implementation github branch tokumxbut yet ship itthat paper heavy reading simple explanation done yet next series post explaining layman term algorithm smaller digestible piece stay shortcoming ark nothing perfect hope community give u feedback done biggest shortcoming ark currently address replica set reconfigurations raft paper addressed reconfigurations yet looking improve area step short busy need manage project incrementally want get important first step fixing majority write concern important issue done addressing hope rare scenario handling configuration change network partitionswhat community short give u feedback feedback feedback reading tech report love hear feedback reading code love hear anyone would like run code please let u know happily provide notforproduction binary containing new algorithmso short tell u anythingrelated
516,Lobsters,scaling,Scaling and architecture,Revisiting 1 Million Writes per second,http://techblog.netflix.com/2014/07/revisiting-1-million-writes-per-second.html,revisiting million writes per second,revisiting million writes per second benchmarking cassandra scalability aws million writes per second node count software version configuration c cluster client cassandra stress network topology zoneaware astyanax tokenaware latency throughput measurement priam atlas latency read write throughput read write test write cl one,revisiting million writes per secondby christos kalantzisin article posted november benchmarking cassandra scalability aws million writes per second showed cassandra c scale linearly add node clusterwith advent new instance type decided revisit test unlike initial post interested proving c scalability instead looking quantify performance newer instance type providewhat follows detailed description new test well throughput latency result testsnode count software version configurationthe c clusterthe cassandra cluster ran datastax enterprise incorporates c c cluster node instance type used ran jvm set heap o ubuntu lts data log mount point mount point notice previous test used instance test although could similar write throughput result le powerful instance type production majority cluster read write choice ssd backed instance type realistic better showcase read throughput latenciesthe full schema follows create keyspace placementstrategy networktopologystrategyand strategyoptions useast durablewrites true use create column family columntype standard comparator asciitype defaultvalidationclass bytestype keyvalidationclass bytestype readrepairchance dclocalreadrepairchance populateiocacheonflush false gcgrace mincompactionthreshold maxcompactionthreshold replicateonwrite true compactionstrategy orgapachecassandradbcompactionsizetieredcompactionstrategy caching keysonly columnmetadata columnname validationclass bytestype columnname validationclass bytestype columnname validationclass bytestype columnname validationclass bytestype columnname validationclass bytestype compressionoptions sstablecompression notice mincompactionthreshold maxcompactionthreshold set high although set parameter exactly value production reflect fact prefer control compaction take place initiate full compaction schedulethe clientthe client application used cassandra stress client node instance type used instance type half core instance used previous test however instance still able push load using le thread required reach throughput almost half price client running jvm ubuntu ltsnetwork topologynetflix deploys cassandra cluster replication factor also spread cassandra ring across availability zone equate c rack amazon availability zone az way event availability zone outage cassandra ring still copy data continue serve requestsin previous post client launched az differs actual production deployment stateless application also deployed equally across three zone client one az attempt always communicate c node az call zoneaware connection feature built astyanax netflix c java client library speed enhancement astyanax also inspects record key sends request node actually serve token range record written read although c coordinator fulfill request node part replica set extra network hop call making tokenaware requestssince test us cassandra stress use tokenaware request however simple grep awkfu test zoneaware representative actual production network topologylatency throughput measurementswe documented use priam sidecar help token assignment backup restores internal version priam add extra functionality use priam sidecar collect c jmx telemetry send insight platform atlas adding functionality open source version priam coming weeksbelow jmx property collect measure latency throughputlatencyavg ile coordinator latenciesread storageproxymbeangetrecentreadlatencyhistogrammicros provides array avg ile calculatedwrite storageproxymbeangetrecentwritelatencyhistogrammicros provides array avg ile calculatedthroughputcoordinator operation countread storageproxymbeangetreadoperations write storageproxymbeangetwriteoperations testi performed following test full write test cl onea full write test cl quoruma mixed test writes read cl onea mixed test writes read cl writeunlike original post test show sustained million writessec many application write data however possible use type footprint telemetry system backend internet thing iot application data fed bi system analysiscl onelike original post test run cl one average coordinator latency little millisecond percentile millisecond
517,Lobsters,scaling,Scaling and architecture,Kafka Producer Pipeline for Ruby on Rails,http://www.shopify.com/technology/14909841-kafka-producer-pipeline-for-ruby-on-rails,kafka producer pipeline ruby rail,,shopify world fastestgrowing commerce platform plan slow working ship quality instead time team continuously deploy new code large scale support hundred thousand online store hundred million request day entrepreneur household brand depending u livelihood tough incredibly rewarding responsibility looking passionate problem solver looking
518,Lobsters,scaling,Scaling and architecture,Crash-only Software,https://www.usenix.org/legacy/events/hotos03/tech/full_papers/candea/candea.pdf,crashonly software,,obj linearized h l e n endobj xref n n n n n n n n n n n n trailer size info r encrypt r root r prev id startxref eof obj type catalog page r outline r endobj obj filter standard r u p v endobj obj filter flatedecode length r stream sbp cm s endstream endobj obj endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text font r r extgstate r r endobj obj type font subtype encoding macromanencoding basefont timesroman endobj obj length filter flatedecode stream ec r endstream endobj obj type font subtype encoding winansiencoding basefont helvetica endobj obj type extgstate sa false sm tr identity endobj obj type extgstate sa true sm tr identity endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r r r r r r r r r r r xobject r extgstate r colorspace r endobj obj length filter flatedecode stream b q fu cjq rg
519,Lobsters,scaling,Scaling and architecture,"StackOverflow Update: 560M Pageviews a Month, 25 Servers",http://highscalability.com/blog/2014/7/21/stackoverflow-update-560m-pageviews-a-month-25-servers-and-i.html,stackoverflow update pageviews month server,update stackexchange architecture stackoverflow take run stack overflow stats platform ui server ssds change use search high availability databasing coding caching dapper deploying teaming budgeting testing monitoring logging opserver clouding performance feature heavy emphasis performance lesson learned use redis use m product gabeech overkill strategy ssds rock know readwrite workload keeping thing efficient mean new machine needed often afraid specialize need done reinvention ok go bare metal bureaucracy garbage collection driven programming cost inefficient code higher think related article,folk stack overflow remain incredibly open time another update stack overflow network site make stackexchange includes stackoverflow ranked traffic world site growing rate month million user million answer million pageviews month server everything high availability load balancing caching database searching utility function relative handful employee quality engineering update based architecture stackoverflow video marco cecconi take run stack overflow post nick craver addition merged comment various source doubt detail date meant write article long ago still representative stack overflow still us microsoft product microsoft infrastructure work cheap enough compelling reason change yet pragmatic use linux make sense purity push make everything linux keep everything microsoft efficient stack overflow still us scaleup strategy cloud site sql server loaded gb ram ssd aws would cost fortune cloud would also slow making harder optimize troubleshoot system issue plus need horizontal scaling strategy large peak load scaling make sense problem quite successful sizing system correctly appears jeff atwood quote hardware cheap programmer expensive still seems living lore company marco ceccon talk say talking architecture need answer question first kind problem solved first easy part stackexchange take topic creates community around creates awesome question answer site second part relates scale see next stackexchange growing quite fast handle lot traffic let take look stats stackexchange network site growing rate month million user million question million answer network site traffic world year year growth million pageviews month peak like requestssec weekday programming profession mean weekday significantly busier weekend server tb sql data stored ssds web server ssds raid elasticsearch box gb also using ssds stack overflow readwrite ratio db server average cpu utilization web server using ii load balancer active using haproxy active database node using m sql application server implementing tag engine anything searching tag hit machine search elasticsearch machine distributed cache messaging using redis network nexus fabric extenders cisco asas think firewall cisco router readonly sql server used mainly stack exchange api vms also perform function like deployment domain controller monitoring ops database sysadmin goody etc platform ui ui message inbox sent message get new badge receive message significant event etc done using websockets powered redis search box powered elasticsearch using rest interface many question impossible show newest question would change fast question every second developed algorithm look pattern behaviour show question would interest us complicated query based tag specialized tag engine developed server side templating used generate page server server much cpu load low calculated could run server database server except burst performing backup low database server ram web server cpu usage scaleup still working scaleout site similar number pageviews tend run server simple system built net project others system reason project compilation lightning fast requires planning beginning compilation take second single computer line code small number given minimalist approach come problem one problem many test test needed great community metastackoverflow discussion site community bug reported metastackoverflow also beta site new software user find problem report bug found sometimes solutionpatches window used new york upgrading oregon already linux system centos load really almost server metastackexchangecom metastackoverflowcom development tier server also run around cpu mean quite bit headroom available ssds intel default web tier etc intel mid tier writes like elastic search intel database tier simply successor high endurance series exclusively raid raid array drive failure problem even hundred intel ssds production single one failed yet one spare part kept model multiple drive failure nt concern elasticsearch performs much better ssds given writesreindexes frequently ssd change use search lucenenet handle concurrent workload due locking issue moved elasticsearch turn lock around binary reader really nt necessary ssd environment scaleup problem far ssd space sql box due growth pattern reliability v space nonconsumer space isdrives capacitor power loss high availability main datacenter new york backup datacenter oregon redis slave sql replica tag engine node elastic node service high availability well exists data center everything slaved data center temporary cache data needed eat bandwidth syncing etc big item still shared cache case hard active data center start without cache possible nt graceful nginx used ssl transition made using haproxy terminate ssl total http traffic sent total traffic sent replication happening secondary data center oregon well vpn traffic majority traffic data replication sql replica redis slave oregon databasing m sql server stack exchange one database persite stack overflow get one super user get one server fault get one schema approach different database effectively form partitioning horizontal scaling primary data center new york usually master readonly replica cluster also readonly replica async dr data center oregon running oregon primary new york replica readonly async wrinkle one network wide database thing like login credential aggregated data mostly exposed stackexchangecom user profile apis career stack overflow stackexchangecom area unique database schema schema change applied site database time need backwards compatible example need rename column worst case scenario multiple step process add new column add code work column back fill new column change code work new column remove old column partitioning required indexing take care everything data large enough something warrant filtered index make way efficient indexing deletiondate null common pattern others specific fk type enums vote table per item example table post vote table comment vote page render realtime caching anonymous user given cache update requery score denormalized querying often needed id date post vote table row currently query millisecond due indexing tag engine entirely selfcontained mean depend external service core functionality huge inmemory struct array structure optimized use case precomputed result heavily hit combination simple window service running box working redundant team cpu almost always three box needed load redundancy fail local web server load tag engine memory keep going dapper lack compiler checking query compared traditional orm compiler checking told database look like help lot thing still fundamental disconnect problem get runtime huge problem tradeoff generated sql nasty finding original code came often nontrivial lack ability hint query control parameterization etc also big issue trying optimize query example literal replacement added dapper help query parameterization allows use thing like filtered index dapper also intercept sql call dapper add add exactly came save much time tracking thing coding process programmer work remotely programmer code batcave compilation fast test run compiled code moved development staging server new feature hidden via feature switch run hardware rest site moved metastackoverflow testing user per day use site good test pass go live network tested larger community heavy usage static class method simplicity better performance code simple complicated bit packaged library open sourced maintained number net project stay low community shared part code used developer get two three monitor screen important help productive caching cache thing level cache network level cache caching browser cdn proxy given free net framework called httpruntimecache inmemory per server cache redis distributed inmemory keyvalue store share cache element across different server serve site stackoverflow server server able find cached item sql server cache entire database cached inmemory entire thing ssd usually hit sql server cache warming example every help page cached code access page terse static method static class used really bad oop perspective really fast really friendly towards terse code code directly addressed caching handled library layer redis dapper micro orm get around garbage collection problem one copy class used template created kept cache everything measured including gc operation statistic known layer indirection increase gc pressure point noticeable slowness cdn hit vary since query string hash based file content refetched build typically million hit day gb bandwidth cdn used cpu io load help user find answer faster deploying want deploy time day build grand gigantic thing put live important teamcity build copy web tier via powershell script step server tell haproxy take server rotation via post delay let ii finish current request sec stop website via pssession following robocopy file start website reenable haproxy via another post almost everything deployed via puppet dsc upgrading usually consist nuking raid array installing pxe boot fast know done rightrepeatable teaming team sre system reliability engineering people core dev q site people core dev mobile people career team development solely career product people devops developer team really closeknit lot movement team employee work remotely office mostly sale denver london exclusively else equal slightly prefered people nyc inperson time plus casual interaction happens getting thing done set make possible real work official team collaboration work almost entirely online learned inperson benefit outweighed much get able hire best talent love product anywhere one willing live city happen common reason someone going remote starting family new york great spacious office manhattan lot talent data center need crazy distance away since always improved also slightly faster connection many backbone nyc location though talking millisecond difference making awesome team love geek early microsoft example full geek conquered world hire stack overflow community look passion coding passion helping others passion communicating budgeting testing move fast break thing push live major change tested pushing development equally powerful sql server run web tier performance testing nt bad test stack overflow nt use many unit test active community heavy usage static code infrastructure change everything backup old configuration whenever possible quick failback mechanism example keepalived failback quickly load balancer redundant system fail pretty often regular maintenance sql backup tested dedicated server restoring constantly free license plan start full data center failovers every month secondary data center readonly time unit test integration test ui test run every push test must succeed production build run even possible mixed message going testing thing obviously test test mean thing touch money career product easily unittestable feature core end thing known input eg flagging new top bar etc thing functionality test hand push incubating site formerly metastackoverflow metastackexchange monitoring logging considering using http logstashnet log management currently dedicated service insert syslog udp traffic sql database web page add header timing way captured haproxy included syslog traffic opserver realog many metric surfaced realog logging display system built kyle brandt matt jibson go logging haproxy load balancer via syslog instead via ii lot versatile ii log clouding performance feature stackoverflow put heavy emphasis performance goal main page load le low programmer fanatic reducing page load time improving user experience timing every single request network recorded kind metric make decision improve system primary reason server run low utilization efficient code web server average cpu gb ram used mb network traffic sql server average around cpu gb ram used mb network traffic three major benefit general room grow upgrade necessary headroom stay online thing go crazy bad query bad code attack whatever may ability clock back power needed lesson learned use redis use m product gabeech o evangelism run thing platform run best period c run best window machine use ii redis run best nix machine use nix overkill strategy nick craver network provisioned gb massive overkill bet as active sql server average around mb gb pipe however thing like backup rebuilds etc completely saturate due much memory ssd storage present serve purpose ssds rock database node use ssd average write time millisecond know readwrite workload keeping thing efficient mean new machine needed often new project come along need different hardware reason new hardware added typically memory added efficient code low utilization mean nt need replacing typically talking adding ssds space b new hardware new project afraid specialize us complicated query based tag specialized tag engine developed need done test necessary active community acceptance testing add project required add line code necessary aint gone need really work reinvention ok typical advice reinvent wheel make worse making square example nt worry making square wheel developer write something lightweight already developed alternative go go bare metal go il assembly language net coding il c look sql query plan take memory dump web server see actually going discovered example split call generated garbage bureaucracy always tool team need example editor recent version visual studio etc make happen without lot process getting way garbage collection driven programming go great length reduce garbage collection cost skipping practice like tdd avoiding layer abstraction using static method extreme result highly performing code hundred million object short window actually measure pause app domain gc run pretty decent impact request performance cost inefficient code higher think efficient code stretch hardware reduces power usage make code easier programmer understand related article
520,Lobsters,scaling,Scaling and architecture,iosnoop for Linux,http://www.brendangregg.com/blog/2014-07-16/iosnoop-for-linux.html,iosnoop linux,iosnoop tool linux option man page example file latency outlier comms pid iosnoop ftrace perfevents function counting overhead conclusion iosnoop,probably dreaming ported popular iosnoop tool linux iosnoop tracing block io ctrlc end comm pid type dev block byte latms supervise w supervise w supervise w supervise w java r supervise w java r java r show one line summary per block device io disk io like tcpdump disk random io workload system rotational disk comm pid type dev block byte latms randread r randread r randread r randread r randread r randread r notice random disk location block result latency io time millisecond running workload ssds comm pid type dev block byte latms randread r randread r randread r randread r randread r randread r excellent latency much better post summarize iosnoop option usage provide example debugging latency outlier discus caveat tool internals overhead wrote option summarized usage message also man page example file iosnoop h usage iosnoop hqst device iotype p pid n name duration device device string eg iotype match type eg r read n name process name match io issue p pid pid match io issue q use queue insert start time include start time io include completion time io h usage message duration duration second use buffer eg iosnoop watch block io live unbuffered iosnoop trace sec buffered iosnoop q include queueing time latms iosnoop t include start end timestamps iosnoop r trace read iosnoop p show io issued pid oncpu iosnoop qp show io queued pid queue time see man page example file info use option postprocessing output r gnuplot digging harder issue including latency outlier latency outlier randread program ssd server aws instance usually experience m io latency outlier high millisecond excerpt iosnoop t tracing block io ctrlc end start end comm pid type dev block byte latms randread r randread r supervise w supervise w supervise w supervise w supervise w supervise w randread r randread r randread r important sort io completion time end case already correct order m read happened large group supervise writes completed truncated dozen supervise write line keep example short latency outlier output file showed sequence slow read batch writes note io request timestamp start show m read issued supervise writes sitting queue debugged type issue many time one different writes different device would assumed would different queue would nt interfere somewhere system xen guest look like shared queue discovered using iosnoop ca nt yet tell queue hope identifying would way tune queueing behavior eliminate reduce severity outlier update see comment output contains comm pid column explained man page treat p n option besteffort comms pid like original iosnoop tool comm pid column show oncpu io issued usually task issued could unrelated driven system high write load started seeing entry like comm pid type dev block byte latms awk w sshd w awk w awk w awk w awk w sshd w awk w well awk sshd oncpu nt likely origin io misleading captured stack trace block io issued using perfevents find example stack showed correct process one showing awk instead awk block blockrqissue w awk blkpeekrequest kernelkallsyms doblkifrequest kernelkallsyms blkrunqueue kernelkallsyms blkstartqueue kernelkallsyms kickpendingrequestqueues kernelkallsyms blkifinterrupt kernelkallsyms handleirqeventpercpu kernelkallsyms handleirqevent kernelkallsyms handleedgeirq kernelkallsyms kernelkallsyms xenevtchndoupcall kernelkallsyms xenevtchndoupcall kernelkallsyms xendohypervisorcallback kernelkallsyms brk awk brk syscall likely normal behavior expands heap since aws xen guest performs hypercall xendohypervisorcallback optimize memory operation happens call kickpendingrequestqueues issue work already queued call blkrunqueue leading issuing block io likely queued another process fetch originating pid tracing higher stack although may best keep iosnoop simple block devicelevel tracer could also dig file system pathname io related earlier iosnoop version like pid comm nt reliable depended file system type iosnoop zfs ca nt really get filename moment hf o x get partial pathnames useful update added q option iosnoop us block queue insertion starting tracepoint instead block io issue provides time include block io queueing analyze problem queueing load also improves acccuracy pid comm column since originating process much likely still oncpu time queue insert tempted trace insert issue tracepoints time better pid comm show io queued time device time separately would cost lot overhead linux kernel enhanced tracing framework cheaply iosnoop use blktrace disk io first wrote iosnoop blktrace existed port well showed source colleague stunned work shell script work large number server use running kernel without software addition using systemtap ktap even perfevents using bash shell awk file sys file control kernel ftrace framework experiment began perfevents hit problem scripting mode since use case little different designed perfevents provides perf command way nicer safer frontend framework tracepoints kprobes ftrace us iosnoop least perfevents script handle use case script advanced framework ifwhen integrated leaf present realized could whip ftracebased iosnoop use solve real issue nicer future arrived also curious see problem run ftrace interface worked really well issue snapshot losing event nt use also write error marker event nt use either still debugging nt know using wrong issue implementation previous post used loweroverhead mode ftrace function counting count inkernel iosnoop read individual event data ftrace process user space particularly interested overhead approach overhead iosnoop version print event happen produce snappy output high iop system consume noticeable cpu resource duration mode us buffering instead populates buffer read example using duration second time iosnoop t real user sys wc iosnoop ate half second cpu process buffer event really bad analyze tune awk performance script reduced cpu overhead downside using buffer like limited size overflow lose event default setting bufsizekb happen around io lot different way handle need easiest way drop duration event printed without buffering cost much cpu second time iosnoop t c real user sys wc l lh rwxrwsrx root jul captured event wrote mbyte output file cost second cpu time one cpu run also mean calculate upper bound unbuffered approach hardware software version would iop really want trace iop long duration beyond scope temporary tool future imagine ditching iosnoop implementation rewriting point handle higher load conclusion iosnoop useful tool implemented using nothing linux ftrace part kernel since day implemented something robust like perfevents advanced tracer systemtap ktap handle buffering concurrent user sensibly ftracebased iosnoop really experiment see far could go surprised dropping shell script different kernel work amazing hope dreaming warning apply ftrace tool hack know test first use risk future kernel development one day rewritten move hack stable tool
522,Lobsters,scaling,Scaling and architecture,"OpenRoss - fast, scalable, on-demand image resizer",http://developers.lyst.com/data/images/2014/06/23/openross/,openross fast scalable ondemand image resizer,click redirected,click redirected
524,Lobsters,scaling,Scaling and architecture,Simple Binary Encoding,http://mechanical-sympathy.blogspot.com/2014/05/simple-binary-encoding.html,simple binary encoding,opra overview fix fast community todd montgomery simple binary encoding olivier deheurles adaptive osi design principle memory access pattern microbenchmarks sbetool schema mutabledirectbuffer bytebuffer unsafeallocatememory long mappedbytebuffer transferred endian feedback specification,financial system communicate sending receiving vast number message many different format people use term like vast normally think really many let quantify vast finance industry market data feed financial exchange typically emitting ten hundred thousand message per second aggregate feed like opra peak million message per second volume growing yearonyear presentation give good overview crazy world still see significant use ascii encoded presentation fix tag value slightly sane binary encoded presentation like fast market even commit sin sending market data xml well complain much time provided good income writing ultra fast xml parser last year cme member fix community commissioned todd montgomery lbm fame build reference implementation new fix simple binary encoding sbe standard sbe codec aimed addressing efficiency issue lowlatency trading specific focus market data cme working within fix community done great job coming encoding presentation efficient maybe suitable atonement sin past fix tag value implementation todd worked java c implementation later helped net side amazing olivier deheurles adaptive working cool technical problem team dream job sbe overview sbe osi layer presentation encodingdecoding message binary format support lowlatency application many application profile performance issue message encodingdecoding often significant cost seen many application spend significantly cpu time parsing transforming xml json executing business logic sbe designed make part system efficient sbe follows number design principle achieve goal adhering design principle sometimes mean feature available codecs offered example many codecs allow string encoded field position message sbe allows variable length field string field grouped end message sbe reference implementation consists compiler take message schema input generates language specific stub stub used directly encode decode message buffer sbe tool also generate binary representation schema used onthefly decoding message dynamic environment log viewer network sniffer design principle drive implementation codec ensures message streamed memory without backtracking copying unnecessary allocation memory access pattern underestimated design highperformance application lowlatency system language especially need consider allocation avoid resulting issue reclamation applies managed runtime native language sbe totally allocation free three language implementation end result applying design principle codec time greater throughput google protocol buffer gpb low predictable latency observed microbenchmarks realworld application use typical market data message encoded decoded compared message gpb hardware xml fix tag value message order magnitude slower sweet spot sbe codec structured data mostly fixed size field number bitsets enums array work string blob many find restriction usability issue user would better another codec suited string encoding message structure message must capable read written sequentially preserve streaming access design principle ie need backtrack codecs insert location pointer variable length field string type indirected access indirection come cost extra instruction plus losing support hardware prefetchers sbe design allows pure sequential access copyfree native access semantics figure sbe message common header identifies type version message body follow header followed root field message fixed length static offset root field similar struct c message complex one repeating group similar root block follow repeating group nest repeating group structure finally variable length string blob come end message field may also optional xml schema describing sbe presentation found sbetool compiler use sbe first necessary define schema message sbe provides language independent type system supporting integer floating point number character array constant enums bitsets composite grouped structure repeat variable length string blob message schema input sbetool compiled produce stub range language generate binary metadata suitable decoding message onthefly java doptionvalue jar sbejar messagedeclarationsfilexml sbetool compiler written java tool currently output stub java c c programming stub full example message defined schema supporting code found generated stub follow flyweight pattern instance reused avoid allocation stub wrap buffer offset read sequentially natively write message header first messageheaderwrap directbuffer bufferoffset messagetemplateversion blocklength carsbeblocklength templateid carsbetemplateid schemaid carsbeschemaid version carsbeschemaversion write body message carwrapforencode directbuffer bufferoffset serialnumber modelyear available booleantypetrue code modela putvehiclecode vehiclecode srcoffset message written via generated stub fluent manner field appears generated pair method encode decode read header lookup appropriate template decode messageheaderwrap directbuffer bufferoffset messagetemplateversion final int templateid messageheadertemplateid final int actingblocklength messageheaderblocklength final int schemaid messageheaderschemaid final int actingversion messageheaderversion template located field decoded carwrapfordecode directbuffer bufferoffset actingblocklength actingversion final stringbuilder sb new stringbuilder sbappend ncartemplateid append carsbetemplateid sbappend ncarschemaid append schemaid sbappend ncarschemaversion append carsbeschemaversion sbappend ncarserialnumber append carserialnumber sbappend ncarmodelyear append carmodelyear sbappend ncaravailable append caravailable sbappend ncarcode append carcode generated code language give performance similar casting c struct memory onthefly decoding compiler produce intermediate representation ir input xml message schema ir serialised sbe binary format used later onthefly decoding message stored also useful tool network sniffer compiled stub full example ir used found direct buffer sbe via agrona provides abstraction java mutabledirectbuffer class work buffer byte heap direct bytebuffer buffer heap memory address returned unsafeallocatememory long jni lowlatency application message often encodeddecoded memory mapped file via mappedbytebuffer thus transferred network channel kernel thus avoiding user space copy c c builtin support direct memory access require abstraction java version directbuffer abstraction added c support endianess encapsulate unsafe pointer access message extension versioning sbe schema carry version number allows message extension message extended adding field end block field removed reordered backwards compatibility extension field must optional otherwise newer template reading older message would work template carry metadata min max null timeunit character encoding etc accessible via static class level method stub byte ordering alignment message schema allows precise alignment field specifying offset field default encoded little endian form unless otherwise specified schema maximum performance native encoding field word aligned boundary used penalty accessing nonaligned field processor significant alignment one must consider framing protocol buffer location memory message protocol often see people complain codec support particular presentation single message however often possible address protocol message protocol great way split interaction component part part often composable many interaction system example ir implementation schema metadata complex supported structure single message encode ir first sending template message providing overview followed stream message encoding token compiler ir allows design fast otf decoder implemented threaded interpreter much le branching typical switch based state machine protocol design area developer nt seem get opportunity learn feel great loss fact many developer call encoding ascii protocol telling value protocol obvious one get work programmer like todd spent life successfully designing protocol stub performance stub provide significant performance advantage dynamic otf decoding accessing primitive field believe performance reaching limit possible general purpose tool generated assembly code similar compiler generate accessing c struct even java regarding general performance stub observed c marginal advantage java believe due runtime inserted safepoint check c version lag little behind due runtime aggressive inlining method java runtime stub three language capable encoding decoding typical financial message ten nanosecond effectively make encoding decoding message almost free application relative rest application logic feedback first version sbe would welcome feedback reference implementation constrained fix community specification possible influence specification please nt expect pull request accepted significantly go specification support javascript python erlang language discussed would welcome update thanks feedback kenton varda creator gpb able improve benchmark get best performance gpb result change java benchmark c gpb example optimisation show approximately doubling throughput compared initial result noted often opposite java gpb compared c get performance improvement allocate object rather reuse gpb optimisation mode thr cnt sec mean mean error unit exec ucrprotobufcarbenchmarktestdecode thrpt opsms exec ucrprotobufcarbenchmarktestencode thrpt opsms exec ucrprotobufmarketdatabenchmarktestdecode thrpt opsms exec ucrprotobufmarketdatabenchmarktestencode thrpt opsms exec ucrsbecarbenchmarktestdecode thrpt opsms exec ucrsbecarbenchmarktestencode thrpt opsms exec ucrsbemarketdatabenchmarktestdecode thrpt opsms exec ucrsbemarketdatabenchmarktestencode thrpt opsms gpb optimisation mode thr cnt sec mean mean error unit exec ucrprotobufcarbenchmarktestdecode thrpt opsms exec ucrprotobufcarbenchmarktestencode thrpt opsms exec ucrprotobufmarketdatabenchmarktestdecode thrpt opsms exec ucrprotobufmarketdatabenchmarktestencode thrpt opsms throughput msgms gpb optimisation testprotocol bufferssberatio car car market data market data throughput msgms gpb optimisation testprotocol bufferssberatio car car market data market data
525,Lobsters,scaling,Scaling and architecture,Building Products at SoundCloud Part I: Dealing with the Monolith,http://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith,building product soundcloud dealing monolith,scala clojure jruby used thousand thirdparty application next soundcloud run database migration scale smarter rail access database process huge number message microservices architecture bounded context sebastian ohm senart presented detail use amqp visual sound new stats system,product written scala clojure jruby always case like startup soundcloud created single monolithic ruby rail application running mri official interpreter backed memcached mysql affectionately call system mothership architecture good solution new product used several hundred thousand artist share work collaborate track discovered industry rail codebase contained public api used thousand thirdparty application userfacing web application launch next soundcloud interface world became mostly public api built client application top api partner developer used day hour music sound uploaded every minute hundred million people use platform every day soundcloud combine challenge scaling large social network medium distribution powerhouse scale rail application level developed contributed published several component tool help run database migration scale smarter rail access database process huge number message end decided fundamentally change way build product felt always patching system resolving fundamental scalability problem first change architecture decided move towards known microservices architecture style engineer separate domain logic small component component expose welldefined api implement bounded context persistence layer infrastructure need bigbang refactoring bitten u past team decided best approach deal architecture change would split mothership immediately rather add anything new new feature built microservices whenever larger refactoring feature mothership required extract code part effort started well soon enough detected problem much logic still rail monolith pretty much microservices talk somehow one option around problem microservices accessing directly mothership database common approach corporate setting database public published interface usually lead many problem need change structure shared table instead went published interface public api internal microservices would behave exactly like application developed thirdparty organization integrate soundcloud platform soon enough realized big problem model microservices needed react user activity pushnotifications system example needed know whenever track received new comment could inform artist scale polling option needed create better model already using amqp general rabbitmq specific rail application often need way dispatch slow job worker process avoid hogging concurrencyweak ruby interpreter sebastian ohm senart presented detail use amqp several iteration developed model called semantic event change domain object result message dispatched broker consumed whichever microservice find message interesting architecture enabled event sourcing many microservices deal shared data remove need query public api example might need fan artist email address notify new track data available public api constrained rule enforced thirdparty application possible example microservice notify user activity private track user could access public information explored several possible solution problem one popular alternative extract activerecord model mothership ruby gem effectively making rail model class published interface shared component several important issue approach including overhead versioning component across many microservices became clear microservices would implemented language ruby therefore think different solution end team decided use feature engine plugins depending version create internal api available within private network control could accessed internally used oauth application acting behalf user different authorisation scope depending microservice need data although constantly removing feature mothership push pull interface old system make sure couple new microservices old architecture microservice architecture proven crucial developing productionready feature much shorter feedback cycle externalfacing example visual sound new stats system
526,Lobsters,scaling,Scaling and architecture,PyParallel: How we removed the GIL and exploited all cores,https://speakerdeck.com/trent/pyparallel-how-we-removed-the-gil-and-exploited-all-cores,pyparallel removed gil exploited core,fewer faster,copyright fewer faster llc slide content description owned creator
528,Lobsters,scaling,Scaling and architecture,Lock-free Data Structures. The Inside. Memory Management Schemes,http://kukuruku.co/hub/cpp/lock-free-data-structures-the-inside-memory-management-schemes,lockfree data structure inside memory management scheme,previous article tagged pointer libcds example tagged pointer use epochbased reclamation hazard pointer r r n msqueue performed hazard pointer hazard pointer implementation libcds k r p reconstructing lost data type api cd gc hp class k p hazard pointer reference counter pas buck summary reference practical lock freedom practical efficient lockfree garbage collection based reference counting pragmatic implementation nonblocking linked list repeat offender problem mechanism supporting nonblocing memory management support dynamicsized data structure safe memory reclamation dynamic lockfree object using atomic read writes hazard pointer safe memory reclamation lockfree object simple fast practical nonbloking blocking concurrent queue algorithm parallelism shift memory model,already mentioned previous article main difficulty implementing lockfree data structure aba problem memory reclamation separate two problem even though connected algorithm solve one article going review popular method safe memory reclamation lockfree container demonstrate application one another method classic lockfree queue michaelscott tagged pointer tagged pointer scheme introduced ibm solve aba problem perhaps popular algorithm solution according scheme pointer represents atomic pair memory cell address tag integer number template typename struct taggedptr ptr unsigned int tag taggedptr ptr nullptr tag taggedptr p ptr p tag taggedptr p unsigned int n ptr p tag n operator const return ptr tag version number incremented every ca operation tagged pointer never reduced strictly growing delete element container instead physically removing element place list free element freelist quite possible call deleted element freelist structure lockfree one thread deletes x element another thread local copy tagged pointer refer element field separate freelist type many case quite illegal call destructor type data placing element freelist due parallel access destructor operation another thread read data element tagged pointer scheme following drawback scheme implemented platform atomic ca primitive double word dwcas requirement fulfilled modern system dwacas operates word modern architecture complete set instruction least dwcas required operation mode implemented architecture stuff nonsense experienced lockfree programmer object necessary wide ca tagged pointer implementation considering fact modern processor use bit addressing older bit address free use tag counter like boostlockfree two snag approach guarantee address used future soon another breakthrough area memory chip volume rise steeply vendor introduce new processor complete addressing bit enough tag storage research carried respect showed following result bit enough overflow quite possible potentially lead aba problem bit enough really bit value tag modern operational system time slice thread thousand assembler instruction taken internal correspondence linux developer processor performance grows time slice grow well quite possible execute thousand even difficult operation like ca possible today tomorrow thus using tag risking facing aba problem freelist implementation lockfree stack lockfree queue fact brings negative contribution performance least one ca call removing element freelist adding hand freelist availability contribute performance increase whereas freelist empty necessary refer system function usually slow synchronized memory allocation availability separate freelist every data type unattainable luxury application lead inefficient memory use example lockfree queue consists element average size increase million freelist size spike million behavior often illegal tagged pointer scheme example algorithm solve aba problem solve problem memory reclamation moment libcds use tagged pointer implementation lockfree container despite relative simplicity scheme lead unmanageable growth used memory due lockfree availability every containerobject libcds focused lockfree algorithm predictable memory use without applying dwcas boostlockfree library good example applying tagged pointer scheme example tagged pointer use prefer wallpaper msqueue pseudocode tagged pointer yes lockfree algorithm verbose indeed left std atomic application simplicity template typename struct node taggedptr next data template typename class msqueue taggedptr volatile mhead taggedptr volatile mtail freelist mfreelist public msqueue allocate dummy node head tail point dummy node mheadptr mtailptr new node void enqueue const value node pnode mfreelistnewnode data value nextptr nullptr taggedptr tail mtail taggedptr next next tail tail tail point last element nextptr nullptr trying add element end list ca next next taggedptr node success leave loop break else tail point last element trying relocate tail last element ca mtail tail taggedptr nextptr end loop trying relocate tail inserted element ca mtail tail taggedptr pnode bool dequeue dest taggedptr head mhead taggedptr tail mtail taggedptr next next head tail next consistent head mhead queue empty tail last headptr tailptr queue empty nextptr nullptr queue empty return false tail last element trying move tail forward ca mtail tail taggedptr nextptr else tail position read value ca otherwise another dequeue deallocate next dest data trying move head forward ca mhead head taggedptr nextptr break success leave loop end loop deallocate old dummy node mfreelistadd headptr return true result dest look attentively algorithm enqueue deueue operation example see several standard approach lockfree structure building draw attention method contain loop content part operation repeated successfully executed successful execution impossible example dequeue empty queue repeating help loop typical method lockfree programming first element queue mhead point dummy node dummy node presence guarantee pointer queue beginning end never null sign empty queue head mtail mtail next null condition line last mtail next null condition significant adding queue change mtail line change mtail next first glance enqueue method break queue structure fact change mtail tail take place different method andor different thread enqueue operation check line adding element mtail point last element mtail next null case try move pointer end line line dequeue executing responsibility also change mtail point end line line demonstrates popular approach lockfree programming thread mutual help helping algorithm one operation container operation one operation hope following operation call maybe another one finish work maybe another thread another fundamental observation operation store local variable pointer value need operation line line calculated value compared original typical lockfree method unnecessary nonconcurrent programming since moment read original value change want compiler optimize access shared data queue compiler even delete line comparison declare mhead mtail atomic volatile pseudocode also remember ca primitive compare destination address value given one equal change data new value according destination address ca primitive necessary indicate local copy current value ca val val newval almost always successful error u consider situation dequeue method data copying take place line removing element queue line taking account element moving mhead forward deletion fail data copying repeated c point view mean data stored queue complex burden assignment operator great ca primitive failure possible condition high load try algorithm taking line outside loop limit lead error next element deleted another thread whereas considered implementation based tagged pointer scheme element never deleted lead situation return wrong data present queue moment line successful execution peculiarity queue msqueue really interesting mhead always point dummy node first element nonempty queue next mhead element dequeue first element mheadnext read nonempty queue dummy element deleted following element becomes next dummy element head element value return physically add element next dequeue operation peculiarity cause much trouble want use intrusive variant cd intrusive msqueue queue epochbased reclamation fraser introduced scheme based epoch variant apply delayed deletion safe moment absolutely sure none thread reference deleted element epoch provide guarantee global nglobalepoch epoch thread work local nthreadepoch epoch entering code guarded epochbased scheme local epoch thread incremented exceed global epoch soon thread reach global epoch nglobalepoch incremented scheme pseudocode global epoch static atomic unsigned int mnglobalepoch const epochcount tl data struct threadepoch global epoch thread unsigned int mnthreadepoch list retired element list void marrretired epochcount threadepoch mnthreadepoch void enter mnthreadepoch mnglobalepoch mnthreadepoch mnglobalepoch void exit thread epoch mnglobalepoch mnglobalepoch empty delete element marrretired mnglobalepoch epochcount thread emptied element lockfree container place local thread list marrretired mnthreadepoch epochcount element waiting deletion soon thread passed global mnglobalepoch epoch empty list thread mnglobalepoch epoch increment global epoch every operation lockfree container enclosed threadepoch enter threadepoch exit call similar critical section lockfreeop getcurrentthread threadepochenter lockfree operation container inside critical epochbased scheme sure one delete data working getcurrentthread threadepochexit scheme quite simple protects local reference reference inside container operation element lockfree container scheme provide protection global reference outside container operation implement concept iterators element lockfree container help epoch based scheme drawback fact thread program move following epoch ie refer lockfree container least one thread go following epoch deletion retired element thread priority lowpriority thread bring uncontrolled growth highpriority thread element delayed deletion thus epoch based scheme lead necessary lead case one thread failure unlimited memory consumption libcds library implementation epoch based scheme manage built quite effective algorithm order define whether thread reached global epoch maybe reader could recommend solution hazard pointer scheme introduced michael meant protection local reference element lockfree data structure perhaps popular studied scheme delayed deletion based atomic read write use synchronization primitive ca type scheme corner stone duty declare pointer element lockfree container hazard inside operation lockfree data structure working element place hp array current thread hazard pointer hp array private thread owner thread write thread read scan procedure attentively analyzed operation various lockfree container noticed size hp array number one thread hazard pointer exceed burden scheme support great huge data structure fair say data structure requiring hazard pointer cite skiplist cd container skiplistmap example stochastic data structure matter fact list list variable level element container fit hazard pointer scheme though skiplist implementation scheme libcds pseudocode hazard pointer scheme constant p number thread k number hazard pointer one thread n total number hazard pointer k p r batch size n example n perthread variable array hazard pointer thread ownerthread write thread read void hp n current size dlist value r unsigned dcount array data ready deletion void dlist r data deletion place data dlist array void retirenode void node dlist dcount node array filled call basic scan function dcount r scan basic function deletes element dlist array declared hazard pointer void scan unsigned unsigned unsigned newdcount n void hptr plist n newdlist n stage traverse hp thread collect total plist array protected pointer unsigned p void phpthread getthreaddata hp n hptr phpthread hptr nullptr plist p hptr stage sorting hazard pointer sorting necessary following binary search sort plist stage deleting element declared hazard r dlist conforms plist list hazard pointer dlist deleted binarysearch dlist plist newdlist newdcount dlist else free dlist stage forming new array retired element newdcount dlist newdlist dcount newdcount deleting retirenode pnode element pnode lockfree container j thread place pnode local j thread dlist list retired requiring deletion element soon size dlist list r r comparable n p k n example r call scan procedure charge deleting retiredelements r p k condition essential condition fulfilled guarantee scan able delete something list retireddata condition broken scan delete anything array get algorithm error array completely filled reduce size scan consists four stage first prepare plist array current hazard pointer include list divergent null hazard pointer thread first stage read shared data array hp thread stage work local data stage sort plist array order optimize following search also delete tallyelements plist stage deletion scroll element dlist array current thread dlist element plist thread working pointer delete leave dlist relocate newdlist delete dlist thread working stage copy nondeleted element newdlist dlist whereas r n scan procedure necessarily reduce size dlist array ie element deleted without fail rule declaring pointer hp carried following way std atomic atomicptr localptr localptr atomicptrload std memoryorderrelaxed hp localptr localptr atomicptrload std memoryorderacquire read atomicptr atomic pointer localptr local variable work later free hp slot hp array current thread hazard pointer check reading atomicptr value changed another thread order perform checkup read atomicptr one time compare read localptr value happens way place true moment declaring hazard value atomicptr hp array pointer array hazard pointer ie declared hazard deleted physically thread reference pointer lead reading hash writing free memory area hazard pointer hp scheme analyzed detail relation c atomic operation memory ordering work msqueue performed hazard pointer lockfree queue michael scott performed hazard pointer providing pseudocode without libcds specific feature template typename class msqueue struct node std atomic node next data node next nullptr node const v next nullptr data v std atomic node mhead std atomic node mtail public msqueue node p new node mheadstore p std memoryorderrelease mtailstore p std memoryorderrelease void enqueue const data node pnew new node data true node mtailload std memoryorderrelaxed declaring pointer hazard hp threadprivate array hp necessarily verify mtail changed mtailload std memoryorderacquire continue node next nextload std memoryorderacquire mtail continue next nullptr mtail point last element move mtail forward mtailcompareexchangeweak next std memoryorderrelease continue node tmp nullptr nextcompareexchangestrong tmp pnew std memoryorderrelease break mtailcompareexchangestrong pnew std memoryorderacqrel hp nullptr zero hazard pointer bool dequeue dest true node h mheadload std memoryorderrelaxed setup hazard pointer hp h verify mhead changed h mheadload std memoryorderacquire continue node mtailload std memoryorderrelaxed node next h nextload std memoryorderacquire head next also mark hazard pointer hp next mhead changed start everything anew h mheadload std memoryorderrelaxed continue next nullptr queue empty hp nullptr return false h help enqueue method moving mtail forward mtailcompareexchangestrong next std memoryorderrelease continue dest next data mheadcompareexchangestrong h next std memoryorderrelease break zero hazard pointer hp nullptr hp nullptr place old queue head array data ready deletion retirenode h hazard pointer scheme multipurpose applied data structure described form number hazard pointer limited k constant data structure condition limited hazard pointer number fulfilled number hp usually pretty small algorithm estimating number concurrently required hazard pointer impossible sorted harris list used example algorithm removing element list chain indefinite length deleted make hp scheme inapplicable strictly speaking generalize hp scheme case unlimited number hazard pointer scheme author provides detailed instruction decided limit libcds classic algorithm order complicate hp scheme make implementation difficult especially another le popular scheme pas buck quite similar hazard pointer scheme approach hazard pointer unlimited number used tell later hazard pointer implementation libcds picture demonstrates inner build hazard pointer algorithm libcds core algorithm hazard pointer manager represents singleton taken libcdsdllso dynamic library every thread object thread hp manager framework hp array k size array retired pointer r size located strcutures thread hp manager bound list maximum number thread p libcds default size hazard pointer array k number thread p size retired ready deletion data r k p hp scheme implemented libcds form three stage core independent data type low level implementation scheme namespace cd gc hzp whereas core typified depend type data deleted taken dynamic library information data type lost call data destructor fair mark always physical example intrusive container call disposer functor stimulation safely event know event processor behind implementation level typified implementation scheme located namespace cd gc hzp level represents set template core shell structure called save data typification somehow similar type erasure level used program interface level api cd gc hp class used used lockfree container libcds actually parameter value one value scheme well gc container template code point view cd gc hp class thin wrapper around implementation level score small class reconstructing lost data type data type core destructor call rather type reconstruction executed simple every log array ready deletion core time delayed retired look like following struct retiredptr typedef void fndisposer void void ptr retiredpointer fndisposer pdisposer disposer function retiredptr void p fndisposer ptr p pdisposer retired pointer function deletion stored scan method hp scheme call pdisposer ptr element pdisposer function know argument type implementation level charge forming given function example physical deletion carried following way template typename struct makedisposer static void dispose void p delete reinterpretcast p template typename void retireptr p place p arrretired array ready deletion data note arrretired private data thread arrretiredpush retiredptr p makedisposer dispose call scan array filled arrretiredfull scan approach bit simplified guess idea clear want use container basis hp scheme libcds library enough declare object cd gc hp type main connect every thread using container based hp scheme quite another matter need write container class based cd gc hp case know api hp scheme api cd gc hp class method cd gc hp class static accent class wrapper singleton constructor hp sizet nhazardptrcount sizet nmaxthreadcount sizet nmaxretiredptrcount cd gc hzp scantype nscantype cd gc hzp inplace nhazardptrcount maximum number hazard pointer k constant scheme nmaxthreadcount maximum number thread p constant scheme nmaxretiredptrcount array dimension retired pointer r constant scheme nscantype small optimization cd gc hzp classic value point necessarily follow pseudocode scan algorithm cd gc hzp inplace value allows get rid newdlist array scan use dlist instead descend keep mind one cd gc hp object constructor actually call static function order initialize core trying declare two object cd gc hp class lead creating two hazard pointer scheme recurring initialization safe useless placing pointer retired array time delayed deletion current thread template class disposer typename static void retire p template typename static void retire p void pfunc disposer argument pfunc defines deleting functor disposer first case call quite pretentious struct foo struct foodisposer void operator foo p const delete p calling mydisposer disposer pointer foo foo p new foo cd gc hp retire foodisposer p static void forcedispose forced call scan algorithm hazard pointer scheme sure useful real life sometimes necessary libcds besides three important sub class declared cd gc hp class threadgc represents wrapper around code initializing private thread data referring hazard pointer scheme class represents constructor carry connection hp scheme thread destructor disconnecting thread scheme guard pointer template guardarray array hazard pointer applying hp scheme often required declare several hazard pointer time better one array time several object guard type guard guardarrayclasses represent superstructure internal hazard pointer array private thread work allocator internal array guard class essence hazard slot following api template typename protect cdsatomic atomic const toguard template typename class func protect cdsatomic atomic const toguard func f declaring atomic pointer type usually pointer hazard loop hidden inside method described read value toguard atomic pointer assign hazard pointer check read pointer changed second syntax func functor necessary need declare hazard pointer pointer derived specific character intrusive container container manages pointer node pointer real data may differ example node field real data func functor following signature struct functor valuetype operator p method return pointer declared hazard template typename assign p template typename int bitmask assign cd detail markedptr bitmask p method declare p hazard loop contrast protect relocate p hazard slot second syntax meant marked cd detail markedptr pointer lower bit always leveled data used marked pointer storage bit flag popular method lockfree programming place pointer cleared flag bit hazard slot bitmask mask method return pointer declared hazard template typename get const read value current hazard slot sometimes necessary void copy guard const src copy hazard slot value src result two hazard slot contain value void clear zero hazard slot value destructor guard class guardarray class similar api index also indicated array template typename protect sizet nindex cdsatomic atomic const toguard template typename class func protect sizet nindex cdsatomic atomic const toguard func f template typename assign sizet nindex p template typename int bitmask assign sizet nindex cd detail markedptr bitmask p void copy sizet ndestindex sizet nsrcindex void copy sizet nindex guard const src template typename get sizet nindex const void clear sizet nindex alert reader notice unknown word cdsatomic macro declaring appropriate namespace std atomic compiler support implement atomic cdsatomic std support cd namespace next version libcds possible use boostatomic cdsatomic boost hazard pointer reference counter drawback hazard pointer scheme meant guarding local reference node lockfree container guard global reference necessary say implementing concept iterators unlimited size hazard pointer array would necessary specifying matter fact implement iterators using hp scheme iteratorobject contain hp slot protecting iterator pointer result get quite specific iterator bound thread remember hazard slot private data thread considering fact also finite set hazard pointer conclude impossible implement iterators help hp scheme classical greek programmer thought reference counting multipurpose instrument saving error know ancient mistaken popular method recognize whether object used reference counting method refcount work valois one pioneer lockfree approach scheme based reference counting used safe deletion container element refcount scheme drawback main difficult implementation circular data structure element refer besides many researcher mark inefficiency refcount scheme lockfree implementation us fetchandadd primitive often actually every use pointer number counter reference increased decremented study group gothenburg university published work work hazard pointer refcount method combined hazard pointer scheme used efficient guarding local reference inside operation lockfree data structure refcount used guarding global reference keeping data structure integrity name scheme hrc hazard pointer refcounting applying hazard pointer allowed avoid difficult operation incrementingdecrementing number reference element increase efficiency refcounting scheme whole hand applying two method time somewhat complicated algorithm combined scheme providing complete pseudocode due abundance technical detail refer hazard pointer scheme need special support element lockfree container hrc relies presence two helper method element void cleanupnode node pnode void terminatenode node pnode terminatenode procedure zero inside pnode element pointer container element use cleanupnode procedure sure pnode element refers element marked deletion data structure changing moving forward reference necessary whereas every reference refcount container accompanied increase element reference counter refer cleanupnode also reduces reference counter deleted element void cleanupnode node pnode x pnode link x node referencecounted retry dereflink pnode link x set hp null isdeleted dereflink link x set hp change reference increment reference counter old element compareandswapref pnode link x releaseref clear hp releaseref clear hp goto retry new reference also deleted repeat releaseref clear hp thanks change accent managing lockfree container scheme core container element used well virtual c function hrc scheme element becomes independent particularly implemented lockfree data structure note cleanupnode algorithm break data structure integrity short term change reference inside element one one unacceptable case use program multicas emulation atomic application connection element lockfree container accept violation well hazard pointer scheme number retired element limited top algorithm physical deletion similar scan algorithm hazard pointer scheme change connected reference counter management cleanupnode call main difference following guaranteed hp scheme choosing r n p k scan procedure definitely delete something least one retired element guarded hazard pointer scan procedure call hrc scheme failure due mutual element reference reference increment counter scan fails call cleanupall support procedure go array retired pointer thread call cleanupnode procedure every every remote element thereby making second scan call successful implementing hrc scheme libcds hrc scheme implemented libcds analogy hp scheme main class cd gc hrc api class absolutely analogous cd gc hp api main advantage hrc scheme possibility embodying iterator concept implemented libcds working library came conclusion generally iterators applicable lockfree container iterators assume safe reference object iterator pointing also safe round trip entire entire container impossible provide last condition round trip lockfree container general case always concurrent thread deleting element iterator based result impossible refer safely node field node protected hp scheme deleted physically either get following element node removed lockfree container thus hrc scheme presented libcds specific case hp scheme implementation example see adding additional condition reference counter make hp scheme complex test example hrc container several time slower hp container also face common fullfledged garbage collector impossible delete something scan call example due circular reference start cleanupall procedure running retired node hrc scheme used libcds variant hp like scheme allows forget generality construction due specific internal structure hrc scheme generalization hrcbased hpbased container usually interesting task pas buck working problem memory reclamation lockfree data structure herlihy al created passthebuck algorithm ptb similar hp scheme michael essentially differ implementation detail well hp scheme ptb scheme also requires declaring pointer guarded analog hazard pointer hp scheme ptb scheme initially meant indefinite advance number guard hazard pointer enough retired data call liberate procedure analog scan hp scheme return array pointer safely deleted unlike hp scheme array retired pointer private thread array retired data entire thread ptb scheme structure guard hazard pointer interesting contains guarded pointer also pointer retired data socalled handoff deleting process liberate procedure detects retired data guarded place retired record slot handoff guard handoff data de deleted next liberate call guard attached changed meaning pointer guarded data author provide two algorithm liberate waitfree lockfree waitfree requires dwcas ca double word make algorithm dependent dwcas support target platform lockfree algorithm interesting ability work data change data guard retired pointer remain unchanged call lockfree liberate version circularity possible algorithm delete possible retired data call time data remain unchanged end program peeling executed singleton destructor ptb scheme also call liberate bothered head circularity decided change liberate algorithm ptb scheme making analogy hp scheme result ptb implementation libcds became familiar hp scheme variant arbitrary number hazard pointer entire array retired data essentially affected capability hp scheme faster ptb ptb preferable due restriction guard number implementation libcds libcds library ptb scheme presented cd gc ptb class implementation detail located cd gc ptb namespace api cd gc ptb absolutely analogous cd gc hp except constructor argument constructor accepts following argument ptb sizet nliberatethreshold sizet ninitialthreadguardcount nliberatethreshold threshold liberate call soon entire array retired data reach size call liberate ninitialthreadguardcount initial number guard creating thread rather connecting thread libcds internal infrastructure next case lacking guard new one automatically created summary article reviewed algorithm safe memory reclamation focusing hazard pointer scheme hp scheme variant represent quite good way providing safe memory control lockfree data structure everything mentioned relates area creating lockfree data structure interested libcds enough initialize chosen scheme forget attaching thread replace scheme class first argument gc container libcds library reference guard scan liberate call etc inside container implementation left one algorithm overboard rcu differs hplike scheme tell one following article reference keir fraser practical lock freedom technical report based dissertation submitted september kfraser degree doctor philosophy university cambridge king college anders gidenstam marina papatriantafilou hakan sundell philippas tsigas practical efficient lockfree garbage collection based reference counting technical report computer science engineering chalmers university technology goteborg university timothy harris pragmatic implementation nonblocking linked list herlihy v luchangco moir repeat offender problem mechanism supporting dynamicsized lockfree data structure technical report sun microsystems laboratory mherlihy vluchangco pmartin mmoir nonblocing memory management support dynamicsized data structure acm transaction computer system vol may page maged michael safe memory reclamation dynamic lockfree object using atomic read writes maged michael hazard pointer safe memory reclamation lockfree object maged michael michael scott simple fast practical nonbloking blocking concurrent queue algorithm johan torp parallelism shift memory model chapter
529,Lobsters,scaling,Scaling and architecture,Big Data Machine Learning with H2O,http://www.lauradhamilton.com/big-data-machine-learning-with-h2o,big data machine learning,srisatish ambati gave talk chicago big data meetup h github netflix using h machine learning h r package,yesterday srisatish ambati gave talk chicago big data meetup use machine learning big data srisatish founder ceo startup called pronounced hexadata developed opensource machine learning tool github number powerful machine learning algorithm run make different r program really good performance handle huge amount data split algorithm data little chunk process parallel many machine cluster netflix using machine learning netflix one pioneer big data machine learning look forward hear using run data stored hadoop distributed file system hdfs also upload data directly cluster written java work data analysis layer play nice system currently support following algorithm random forest generalized linear modeling glm logistic regression kmeans srisatish said team working add algorithm particular srisatish told u key area focus developing set statistic math algorithm unbalanced datasets use fraud detection also r package r user run distributed algorithm r command line algorithm also run via selfhosted web interface srisatish said company goal enable developer use realtime machine learning service enable smartphone app web app learn continually obtained data real goal make algorithm embeddable application ambati ceo
530,Lobsters,scaling,Scaling and architecture,How WhatsApp is scaling up,https://raw.githubusercontent.com/reedr/reedr/master/slides/efsf2014-whatsapp-scaling.pdf,whatsapp scaling,, obj stream dgnb endstream endobj obj endobj obj stream jfif c c ltw n x b e g  p g m  x  g eri e qd ri r r r rv r lv f w fdi aqq ml  n g h nuu l n l j  p j j qpo  b ku r b j ul t
531,Lobsters,scaling,Scaling and architecture,Linus Torvalds on CPU page fault handling performance,https://plus.google.com/102150693225130002912/posts/YDKRFDwHwr6,linus torvalds cpu page fault handling performance,one account google sign google account,one account google sign google account
532,Lobsters,scaling,Scaling and architecture,How Disqus Went Realtime with 165K Messages Per Second and Less than .2 SecondsLatency,http://highscalability.com/blog/2014/4/28/how-disqus-went-realtime-with-165k-messages-per-second-and-l.html,disqus went realtime message per second le second latency,update disqus still realtime go demolishes python adam hitchcock making disqus realtime slide twitter must solve stream capability stats engagement selltrade data work code likely fail code endtoend acks good expensive greenlets free publish free sometimes big win understand use case load testing really test system test real traffic use python increasing server count response scale sign architecture may need tuning use shelf technology,update disqus still realtime go demolishes python add realtime functionality web scale application adam hitchcock software engineer disqus talk excellent talk making disqus realtime slide disqus take commenting system add realtime capability something easy time talk hit billion unique visitor month disqus developed realtime commenting system called realertime tested handle million concurrently connected user new connection per second messagessecond le second latency endtoend nature commenting system io bound high fanout comment come must sent lot reader problem similar twitter must solve disqus solution quite interesting path solution tried different architecture settled solution built python django nginx push stream module thoonk unified flexible pipeline architecture process able substantially reduce server count easily handle high traffic load one point talk adam asks pipelined architecture good one disqus message filtering series transforms perfect match old idea unix system long stream capability creating flexible pipeline architecture incredibly flexible powerful way organizing code let see disqus evolved realtime commenting architecture created something old new process stats current million website use disqus commenting system half billion people engaged conversation every month million comment every month march platform python disqus service written python language django thoonk redis queue queue library top redis nginx push stream module pure stream http push technology nginx setup comet made easy really scalable gevent coroutinebased python networking library us greenlet provide highlevel synchronous api top libev event loop long polling using eventsource browser sentry realtime platformagnostic error logging aggregation platform scale track server state statistic allowing see server run raw metal architecture motivation realtime engagement realtime distribution comment encourages user stay page longer people comment realtime selltrade data create firehose product global comment stream old realtime system disqus app written django would post memcache many key forum id thread id user id post id maybe someone future might find interesting since pubsub cheap allows later innovation frontend client would poll memcache key every couple second client would display new comment problem scale network could use product one time first solution approach new post disqus redis pubsub flask web framework front end cluster haproxy client client would connect haproxy haproxy used handle million connection problem rapidly ran cpu flask machine redundant work two subscriber listening thread message would formatted twice second approach backend server created dedupe formatting work new flow new post disqus redis queue python glue gevent formatting server server redundancy redis pubsub server flask fe front end cluster big server ha proxy server client worked well except scaled using server especially flask cluster redis pubsub cluster also growing quickly third winning approach us pipelined architecture message pas queue queue acted upon filter switched nginx push stream module replaced redis pubsub flask server haproxy cluster new flow look like new post disqus redis queue python glue gevent formatting server server http post nginx pub endpoint nginx push stream module server client pubsub redis used nginx push stream module supported functionality push stream server required network memory limitation kernel socket allocation problem lot socket open otherwise could run server including redundancy disqus part flow django web app us postsave postdelete hook put stuff onto thoonk queue hook useful generating notification realtime data thoonk queue library top redis already thoonk used instead spinning ha cluster rabbitmq machine ended really liking thoonk implemented state machine easy see job claimed claimed etc make cleanup failure easy since queue stored redis using zsets range query performed queue useful implement endtoend acks ask message processed yet example take appropriate action python glue program listens thoonk queue performs formatting computation client includes cleaning formatting data originally formatting flask cluster took much cpu found gzipping individual message win enough redundancy message generate sufficient saving compression gevent run really fast io bound system like watchdog make sure greenlet always running say work always performed greenlet microthread implicit scheduling coroutines monitor watch lot failure raise alert observed pipelined architecture python glue program structured data pipeline stage data must go parsing computation publish another place run greenlet mixins used implement stage functionality jsonparsermixin annomizedatamixin supersecureencryptdatamixin httppublisher filepublisher idea compose pipeline message would come thoonk run pipeline jsonannonhttppipeline jsonsecurehttppipeline jsonannonfilepipeline pipeline share functionality yet still specialized great bringing new feature make new pipeline stage make new pipeline old pipeline run side side new pipeline old new feature happily coexist test also composable within pipeline run test insert filtermodulemixin pipeline test get run easy reason mixin easy understand one thing new engineer project much easier time groking system designed way nginx push stream handle pubsub aspect web serving aspect system well recently hit two million concurrent user server hit peak subscriber per machine mbytessecond per machine cpu usage continually write data socket test socket still open cleaned make room next connection configuration publish endpoint subscribe endpoint map data good monitoring builtin accessible push stream status endpoint memory leak module requires rolling restarts throughout day especially couple hundred thousand concurrent connection per process browser know quickly disconnected restart reconnect long er polling browserjavascript client side thing currently using websockets fast moving eventsource built browser browser handle everything register message type give callback handler testing darktime testing disqus installed million website need test million concurrent connection use existing network load test rather create faux setup instrumented client say user exactly website flow new system example darkesttime something important world happening couple website getting mega traffic took traffic sent single pubsub key system helped identify lot hot spot code measure measure thing pipelined system measure input output every stage reconcile data system like haproxy without measurement data way drill find wrong express metric really understand one sentry help find problem code measurement make easy create pretty graph pope selected white smoke seen traffic peaked mb per second tb data transferred day peak cpu lesson learned work large fanout architecture work one place send consumer repeat work consumer code likely fail code smart really smart people wrote redis product concerned code part system endtoend acks good expensive necessary customer want delivery every frontend user greenlets free use make code much easier read publish free publish channel able make great realtime map traffic without prior planning message published channel sometimes big win discovering nginx push stream module simplified huge chunk system reduced server count understand use case load testing really test system test real traffic much easier approach system get larger generating synthetic load would huge project use python really like python django though backend stuff written go increasing server count response scale sign architecture may need tuning take look one change architecture use resource efficiently use shelf technology feel like build everything scratch leverage code keep team small related article
534,Lobsters,scaling,Scaling and architecture,"Concurrency primitives, safe memory reclamation mechanisms and non-blocking data structures designed",http://concurrencykit.org/,concurrency primitive safe memory reclamation mechanism nonblocking data structure designed,architecture compiler feature,concurrency primitive safe memory reclamation mechanism nonblocking data structure research design implementation high performance concurrent system architecture arm power compiler gcc clang cygwin icc suncc feature atomic operation hardware transactional memory memory barrier hash table list ring stack fifo bitmap safe memory reclamation scalable lock execution barrier asymmetric synchronization
536,Lobsters,scaling,Scaling and architecture,Six Lessons Learned the Hard Way About Scaling a Million User System,http://highscalability.com/blog/2014/4/16/six-lessons-learned-the-hard-way-about-scaling-a-million-use.html,six lesson learned hard way scaling million user system,martin kleppmann six thing wish known scaling realistic load testing hard data evolution difficult database connection real limitation read replica operational pain think memory efficiency change capture underappreciated change capture system cache cache invalidation mysteriousllama,ever come point feel learned enough share experience hope helping others traveling road martin kleppmann done lovingly written six thing wish known scaling article well worth time advice scaling twitter building million user system sweet spot lot project conclusion ring true building scalable system sexy roflscale fun lot plumbing yak shaving lot hacking together tool really ought exist already open source solution bad end bad least solves particular problem gloss six lesson plus bonus lesson realistic load testing hard testing large distributed system like scientific experiment conducted ideal condition hard scientific minded accept knowing actual access pattern hard testing synthetic data set larger actually hard comparing correctness old new system hard prepared rollback new code nt work practice data evolution difficult data place database log blob updating data format change enormous time sink larger company often advantage automating orchestrating process database connection real limitation database connection add surprisingly fast system grows service node count connection eats resource machine developer figure deal use connection pooler write data access layer wrapping database access behind api read replica operational pain read replica offload database access master common scaling strategy also take lot work setup maintain system failure handling constant source problem think memory efficiency latency spike often caused memory problem using ram efficiently difficult hard figure ram actually used many performance problem solved buying ram fit index ram possible index hash string instead string change capture underappreciated data change system must flow many service like database search index graph index read replica cache invalidation etc could application write multiple location every time make update never work practice could apps read database log nt possible every system good solution use change capture system receives store writes database application receive update realtime andor stream history change change capture system becomes single source truth data application big advantage approach data producer consumer decoupled give great freedom experiment without fear bringing main site cache cache invalidation bonus lesson comment mysteriousllama article without proper caching good invalidation strategy database get pounded use redis memcache cache everything possible nt even connect database unless ensure invalidate cache entry easily keep thing atomic run race condition use locking ensure cache expires database get dogpile multiple copy query think querycache database choice may efficient trust even close also cache higherlevel object simple query depending reliability requirement may even consider treating cache writeback batched database writes background generally efficient individual writes due variety factor worked several ranking site always one main goto strategy scaling database suck avoid querying
537,Lobsters,scaling,Scaling and architecture,"Share Nothing, Scale Everything",https://blog.engineyard.com/2014/scale-everything,share nothing scale everything,previous post series replacing file system none server updated config gitdocs stateful approach positive negative pet server stateless approach positive cattle negative alternative cap theorem acid conclusion stateless environment next final post,previous post series explained sharednothing architecture place additional constraint cloud app developer also explained embracing constraint enables apps high scalability high availability present post explain adapt app cloud removing dependency file system order make compatible sharednothing architecture replacing file system putting file system cloud asking trouble deploying existing app cloud whether internal app offtheshelf app may find point contention common problem found apps designed traditional hosting environment expect file system behave like database write file expect file going exist point future problem language like php many offtheshelf apps existed since long cloud popular apps generally assume using one server master copy site life server unfortunately cause problem model work want scale across multiple server want keep master copy site revision control system like git let take one example wordpress default wordpress configuration requires write access wpcontent directory local file system log wordpress administration console make change wordpress may update file local file system multiple server none server updated config hand redeploy git configuration change overwritten could try use something like gitdocs automatically propagate change happens merge conflict local state becomes particularly byzantine app could fail instantly may even experience hidden corruption failing later way make difficult debug solution two primary approach take stateful approach allow write access file system application server positive requires modification app code environment configuration take stock wordpress release instance get running matter minute negative dealing pet server run single server since local file system change attempt run app across multiple server quickly get sync since server disappear without notice run risk downtime server run difficulty need rebuild snapshot addition deploying new code revert local change made stateless approach disallow write access application server number way achieve depending app deploying might simple switching uploading file instead file system though also mean converting code use library manipulating file something like wordpress might install one existing plugins something cover detail later post positive treat server cattle run many need increase decrease number server server run difficulty replace without noticeable downtime endusers negative configuration change require write access file system made production server one solution deploy app locally make configuration change commit change result source tree deploying locally make configuration change deploying production may seem unusual keep configuration change git seen reverted etc allows cluster hence scale application alternative another alternative use distributed file system way multiple app instance sharing single file system unfortunately reality complex file system distributed need think cap theorem consistency availability partition tolerance pick two sacrificing thing going hard dealing file system application expects file system always always consistent also start thinking acid file system level locking short mess distributed file system posix interface scaled decentralised way without sacrificing either function performance likely attempt scale posix file system like incur problem avoid approach prepared work around oddity distributed file system application conclusion primary obstacle adapting app compatible cloud handling file system discovered three approach eliminating reliance one stateless model easy setup work well cloud architecture sometimes luxury writing app scratch time make got luckily standard way deploy stateful application stateless environment cover next final post series well wrapping everything covered far difficulty dealing file system cloud app approach take war story share trying use distributed file system share thought comment
538,Lobsters,scaling,Scaling and architecture,"Manhattan, our real-time, multi-tenant distributed database for Twitter scale",https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale,manhattan realtime multitenant distributed database twitter scale,manhattan holistic view storage system twitter reliability availability extensibility operability low latency realworld scalability developer productivity reliability scale building storage system core sooner firstclass citizen predictability service layer core ostrich consistency model achieving consistency replica reconciliation storage engine storage service batch hadoop importing strong consistency service timeseries counter service interface tooling storage service storage service focus customer multitenancy qos quality service looking ahead acknowledgment armond bigian peter schuller chris goffinet boaz avital spencer fang ying xu kunal naik yalei wang danny chen melvin wang bin zhang peter beaman sree kuchibhotla osama khan victor yang ye esteban kuber tugrul bingol yi lin deng liu tyler serdar bulut andy gross anthony asta evert hoogendoorn lin lee alex peake yao yue hyun kang xin xiang sumeet lahorani rachit arora sagar vemuri petch wannissorn mahak patidar ajit verma sean wang dipinder rekhi satish kotha johan harjono alex young kevin donghua liu pascal borghino istvan marko andres plaza ravi sharma vladimir vassiliouk ning li liang guo inaam rana,twitter grown global platform public selfexpression conversation storage requirement grown last year found need storage system could serve million query per second extremely low latency realtime environment availability speed system became utmost important factor need fast needed scalable across several region around world year used made significant contribution many open source database found realtime nature twitter demanded lower latency existing open source product offering spending far much time firefighting production system meet performance expectation various product standing new storage capacity use case involved much manual work process experience developing operating production storage scale made clear situation simply sustainable began scope build next generation distributed database call manhattan needed take account existing need well put u position leapfrog exists today holistic view storage system twitterdifferent database today many capability experience identified requirement would enable u grow way wanted covering majority use case addressing realworld concern correctness operability visibility performance customer support requirement build reliability twitter service need durable datastore predictable performance trust failure slowdown expansion hotspot anything else throw availability use case strongly favor availability consistency alwayson eventually consistent database must extensibility technology built able grow requirement change solid modular foundation build everything new storage engine strong consistency additionally schemaless keyvalue data model fit need allowed room add structure later operability cluster grow hundred thousand node simplest operation become pain time sink operator order scale efficiently manpower make easy operate day one every new feature think operational complexity ease diagnosing issue low latency realtime service product require consistent low latency make proper tradeoff guarantee low latent performance realworld scalability scaling challenge ubiquitous distributed system twitter need database scale certain point continue grow new height every metric cluster size request per second data size geographically number tenant without sacrificing cost effectiveness ease operation developer productivity developer company able store whatever need build service self service platform require intervention storage engineer system view reliability scalewhen started building manhattan already many large storage cluster twitter understood challenge come running system scale informed kind property wanted encourage avoid new system reliable storage system one trusted perform well state operation kind predictable performance difficult achieve predictable system worstcase performance crucial average performance much well implemented correctly provisioned system average performance rarely cause concern throughout company look metric like latency care slow slowest request system design provision worstcase throughput example irrelevant steadystate performance acceptable periodic bulk job degrades performance hour every day priority predictable plan good performance potential issue failure mode customer interested implementation detail excuse either service work twitter even make unfavorable tradeoff protect unlikely issue must remember rare event longer rare scale scale come large number machine request large amount data also factor human scale increasing number people use support system manage focusing number concern customer cause problem problem limited customer spread others simple u customer tell issue originates storage system client potential issue must minimize time recovery problem detected diagnosed must aware various failure mode manifest customer operator need deep comprehensive knowledge storage system complete regular task diagnose mitigate issue finally built manhattan experience operating scale complexity one biggest enemy ultimately simple working trump fancy broken prefer something simple work reliably consistently provides good visibility something fancy ultraoptimal theory practice implementation work well provides poor visibility operability violates core requirement building storage systemwhen building next generation storage system decided break system layer would modular enough provide solid base build top allow u incrementally roll feature without major change designed following goal mind keep core lean simple bring value sooner rather later focus incremental multitenancy quality service qos selfservice firstclass citizen focus predictability storage service technology layerswe separated manhattan four layer interface storage service storage engine core corethe core critical aspect storage system highly stable robust handle failure eventual consistency routing topology management intra interdatacenter replication conflict resolution within core system crucial piece architecture completely pluggable iterate quickly design improvement well unit test effectively operator able alter topology time adding removing capacity visibility strong coordination topology management critical store topology information zookeeper strong coordination capability managed component infrastructure twitter though zookeeper critical path read writes also put lot effort making sure extreme visibility core time extensive set ostrich metric across host correctness performance consistency modelmany application fit well eventually consistent model favor high availability consistency almost use case natural build manhattan eventually consistent system core however always application require strong consistency data building system high priority adopting customer strong consistency optin model developer must aware tradeoff strongly consistent system one typically form mastership range partition many use case twitter hiccup second unavailability simply acceptable due electing new master event failure provide good default developer help understand tradeoff model achieving consistencyto achieve consistency eventually consistent system need required mechanism call replica reconciliation mechanism need incremental always running process reconciles data across replica help face bitrot software bug missed writes node going long period time network partition datacenters addition replica reconciliation two mechanism use optimization achieve faster convergence readrepair mechanism allows frequently accessed data converge faster due rate data read hintedhandoff secondary delivery mechanism failed writes due node flapping offline period time storage enginesone lowest level storage system data stored disk data structure kept memory reduce complexity risk managing multiple codebases multiple storage engine made decision initial storage engine designed inhouse flexibility plugging external storage engine future needed give u benefit focusing feature find necessary control review change go currently three storage engine seadb readonly file format batch processed data hadoop sstable logstructured merge tree based format heavy write workload btree btree based format heavy read light write workload storage engine support blockbased compression storage serviceswe created additional service sit top core manhattan allow u enable robust feature developer might come expect traditional database example batch hadoop importing one original use case manhattan efficient serving layer top data generated hadoop built importing pipeline allows customer generate datasets simple format hdfs specify location self service interface watcher automatically pick new datasets convert hdfs seadb file imported cluster fast serving ssds memory focused making importing pipeline streamlined easy developer iterate quickly evolving datasets one lesson learned customer tend produce large multiterabyte datasets subsequent version typically change le data baked optimization reduce network bandwidth producing binary diffs applied download data replica substantially reducing overall import time across datacenters strong consistency service strong consistency service allows customer strong consistency certain set operation use consensus algorithm paired replicated log guarantee inorder event reach replica enables u operation like checkandset ca strong read strong write support two mode today called localcas globalcas global ca enables developer strongly consistent operation across quorum datacenters whereas local ca operation coordinated within datacenter issued operation different tradeoff come latency data modeling application timeseries counter service developed specific service handle high volume timeseries counter manhattan customer drove requirement observability infrastructure needed system could handle million increment per second level scale engineer went exercise coming agreed upon set design tradeoff thing like durability concern delay increment needed visible alerting system kind subsecond traffic pattern could tolerate customer result thin efficient counting layer top specially optimized manhattan cluster greatly reduced requirement increased reliability previous system interfacesthe interface layer customer interacts storage system currently expose keyvalue interface customer working additional interface graph based interface interact edge toolingwith easy operability cluster requirement put lot thought best design tool daytoday operation wanted complex operation handled system much possible allow command highlevel semantics abstract away detail implementation operator started tool allow u change entire topology system simply editing file host group weight common operation like restarting node single command even early tooling started become cumbersome built automated agent accepts simple command goal state cluster able stack combine execute directive safely efficiently attention operator storage servicea common theme saw existing database designed setup administered specific set usecases growth new internal service realized efficient business solution storage service provided major productivity improvement engineer operational team building fully selfservice storage system put engineer control engineer provision application need storage size query per second etc start using storage second without wait hardware installed schema set customer within company run multitenant environment operational team manage managing self service multitenant cluster imposes certain challenge treat service layer firstclass feature provide customer visibility data workload builtin quota enforcement ratelimiting engineer get alerted go defined threshold information fed directly capacity fleet management team analysis reporting making easier engineer launch new feature saw rise experimentation proliferation new usecases better handle developed internal apis expose data cost analysis allows u determine use case costing business well one used often focus customereven though customer fellow twitter employee still providing service still customer must provide support call isolate action one application another consider customer experience everything developer familiar need adequate documentation service every change addition storage system requires careful consideration feature seamlessly integrated selfservice different requirement one need intervention operator customer problem must make sure design service quickly correctly identify root cause including issue emergent behavior arise many different client application engineer access database lot success building manhattan ground service piece technology multitenancy qos quality service supporting multitenancy allowing many different application share resource key requirement beginning previous system managed twitter building cluster every feature increasing operator burden wasting resource slowing customer rolling new feature quickly mentioned allowing multiple customer use cluster increase challenge running system must think isolation management resource capacity modeling multiple customer rate limiting qos quota addition giving customer visibility need good citizen designed rate limiting service enforce customer usage resource quota monitor needed throttle resource usage across many metric ensure one application affect others system rate limiting happens coarse grain subsecond level tolerance kind spike happen real world usage consider automatic enforcement control available manually operator help u recover issue mitigate negative effect customer including one going capacity built apis needed extract data every customer send capacity team work ensure resource always ready available customer small medium requirement twitter standard engineer get started without additional help u integrating directly selfservice allows customer launch new feature large multitenant cluster faster allows u absorb traffic spike much easily since customer use resource time looking aheadwe still lot work ahead u challenge increasing number feature launched internally manhattan growing rapid pace pushing harder better smarter drive u core storage team take pride value make twitter better make customer successful plan release white paper outlining even technical detail manhattan learned running two year production stay tuned acknowledgmentswe want give special thank armond bigian helping believe team along way championing u make best storage system possible twitter following people made manhattan possible peter schuller chris goffinet boaz avital spencer fang ying xu kunal naik yalei wang danny chen melvin wang bin zhang peter beaman sree kuchibhotla osama khan victor yang ye esteban kuber tugrul bingol yi lin deng liu tyler serdar bulut andy gross anthony asta evert hoogendoorn lin lee alex peake yao yue hyun kang xin xiang sumeet lahorani rachit arora sagar vemuri petch wannissorn mahak patidar ajit verma sean wang dipinder rekhi satish kotha johan harjono alex young kevin donghua liu pascal borghino istvan marko andres plaza ravi sharma vladimir vassiliouk ning li liang guo inaam rana
541,Lobsters,scaling,Scaling and architecture,WebScaleSQL: A collaboration to build upon the MySQL upstream,https://code.facebook.com/posts/1474977139392436/webscalesql-a-collaboration-to-build-upon-the-mysql-upstream/,webscalesql collaboration build upon mysql upstream,built far working nonblocking client logical readahead mechanism expect future webscalesqlorg,help billion people use facebook share connect build expansive incredibly advanced infrastructure including one largest deployment mysql world along way learned benefited code change made mysql community today announcing webscalesql collaboration among engineer several company face similar challenge running mysql scale seek greater performance database technology tailored need webscalesql currently includes contribution mysql engineering team facebook google linkedin twitter together working share common base code change upstream mysql branch use made available via open source collaboration expand existing effort mysql community continue track upstream branch latest productionready release currently mysql goal launching webscalesql enable scaleoriented member mysql community work closely together order prioritize aspect important u aim create integrated system knowledgesharing help company leverage great feature already found mysql building adding feature specific deployment large scale environment last month engineer four company contributed code provided feedback develop new unified collaborative branch mysql effective collaboration far know one trying solve particular challenge keep webscalesql open go encourage others scale resource customize mysql join effort course welcome input anyone want contribute regardless currently working built far want webscalesql able collaborate effectively move fast end set system collaborating reviewing code reporting bug example introduce code change webscalesql engineer propose change webscalesql engineer another company review code provide feedback engineer agree change make sense functional pushed webscalesql branch everyone use beyond organization may customize webscalesql suit need today already produced exciting result working together engineer involved webscalesql made major change aid development new branch including automated framework proposed change run publish result mysql builtin test system mtr several change test already found mysql structure existing code avoid problem otherwise safe code change previously caused test fail caused unnecessary conflict change make easier work code helped u get started creating webscalesql working initial accomplishment started work number improvement upstream mysql activity facebook webscalesql team currently working contributing asynchronous mysql client link mean querying mysql wait connect send retrieve nonblocking client currently codereviewed webscalesql team used production facebook many month preparing move facebook productiontested version table user compression statistic webscalesql preparing push remaining component facebook current productiontested version compression already included mysql webscalesql adding logical readahead mechanism proven production achieve large quantifiable speed improvement full table scan nightly logical backup expect future keep webscalesql work open create useful branch others within mysql community focused scale deployment continue follow uptodate upstream version mysql long mysql community release continue committed remaining branch fork mysql excited expand existing work webscalesql think collaboration represents opportunity scaleoriented member mysql community work together efficient transparent way benefit u learn get involved visit webscalesqlorg
542,Lobsters,scaling,Scaling and architecture,"Scout Realtime: live, in-browser CPU/memory stats for your server",http://scoutapp.github.io/scout_realtime/,scout realtime live inbrowser cpumemory stats server,question free source github scout secure operating system supported proc file system version ruby required server need public ip domain name multiple people view realtime stats ssh tunnel pain way set persistent access specify different port run specify log andor pid written much cpu memory consume leave running installing gem nt work log file stop daemon uninstall stuck talk human open github issue support scoutappcom,question scoutrealtime free yes opensource view source github scoutrealtime maintained team scout saas server monitoring solution trusted ten thousand developer scoutrealtime secure daemon fetch key metric server store memory render browser inbound communication nothing written disk run production server operating system supported scoutrealtime relies heavily proc file system fetch metric procfs available linuxbased distribution osx freebsd full support procfs supported version ruby required need ruby server run scoutrealtime server need public ip domain name yes need public ip domain name view scoutrealtime browser multiple people view realtime stats yes open port firewall instead using ssh tunnel sudo iptables input p tcp dport j accept ssh tunnel pain way set persistent access yes open port firewall using iptables command specify different port run yes scoutrealtime help option specify log andor pid written yes scoutrealtime help option much cpu memory consume leave running clocked cpu usage scoutrealtime daemon intel xeon cpu memory usage around mb turn metric collection clicking pause button web page cpu usage effectively drop still able visit web page reenable metric time installing gem nt work need ruby server run scoutrealtime try ruby confirm ruby ruby v show ruby version gem confirm ruby gem ruby package manager available log file default log written scoutscoutrealtimelog stop daemon scoutrealtime stop uninstall scoutrealtime gem uninstall scoutrealtime stuck talk human open github issue shoot u email support scoutappcom
543,Lobsters,scaling,Scaling and architecture,Facebook News Feed: Social Data at Scale,http://www.infoq.com/presentations/Facebook-News-Feed,facebook news feed social data scale,infoq homepage presentation facebook news feed social data scale summary bio conference related sponsored content,infoq homepage presentation facebook news feed social data scale facebook news feed social data scale summary serkan piantino discus news feed facebook basic infrastructure used feed data stored centrifuge storage solution bio serkan piantino engineering manager facebook head facebook new engineering office new york city manages team power facebook message chat previous experience includes building infrastructure ranking algorithm news feed managing development new version facebook home page leading development timeline conference software changing world qcon aim empower software development facilitating spread knowledge innovation enterprise software development community achieve qcon organized practitionerdriven conference designed people influencing innovation team team lead architect project manager engineering director recorded nov related sponsored content
544,Lobsters,scaling,Scaling and architecture,Building a Better PHP  Part 2: Using HHVM,http://ey.io/1oUWylM,building better php part using hhvm,using hhvm part one series composer composer numerous tweet note note setting web server zend framework note phpunit using fastcgi unix socket scheduled march note coming,using hhvm continuing part one series working hhvm environment start use something practical phpinfo composer one first thing today improve workflow start using hhvm composer noted numerous tweet make composer run faster use le resource standalone process easy switch away phpnet importantly switch back thing go awry first let start installing composer hhvm default http request timeout want change setting change need set httpslowquerythreshold configuration option set serverhdf config affecting request adding http slowquerythreshold via command line using v flag using php flag emulate php cli flag hhvm v hhvm php note may require higher threshold depending internet connection install couple dependency sudo aptget install curl git unzip install composer simply curl s http getcomposerorginstaller sudo hhvm php sudo mv composerphar usrlocalbincomposer us php flag allow u pipe composer installer code directly hhvm next setup alias composer allow u run via hhvm set two network configuration variable alleviate issue slower connection additionally set evaljitfalse jit beneficial longrunning process used cli would case composer cause hhvm startup slower alias composer hhvm v v v evaljitfalse usrlocalbincomposer make permanent add line bashrc run source bashrc run composer setup project using example zend framework skeleton application composer createproject prefersource zendframeworkskeletonapplication devmaster note got errorexception undefined variable inputstream however completed successfully nonetheless setting web server zend framework next update nginx hhvm server configuration point document root nginx edit etcnginxsitesavailablehhvmconf change following two line root inside location blockfastcgiparam scriptfilename fastcgiscriptname hhvm change serversourceroot configuration sourceroot next restart service sudo service nginx restart sudo service hhvm restart note prior hhvm init script ubuntu least broken instead use sudo killall hhvm sudo service hhvm start restart access web server browser see default skeleton application start develop new application phpunit another common task improved switching hhvm running unit test first install phpunit using composer composer global require complete installation add following alias bashrc run source bashrc alias phpunithhvm v evaljitfalse homevagrantcomposervendorbinphpunit phpunit command run via hhvm like composer want run test suite running following cd composer install cd test phpunit using fastcgi unix socket next version hhvm scheduled march see support fastcgi unix socket unix socket theory perform better tcp socket easy use choose make change sure benchmark change ensure actually seeing performance increase resource usage decrease use unix socket need change server configs hhvm change etchhvmserverini like hhvmserverport varrunhhvmserversock nginx change etcnginxsitesavailablehhvmconf location hhphp fastcgipass fastcgipass unix varrunhhvmserversock restart daemon service hhvm restart service nginx restart note use feature must install current nightly build vagrantfile default coming well overview using hhvm replace phpnet two practical application hhvm put use immediately next part series take indepth look hack feature brings table well review might mean phpnet tried hhvm yet experience think hhvm far let u know comment
545,Lobsters,scaling,Scaling and architecture,Building a Better PHP with HHVM and Hack: Part 1,http://ey.io/1lTljNn,building better php hhvm hack part,release hack hhvm brief history hiphop hhvm v phpnet environment impact found generator arrayof sara golemon extension zephir phpcpp mongofill hhvmnative interface hni hhvm documentation extension api hack note xhp extension installing hhvm vagrant vm running hhvm serverhdf serverini note hhvm cloud php stack coming today,official release hack hhvm today facebook thought would good idea take look hhvm hack detail facebook probably largest php installation planet yet recent year turned away favor homegrown solution point even call php shop answer lie somewhere absolutely crazy brief history hiphop facebook initially created hphpc php c compiler would translate huge codebase c code compile process took long time hour gave performance gain negative impact developer even bigger alongside facebook also created hphpi developer mode require compiling entire codebase every time hphpd debugger hphpc way success facebook giving necessary performance gain allow scale beyond previous ability hardware built php could support subset language example support dynamic function created using createfunction dreaded eval order move forward something new needed facebook released first public version hiphop virtual machine hhvm deprecated hphpc sibling favor fastperforming time compiler jit took php code compiled machine code fly initial release hhvm almost compatible php continues add support new language feature php mantra hear team behind hhvm bugforbug parity could create perfect php implementation trying recreate edgecases oddity php language migration path easier said committed fixing actual bug phpnet possible fix without breaking backwards compatibility huge way see facebooks effort achieve parity comparing popular open source test suite result hhvm phpnet hhvm v phpnet hhvm faster much faster lot case speed everything people focus performance improvement hhvm phpnet primary differentiator definitely thing consider environment impact carbon though given fact hhvm drastically reduce number server therefore power requirement argument made impact technical environment release hhvm added support fastcgi interface mean currently using phpfpm hhvm effectively dropin additionally possible run phpnet hhvm simultaneously simply routing request different fastcgi proxy sysadmins using ubuntu debian fedora package available directly hhvm team one biggest pain point developer lack ide support specifically debugging may also allow set hhvm binary inplace php binary running command line script thing like phpunit perhaps biggest deficiency point however lack support standard php extension mean use pecl extension probably work hhvm box team actively trying implement common extension however making great progress list currently available extension found believe hhvm fragment php community certainly possibility however hhvm team actively involved least trying contribute back phpnet example feature first implemented hhvm later php generator added php another example recent rejected rfc arrayof syntax overlapped heavily generic supported hhvm hack language later hhvm team via sara golemon participated heavily discussion around extension hugely renewed interest php extension lately tool like zephir phpcpp possibly spurred success hhvm undoubtedly maturation community enterprise space last decade hhvm long extension wrap cc library simply write php hack hhvm good job compiling machine code fast faster writing thing c example community contributed mongofill mongo extension replacement hhvm case need drop c hhvm provides hhvmnative interface hni allows stub function phphack write implementation c full detail implementing extension hhvm see hhvm documentation extension api hack people think hhvm alternative php runtime additionally runtime facebooks language known hack best described syntactical sibling php support number new feature scalar type hinting argument return value generic collection constructor argument promotion besides hack supported alongside standard php syntax major feature fact developer tool paired hhvm static analysis tool get realtime information thing like type mismatch additionally allows hhvm make informed decision compiling code however compiled hack php code identical far hhvm concerned hack hhvm great place trial new potential feature php general hope see lot great feature back ported phpnet future yes possibly ungoogleable name ever hack code start hh open tag rather standard php open tag use closing tag hack interspersed html like regular php great separation concern support native xhtml firstclass syntax note xhtml syntax added phpnet using xhp extension hhclass calculator public function add int left int right int return left right calc new calculator calc add calc add one three fatal error running result fatal error fatal error argument passed calculator add must instance int string given file line hhvms static analysis tool error found without even running code even noncommon code path see hack part series installing hhvm installing hhvm easy booting vagrant vm simply place following vagrantfile vagrantfileapiversion vagrantconfigure vagrantfileapiversion config use ubuntu lts configvmbox configvmnetwork publicnetwork create bridge network install hhvm configvmprovision shell inline shell aptget update aptget install pythonsoftwareproperties forceyes addaptrepository ppa mapnikboost wget http dlhhvmcomconfhhvmgpgkey sudo aptkey add echo deb http dlhhvmcomubuntu precise main sudo tee etcaptsourceslistdhhvmlist aptget update aptget install hhvmnightly forceyes aptget install screen vim forceyes debconfsetselections mysqlserverrootpassword password pa debconfsetselections mysqlserverrootpasswordagain password pa aptget install mysqlserver forceyes shellend vagrantfile ready go simply issue vagrant command wait machine boot able ssh using vagrant ssh play hhvm running hhvm hhvm installed simply call using hhvm command example assuming calculator class calcphp might run addition simple command line usage two way run hhvm web first builtin server way run hhvm recently second option fastcgi added use hhvm fastcgi need sit behind web server like nginx similar phpfpm install simply sudo aptget install nginx sudo mkdir varwww sudo chown vagrant daemon varwww next add new nginx config file etcnginxsitesavailablehhvmconf server servername hhvmdev root varwww index indexphp location hhphp fastcgipass fastcgiindex indexphp fastcgiparam scriptfilename varwww fastcgiscriptname include fastcgiparams install place default sudo rm etcnginxsitesenableddefault sudo ln etcnginxsitesavailablehhvmconf etcnginxsitesenabledhhvmconf sudo service nginx restart next setup hhvm run fastcgi updating etchhvmserverhdf etchhvmserverini current nightlies config file serverhdf server port type fastcgi sourceroot varwww log level error uselogfile true file varloghhvmerrorlog access file varloghhvmaccesslog format h l u r b repo central path varloghhvmhhvmhhbc serverini php optionspid varrunhhvmpid hhvm specifichhvmserverport fastcgihhvmserversourceroot varwwwhhvmserverdefaultdocument indexphphhvmloglevel errorhhvmloguselogfile truehhvmlogfile varloghhvmerrorloghhvmrepocentralpath varrunhhvmhhvmhhbc restart start hhvm note prior hhvm recent nightlies init script ubuntu least broken instead use sudo killall hhvm sudo service hhvm start restart finally add indexphp webroot varwww access web server get ip calling ifconfig case hhvm output simply hhvm cloud engine yard constantly track new technology tracking hhvm closely year excited seeing yet support hhvm stack working hard integrate php stack hhvm cloud mean better utilization cloud resource allowing stretch budget le coming next part series look practical application hhvm start using today even ready move codebase
546,Lobsters,scaling,Scaling and architecture,Reliable real-time processing at Spotify,https://www.jfokus.se/jfokus14/preso/Reliable-real-time-processing-with-Kafka-and-Storm.pdf,reliable realtime processing spotify,,obj length r filter flatedecode stream ya v endstream endobj obj endobj obj type page parent r resource r content r mediabox annots r endobj obj procset pdf text imageb imagec imagei colorspace r font r r xobject r r r endobj obj r endobj obj length r filter flatedecode type xobject subtype form formtype bbox resource r group transparency c r true k false stream j v g k id  uo endstream endobj obj endobj obj procset pdf extgstate r r r r r xobject r r shading r endobj obj length r filter flatedecode type xobject subtype form formtype bbox resource r group transparency c r true k false stream g endstream endobj obj endobj obj procset pdf text colorspace r font r endobj obj length r filter flatedecode type xobject subtype form formtype bbox resource r group transparency c r true k false stream  endstream endobj obj endobj obj procset pdf colorspace r extgstate r endobj obj length r filter flatedecode type xobject subtype form formtype bbox resource r group transparency c r true k false stream k endstream endobj obj endobj obj procset pdf extgstate r r shading r endobj obj colorspace r shadingtype coords domain extend true true function r endobj obj colorspace r shadingtype coords domain extend true true function r endobj obj length r type xobject subtype image width height interpolate true colorspace r intent perceptual smask r bitspercomponent filter flatedecode stream h e  ya  hp a l f uy l r  endstream endobj obj endobj obj length r type xobject subtype image width height colorspace devicegray interpolate true bitspercomponent filter flatedecode stream k r wtxxz u endstream endobj obj endobj obj type extgstate ca endobj obj type extgstate opm endobj obj type extgstate bm softlight endobj obj type extgstate ca endobj obj type extgstate smask g r luminosity bc endobj obj type extgstate bm darken endobj obj separation black r r endobj obj length r n alternate devicergb filter flatedecode stream vdf c n z v l r dgry j e e n c l q wt w endstream endobj obj endobj obj iccbased r endobj obj length r n alternate devicecmyk filter flatedecode stream  h k l n z  un h
547,Lobsters,scaling,Scaling and architecture,The Four Hamiltons Framework for Mitigating Faults in the Cloud,http://highscalability.com/blog/2014/3/3/the-four-hamiltons-framework-for-mitigating-faults-in-the-cl.html,four hamilton framework mitigating fault cloud,patrick eaton avoid mask bound fix fast mask published postmortem application conclusion,guest post patrick eaton software engineer distributed system architect stackdriver stackdriver provides intelligent monitoringasaservice cloud hosted application behind easytouse service large distributed system collecting storing metric event monitoring alerting analyzing serving result web ui run cloud mostly aws spend lot time thinking deal fault cloud developed framework thinking fault mitigation large cloudhosted system endearingly call framework four hamilton inspired article james hamilton vice president distinguished engineer amazon web service article led framework called power failure seen around world hamilton analyzes cause power outage affected super bowl xlvii early article hamilton writes looking system fault tool mitigate impact avoid fault entirely protect fault redundancy minimize impact fault small fault zone minimize impact fast recovery mitigation option roughly ordered increasing impact customer article refer strategy order avoid mask bound fix fast hamilton enumerates technique dealing fault architecture public cloud determine type system fault expect design solution amazon web service aws google cloud expose three fault domain smallest scale instance single virtual machine fails independently virtual machine largest scale region geographic area often multiple data center fails independently region region comprised multiple zone typically single data center isolated zone region rackspace also expose multiple geographic region claim data center network always available thus concept zone needed combining fault domain technique addressing fault domain produce grid shown guide u consider survive fault cloudhosted application section discus strategy applied fault domain cloud also highlight employ strategy system stackdriver avoid first obvious strategy surviving failure avoid altogether fault never happen easiest one fix impact user experience traditionally avoiding failure required highquality hardware reliable enterprisegrade server disk networking gear cloud however built commodity component cloud infrastructure provider make guarantee reliability hardware consequently system designer application builder little control reliability virtual hardware provision control however software fault avoidance cloud depends solid system architecture disciplined software engineering avoid failure instance level write highquality software test manage software complexity choose high quality software component web server database etc library preferably one battle tested track configuration carefully automate management avoid errorprone manual change application builder generally avoid fault bigger fault domain region zone control domain however way avoid fault let someone else handle cloud provider offer hosted service claim protect zone region outage certainly component hosted service fail key point though perspective application builder hosted service provides abstraction reliable cloud provider mask fault service example hosted service avoids fault aws relational database service rds offer multiaz zone deployment option includes synchronous replica automatic failover mask zone failure making appear reliable database application builder another example aws simple queue service sqs documentation suggests store copy message across data center single region perspective system builder reliable presence instance zone failure largest fault domain aws route hosted dns service support failover region provide appearance reliable name resolution even presence region failure stackdriver approach avoid software error support goal highquality software use rollbar http rollbarcom software error monitoring reporting software fails message sent rollbar api record error bug trigger report quickly squashed avoid possible future impact avoid configuration error instance level use puppet http puppetlabscom ensure instance running known tested stable configuration puppet define software running instance version software configuration software provision deploy new instance without introducing error even responding quickly incident avoid error zone region level make heavy use hosted service provide reliable service despite component failure hosted service relational database use aws rds multizone replication avoid instance zone failure reliable load balancing use aws elb protect instance zone failure finally much lowvolume messaging use aws sqs reliable queueing service affected instance zone failure mask avoid fault next best alternative mask fault redundancy masked failure generally affect availability though performance service may change incrementally example failure request latency may increase several technique providing redundancy refer generically replication thus solution provide masking take several form system fully replicate functionality across multiple peer allow resource handle request system use single master primary node one slave secondary node automatic failover master fails still system use clustering approach quorum determining valid state value mitigate failure individual instance must ensure replica online take instance fails mitigate failure zone must ensure application service distributed across one zone also must sure service sufficiently overprovisioned distributed handle increased load healthy zone event zone failure similarly handling failure region requires service distributed across one region capacity sufficient handle load one region unavailable stackdriver approach many service system employ typical scaleout design strategy masking instance zone failure service use multiple worker consuming message single aws sqs queue instance hosting worker fails failed worker stop consuming message worker continue process request going one level deeper sqs service mask failure appear reliable application builder service mask instance zone failure deploying multiple worker behind aws elastic load balancer elb instance hosting worker fails becomes unavailable due zone failure load balancer remove worker continues distributing load among remaining worker going level deeper elb hosted service provides abstraction reliable load balancer use cassandra primary data store time series data must accessed interactive latency basic design cassandra ensures survives instance failure cassandra deployment span multiple zone single region configured replication level strategy allows survive failure entire zone key replicated three time way two replica hosted zone via awsaware replica placement support endpoint monitoring feature measure availability latency customer internetfacing service run service initiate probe request deploy probe across multiple region even multiple provider ensure probe query endpoint even event failure region using multiple provider feature could even tolerate complete outage single provider bound unfortunately always possible avoid mask failure incident masked try minimize scope failure customer may impacted incident strategy considers way reduce impact failure level instance zone region cause problem bounding impact best mitigation strategy design engineering effort varies depending type failure limited choice consider similar typical choice detailed limit impact customer common approach strategy sharding sharding different customer served resource different fault domain resource fail customer hosted failed resource affected prioritize certain class request failure related limited capacity give preference certain type request could prioritize traffic paying customer trial customer could prioritize highprofit request lowprofit request could prioritize lowlatency request longlatency request shedding low priority traffic ease load allow fix system degrade service gracefully properly architected subsystem application independent much possible case large part application continue function even feature failing example app could provide browse functionality even search working app could allow downloads even uploads possible app could allow purchase even feedback feature available stackdriver approach reduce load system customer view data long interval system run batch aggregation job precompute rollups various function multiple time scale eg average minute interval aggregation function fall behind due reduced capacity instance zone failure stackdriver currently run primary data pipeline single region aggregation process fall behind service retrieves data graph aggregate data dynamically raw data cost dynamic aggregation additional load system latency customer longrange graph remain available stackdriver design different pipeline data processing alerting pipeline operate independently data processing delayed due failure instance zone data processing delayed alerting pipeline continues function unimpeded losing ability view current data dashboard undesirable alerting feature important customer continue working even within data processing pipeline different function operate independently data processing pipeline archive data durable object store index cassandra elasticsearch different function impacted instance zone failure example elasticsearch unavailable system still push data archive cassandra elasticsearch available delayed data flushed pipeline note also archival pipeline designed simple possible since among critical function system fix fast worst case failure happens hide sugarcoating service user see outage use service best option fix fast rely strategy often wrong significant customervisible downtime several design engineering approach help fix problem fast revert code approach useful instance failure due single faulty commit simply revert commit redeploy last known working code recovery completed time take deploy code provision deploy new instance approach address instance zone region failure failure due resource crashing dying otherwise becoming unavailable unreachable replace lost resource stateless replacing failed resource restore service recovery completed time take provision configure new resource restore replica approach address instance zone region failure handle zone region failure replica must obviously located outside impacted fault domain failure due lost state restore state replica backup replica kept online database slave recovery completed promoting slave backup online recovery delayed state recovery backup document procedure recovering failure identify biggest threat system predict impact think recovery plan scenario practice disaster recovery probably want bring whole application fire drill induce failure practice recovery regularly exercise help test design fault tolerance recovery procedure tool recovery test help find way recover faster real failure future stackdriver approach thankfully many story tell illustrate mitigation strategy fault occur system system fault handled via masking bounding allowing u make fix background without affecting customer automation system allow u revert code provision new instance quickly needed typically however enough resource online mask problem also designed response several disaster scenario one major incident tested recovery plan fault domain one one far considered failure one core subsystem october cassandra cluster storing serving time series measurement data failed catastrophically due error realized would able recover cluster began execute previouslydiscussed plan provision new cassandra cluster repopulate historical data read incident published postmortem recovery still took hour procedure previously designed automation previously built key recovering quickly example developed script provision deploy new cassandra cluster automatically also built many component ability recover state historical data archive application use framework help think failure happen system system currently responds evaluate system iterating system component compare fault mitigation strategy four hamilton rely fix fast often building system designed frequent downtime improve reliability system upgrading fault mitigation strategy admittedly cost associated implementing strategy typically cost higher strategy le customer impact pragmatically may decide use mitigation strategy bigger customer impact due tradeoff development cost operation cost outage cost conclusion delivering large highlyavailable service public cloud relatively new art best practice still emerging designing faulttolerant distributed system provide alwayson service requires understanding type fault threaten system technique mitigating fault james hamilton give u list fault mitigation strategy avoid fault avoid mask fault redundancy mask minimize fault impact small fault zone bound minimize fault impact fast recovery fix fast public cloud define fault domain must considered instance zone region combining list provides rich framework considering alternative handling failure big system applying framework make system reliable make customer happy wellrested
548,Lobsters,scaling,Scaling and architecture,Caching the uncacheable: CloudFlare's Railgun,http://blog.cloudflare.com/cacheing-the-uncacheable-cloudflares-railgun-73454,caching uncacheable cloudflare railgun,railgun cloudflare business enterprise latency problem dave fayram caching uncachable technical detail blowing door standard gzip sdch tcp connection time slow start learn railgun upgrade cloudflare business enterprise account,cloudflare recently rolled premium service called railgun available cloudflare business enterprise customer railgun web optimization software designed speed delivery content cached one major advantage using cloudflare cacheable content image javascript cs html cached cloudflare delivered data center around world cloudflare data center covering entire globe cached content get delivered quickly web surfer wherever overcomes latency problem content cacheable must obtained real origin web server railgun overcomes problem using scheme able cache dynamically generated personalized web page dramatically reducing bandwidth used improving download time image credit dave fayram caching uncachable railgun work recognizing uncacheable web page change rapidly example captured cnn homepage html minute one hour page size five minute still one hour later cnn set caching page second one minute necessary download entire page looking inside page much changed fact change version order byte almost screenshot one small binary difference cnn home page five minute interval yellow byte changed rest experiment cloudflare revealed similar change value across web example redditcom change five minute hour new york time home page change five minute hour bbc news change five minute hour although dynamic web cacheable also changing quickly mean moment moment small change version page railgun us fact achieve high rate compression similar video compression look change frame frame railgun look change page download download technical detail railgun consists two component sender listener sender installed every cloudflare data center around world listener software component premium customer install network sender listener establish permanent tcp connection secured tl tcp connection used railgun protocol binary multiplexing protocol allows multiple http request run simultaneously asynchronously across link web client railgun system look like proxy server instead server widearea link special property one property performs compression noncacheable content synchronizing page version end railgun link keep track last version web page requested new request come page railgun already seen change sent across link listener component make http request real origin web server uncacheable page make comparison stored version sends across difference sender reconstructs page cache difference sent side blowing door standard gzip course compression used web page today common technique gzip page cnn actually sends byte gzipped data decompressed become byte page page compressed original size google proposed somewhat complex dictionary based scheme called sdch widely deployed railgun compression technique go much compression version page five minute interval result byte difference data sent compression original page size one hour difference version byte compression original page size clearly railgun compression outpeforms gzip enormously page frequently accessed delta often small fit inside single tcp packet connection two part railgun kept active problem tcp connection time slow start eliminated railgun mean premium cloudflare customer entire web becomes almost completely cacheable learn railgun upgrade cloudflare business enterprise account enable site
549,Lobsters,scaling,Scaling and architecture,ZooKeeper Resilience at Pinterest,http://engineering.pinterest.com/post/77933733851/zookeeper-resilience-at-pinterest,zookeeper resilience pinterest,win learn,win learn pinterest growing learning experimentation believe spirit exploration inside pinterest wall learn success epic failure
550,Lobsters,scaling,Scaling and architecture,An analysis of Facebook photo caching,https://code.facebook.com/posts/220956754772273/an-analysis-of-facebook-photo-caching,analysis facebook photo caching,haystack academic study sosp,every day people upload million photo facebook dec view many news feed friend timeline facebook store photo haystack machine optimized store photo also deep distributed photoserving stack many layer cache delivers photo people view recently published academic study stack sosp post describe stack cover four many finding effective layer stack popularity distribution photo change across layer effect increasing cache size effect advanced caching algorithm
551,Lobsters,scaling,Scaling and architecture,Are your servers pets or cattle?,http://www.lauradhamilton.com/servers-pets-versus-cattle,server pet cattle,randy bias cloudscaling christian haugen flickr thskyt flickr number tool conformity monkey doctor monkey security monkey designed service kill healthy server pet v cattle blog post,yesterday attended talk randy bias cloudscaling talk called pet v cattle elastic cloud thought randy perspective infrastructure system administration really interesting going explain key point shared u yesterday olden day cloud know company favored enterprise computing model system administration enterprise computing model guidriven ticketbased reserved scaleup smart hardware proprietary siloed oldfashioned enterprise computing infrastructure model server given cutesy name like cookie dakota reagan aardvark server procured individually configured hand often several different people server configured manually two server exactly like machine like special snowflake one server suddenly fall stopped allhandsondeck bring server back life basically server treated like pet image credit christian haugen flickr instead treating machine pet randy told audience treating cattle one get sick shoot em head replace em new one bias ceo cloudscaling image credit thskyt flickr system administration model configuration deployment automated server expendable apidriven selfservice automated scaleout commodity hardware open source collaborative configuration automated deployment automated one server start issue get rid spin brand new one place best practice said randy develop software automatically detects malfunctioning machine retires spin new one place netflix really cutting edge cloud computing several year number tool sort simian theme designed identify remove underperforming machine conformity monkey find instance fail conform best practice shuts doctor monkey run health check instance remove sick instance service security monkey find security violation vulnerability terminates offending instance netflix even take step actually designed service kill healthy server order test fault tolerance infrastructure netflix developed system love chaos monkey chaos monkey job randomly kill instance service within architecture constantly testing ability succeed despite failure likely work matter event unexpected outage tech blog randy posted slide talk also check noah slater pet v cattle blog post
552,Lobsters,scaling,Scaling and architecture,The WhatsApp Architecture Facebook Bought For $19Billion,http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html,whatsapp architecture facebook bought billion,center engagement purpose built apps investment sequoia capital used deal focus messaging privacy facebook chat optimization dark grungy work suitable troll engineer get data need write tool patch tool add knob measure remove bottleneck test repeat erlang rock crack virality code profit value employee count officially divorced something brutal focus user idea limit cause simplicity ok age thing start simply customize keep server count low purposely overprovision hardware growth stall charge money inspiration come strangest place hacker news whatsapp actually worth say facebook zuckerberg internetorg sealed deal,rick reed upcoming talk march titled billion b scaling next level whatsapp reveals eye popping whatsapp stats hundred node thousand core hundred terabyte ram hope serve billion smartphones soon reality around globe erlangfreebsdbased server infrastructure whatsapp faced many challenge meeting evergrowing demand messaging service continue push envelope size core speed erlang message per second serving system since talk yet let take look talk rick reed gave two year ago whatsapp scaling million simultaneous connection built high performance messaging bus c yahoo rick reed new world high scalability architecture founder also exyahoo guy little experience scaling system whatsapp come scaling prowess honestly since big hairy audacious goal every smartphone world could many billion phone year need make experience get fact let digress moment absolutely fascinating conundrum whatsapp possibly worth billion facebook programmer ask whatsapp worth much answer expletive sending stuff network get real also guy thought need blogging platform hard remote login server edit indexhtml file vi write post html taken quite realize code stupid getting user love use product hard part buy love make whatsapp valuable technology ignore people say could write whatsapp week php simply true see pretty cool technology certainly facebook sufficient chop build whatsapp wished let look feature know whatsapp gimmick ad gimmick game product loyal user across world offer free texting cruel world sm charge abusive sheltered american surprised see many real people use whatsapp really stay touch family friend get whatsapp likely people know already since everyone phone mitigates empty social network problem aggressively cross platform everyone know use work work phrase often used full featured shared location video audio picture pushtotalk voicemessages photo read receipt groupchats send message via wifi done regardless whether recipient online handle display native language well using cell number identity contact list social graph diabolically simple email verification username password credit card number required work impressive worth billion product compete feature google wanted possible reason threat cent user facebook desperate phone book metadata even though whatsapp keep none million active user user based growing one million user day potential billion user facebook need whatapp next billion user certainly must part cost user seem unreasonable especially bulk paid stock facebook acquired instagram per user twitter user worth benedict evans make great case mobile trillion dollar business whatsapp disrupting sm part industry globally billion revenue sending billion sm message day global sm system sends billion sm message day fundamental change transition pc nearly universal smartphone adoption size opportunity much larger addressable market facebook normally play facebook promised ad interference win interesting development business use mobile whatsapp used create group conversation project team venture capitalist carry deal flow conversation whatsapp instagram used kuwait sell sheep wechat whatsapp competitor launched taxicab hailing service january first month million cab hailed future ecommerce looking like funneled mobile messaging apps must ecommerce play business using whatsapp application desktop web police officer spain use whatsapp catch criminal people italy use organize basketball game commerce application jumping mobile obvious reason everyone mobile messaging application powerful free cheap use longer need desktop web application get thing done lot functionality overlayed messaging app messaging threat google facebook desktop dead web dying messaging mobile entire ecosystem sidestep channel messaging become center engagement mobile search changing thing found nature application win future prepagerank preweb facebook need get market become irrelevant move mobile seeing deportalization facebook desktop web interface facebook portal style interface providing access feature made available backend big complicated creaky really love facebook ui facebook moved mobile tried portal approach work going strategy smaller focussed purpose built apps mobile first much small screen mobile easier go find special app find menu buried deep within complicated portal style application facebook going one step creating purpose built apps providing multiple competing apps provide similar functionality apps may even share backend infrastructure see messenger whatsapp instagram facebook photo app paper alternate interface facebook provides limited functionality well conway law may operating idea organization design system constrained produce design copy communication structure organization monolithic backend infrastructure get borglike portal design move mobile free organization way thinking apps built provide view slice facebook infrastructure apps built use facebook infrastructure nt need facebook infrastructure free built facebook exactly facebook facebook ceo mark zuckerberg take saying keynote presentation mobile world congress facebook acquisition whatsapp closely related internetorg vision idea develop group basic internet service would free charge use internet could social networking service like facebook messaging service maybe search thing like weather providing bundle free charge user work like gateway drug sort user may able afford data service phone day see point would pay data service would give context important lead paying service like hope go long play game huge reservoir valuable stock allows play reached conclusion think stunning dollar amount tenuous apparent immediate reward long term play explanation actually make sense still early day mobile nobody know future look like pay try force future look like past facebook seems enough support million active user engineer let find source warning know lot whatsapp architecture bit piece gathered various source rick reed main talk optimization process used get million connection server using erlang interesting complete architecture talk stats stats generally current system system talk talk current system include hack data storage messaging metaclustering beamotp patch million active user reached number faster company history engineer one developer support million active user billion message every day across seven platform inbound outbound million people sign every day invested advertising million investment sequoia capital billion amount sequoia make much facebook cash used deal hundred node core hundred terabyte ram erlang message per second whatsapp achieved million established tcp session single machine memory cpu spare pushed million tcp connection whatsapp tweeted dec new record day msg inbound msg outbound billion total message processed one day happy platform backend frontend seven client platform iphone android blackberry nokia symbian nokia window phone sqlite hardware product focus messaging connecting people world regardless world without pay lot money founder jan koum remembers difficult connect family world privacy shaped jan koum experience growing ukraine nothing private message stored server chat history stored goal know little user possible name gender known chat history phone general whatsapp server almost completely implemented erlang server system backend message routing done erlang great achievement number active user managed really small server footprint team consensus largely erlang interesting note facebook chat written erlang went away hard find qualified programmer whatsapp server started ejabberd ejabberd famous open source jabber server written erlang originally chosen open great review developer ease start promise erlang long term suitability large communication system next year spent rewriting modifying quite part ejabberd including switching xmpp internally developed protocol restructuring code base redesigning core component making lot important modification erlang vm optimize server performance handle billion message day focus making reliable system work monetization something look later far far road primary gauge system health message queue length message queue length process node constantly monitored alert sent accumulate backlog beyond preset threshold one process fall behind alerted give pointer next bottleneck attack multimedia message sent uploading image audio video sent http server sending link content along encoded thumbnail applicable code usually pushed every day often multiple time day though general peak traffic time avoided erlang help aggressive getting fix feature production hotloading mean update pushed without restarts traffic shifting mistake usually undone quickly hotloading system tend much looselycoupled make easy roll change incrementally protocol used whatsapp app ssl socket whatsapp server pool message queued server client reconnects retrieve message successful retrieval message sent back whatsapp server forward status back original sender see checkmark icon next message message wiped server memory soon client accepted message registration process work internally whatsapp whatsapp used create usernamepassword based phone imei number changed recently whatsapp us general request app send unique digit pin whatsapp send sm indicated phone number mean whatsapp client longer need run phone based pin number app request unique key whatsapp key used password future call permanent key stored device also mean registering new device invalidate key old device google push service used android user android android enjoyable work developer able prototype feature push hundred million user overnight issue fixed quickly io much quest million connection per server experienced lot user growth good problem also mean spend money buying hardware increased operational complexity managing machine need plan bump traffic example soccer game earthquake spain mexico happen near peak traffic load need enough spare capacity handle peak bump recent soccer match generated spike outbound message rate right daily peak initial server loading simultaneous connection per server extrapolated would mean lot server hoped growth pattern server brittle face burst load network glitch problem would occur needed decouple component thing brittle high capacity goal million connection per server ambitious goal given time running connection running server headroom allow world event hardware failure type glitch would require enough resilience handle high usage level failure tool technique used increase scalability wrote system activity reporter tool wsar record system stats across system including o stats hardware stats beam stats build easy plugin metric system like virtual memory track cpu utilization overall utilization user time system time interrupt time context switch system call trap packet sentreceived total count message queue across process busy port event traffic rate byte inout scheduling stats garbage collection stats word collected etc initially ran minute system driven harder one second polling resolution required event happened space minute invisible really fine grained stats see everything performing hardware performance counter cpu pmcstat dtrace kernel lockcounting fprof dtrace mostly debugging performance patched beam freebsd include cpu time stamp wrote script create aggregated view across process see routine spending time biggest win compiling emulator lock counting turned issue earlier saw time spent garbage collection routine brought saw issue networking stack tuned away issue lock contention emulator show strongly output lock counting measurement result started simultaneous connection per server first bottleneck showed system ran lot contention work stopped instrumented scheduler measure much useful work done sleeping spinning load started hit sleeping lock cpu used across system scheduler utilization first round fix got million connection vm usage cpu beam emulator running utilization match closely user percentage good emulator run user ordinarily cpu utilization good measure busy system scheduler us cpu month later tackling bottleneck million connection per server achieved beam utilization close freebsd might start paging cpu double connection scheduler hitting contention running pretty well seemed like good place stop started profiling erlang code peaked connection per server tried million connection failed see long message queue system trouble either single message queue sum message queue added beam instrumentation message queue stats per process many message sentreceived fast sampling every second could see process message message queue dequeue rate delay second projected drain time second finding erlang beam fix awesome smp scalability nearly linear scalability remarkable box run system cpu utilization keeping running production load run like day testament erlang program model longer server accumulate long running connection mostly idle handle connection connection busy per connection contention biggest issue fix erlang code reduce beam contention issue patched beam partitioning workload work cross processor lot timeofday lock every time message delivered port look update timeofday single lock across scheduler mean cpu hitting one lock optimized use timer wheel removed bif timer check io time table grows arithmetically created vm thrashing hash table would reallocated various point improved use geometric allocation table added write file take port already open reduce port contention mseg allocation single point contention across allocator make per scheduler lot port transaction accepting connection set option reduce expensive port interaction message queue backlog became large garbage collection would destabilize system pause gc queue shrunk avoiding common thing come price backported tse time counter freebsd cheaper read timer fast get time day le expensive going chip backported igp network driver freebsd issue multiple queue nics locking increase number file socket pmcstat showed lot time spent looking pcbs network stack bumped size hash table make lookup faster beam patch previously mentioned instrumentation patch instrument scheduler get utilization information statistic message queue number sleep send rate message count etc done erlang code procinfo million connection slow stats collection efficient gather run production stats kept different decay interval second interval allows seeing issue time make lock counting work larger async thread count added debug option debug lock counter tuning set scheduler wake threshold low scheduler would go sleep would never wake prefer mseg allocator malloc allocator per instance per scheduler configure carrier size start big get bigger cause freebsd use super page reduced tlb thrash rate improves throughput cpu run beam realtime priority thing like cron job interrupt schedule prevents glitch would cause backlog important user traffic patch dial spin count scheduler spin mnesia prefer o timestamp erlang using transaction remote replication ran backlog parallelized replication table increase throughput actually lot change made lesson optimization dark grungy work suitable troll engineer rick going change made get million connection server mind numbing notice immense amount work went writing tool running test backporting code adding gob instrumentation nearly every level stack tuning system looking trace mucking low level detail trying understand everything take remove bottleneck order increase performance scalability extreme level get data need write tool patch tool add knob ken relentless extending system get data needed constantly writing tool script data needed manage optimize system whatever take measure remove bottleneck test repeat erlang rock erlang continues prove capability versatile reliable highperformance platform though personally tuning patching required cast doubt claim crack virality code profit virality allusive quality whatsapp show figure man worth lot money value employee count officially divorced lot forcemultipliers world today advanced global telecom infrastructure make apps like whatsapp possible whatsapp make network phone etc would never happen powerful cheap hardware open source software availability course another multiplier right place right time right product front right buyer something brutal focus user idea whatsapp focussed simple messaging app gaming network advertising network disappearing photo network worked guided ad stance ability keep app simple adding feature overall brainer work philosohpy phone limit cause simplicity ok identity tied phone number change phone number identity gone uncomputer like make entire system much simpler design age thing age discrimination prevented whatsapp cofounder brian acton getting job twitter facebook shame shame shame start simply customize chat launched initially server side based ejabberd since completely rewritten initial step erlang direction experience scalability reliability operability erlang initial use case led broader broader use keep server count low constantly work keep server count low possible leaving enough headroom event create shortterm spike usage analyze optimize point diminishing return hit effort deploy hardware purposely overprovision hardware ensures user uninterrupted service festivity employee able enjoy holiday without spending whole time fixing overload issue growth stall charge money growth super fast whatsapp free downloads day early day switching paid declined day end year adding picture messaging settled charging onetime download fee later modified annual payment inspiration come strangest place experience forgetting username password skype account drove passion making app work related article hacker news keynote benedict evans incontext related slide whatsapp benedict evans whatsapp blog telling diary billion dollar startup nice timeline event andre bourque rick change erlang github whatsapp blog whatsapp inside story open source project used whatsapp whatsapp facebook erlang realtime messaging started ejabberd quora whatsapp work whatsapp work mobile network whatsapp grow big whatsapp broken really broken early security problem whatsapp ceo jan koum hate advertising tech rumor mill full dive video singapore progressively business whatsapp four number explain facebook acquired whatsapp announcement mark zuckerberg millionuser comet application mochiweb part inside erlang rare programming language behind whatsapp success whatsapp actually worth say facebook zuckerberg internetorg sealed deal facebook buy whatsapp billion value pricing perspective facebook billion craving explained mark zuckerberg imho lesson learned whatsapp may use whatsapp rest world sure whatsapp story challenge valley conventional wisdom whatsapp right according jan koum video facebook buy whatsapp someone explain whatsapp valuation google unusual offer whatsapp
553,Lobsters,scaling,Scaling and architecture,Cache Oblivious Algorithms,http://www.1024cores.net/home/parallel-computing/cache-oblivious-algorithms,cache oblivious algorithm,let apply idea pairwise predicate problem see performancescalability achieve,idea behind cacheoblivious algorithm efficient usage processor cache reduction memory bandwidth requirement thing equally important singlethreaded algorithm especially crucial parallel algorithm available memory bandwidth usually shared hardware thread frequently becomes bottleneck scalability name may somewhat misleading oblivious fact presence cache opposite oblivious particular cache hierarchy cache parameter make efficient use whatever cache hierarchyparametersin order understand let consider typical cache structure modern computer yes sense memory cache slow disk disk cache network whatever lower level expensive thus lower capacity however also faster particular significantly lower access latency cache typically latency processor cycle large subsequent level roughly order magnitude larger access latencythere another dimension structure cache private core others shared several core usually cache potentially per core modern processor cache currently usually shared core processorso without loss generality let consider bit simpler structure use term cache memory generalized sense may represent real cache memory cache cache memory disk reality scheme actually applied several time recursively assume cache fast private memory slow shared hereinafter analyze optimize access slow shared memory let consider following problem array n element want apply pairwise predicate possible pair element struct itemt x bool predicate itemt const itemt const return x x x std vector itemt data n straightforward way accomplish task use doubly nested loop straightforward way parallelize put pragma omp parallel outer loop sizet kernel itemt const itemt const itemt const itemt const sizet count itemt const itemt const predicate count return count sizet calculatestraightforwardparallel itemt const begin sizet count unsigned threadcount sizet re pragma omp parallel reduction re numthreads threadcount int int count re kernel begin begin begin begin count return re let analyze number memory transfer mt n memory cache quite evident mt n b size block used transfer cache line size reloading whole array iteration inner loop assume data fit cache entirely good compare computational complexity may conclude bad end memory transfer per unit computation perfectly exploit spatial locality cache line loaded use data contained itlet take look performance graph processor x core amd machine dataset size element seen scalability beyond thread mediocre scalability beyond thread reason reason thread constantly access memory shared resource read potential bottleneck indeed thread must memory transfer b computationsso missed design algorithm missed possibility data reuse cache based temporal locality loaded data cache want much computation possible ideally cacheoblivious algorithm kick actioncacheoblivious algorithm work recursively dividing problem dataset smaller part much computation part possible eventually subproblem dataset fit cache significant amount computation without accessing memory note nt need know exact cache hierarchyparameters work recursively divide dataset inevitably eventually fit cache also note done part fit need load new part load satisfied rather memory recursive division automatically optimally exploit whatever cache hierarchy havenow let apply idea pairwise predicate problem see performancescalability achieve
554,Lobsters,scaling,Scaling and architecture,Scaling WhatsApp to Millions of Simultaneous Connections,http://www.erlang-factory.com/upload/presentations/558/efsf2012-whatsapp-scaling.pdf,scaling whatsapp million simultaneous connection,, obj stream  endstream endobj obj endobj obj stream z u g qn v  j p ta du b p c c h l z y qj  cpe n f tcu  u z g l  w x g x  zg ard x x x mr f f nt j il b kl v g c rrj uf g k yu
555,Lobsters,scaling,Scaling and architecture,A Scalable and Explicit Event Delivery Mechanism for UNIX (1999),https://www.usenix.org/legacy/event/usenix99/full_papers/banga/banga.pdf,scalable explicit event delivery mechanism unix,, obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r r xobject r extgstate r endobj obj length stream q cm g j j w g l q bt tf tm rg g tc tw tj et q cm rg j j w l l q bt tm connection duration second tj et q cm rg j j w l l l l l l l l l l q bt tm tc tj et q cm rg j j w l l l l l l l l l l q bt tm tc tj et q cm rg j j w l l l l l l l l l l q bt tm tc tj et q cm rg j j w l l l l l l l l l l q bt tm tj et q cm rg j j w l l l l l l l l l l q bt tm tc tj et q cm rg j j w l l l l l l l l l l q bt tm tc tj et q cm rg j j w l l q bt tm tc tj td tj et q cm rg j j w l q bt tm tc tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj tm tc fraction connection tj et q cm rg j j w l l l l l l l l l l l l l l q bt tm median tj et c c c c f q cm rg j j w l l l l l l l l l l l l q bt tm tc tw mean tj et c c c c f q cm rg j j w l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l q bt tm g tc tw n h ttp connect ons tj td tc f r ct ober h r ough c ober tj tm tc f g cum u l v e di ri b u n f p roxy c onne c n dura ons tj td h e k e probl e w h h e tj tf td tc e l ect tj tf td tc te r f c e th tj td tc r e q u r e h e ap p l cat n n f r h e k e r n el n tj tc cal l f h e e n r e et f i n er e n g   l e e cr p r e tj tc th e f r w h ic h h e p p lic tio n w n t c h e c k r e e tj tc f r e v en h cau e e f f r n mo n p r tj td tc opor tj td tc ona l num r f nt e r e n g  l e de c ri pt or nc e tj h e num r f  l e de c ri pt or norm l l proport ona l tj td tc th e e v e n r te h e ta l c f tj tf td tc e l ect tj tf td tc act iv cal e tj td tc roughl w h h e qua h e e v e nt ra e tj td tc n h p p e r w e e x p l ai n h e n ct n b et w een e tj td tc b e ech m u c h tj tf td tc e l ect tj tf td w h ch ch eck h e tj td tc c u rre nt u num e r ous de c ri pt or n e v e nt ba e tj tc e c h n w h c h e li v e r e x p lic e v e n n ti c tio n tj tc w e p r e en n e w u n x e v e n b ed p ap p l cat n tj tc p r g r ammi n g n e r f ace h ap p l cat n ay u e n tj tc ead f tj tf td e l ect tj tf td w ai f r e v en n  l e e cr p r tj td tc h e p llo w n p p lic tio n r e g te r te r e tj  le e c r p r n c e r th e r th n e v e r tim e w f r tj tc e v en wh en e v en ccu r n n e f h e e n er e tj td tc n g  l e e c r p r h e k er n e l p l ace n  cat n n tj td tc q u e u e n th e p llo w th e p p lic tio n e f  c e n tly tj tc de que ue e v e n  c ons tj td tc w e w ill h w th th n e w n e r f c e p le e ily tj td tc pl e e n e n p e rform nde pe nde nt l h e num r tj  l e e c r p r f e x pl e w h c onne c ons tj tc p p ro v e x um h roughput tj tf tm tc pr oble w ith tj tf td tc sel ect tj tf tm tc w e b e g n b r e v ie w n g th e e ig n n im p l e e n ta tio n tj td tc h e tj tf td tc e l ect tj tf td p th e e cal l ecl ar ed tj tf tm tc int select tj td int nfds tj fdset readfds tj td fdset writefds tj td fdset exceptfds tj struct timeval timeout tj tf tm tc tj tf td tc fd tj et q cm q bt tm tc se tj tf td tc p ly b itm p h e x u iz e tj td b f h e e b itm p h e la r g e l e g l  l e e c r ip r tj td tc v l u e w h c h em p eci  c p r met e r th e tj tf td tc r e ad tj td tc fd tj tf td tc tj tf td tc wr ite fd tj tf td tc n tj tf td tc e xcep f tj tf td ar e n u ar g u en r e p ect tj td tc v e l c orre pondi ng h e e f  l e de c ri pt or ha tj tc  nt e r e ng r e ng w r ng n e xc e p ona l c tj tc di ons gi v e n  l e de c ri pt ght n h n tj tc one h e e e h e tj tf td nf d tj tf td tc ar g u en g iv e h e l ar g e tj td tc bi p nde x c ua l l u e tj tf td tc tim e u tj tf td tc r gum e n c tj td r ol w h e r nd ho w oon tj tf td tc e l ect tj tf td tc w ill r e tu r n n  l e tj td tc de c ri pt or b e c e tj td tc fore tj tf td tc e l ect tj tf td tc c lle th e p p lic tio n c r e e n e tj td tc h e tj tf td tc r e adf d tj tf td tc tj tf td tc wr ite fd tj tf td tc r tj tf td tc e xcep f tj tf td tc b itm p b tj td tc e rt n g b c rre pondi ng h e e n e n g  l e tj de c ri pt or n u rn tj tf td tc e l ect tj tf td tc v e r w r ite h e e b tj td tc p w h ne w v l ue c rre pondi ng ubs e f tj nput e ndi c n g w hi c h  l e e c r p r r e v l bl e tj tc f r memb er f h e tj tf td tc r e adf d tj tf td tc e v l bl e f tj td tc n v l bl e nput da e b e r tj tf td tc wr ite fd tj tf td tc c tj td tc e r e w r b l e f h e v ai l b l e b u f f e r p ace e x ceed tj tc e pe c  c pa ra e e r u ua l l byt e f c p tj td tc ck et h e ap p l cat n h e n h e r e u l b ap tj td tc di c v e r b l e w r bl e  l e de c ri pt or n tj norm l l n v oke h ndl e r f ho e e c r p r tj td tc f g u r e v e r p l  ed e x amp l e f h w ap tj td tc p lic tio n p c lly u e tj tf td tc e l ect tj tf td tc ne u ha ho w n tj td tc th th e p r g r n g l e u e h e r e q u ite e f  c ie n tj tc l r ge num r  l e e c r p r nde pe nde nt h e tj probl e w h tj tf td tc e l ect tj tf td tc f e x pl e h e c ons r uc n tj td tc h e nput bi p l n e hrough f gure tj tc hould done e xplic itly fore tj td tc cal l tj tf td e l ect tj tf td tc tj td tc n e ppl c n houl nt n ha w c opi e tj td h e nput bi p n pl c opy h e e ha w tj tf td r e adf d tj tf td tc tj tf td tc wr ite fd tj tf td tc l th e c n f th e r e u l b tj td tc p w hi c h r e u u l l qui e p r e b e done w rd tj byw rd r r ha n b bybi tj td tc n ce n e h e l n ed h e e n e f  ci en ci e h w e v er tj tf td e l ect tj tf td tc till q u ite c tly p r f h c c e f r tj td th e u e f b itm p w h ic h u b e c r e e c p e tj tc h e k e r ne l c nne b ke rne l ubs e e c opi e tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r xobject r extgstate r endobj obj length stream bt tf tm g g tc tw fdset readfds writefds tj td struct timeval timeout tj td int numready tj tj td timeouttvsec timeouttvusec tj td tj true tj fdzero readfds fdzero writefds tj td endobj obj procset pdf text imageb font r r r r xobject r extgstate r endobj obj length stream g g q cm q q cm q bt tf tm tc tw cp u tj et q cm q bt tm tc n n id le tj et q cm q bt tm tc p r c e dur e tj et q cm q bt tm tc mo de tj et q cm q q cm q q cm q bt tm tc cp u tj et q cm q q cm q q cm q q cm q q cm q q cm q bt tf tm tc tj et q cm q bt tm tj et q cm q bt tm tc l l noni l e e tj et q cm q bt tm ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q q cm q bt tm tc l l l e tim e tj et q cm q bt tm tc ke rne l tj et q cm q q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tc al l el ect f u n c n tj et q cm q bt tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm tc e l ect tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm ne w tj et q cm q bt tm tc tj et q cm q bt tm tc e l ect tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm ne w tj et q cm q bt tm tc se l c n tj et q cm q bt tm tc one tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm ne w tj et q cm q bt tm undo tj et q cm q bt tm tc sc n tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm tc llo c tj tf td r e la te c e tj et q cm q bt tm tc u e r tj et q cm q q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc tj et q cm q bt tm tc pc bl ook tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm l l cp func ons tj et q cm q bt tm tc ke rne l tj et q cm q q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm tc mc p tj et q cm q bt tf tm tc u e r tj et q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc mse tj et q cm q bt tf tm tc u e r tj et q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm bc opy tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tf tm tc tj et q cm q bt tm tc io tj et q cm q bt tm tc por tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q q cm q bt tf tm dopr nt tj et q cm q bt tf tm u e r tj et q cm q q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc co tj et q cm q bt tm e l ect tj et q cm q bt tf tm tc u e r tj et q cm q q cm q bt tm tc p r o l e p tj td tc mean l ad r eq u e ec tj td tc p eak l ad ca r e q u e e c tj td tc bl e p ro l e odi  e k e r ne l qui n l v e p roxy tj td tc men f r tj tf td tc e l ect tj tf td tc tj tf td tc h e tj tf td tc p tj tf td tc sy st e c l l tj tf td tc n th e te v u n x e n v ir n e n p p lic tio n u e tj td th e tj tf td p tj tf td tc em cal l n ead f tj tf td e l ect tj tf td h cal l e tj td cl ar ed tj tf tm tc struct pollfd tj td int fd tj td short event tj short revents tj td tj td int poll tj td struct pollfd filedes tj td unsigned int nfds tj td int timeout millisecond tj tf tm tc th e tj tf td tc  l ed e tj tf td tc r gum e n n nout rra w h one e l e tj td tc men f r  l e e cr p r f n er e tj tf td tc nf d tj tf td gi v e tj td tc rra l e ngt h n nput tj tf td tc e ven tj tf td tc  e l f el emen tj td tc e l l h e k e r ne l w hi c h e c ondi ons r e f n tj tc e r e f r h e ci ed  l e e cr p r tj tf td tc fd tj tf td tc n r e urn h e tj tf td tc r e ven tj tf td tc  e l h w w ha ubs e ho e c ondi ons hol tj td tc r ue e  e l r e p e nt om e w ha b roa e r e f tj td tc c n itio n th n th e h r e e b itm p u e b tj tf td tc e l ect tj tf td tc tj td tc th e tj tf td tc p tj tf td tc p p p ear h v e w ad v n ag e v e r tj tf td e l ect tj tf td tc rra c om pa c l pre e n onl  l e tj td tc de c ri pt or f nt e r e n doe de roy nput tj tc  e ld f u r g u e n h w e v e r th e f r e r tj v n g e p r b b l illu r c e tj tf td tc e l ect tj tf td tc onl c opi e tj td tc b pe r  l e de c ri pt w hi l e tj tf td tc p tj tf td tc c opi e bi f tj td tc h e n u b e r f n er e n g e cr p r e x ceed f h e tj td tc hi ghe num c v e  l e de c ri pt tj tf td tc p tj tf td tc doe ore tj td tc c opyi ng h n tj tf td tc e l ect tj tf td tc n n e v en h ar e h e ame tj td tc c l n g p robl e doi ng w rk proport ona l num r tj tc f te r e tin g e c r ip r r th e r th n c n ta n e f f r p e r tj tc ev e n tj tf tm tc e v e ntba e v ta te ba e tic io n tj td tc ech tj tf tm tc r ecal l h w e w h p r v e ap p l cat n w h tj td ef  c e n cal ab l e mean eci e w h c h f  l e tj td tc de c ri pt or f proc e ng w e c n pproa c h tj tc th e ith e r f tw w tj td tc tj tf td e ba e tj tf td tc vi e w n w hi c h h e k e r ne l nform tj td tc th e p p lic tio n f h e c u r r e n e f  l e tj tc de c ri pt e g w h e r n c urre nt l tj tc v l bl e f ng tj td tc n tj tf td tc e ven b ed tj tf td tc vi e w n w hi c h h e k e r ne l nform tj td tc h e ppl c n f oc c u rre nc e f e n ngful tj tc e v e n f  l e de c ri pt e g w h e r n e w da tj ha b e e n dde c k e nput b u f f e r tj td tc th e tj tf td tc e l ect tj tf td tc e c h n f llo w th e ta te b e p tj td tc proa c h f e x pl e f tj tf td tc e l ect tj tf td e cr p r r ead tj td tc r e ng h e n h e r e n nput b u f f e r f p tj tc p l cat n r ead j u p r n f h h e n cal l tj tf e l ect tj tf td ag ai n b ef r e r e r r v e tj tf td e l ect tj tf td w l l ag ai n tj td tc port ha de c ri pt f ng tj td h e e ba e pproa c h nhe nt l qui ke r tj td tc n e l c h e c k n e v e r n ti c tio n w c th e ta tu tj td tc f memb er f h e et f e c r p r w h e e tj tc b e g e te u r im p r v e p le e n tio n f tj tf td tc se tj td tc l ect tj tf td tc one c n e l e p r f hi v e rhe w c hi ng tj td tc e v e n t th c h n g e th e ta te f e c r ip r f r u n r e tj tc r ead th e k er n e l n eed n r e p eat ed l r e e h e e tj tc e c r p r kno w n unre tj td tc ho v e r n c e tj tf td tc e l ect tj tf td tc ha ol ppl c n ha tj td tc de c ri pt h e ppl c n ght ght tj tc pe rform ope ra ons v e r e hi e c ha nge f e x tj tc pl e ght n yt hi ng l l dy tj tc n g nput de c ri pt r ght l l f tj td h e p e ndi ng da fore onc e tj tf td tc e l ect tj tf td tc ha r e port e tj td h e c r p r r e c nnot p l gnore ha tj tc de c ri pt fut u c l l u e ha e c r p r tj tc e l ea u n l b eco me u n r ead e v en f n f u r tj h er e v e n ccu r n e h el emen f tj tf td tc wr ite fd tj tf td tc ar e tj td tc u ua l l tj td tc l hough tj tf td tc e l ect tj tf td tc f llo w th e ta te b e p p r c h tj td th e k e r n e l u b te e l w ith e v e n t p c k tj tc e rri v e c kno wl e dge e nt rri v e k b l c k rri v e tj tc et c th er ef r e h e tj tf td tc e l ect tj tf td tc im p l e e n ta tio n u r n tj td tc form  c ons n nt e r na l e v e nt ba e vi e w tj td tc e x e r n al e b e v e w b u h e e v e n r v e n  ap tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r r r r xobject r extgstate r endobj obj length stream bt tf tm g g tc tw p lic tio n th u e tj tf td tc e l ect tj tf td tc b n n ti c tio n u lti tj td te ly f llo w h e e v e n b e v ie w n h u p e n e f tj td tc fort r n form n g nform n b c k e ba e tj tc ode l h e e dua l ra n form ons c r e e e xt ra w rk tj td tc u r n e w p f l l w h e e v e n b ed ap p r ach n tj td tc th e l th e k e r n e l im p l r e p r t tr e f e v e n t tj tc h e ppl c h e e e v e nt onot oni c n h e e n e tj h h e ne v e r e c r e e ount b l e da tj tc w r b l e b u f f e r p ace f r e c r p r h e r e f r e n c e tj e v en h ar r v e f r e cr p r h e p p l cat n tj tc e h e r proc e de c ri pt e di e l ke e tj h e e v e nt n e f e r h e p roc e ng h e k e r ne l doe tj td r c k h e r e n e f n de c ri pt doe pe r tj td tc form w rk proport ona l num r f e c r p r tj tc onl p e rform w ork p roport ona l num r f e v e nt tj td p u e v e n ba e p h v e w probl e tj td tc f r eq u e n e v en r r v l cr eat e e x ce v e co tj td mu n cat n v e r h ead e p eci al l f r n p p l cat n tj tc h n e e n e e ng e v e r ndi vi dua l tj tc ev e n tj td tc h e p prom e de l v e r nform n bout tj td tc n v u l e v e n u l l cat e r g e p r tj tc port ona l e v e n r e tj td u r p doe de l v e r e v e nt ync hronous l tj td w oul gna l ba e e c h n e e e c n tj w h c h h e l p e l n e  r probl e n e tj tc th e p llo w n p p lic tio n e f  c e n tly c v e r tj tc de c ri pt or ha h v e h e v e n rri v l nc e n e v e nt tj tc ha rri v e f e c r p r h e k e r ne l c oa l e c e ub tj tc e q u e n e v e n r r v l f r h e c r ip r u n til th e p p lic tj td tc n l e r n h e  r one hi r e duc e h e c om uni c tj td tc n r e v h e n eed r e p er e v e n n f r tj tc w e l e v e h ppl c ons ne e e xpl tj tc c p e r e v e nt n form yond h v l bl e nba nd tj tc th e tr e tj td tc b mp l f n g h e eman c f h e p co mp ar ed tj td tc tj tf td tc e l ect tj tf td tc w e r e v e th e n e c e ity ta f r tj td tc n n ke rne l h ght n e h e tj tc ppl c w e l v e p r f ra n form ons tj tc b e w een h e e v e n b ed e b ed v e w th tj tc p r v e h e c l b ility f th e k e r n e l p le e n tio n n tj le v e th e p p lic tio n u f  c ie n  e x b ility im p l e e n tj td tc h e ppropri e e v e n na ge e nt l gori h tj tf tm tc et ail h e p r ogram ng int erf ace tj tf tm tc n ppl c n ght l w y n e e n tj td tc e v e n rri vi ng l l f ope n  l e de c ri pt or f r tj tc e x pl e e nt one n e c n qui p roxy tj tc e rv e r e pora r l gnore da rri vi ng n dri bbl e tj tc w oul r r p roc e l r ge b u f f e r f po bl e tj td h e r e f ore p nc l ude e c l l l l w ng tj td tc th r e e c l r e te r e r l c k f te r e n  l e tj tc de c ri pt tj tf tm tc define eventread tj td define eventwrite tj td define eventexcept tj td int declareinterest int fd tj td int interestmask tj int statemask tj tf tm tc h e h r e c lls th p r c e u r e w ith th e  le e c r p r tj td tc n que h e tj tf td tc te r e k tj tf td tc ndi c e w h e r tj td tc th e h r e te r e e r e g f r r w r itin g h e tj tc de c ri pt r n e xc e p n e v e nt tj tf td tc te r e k tj tf td tc zer tj td tc h e n h e hre l onge r nt e r e e n n e v e n tj tc th e e c r ip r clo g e c r p r im p lic itly r e v e tj tc ecl ar ed n e r e tj td n ce h e h r ead h ecl ar ed n e r e h e k e r n el tj td tc r c k e v e nt rri v l de c ri pt e c h rri v l tj tc e p e r th r e q u e u e f u ltip le th r e r e te r tj td tc e e n e c r p r p e r ck et p n el ect b et w een tj td tc w w c hoos e prope r que ue que ue tj tc de fa ul e nque ue n e v e n rri v l r e c rd tj td tc n tj td tc te r e e th r e b u b e ttin g h e tj et q cm q bt tm tc w ake tj et q cm q bt tm one tj td tc  g th e p p lic tio n n c e th w n n e v e n r tj tc r v l el iv er ed n l h e  r el g b l e h r ead tj td tc h e tj tf td tc e k tj tf td tc r gum e n nonn u l l h e n tj tf td tc de tj td tc cl r e tj et q cm q bt tm tc te r e tj tf td tc l port h e c urre nt e f  l e tj td tc de c ri pt f e x pl e f h e e v e n tj et q cm q bt tm tc bit e tj td tc th v lu e th e n th e e c r ip r r e f r r e g tj tc th f eat u r e v r ace n w h c h e c h n g e ccu r tj td tc af e r h e  l e h b een p e n e p e r h ap v tj tf td ccep tj tf td em cal l b u b ef r e tj tf td ecl r e tj et q cm q bt tm tc te r e tj tf td tc h b een cal l e tj td tc h e p le e n tio n g u r n e e th th e tj tf td tc e k tj tf td tc va l u e tj td tc  e c de c ri pt e b e f ore n e v e n r e tj tc de h que ue h e r w e v ng tj n e v e nt h e ppl c n w oul h v e p e rform non tj tc bl oc ki ng tj tf td tc tj tf td tc tj tf td tc wr ite tj tf td tc f te r c llin g tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td tc tj td tc w ddi ona l e v e nt hre n v oke r tj td tc n e w te c tj tf tm tc typedef struct tj td int fd tj td unsigned mask tj td eventdescrt tj td int getnextevent int arraymax tj td eventdescrt evarray tj td struct timeval timeout tj tf tm tc th e tj tf td tc ev tj et q cm q bt tm tc ar r tj tf td tc r gum e n poi nt e r n rra tj td tc l e ngt h tj tf td ar r tj et q cm q bt tm tc x tj tf td tc f v l ue f ype tj tf td tc e ven tj et q cm q bt tm tc de c r tj et q cm q bt tm tc tj tf td tc f n tj td tc e v e n r e p e ndi ng h ke rne l de que ue tj n f f orde r tj tf td tc ar r tj et q cm q bt tm tc x tj tf td tc ev e n tj tf tm tc tj tf tm tc r e p r t th e e tj td tc de que ue e v e nt n tj tf td tc ev tj et q cm q bt tm tc ar r tj tf td tc u l rra h e tj tf td tc sk tj tf td tc b n tj tf td e ven tj et q cm q bt tm tc de c r tj et q cm q bt tm tc tj tf td tc r eco r w h h e ame e n tj td tc ons u e n tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td tc ndi c e c u rre nt tj et q cm q bt tf tm tc tj tf tm tc f f r er n g n r n c de gn n anot p aper tj td tc w e descr n e w ker n el mechani cal l e tj tf td tc r e ce cont ai ner tj tf td tc tj td tc w h ic h llo w n p p lic tio n p e c fy th e p rio r ity w h ic h h e k e r tj td tc nel e nqueues e vent tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r xobject r extgstate r endobj obj length stream bt tf tm g g tc tw e f c rre pondi ng de c ri pt tj tf td tc fd tj tf td tc func n r e tj td tc u rn v l u e g v e h e num r f e v e nt c ua l l port e tj td tc b al l w n g ap p l cat n r eq u e ar b r ar tj td tc num r f e v e nt port n one c l l c n ort z e tj tc c f th c l l v e r u ltip le e v e n t h w e v e r f l e tj tc one e v e n que ue w n c l l e u rn tj e di e l w e bl oc k h p l  l l u p tj tf tc ev tj et q cm q bt tm tc ar r tj tf td tc tj td tc e v e n r e que ue f h e hre n c l l tj td bl oc k unt l l e one e v e n rri v e unt l h e e tj tc e xpi tj td tc n te th u lti th r e e p p lic tio n r n p tj td p lic tio n w h e r e th e e c k e r  le u lta n e u ly tj td tc p e n v e v e r l e cr p r r ace co u l ak e h e tj tc de c ri pt unre b e f ore ppl c n r e tj tf td tc sk tj tf td tc bi ppl c n houl u e nonbl oc ki ng ope ra ons tj tc r e r w r ite th e e e c r ip r e v e n f h e p p e r tj b e r e h e p le e n tio n f tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc doe tj td tc e p ry port c u rre nt e f de c ri pt tj tc r h er h mp l r e p r n g h e mo r ecen e r tj tc n nt e r na l l uppre e n port h r e n tj l onge r e n ngful hi houl r e duc e fre que nc f tj tc u ch r ace tj td tc h e p le e n tio n l tte p t c l e c e u l tj td tc p l e port e de c ri pt hi tj tc v lu e w h e n f r e x p le b u lk ta tr n f e r r r v e tj tc er e f mal l p ack et th e p p l cat n g h tj tc c ons um e l l h e b uf fe n one e c l l tj tc w oul b e ne f  c e n f ppl c n h c ons um e tj tc doz e n f que ue e v e nt  c ons c rre pondi ng tj one l r g e b uf fe r e h w e v e r po bl e e n tj tc r el e l n e u p l cat e n  cat n b ecau e f r ace tj tc w e e n ne w e v e nt rri v l n tj tf td tc tj tf td tc tj tf td tc wr ite tj tf td tc r im ila r tj td tc sy st e c l l tj tf tm tc u e h e p r ogram ng int erf ace tj tf tm tc f gure h w hi ghl pl  e e x pl e f h w tj td one ght u e ne w p w r e pa rt f n e v e nt tj tc dri v e n e rv e r w e port nt de l u c h e rror tj tc h n lin g u lti th r e g n n p r c e u r e e  n tj tc ons tj td tc th e tj tf td tc n tj et q cm q bt tm tc l oop tj tf td tc p r ced u r e h e cen r al e v en tj td tc p tc h e r e c h ite r tio n ta r b tte p tin g e q u e u e tj td tc b ch f e v en h er e u p p e r b ch u n g tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td l n e f h e em cal l e u tj td tc th e p p lic tio n e tim e u r e la te p r c e n g h tj tc e r w e l oops v e r ba c h f e v e nt n pa c tj tc e v en h l er f r e v en l n e h er e p e tj tc c l c e f h e oc ke w h c h ppl c n tj l e ni ng n e w c onne c ons w hi c h ha ndl e di f f e r tj tc en l f r car r n g c k e tj td tc w e h w onl one ha ndl e r f h e e pe c l l e n tj td tc c k e n itia liz tio n c e n h w n h e r e th e e tj tc l e n c k e h v e b e e n e u e h e nonbl oc ki ng op tj td tc n th er ef r e h e tj tf td tc ccep tj tf td tc c l l lin e w ill n e v e r tj td b l c k e v e n r c e w ith th e tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td cal l e tj td tc ho w c u e hi c ode run oo oft e n f e x pl e tj td e c l e nt ght c l o e n e w c onne c n b e f ore w e tj tc h v e c h n c e accep f tj tf td ccep tj tf td tc doe uc c e f ul l tj td tc u rn h e oc ke f n e w c onne c l n e e tj u e nonbl oc ki ng l n e tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td te lls tj td tc h e k e r ne l ha ppl c n w n kno w bout fut u tj tc r e n w r ite e v e n t l e e t e e f n ta b e tj tc came v l b l e b ef r e w e cal l e tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td tc f tj td tc w e r ead mmed el tj tf tm tc mp l e n n tj tf tm tc w e pl e e n e ne w p odi fyi n g g l tj td tc u n x v w e rt e w h pro v e tj tf td tc e l ect tj tf td tc im tj td tc pl e e n n r e u ng e da ruc ure n tj port f unc ons ha e f f ort h l l l w u tj e ure ne w p g n kno w n tj tf td tc e l ect tj tf td tc im tj td tc pl e e n n w hout v ryi n g n h n g e l e u r c urre nt tj tc pl e e n n w orks onl f c k e b u c oul b e e x tj e nde h e r de c ri pt ype f e r e n c e l w tj td tc h e  prot oc ol c k  w oul n nc l ude  l e y e nd tj td tc de vi c e dri v e r c ode tj td tc f r ne w p w e dde bout l n e c ode tj td tc th e tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc c l l r e qui bout l n e tj tf td tc de tj td tc cl r e tj et q cm q bt tm tc te r e tj tf td tc qui n nde r c v e r tj td tc c h nge p rot c l c ode n upport f unc ons n c tj r pre v ous odi  c ons tj tf td tc e l ect tj tf td tc dde bout tj td tc l n e f w hi c h w e u e bout l n e n pl e tj tc e nt n g ne w p tj td tc f r ap p l cat n h r ead u r co e mai n n f u r tj td tc da ruc ure e n c l ude e r e e r e n tj tc ter e ted w r e n n ter e ted e x cep h e et tj td tc de c ri pt or e gna e v tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td tc i n er tj td tc e n g  f r r ead n g w r n g e x cep n r e p ect iv el tj tc h e h e r h n f que ue e v e n po e b tj tc h e p r co l ack f r h e h r ead tj td h r ead  r cal l tj tf td ecl r e tj et q cm q bt tm tc te r e tj tf td tc cau e c r e tj td tc tio n f n e r e e e t h e e t r e r e iz e n e tj tc c e ry w h e n de c ri pt or dde h e h que ue tj tc c r e e upon h c r e l l f e r e e r oye tj w h e n h e hre e x w n de c ri pt c l o e tj tc u tic lly r e v e f r r e le v n n e r e e tj tc se tj td tc f gure h w ke rne l da ruc ure n e x tj td tc amp l e n w h ch h r ead h ecl ar ed r ead n e r e n tj tc de c ri pt or n nd w r e n e n de c ri pt tj tc th e h r ee n ter e ted e ar e h w n h e r e n e tj tc b e b ap b ecau e h e h r ead h n ecl ar ed n e r e tj tc n n h ghe r num e c r p r n hi e xa p l e tj tc h e h que ue h c rds hre e pe ndi ng tj tc e v en n e f r e cr p r tj td cal l tj tf td ecl r e tj et q cm q bt tm tc te r e tj tf td tc al ad el emen tj td tc h e c orre pondi ng c k e  r e v e r e ppi ng l h tj tc e l e e n nc l ude bot h poi nt e r h e hre n tj td de c ri pt nde x num r f gure h w ke rne l tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text font r r r extgstate r endobj obj length stream bt tf tm g g tc tw define maxevents tj td struct eventdescrt eventarray maxevents tj td tj mainloop struct timeval timeout tj td tc tj td tc int n tj tj true tj td n getnextevent maxevents eventarray timeout tj td n tj td setnonblocking newfd tj declareinterest newfd eventreadeventwrite tj statemask tj td statemask eventread tj td invokereadhandler newfd tj tj tj tf tm tc f g p l  e e x pl e f h w h e n e w p ght u e tj et rg j j w bt tf tm rg tc thread tj td control tj block tj et bt tm tj et bt tm interestedread tj td tc tj td tc interestedwrite tj td interestedexcept tj et l l l l l l bt tm hint queue tj td tc tj et l l l l bt tm tc tj td tj et w l l l l l l l l l l l l l l l l l l l l l l l l bt tf tm g tc f g p e r h da ruc ure tj td tc r u ct u r e f r n e x mp l e n w h c h p r ce tj td tc p r ce h l r ef er en ce ck et v  l e e cr p r tj tc n r e p ect iv el w h r ead f p r ce tj tc n e h r ead f p r ce ar e n er e e n c k e tj h e r e v er e ap p n g l ci ed w h h e c k e h tj tc poi nt e r l l h e hre tj td w h e n h e p rot c l c ode proc e e n e v e nt uc h tj td tc ar r v l f r ck et c h eck h e r e v e r e map p n g tj tc l f r h r ead n h e l f h e n e x n u b e r tj tc found n h e hre l e v nt e r e e e h e n tj tc n ti c tio n e le e n e h e th r e h n tj td tc que ue tj td tc v oi v e rhe ddi ng n e l e n g tj td tc v e r e ppi ng l ft e n w e n e v e r v e tj tc r e v e r e p p n g ite u n til th e e c r ip r c lo e h tj tc e n ha l upda e o onc e p e r de c ri pt tj l f e e doe dd e l ght pe r e v e n v e rhe tj tc c k e w h ile h r e h r e v k e te r e th tj e c r p r w e b e lie v e th n e g lig ib le tj td w e tte p c le c e u ltip le e v e n n ti c tio n f r tj td tc ngl e e c r p r w e u e r p e r hre bi p n tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r r r r xobject r extgstate r endobj obj length stream rg j j w g l l l l l bt tf tm rg tc tw tj td tj td tj tj tj tj et l l bt tf tm descriptor tj td table tj et bt tm thread tj et bt tm thread tj et l l l l l bt tf tm tj td tj td tj tj tj tj et l l bt tf tm descriptor tj td table tj et bt tm thread tj td process p rocess tj et l l l l l l bt tm socket tj td reverse tj td mapping tj list tj et l l bt tm tc tj et w v c c c l l v c c c c c c c l l l l l l l l l l l v c c l l v c c c l l v c c c c c l l bt tf tm g tc f g p e r c k e da ruc ure tj td de x e b  l e de c ri pt num r e ha h n tj td tc que ue c ont n p e ndi ng e l e e n f h e e c r p r h e tj tc p r c l c e te n e t th e e b itm p e n tr ie th e tj tc ar e c l ear ed n c e tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc h el iv er ed h e c r tj td tc pondi ng  c hus tj tf tm tc n tj tf tm tc e v e n oc ke tj td tc b e tw e e n c lls tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc le ju n e n ti tj td tc  c tj td tc cal l tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc unl e e tj td tc de que ue one  c n e l e e nt tj h n que ue n f f orde r h w e v e r h e h que ue tj tc h z e lim v e r  w w e c r n e tj tc l v e r e v e n n e cr p r r er u n g l n ear ear ch f tj tc h e n ter e ted e  w e w u l r h e r el iv er h n g tj tc n h e w rong orde r ha n b l c k progre h pol c tj tc c u l l e r v tio n th e tj tf td tc ar r tj et q cm q bt tm tc x tj tf td tc pa ra e e r tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc le th n th e n u b e r f e c r ip r tj td tc ay n eed r e v n tj td tc w e e ha r e h e r po bl e p l e e nt ons tj td ne w p f e x pl e one h e non ym ous tj vi e w e r ugge e u n g l nke l f h e p e r hre tj tc que ue pe ndi ng e v e n r e e r vi ng p c e one l e l e tj tc men n ck et r u ct u r e th ap p r ach eems tj tc ha v e e v e ra l dv n ge w n tj et q cm q bt tm tc w ake tj et q cm q bt tm one tj td tc p n et b u mi g h n b e f ea b l e w h en e v en tj tc e li v e r e u ltip le th r e tj tf tm tc p erf orm n c e tj tf tm tc w e e ure h e p e rform n c e ne w p u n g tj td pl e e v e nt dri v e n h p proxy progra hi p roxy tj tc e n cach e r e tj td tc pons e c n c on gure u e tj td tc e ith e r tj tf td tc e l ect tj tf td tc ne w e v e nt p tj td tc l l f e xpe ri e nt p e nt e w e g e n e r tj td tc e l oa u ng w k nd c l e nt h e  hot  c onne c tj tc ons c e e f p roc e e r unni ng h e cl e nt tj tc ft wa de gne g e n e r e r e l c que l tj td tc c h r c e r tic f w n c lie n u r e r lie r w r k tj td tc l u e tj tf td tc l oadaddi ng c l e nt tj tf td ge ne ra e l r ge num tj td r f  c l  c onne c ons l ongdura n dum c tj tc n e c tio n th u la te th e e f f e c f la r g e w n e la tj tc h e l oa da ddi ng c l e nt proc e ope n n e v e ra l tj td tc hous n c onne c ons b ut doe c ua l l e nd n r e tj td tc q u e st n e sse n c e si mu l e l w h g v e n r tj tc r v l r e n u r tio n tr ib u tio n b b r e k g n w tj tc p eces c l e n f r h e r r v l r e l ad n g cl tj tc e n t f r h e u r tio n tr ib u tio n tj td tc h e p roxy l y l l r e que w e b e r v e r ngl e tj td tc proc e e v e nt dri v e n p rogra de ri v e ht p tj tc w h num e r ous pe rform n c e pro v e e nt hi n tj tc ear l v e r n f h e f l h w eb e r v er w e k e car e tj tc e n u r e th th e c lie n th e w e b e r v e r n th e n e tj w r k e l f r e n e v e r b ttle n e c k h u h e p r x e r v e r tj td e th e b ttle n e c k tj tf td tc e xp e r n al e n vi r n e n tj tf td tc h e y e unde r e w h e r e proxy e rv e r run tj td tc h z gi l p e r ona l w orks n l pha tj td b r p e ci nt r unni ng odi  e tj tc v e r n f g l u n x v c l e nt proc e e r un tj four e n c l hz p e nt u p ro c h n e b tj ra f r e e b w e b e r v e r p rogra run tj h z p e n u b r f e b tj td tc w itc f ullduple x b e c f e rne tj td tc co n n ect l l mach n e h e p r x e r v er mach n e h tj tc w n et w r k n e r f ace n e f r c l e n r af  c n e f r tj w e b e r v er r af  c tj tf td tc p fu n c ti c tj tf td w e pe rform e e xpe ri e nt  nd h e b c c f tj td tc u r n e w p c lls e u r g h w th e e c t c l e w ith tj td tc h e num r f c onne c ons pe r p roc e ide l l c tj td tc houl b e bot h l w n c ons nt tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r xobject r extgstate r endobj obj length stream bt tf tm g g tc tw n th e e e t c lie n f w r e u la te h p c li tj td tc e n ge ne ra n g r e que proxy c onc urre nt l tj td l ddi ng c l e nt e bl om e num r f c ol c tj tc n ect n h e p r x e r v er w e r ed meas u r emen tj tc onl ft e r dum run w r e w e b e r v e r  l e tj tc cach e u r n g h e e ea u r emen h e p r tj td tc oxy c p u tj td tc u ra e n proxy ppl c n n e v e r bl oc k n tj tf tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc r e l w e v e nt que ue f de tj td tc li v e r tj td tc h e p roxy ppl c n u e l pha c c l e c ount e r tj td tc meas u r e h e el ap e e p e n n em cal l w e tj tc port e v e ra ge v e r c l l tj td tc e ure c tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc w e u e tj td tc cl e nt g e n e r n g r e que f byt e  l e hus tj tc c u ng hous nd e v e n pe r c onne c w e ra n ri tj tc l w ith tj tf td tc ar r tj et q cm q bt tm tc x tj tf td tc xi u num r f e v e nt e tj td tc l v e r e p er cal l v r n g b et w een n w e al v r ed tj tc h e num r f cl e nt proc e e f gure h w ha tj tc th e c p e r c w ith c ld c n n e c tio n v r ie lin tj e r ly w ith tj tf td tc ar r tj et q cm q bt tm tc x tj tf td tc u p poi nt l e ppa nt l tj td h e c onc urre nc f cl e nt tj td tc fo r g v e n tj tf td tc ar r tj et q cm q bt tm tc x tj tf td tc v l u e w e found h v ryi n g tj td tc h e num r f c ol c onne c ons w e e n nd ha tj td tc al mo n ef f ect n h e c f tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc c c ount tj td tc n g f v ri n f o v e r hi r nge tj td w e l found h n c r e n g hot c onne c n tj td ra e di ppe r n c r e e pe r e v e n c o f tj tf tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc n f act h e e v en b c h n g mech tj td tc duc e h e p e r e v e nt c proxy fa l l furt r b e tj tc hi nd h e c o f l l e v e n p ope ra ons n pl e tj e nt n nde pe nde nt h e e v e nt ra e l ong h e tj xi u z e f h n que ue c on gure l r g e tj tc e nough hol one e n r f tj td tc e c r p r f h e p r tj td tc c e s tj td tc e ure c h e tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td tc sy st e tj td tc c l l w e u e cl e nt k n g r e que f k b yt e tj td tc  l e w e mad e e p r e meas u r emen f r h e d ecl ar tj tc n g nt e r e  c e ddi ng n e w de c ri pt n e r tj tc e e e n  r e v oki ng n e  c e vi ng tj e c r p r h e f orm e r c e h l onge r c ode pa h tj tc f gure h w l ght c v ri ons w h c h nge n tj num r f c ol c onne c ons b ut h e e e ure tj tc men ar f act tj tf td tc p r x erv er p erf r n c e tj tf td w e h en meas u r ed h e act u l p er f r ce f u r tj td tc p l e proxy e rv e r u ng e h e r tj tf td tc e l ect tj tf td tc ne w p tj td tc h e e e xpe ri e nt l l r e que e c tj tc k b e  l e w h ch h er ef r e l w ay cach ed n h e w eb tj td tc e rv e r e ry w e ra n ddi ona l e u n g k byt e tj td tc  l e pa c e doe pe rm h w n g u l b ut h e tj tc di p l na l ogous ha vi tj td tc n th e  r e r ie f e t w e lw u e h tj td tc c onne c ons b ut v ri e h e num r f c ol c onne c ons tj w e e n nd h e hot c onne c n cl e nt tj td c on gure ge ne ra e que f proxy tj td e c n ha ndl e hus w e u ra e proxy b ut ne v e r tj td v e rl oa de f gure pl ot h roughput c h e v e tj hre e ke rne l c on gura ons h e  c l c l  tj tc p l e e n ta tio n f tj tf td tc e l ect tj tf td tc pro v e p l e e nt tj td tc tio n f tj tf td tc e l ect tj tf td tc nd ne w p de c ri n hi tj td pa pe r l l k e r ne l u e c l b l e v e r n f tj tf td tc uf al l c tj tf td tc  l e de c ri pt l l c n f unc n norm l v e r tj tc doe c l e w e l l h e r e ul c l e r l ndi c e ha tj tc ne w p pe rform nde pe nde nt l h e num r f c ol tj tc c onne c ons w hi l e tj tf td tc e l ect tj tf td tc doe w e l found h tj td h e p roxy hroughput nde pe nde nt tj tf td tc ar r tj et q cm q bt tm tc x tj tf td tc tj td tc h e e c ond e ri e e w e  x e h e num r f tj td tc c l c onne c ons n e ure pons e e tj e e n h e c l e nt f gure h w u l w n u tj n g ne w p proxy e e xhi bi u c h l w e r tj l e nc n ura e om e w ha h ghe r r e que l tj tc que e c v que e c tj tc p r ove tj tf td tc e l ect tj tf td tc im p l e e n ta tio n tj td tc bl e h w cp p ro l e h e p roxy e rv e r n h e tj td h e k e r ne l c on gura ons e pro l e w e r e e tj tc u n g c l c onne c ons hot c onne c ons nd tj tc l l oa f que e c h e h w h h e n e w tj td tc e v e n p gni  c n l n c r e e h e ount cp u dl e tj td tc tim e b l e lim tin g h e e v e n n ti c tio n v e r tj h e w h ile th e c la c l tj tf td tc e l ect tj tf td tc im p l e e n ta tio n c n tj td tc u e h e c p u nd pro v e tj tf td tc e l ect tj tf td tc im tj td tc pl e e n n c ons um e ne w p c ons um e l e tj tc h n h e c p u tj tf tm tc r el ed work tj tf tm tc pl c e w rk n c ont e x w e u rv e ot r n v e tj td tc ig tio n th e c l b ility f e v e n n g e e n p tj td tc n de gn e v e n na ge e nt p n ot r ope r tj tc tin g te tj tf td tc e v e nt suppo r n ne b io nd w tj tf td tc th e n et b n e r f ace l l w ap p l cat n tj td tc w nc om ing n ultiple ne tw ork c onne c tions tj tc n e b e n p r v e p r ced u r e cal l n er f ace n tj ead n p p l cat n c r eat e  n e w r k c n r l b l ck  tj tc n cb l oa d ddre nt pe c  c gi e r n n tj tc n v oke n e bio vi ft w n e rrupt n e bio tj tc p r v e c mman r e u l v cal l b ack tj td tc th e n et b r eceiv e  co mman r e u r n cal l tj td tc ba c k w n rri v e n n ne w ork  e on c tj td tc ne c hi l l w n ppl c n w f rri vi ng tj td tc da n r bi r r num r f e ons w hout ha vi ng tj e num e r e e f e ons doe ppe r po tj tc b l e w ai f r r eceiv e n u b et f h e act iv e tj tc se ssi n tj td tc th e  r eceiv e  co mman h n u er u l tj td tc ons om e f w hi c h r e u l none x e n b l e tj de gn h e n cb form l l w e ons tj w h c h b v e ne e hi ghl y c l b l e pl e e n tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r xobject r extgstate r endobj obj length stream q cm g j j w g l q bt tf tm rg g tc tw tj et q cm rg j j w l q bt tm tc tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj td arraymax tj et q cm rg j j w l l l q bt tm tj td tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj tm cost per call usec tj et q cm rg j j w l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l q bt tm sclients tj et q cm rg j j w l l l l l l l l l l l l l l l l l l l l l l l q q cm rg j j w l l l l l l l l l q q cm rg j j w l l q bt tm sclients tj et q cm rg j j w l l q q cm rg j j w l q bt tm g tc f g tj tf td ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc c lin g tj et q cm g j j w l q bt tm rg tc tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj td number cold connection tj et q cm rg j j w l l l q bt tm tj td tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj tm cost per call usec tj et q cm rg j j w l l l l l l l l l l l l l l l l l l q bt tm declaring interest tj et q cm rg j j w l l l l l l l l l l l l l q q cm rg j j w l l l l q q cm rg j j w l l q bt tm revoking interest tj et q cm rg j j w l l q q cm rg j j w l q bt tm g tc f g tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td c lin g tj et q cm g j j w l q bt tm rg tc tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj td number cold connection tj et q cm rg j j w l l l q bt tm tj td tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj tm http throughput connsec tj et q cm rg j j w l l l l l l l l l l l l l l l l l l q bt tm classical select tj et q cm rg j j w l l l l l l l l l l l l l q q cm rg j j w l l l l q q cm rg j j w l l q bt tm improved select tj et q cm rg j j w l l q q cm rg j j w l q c c c c f c c c c f c c c c f c c c c f c c c c f q cm rg j j w l q c c c c f bt tm new api tj et c c c c f q cm rg j j w l q bt tm g tc f g h p ra e v c ol c onne c ons tj et q cm g j j w l q bt tm rg tc tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj td request per second tj et q cm rg j j w l l l q bt tm tj td tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj et q cm rg j j w l q bt tm tj tm response time msec tj et q cm rg j j w l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l q bt tm classical select tj et q cm rg j j w l l l l l l l l l l l l l l l l l l l l l q q cm rg j j w l l l l l l l l q q cm rg j j w l l q bt tm improved select tj et q cm rg j j w l l q q cm rg j j w l q c c c c f c c c c f c c c c f c c c c f c c c c f c c c c f c c c c f c c c c f c c c c f c c c c f q cm rg j j w l l l l l l l l l q c c c c f bt tm tc tw new api tj et c c c c f q cm rg j j w l q bt tm tc tw cold connection tj tm g tc f g l e nc v que ra e tj td tc h e c om nd doe l l w n ppl c n tj td tc c v e r w n onc e ful l put b u f f e r c e w ri bl e tj tc doe ppl k  l e tj td tc h e w progra ng e n vi ronm e n h e u e tj td tc n e b io r ongl c oura g e w n c l ude tj tc proc e dure n e tj tf td tc w itf r mu lt ip l e b je c tj tf td tc ecl ar ed tj tf tm tc dword waitformultipleobjects tj td dword cobjects tj td number handle handle array tj td const handle lphobjects tj td address objecthandle array tj td bool fwaitall tj td flag wait one tj td dword dwtimeout tj td timeout interval millisecond tj td tj tf tm tc h proc e dure k e n rra f w obj e c tj td w hi c h c oul nc l ude h ndl e hre proc e e tj tc u e x e e c nd w e h e r one l l f tj tc c p le te f h e tj tf td fw ita tj tf td tc a g f al se h e n h e r e tj td tc u rne v l u e rra nde x f obj e c tj td tc po ble l e r n bout u ltiple obje c n one c tj tc u n l e h e p p lic tio n w illin g w f r c p le tio n tj tc l l f l e obj e c tj td tc h proc e dure l k e tj tf td tc e l ect tj tf td g h n cal e v er w el l tj td tc l r g e num r  l e h ndl e f l r r e tj td pa e nform n bout l l pot e n l e v e nt ourc e tj td tc e v er e cal l e n ca e h e b j ect h l e r tj tc ra c ont n n ore ha n e l e e nt l nc e tj tf tc w itf r mu lt ip l e b je c tj tf td tc mu b e cal l e r ep eat ed l b tj td tc ta u ltip le e v e n t n th e r r e r c h e lin e r ly tj tc f r e q u e n e v en r e n b j ect ear l n h e r r ar v e tj tc e rvi c e f hi ghe r nde x e obj e c tj td tc w n w n e mo r e ad v n ced mech tj td tc f r e e c tin g e v e n t c lle n c p le tio n p r tj td tc io cp hi e oge h e r h e hre e c h tj tc n w ith th e e c h n n p p lic tio n c lls tj tf tc cr e te co p le tio n p r tj tf td tc cr eat e n c p h e n tj td tc ke n ddi ona l c l l tj tf td tc cr e te co p le tio n p r tj tf td tc ci e n e r e n g  l e h n l e w h h c p tj u ch cal l l p r v e n p p l cat n p eci  e tj c p l et n k e  v al u e h w l l b e ci ed w h h e tj tc  l e h ndl e tj td tc n p p lic tio n h r e w f r c p le tio n e v e n tj td tc u n g tj tf td g e q ue ue dcom pl e onst u tj tf td tc c tj tf tm tc bool getqueuedcompletionstatus tj td handle completionport tj lpdword lpnumberofbytestransferred tj lpdword completionkey tj td lpoverlapped lpoverlapped tj td dword dwmillisecondtimeout tj tf tm tc u pon u rn tj tf td tc co p le tio n k e tj tf td tc v ri b l e hol d h e tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r xobject r extgstate r endobj obj length stream g g q cm q q cm q bt tf tm tc tw cl ssi c l tj et q cm q bt tm tc c al ab l e tj et q cm q bt tm tc ne w e v e nt tj et q cm q q cm q q cm q q cm q bt tm tc e l e c tj et q cm q bt tm e l e c tj et q cm q bt tm tc ap tj et q cm q q cm q q cm q q cm q bt tm cp u tj et q cm q bt tm cp u tj et q cm q bt tm cp u tj et q cm q bt tm tc p r c e dur e tj et q cm q bt tm tc mo de tj et q cm q q cm q q cm q q cm q bt tf tm tc tj et q cm q bt tm tj et q cm q bt tm tj et q cm q bt tm tc l l l e tim e tj et q cm q bt tm tc ke rne l tj et q cm q q cm q q cm q q cm q bt tf tm tc tj tf td tc tj et q cm q bt tf tm tc tj tf td tc tj et q cm q bt tf tm tc tj tf td tc tj et q cm q bt tf tm tc al l k e r ne l tj tf td tc e l ect tj tf td tc e v e n f unc ons tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tc n tj et q cm q bt tm n tj et q cm q bt tf tm tc tj et q cm q bt tm tc e l ect tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q bt tm tc tj et q cm q bt tm tc n tj et q cm q bt tm n tj et q cm q bt tf tm tc e l c tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q bt tm tc tj et q cm q bt tm tc n tj et q cm q bt tm n tj et q cm q bt tf tm tc undo tj et q cm q bt tm tc c tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tc n tj et q cm q bt tf tm tc e l ect tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc n tj et q cm q bt tm tc tj et q cm q bt tm tc n tj et q cm q bt tf tm tc ne w tj et q cm q bt tm tc tj et q cm q bt tm tc e l ect tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q bt tm tc n tj et q cm q bt tm tc tj et q cm q bt tm tc n tj et q cm q bt tf tm tc ne w tj et q cm q bt tm tc se l c n tj et q cm q bt tm tc one tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc n tj et q cm q bt tm n tj et q cm q bt tm tc tj et q cm q bt tf tm ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q bt tm tc n tj et q cm q bt tm n tj et q cm q bt tm tc tj et q cm q bt tf tm tc ecl r e tj et q cm q bt tm tc te r e tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q bt tm tc n tj et q cm q bt tm n tj et q cm q bt tm tc tj et q cm q bt tf tm tc r e vo ke tj et q cm q bt tm tc te r e tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tj et q cm q q cm q bt tf tm tc x y ca l l tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc n tj et q cm q bt tf tm tc u e r tj et q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tm tj et q cm q q cm q bt tf tm tc dopr nt tj et q cm q bt tf tm tc u e r tj et q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc mse tj et q cm q bt tf tm tc u e r tj et q cm q q cm q q cm q bt tm tj et q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc tj et q cm q bt tm tc io tj et q cm q bt tm tc por tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc sy sc l l tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tj et q cm q q cm q bt tf tm tc xen n tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q q cm q bt tm tc tj et q cm q bt tm tj et q cm q bt tm tj et q cm q bt tf tm tc llo c tj et q cm q bt tf tm tc ke rne l tj et q cm q q cm q bt tm c l c onne c ons hot c onne c ons que e c ond b que tj td tc b l e e f f ect f e v en p n em c p u p r  l e tj td v l u e ci ed v tj tf td tc cr e te co p le tio n p r tj tf td tc w ith th e tj td tc c rre pondi ng  l e h ndl e e v e ra l hre ght tj tc b l c k e n h p r c e u r e w itin g f r c p le tio n e v e n tj tc n h e ame c p th e k er n e l el iv er h e e v e n tj tc n f f r e r b u el ect mo n g h e b l ck ed h r ead n tj tc l f orde r duc e c ont e x w c hi ng v e rhe tj td tc th e c p ech eems h v e n n h e r e n l tj td tc c l n g l r ge num r  l e e c r p r tj tc h w e kno w f n e xpe ri e nt l u l c on rm n g tj tc c l b ility h w e v e r tj td tc n ce h l e h b een ci ed w h c p tj td h er e n w ay ci e e x cep b c l n g h e tj td tc h n l e th w h co mp l cat e h e p r g r ammer tj tc k f e x pl e un f e u e h e c om pl e tj tc k e ddre f da ruc ure ha ght l l c tj tc ed w h en  l e h n l e c l e n ead h e ap p l ca tj tc n houl u e nonc e v l ue p l n g r l e v e l tj tc ndi c n obt n ne c e ry poi nt e r nd w h l e tj tc th e p p lic tio n ig h u e e v e r l cp e g r e g te  l e tj tc ha ndl e n di f f e r e n p ri ori c l e c nnot v e tj  l e h ndl e one io cp r w f dj u tj tc g p r io r ity tj td tc e ppl c ons uc h qui p roxy tj td e pora r l gnore e v e n n c v e  l e de c ri pt tj td v oi e r vi c ng da rri vi ng l e ngt hy e ri e tj l l ri bbl e hi e l done w h h e u n x tj tf td tc se tj td tc l ect tj tf td tc c l l vi ng h de c ri pt nput tj td tc bi p c l e r f h c n done u n g n c p tj td h u e l di c u e v e r l di f f e r e n n e v e nt di tj td tc pa c hi ng n c onc urre nc ode l n h e c ont e x f w e b tj tc e rv e r nd h w ho w e rv e r pe rform n c e v ri e tj tc acco r n g h e mo e l c h en h w e v er h e n tj meas u r e h w p e r f r ce cal e w h l r g e n u b e r f tj tc ope n c onne c ons b ut l e r e ure e n tj c onc urre nt c l e nt tj td tc u ry iocp e c h n n w ndo w nt tj td tc l r p w e p ropos e f u n x n p da e tj td tc de gn e v e ra l e r lthough w e w e initia lly tj td tc una w di f f e r e n c e w e e n h e e gns tj gni  c n w e l ook forw r c r e f ul tj tc n l y io cp pe rform n c e c l ng u r c ont ri b u n tj tc h e c onc e p f pe ndi nge v e n que ue b ut ra h e r tj tc p p lic tio n u n x n u r q u n tita ti v e n l f tj c l b ility tj tf td tc q u e u e c mp l e ti gn al n p x tj tf td tc h e p x p llo w n p p lic tio n r e q u e tj td th e e l v e r f g n l f w r e te r r u p w h e n tj tc p b l e f r g v e n  l e e c r ip r h e p x ltim e tj tc g n l e x te n io n llo w n p p lic tio n r e q u e th tj tc de l v e gna l que ue n ha gna l h ndl e r tj tc b e n v k e w h p ar amet er g v n g h e ci ed  l e tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text imageb font r r r r xobject r extgstate r endobj obj length stream bt tf tm g g tc tw e c r p r h e c b tio n f h e e f c ilitie p r v e tj td tc cal ab l e n  cat n ech tj td tc w e e e hre e probl e ha c oura g e u e f g tj td na l f r gna l e l v e r ore e xpe n v e ha n tj p e c l z e e v e n e c h n w e p ropos e n e tj tc em g n l el iv er f r g r eq u r e u ec tj tc v e r u bout u e c f tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc e e  gure tj td tc n u n lik e tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td h e g n l ech tj td tc n b tc h n ti c tio n f r u ltip le e c r p r n n e tj tc n v c e c ond ync hronous n v c n f h nd tj tc l e r pl e u e f om e ort f l oc ki ng e c h n tj w h c h dd v e rhe n c om pl e x f n l l u e f tj td tc g n l e lim e p p lic tio n c n r l v e r w h c h th r e tj td tc n v oke tj tf td p e n ac h tj tf td tc h e c h ope ra n g y e de pe nd e ge tj td ba e c om uni c u n g  port  r e p e nt e tj tc g e e n p n p r r e p r e c e w ith c p b ility lik e tj tc s en r g h   r eceiv e r g h  l l em p e r tj tc ons r e p e rform e u n g e ge e xa p l e v r tj tc tu l e r f u lts r e c n v e r e e g e e n tj tc h e b c k ng ore port f c e e ry obj e c tj tc r c om uni c n ode l uc h cp byt e r e tj tc r e c ons r uc e n op h e ge pa ng l ye r tj td e c h port h que ue pe ndi ng e ge hre tj td tc may u se h e tj tf td tc msg tj et q cm q bt tm tc r ecei tj tf td tc em cal l r et r e v e me tj td tc ge que ue ngl e port w e tj td ge rri v e f h e que ue e pt tj td tc h r ead w h r eceiv e r g h f r man p r may c r e tj td tc e p r et   r cl b j ect co n ai n n g ar b tj r r u b et f h e e r eceiv e r g h h e h r ead may tj tc th e n v k e tj tf td tc msg tj et q cm q bt tm tc r ecei tj tf td n h p r et r h er h n tj td tc h e unde rl yi ng port r tj td tc eceiv n g me g e f r l l f h e tj td tc c ont ne port n f f orde r e c h e ge rke tj tc w ith th e e n tity f th e r g n l r tj td tc eceiv n g p r al l w n g tj td tc th e h r e e u ltip le x h e e g e h e p r e p tj tc p r ach cal e ef  c e n l h e e r eq u r e r et r e v e tj td tc e ge port e houl b e nde pe nde nt h e tj td num r f port n h e tj td tc p rt e r e ppropri e ode l n w hi c h l l c om tj td tc uni c n done w h e ge n n w hi c h h e y tj tc te p r v e h e n e c e r f c ilitie n g e e g e tj tc port ne c e ri l pl e p robl e int r oduc n g tj tc port e nt u n x w h e r e o c om uni c n f ol l w tj byt e ode l ght qui j r c ha nge p tj tc pl c ons n e xi n g c om pone nt tj tf tm tc f utur e w k tj tf tm tc th e tj tf td tc e l ect tj tf td tc e c h n c n b e c n f u g n u lti tj td th r e e p r g r e p e c lly n u ltip r c e r b e tj tc cau e tj tf td e l ect tj tf td tc u rn e f de c ri pt n e tj td tc f e v en n  cat n w h r ead b l ck ed n tj tf td tc se tj td tc l ect tj tf td tc c oul w ke n e e n w oul n e e tj td tc ddi ona l ync hroni z n v ha ndl n g e tj td tc de c riptor ur e v e n tba e p hould k e w riting tj td tc h r ead ed ap p l cat n mo r e n u r l b ecau e w h h e tj td tc tj et q cm q bt tm tc w ake tj et q cm q bt tm tc n e p tio n e c r ib e e c tio n e tj td tc l v e r e v en n ce w e h v e n e e x p l r ed tj tc th r e e ta il tj td tc u r e x n g p r e q u r e h r ead n p r ce tj td cal l tj tf td ecl r e tj et q cm q bt tm tc te r e tj tf td tc f r e c r p r h n e r tj td tc e e n h qui e nt ght e xc e v e v e rhe tj tc u ltithre e progra u ing la r g e pool inte r tj tc ch g eab l e w r k er h r ead w e co u l u g en h e p tj tc w ith n h e r te c tj tf tm tc int declareprocesswideinterest int fd tj td int interestmask tj td int statemask tj tf tm tc h e r e ul f hi y e c l l w oul b e e qui v l e nt tj td n v oki ng tj tf td tc ecl r e tj et q cm q bt tm tc te r e tj tf td e v e r e x tin g n f u tu r e tj td th r e f th e c llin g p r c e g h l im p lic itly e tj td tc tj et q cm q bt tm tc w ake tj et q cm q bt tm tc n e de c ri pt f e r hi c l l tj td tc h r ead f h e p r ce c u l w ai f r e v en n h tj tc de c ri pt u n g tj tf td tc ge tj et q cm q bt tm ne x tj et q cm q bt tm tc e ven tj tf td tc tj td tc n ppl c n h ndl n g hous nd de c ri pt or tj td tc g h w n e e v e n e l v e r p r r itie c n r l h e tj tc orde r n w hi c h h e k e r ne l e l v e r e v e nt r p tj pe r w e nt roduc e h e tj tf td tc r e c e c ont ai ne r tj tf td tc ab r ac tj td tc tio n w h ic h n g h e r b e n e  t llo w n p p lic tio n tj e k e r n e l le v e l p r r itie f r e c r p r p r c e n g n tj tc h pa pe r w e ho w e h w n e v e n ba e p uc h tj tc h e one pre e n e h e r e u e ful c om pone nt e ndt tj td tc e n p ri ori c ont rol n n e w rke ppl c ons w e l ook tj td forw r g ni ng e xpe ri e n c e w h h e c om bi na n f tj tc p r io r ity c n r l n n e v e n b e p c p le x p tj p lic tio n tj tf tm tc sum r tj tf tm tc w e h w e th th e c l b ility f n p e r tin g tj td tc e e v e n n  cat n ech h r ect ef f ect n tj tc p p lic tio n p e r f r n c e c l b ility w e l h w e h tj td th e tj tf td tc e l ect tj tf td tc p h inhe ntly poor c l b ility b ut tha tj td tc b e r e p l aced w h p l e e v e n r en e p w e tj tc pl e e n e hi p n ho w e ha doe nde e tj tc pro v e pe rform n c e r e l ppl c tj tf tm tc ack n wled gm en tj tf tm tc w e w oul l k e ha nk c h l c ht e nbe r g h e l pi ng tj td u unde r nd h e n e bio w n w ndo w n tj td e v e n na ge e nt p w e w oul l l k e ha nk tj td h e non ym ous vi e w e r f h e r ugge ons tj td h w rk w done w h l e g ura v ba nga w u tj td tc de nt ri c e u n v e r w upport e n p r b tj n f g ra nt n e xa tj p g r n n b n e qui pm e n l oa n b tj tc gi l e qui pm e n c orpora n ubs ry com p q tj tc com put e r corpus tj et endstream endobj obj type page parent r resource r content r mediabox cropbox rotate endobj obj procset pdf text font r r r extgstate r endobj obj length stream bt tf tm g g tc tw ref e r e n ce tj tf tm tc j nde r l rc e l cont nuous pro l tj td tc ng w h e r e h v e l l h e c yc l e gone n tj tf td tc pr ceed tj td tc g f th e ix te e n th c p iu n p e r tin g tj td tc em pr n ci p l e tj tf td tc p g e n l f r n c e tj td c tj td tc g b nga n p ru c l e uri n g ca pa tj td tc ci f w eb e r v er n tj tf td tc pr ceed n g f h e tj td tc tj td tc u e n ix sy po u n nt e r ne e c hnol ogi e tj tc sy e tj tf td p g e ont e r e c e c tj td tc g b nga p ru c l n j c ogul r e ourc e tj td tc co n n e r n e w f c ility f r r e u r c e n g e tj tc e n n se r v e r sy st e n tj tf td tc p r oc r p p tj td tc e r tin g te e g n n p l e e n ta tio n tj tf td tc p ag e tj td tc n e w rl e n l f e b rua r tj td g b nga n j c ogul c l b l e ke rne l tj td tc pe rform n c e nt e r ne e r v e r unde r r e l c tj tc lo n tj tf td tc p r oc u e n ix nnual e c hni c l tj td tc conf tj tf td tc p g e n e w rl e n l j une tj td tc use n ix tj td tc c ha nkhunt hod p b nz g c n e e r da e l tj td tc f c h w r tz n k j w r r e h ie r r c h tj tc cal n e r n et b j ect c ach e n tj tf td tc pr ceed n g f h e tj td tc u e n ix e c hni c l c onf e r e n c e tj tf td tc p g e tj td n e go ca j n tj td tc j b c n p e r ona l c om uni c n v tj td tc r p r v e r e v ed p c n er f ace n tj tf td tc pr c tj td tc u en x c h c n f er en ce tj tf td tc p g e burl tj td tc ngt v c obe r tj td tc g ri bbl e nd e bre w e r y e e g n tj td tc u e f r n er n e l e w ar e er v ce ed u c tj tc n f r lar g e c l e n r ace n tj tf td tc p r oc u se nix tj td sy po u n nt e r ne e c hnol ogi e sy e tj tf td tc tj td tc pa ge ont e r e c tj td tc ecemb e r tj td tc tj td tc g runw l p e r ona l c om uni c r tj td tc j h rt tj tf td tc w sy e p r ogr ng tj tf td ddi n tj td w e l e l ongm n r e ng tj td tc j h u p ya ra l n c c h dt e uri n g tj td tc h e p c e v e n pa c hi ng n c onc urre nc tj td tc e l n w e b er v e r p er f r ce v er h g h tj td tc p e e n e w orks n tj tf td tc p r oc g l obal int e r n e conf e r tj td tc en ce h el p r f g l bec tj tf td tc p hoe ni x tj td z n v e b e r tj td tc ibm tj tf td tc l ca l r e n et w r k e c h n ca l r ef er en ce tj tf td tc tj td tc e r c h r ngl e p r k n c h e tj td tc k r oe ge r e l ong n j c ogul tj td tc e xpl ori n g bound w e b l e n c duc n tj tc f r c ach n g n p r ef et ch n g n tj tf td tc p r oc po u tj td tc int e r n e e c hnol ogi e sy e tj tf td tc p ag e  tj td tc n er e c ecemb e r tj td tc u e n ix tj td tc j l ons tj tf td tc l n c e n r n u ni x th e itio n tj td tc wi h sour c e code tj tf td tc p eer p eer c mmu n cat n tj td tc n j o e ca tj td tc j c ogul pe e e r qui c e udy n tj td tc int e rne e rv e r pe rform n c e probl e tj tf td tc l ogi n tj tf td tc p ag e tj td tc f e brua ry tj td h e pe n g roup w b u rn tj tf td tc h e ngl e u nix tj td tc spe c c v e r v ol se f u n ix tj tf td tc tj td tc tj td tc v p p r u c h e l nd w z w e n e poe l f l h n tj td tc e f  c e nt n p ort bl e w e b e rv e r n tj tf td tc p r oc tj td tc u e n ix nnual e c hni c l c onf tj tf td tc n e r e c tj td tc j une u e n ix tj td tc q u h ttp q u id n l n r n e t q u id tj td tc h e nda rd p e rform n c e e v l ua n c orpora n tj td tc spe c spe c w e b r e su l tj tc ht p w w w pe c b e n c h r g g w e u l tj td tc th ttp h ttp w w w c e c f w r e th ttp tj td j v e r w r itin g c l b le p p lic tio n f r w w tj td tc n h p w w w c r o oft c w v tj tc ba e ca l bil h tj td tc w e e l p e r ona l c om uni c e pt e r tj td tc tj td tc g w r ght n w e v e n tj tf td tc c p p llu r te tj td tc vo l u e tj tf td tc ddi onw e l e r e ng tj td oung e v ni n j r r r hi g l ub tj td tc j e ppi nge r j che w w b ol o k b l c k nd tj tc r ba r n h e u lity f e r n c u tj tc n c tio n n h e p l e e n ta tio n f u ltip r c e r tj p e r tin g te n tj tf td tc p r oc h po u n tj td tc p er n g y e pr n ci p l e tj tf td tc p g e u tj td n x n v e b e r tj et endstream endobj obj type font subtype firstchar lastchar width encoding winansiencoding basefont hinnbccourier fontdescriptor r endobj obj type xobject subtype image width height bitspercomponent imagemask true length filter ccittfaxdecode decodeparms k column stream p endstream endobj obj name type font subtype resource r fontbbox fontmatrix firstchar lastchar encoding r charprocs r width endobj obj name type font subtype resource r fontbbox fontmatrix firstchar lastchar encoding r charprocs r width endobj obj type font subtype firstchar lastchar width encoding winansiencoding basefont hipikahelveticabold fontdescriptor r endobj obj type font subtype firstchar lastchar width encoding winansiencoding basefont hiplnchelvetica fontdescriptor r endobj obj name type font subtype resource r fontbbox fontmatrix firstchar lastchar encoding r charprocs r width endobj obj type fontdescriptor ascent capheight descent flag fontbbox fontname hinnbccourier italicangle stemv xheight charset conesbracketrightdtwotagthreeunumbersigniaunderscorefour vfiveewlsixybmampersandsevennocbracelefteightkqexpa renleftbarrninefbracerightfsparenrightnhcolonlsemicolonui asteriskvjlesspluspwkequalcommamxgreaterhhyphenoperiod tpzslashgbracketleftdbzeror r endobj obj filter flatedecode length subtype stream tef av  v vq   
556,Lobsters,scaling,Scaling and architecture,"Littles Law, Scalability and Fault Tolerance: The OS is your bottleneck. What canyou do?",http://highscalability.com/blog/2014/2/5/littles-law-scalability-and-fault-tolerance-the-os-is-your-b.html,little law scalability fault tolerance o bottleneck,repost parallel universe little service client little law little law dominates capacity million latency sickness health hystrix functional callbacking nodejs play rxjava lightweight thread quasar simple mechanism proposed userlevel thread comsat like functional programming actorsrx want learn new apis pulsar actor model previous blog post conclusion quasar comsat related article,guest repost ron pressler founder ceo parallel universe combinator company building advanced middleware realtime application little law help u determine maximum request rate server handle apply find dominating factor limiting server capacity hardware o buy hardware software problem remove software limitation way make code much harder write understand many modern web application composed multiple often many http service often called microservice architecture architecture many advantage term code reuse maintainability scalability fault tolerance post like examine one particular bottleneck approach hinders scalability well fault tolerance various way deal using term scalability loosely post refer software ability extract performance available resource begin trivial example analyze problem explore solution offered various language framework library little service let suppose http service accessed directly client say web browser mobile app call various http service complete task code might look java import path myservice public class myrestresource private static final client httpclient clientbuildernewclient get produce mediatypetexthtml public string getit int failure call foo synchronous string fooresponse null try fooresponse httpclienttarget http request get readentity stringclass catch processingexception e failure call bar synchronous string barresponse null try barresponse httpclienttarget http request get readentity stringclass catch processingexception e failure monitoroperation failure return combineresponses fooresponse barresponse define rest service example us jaxrs though plain servlet framework could used invoke service use jaxrs client though library could used jaxrs client powerful general support integrating library perform actual http request eg jersey jaxrs client integrates netty jetty grizzly client keep example simple instead calling many micro service call two foo bar keep mind real application might call many also want monitor failed service call count report let find bottleneck approach little law little law mathematical theorem useful determining capacity system receives process external request stable system tie average request arrival rate  average time request processed system w number concurrent request pending system l neat little formula l w remarkable result depend precise distribution request order request processed variable might conceivably affected result example request average arrive second take seonds process average system required handle request concurrently normally however system capacity l given request processing time w feature software depends complexity internal latency know l w figure rate request support  lw handle request need increase l capacity decrease w processing time latency happens request arrive greater rate  system longer stable request start queuing first experience much increased latency quickly system resource exhausted server become unavailable dominates capacity l l feature environment hardware o etc limiting factor minimum limit constraining number concurrent request limit well first number concurrent tcp connection server support normally server support several tensofthousands concurrent tcp connection shop success maintaining million open connection second bandwidth unless streaming hd video request response travelling back forth lan kilobyte length total chitchat volume single request usually well given today highbandwidth lan network could support anywhere million concurrent request remember talking concurrent request request per second service require large message volume usually take longer process request take longer second even several second network handle third ram number concurrent request fit ram depends much memory request consumes assuming keep well given low cost ram number probably well million certainly several hundredsofthousands fourth cpu many concurrent request cpu support depends application logic given processing done microservices time request wait microservices responsd waste cpu number anywhere several hundred thousand several million indeed production system employing similar architecture rarely report cpu bottleneck practice far reason believe keep l somewhere million sound great huh one limiting factor o example employ simple familiar threadperrequest model request run single o thread completion done web server free use thread serve request number concurrent request handle also limited number thread o handle thread behave well processing block waiting microservice respond might processing block thread busy cpu saturated sitting idle either o required schedule anywhere dozen time complete request many thread could o handle concurrently depends o usually somewhere beyond thread scheduling add significant latency request latency grows w increase  drop allowing software spawn thread willynilly may bring application knee usually set hard limit number thread let application spawn number somewhere rarely l minimum limit o scheduler suddenly dropped capacity l high million well conclusion use threadperrequest model goodenough hardware l completely dominated number thread o support without adding latency could course buy server cost money incur many hidden cost might particularly reluctant buy extra server realize software problem server already underutilized look model might work around problem issue introduce let turn examine w processing latency latency sickness health suppose two microservices foo bar take average return response including network latency since call sequentially time web service request processing time processing latency second w suppose allowed web server spawn thread l according little law handle  lw request per second become unstable crash figure even taking account traffic spike number good enough problem calculation valid foo bar healthy happens one experience trouble increase latency second w gone w let call round happened  request per second dropped deem unacceptable make service faulttolerant set timeouts service call path myservice public class myrestresource private static final client httpclient static clientconfig configuration new clientconfig configurationproperty clientpropertiesconnecttimeout timeunitsecondstomillis configurationproperty clientpropertiesreadtimeout timeunitsecondstomillis httpclient clientbuildernewclient configuration assigned http client timeout parameter second give leeway mean maximum latency even presence failure second yield maximum request rate  per second fact better foo go bad consistently timesout need try reaching waiting twho whole second time install circuit breaker trip service fails prevents subsequent request attempting call ocassionally side process sample foo see recovered close circuit bring latency back second request handling capacity back request per second even event failure cost added complexity kind circuit breaker mechanism exactly netflix open source hystrix library provides circuit breaker help prevent w rising something go wrong also instead giving server single cap number thread spawn hystrix make easy allocate various capped threadpools different kind operation form bulkheading failure one operation go awry exhaust thread assuming service healthy protected circuit breaker reduce w matter fact quite easily fact example ee call bar foo return latency compounded might ok little example call service instead could serious issue notice need result foo call bar issue two twenty call time let business parallel absorb latency one another java get produce mediatypetexthtml public string getit throw interruptedexception int failure submit request asynchronously future string foofuture httpclienttarget http request async get readentity stringclass future string barfuture httpclienttarget http request async get readentity stringclass collect response synchronously string fooresponse null try fooresponse foofutureget catch processingexception e failure string barresponse null try barresponse barfutureget catch processingexception e failure monitoroperation failure return combineresponses fooresponse barresponse future let u dispatch request wait result go well reduced w processing latency second applies service call result server handle request per second even call quite micro service best unfortunately yes short somehow greatly optimizing foo bar pretty much even though hardware still severely underutilized request per second best might much worse allow latency greater micro service enough functional callbacking taken w much could l still constrained number thread o efficiently handle thing left u somehow increase l option abandon threadperrequest model nodejs server side javascript framework used primarily web application javascript singlethreaded node choice come using o thread scheduler let see nodejs handle problem var http require http httpcreateserver myservice listen function myservice request response var completed var failure var fooresponse var barresponse function checkcompletion completed monitoroperation failure responsewrite combineresponses fooresponse barresponse responseend httpget host http port path foo function resp respon data function chunk fooresponse chunk completed checkcompletion error function e completed failure checkcompletion httpget host http port path bar function resp respon data function chunk fooresponse chunk completed checkcompletion error function e completed failure checkcompletion node request handler thread block instead run need wait say service call give framework callback execute call completes node turned javascript lack threading advantage use threadperrequest model node us asynchronous callback suffer o thread limitation approach completely eschews o thread limit problem introduces several others first accidental blocking handler function even happens running lengthy computation effectively block entire nodejs instance request processed often mitigated running several node instance one machine loadbalancing also help take advantage cpu core waste ram make parallelizing certain computational task difficult latter might might matter depending want accomplish importantly however force asynchronous callbackbased programming style style harder write harder reason code executed order written complex dependent operation style nightmare colloquially known callback hell nodejs trying make simpler adopting comprehensivelyfunctional style something called promise look similar approach right away one thing working node favor however single threaded good keep example code simpler call foo bar submitted asynchronously callback executed time order complete still safely increment failure completed variable whenever callback run run thread never concurrently wrap head around callback thankfully need factor concurrency race equation well jvm support multiple thread use let see play jvm web framework handle problem goal avoid hogging o thread duration request public static promise result getit promise string foopromise wsurl http get map result resulttostring promise string barpromise wsurl http get map result resulttostring actually waiting promise list resulttype result promisewaitall foopromise barpromise return async resultsmap list string r string fooresponse rsget string barresponse rsget monitoring return ok combineresponses fooresponse barresponse play written scala support among style sometimes encourages functional programming style reflected play java api well written code sample java prior version java verbose point exhaustion written functional style scala api requires understanding scala forcomprehensions practice functional style callbackbased usually contains rigorous mechanism combining callback learn make code much methodic function line executed call foo bar respectively complete one line complete two promise essentially future combined promisewaitall failure count keep local variable even class field two callback called thread even concurrently modify shared state race condition bound happen solve need promise return string string additional monitoring value yes know failure specifically reported differently hold true monitoring value may want collect combined new class need define addition callback executed thread original request handler thread one another use library function rely threadlocal state thread gone like play api certainly feel foreign java netflix rxjava project offer functional api operates along principle tied single web framework familiar java programmer lot nicer imo functional style elegant help compose callback nicely idiomatic manner using entail pretty serious learning curve might bring baffling problem raceconditions happen interact code play well functional bit short go functional might find need go functional way least within service risk serious head scratcher especially programming language restricts shared mutable state like clojure erlang haskell inclined aaaand longer using thread guide flow code delineate request l longer dominated o scheduling capacity solely determined hardware capacity whatever hopefully small overhead frameworkslibraries use incur need buy hardware hardware saturated well good except one thing lost threadperrequest model nobody like nesting callback may argue functional style way go matter unfamiliar arguably difficult reason circumstance yet unproven industry also thread nice give u clear program flow along stack intermediate state must completely throw wonderful abstraction window scalability fault tolerance simple easy follow code mutually exclusive luckily lightweight thread remember started realizing server capacity l little formula dominated o thread scheduling capability becasue cap thread relatively small number became precious resource circuitbreakers functional programming style required thread expensive language notably erlang go provide lightweight thread process erlang goroutines go opensource quasar library provides jvm called fiber lightweigh thread scheduled o language library runtime runtime often far better job o scheduling lightweight thread know purpose particular know run short burst block often generally true heavyweight thread runtime scheduler usually employ known n scheduling lightweight thread mapped onto n o thread n like anything else lightweight thread magic tradeoff main problem blocking function call issue lightweight thread must integrated scheduler library function aware scheduler might block entire o thread quasar monitor running fiber issue warning happens also easily handle nonfrequent blocking o thread many circumstance though acceptable price pay keeping code simple gaining scalability faulttolerance also hard enforce blocking call usually perform one small set operation network io file io database call making sure use library aware lightweight thread usually easy fact quasar make easy turn asynchronous callbackbased api fiberblocking api simple mechanism google proposed userlevel thread make linux kernel allow user code schedule o thread quasar integrate reduce even completely remove possible conflict integrating nonfiberaware blocking library new opensource comsat library set standard java api implementation like jaxrs jdbc integrate quasar fiber let see rewrite original example scalable faulttolerant time using lightweight thread via comsat import import coparalleluniversefiberswsrsclientasyncclientbuilder import coparalleluniversefiberssuspendexecution path myservice public class myrestresource private static final client httpclient asyncclientbuildernewclient get produce mediatypetexthtml public string getit throw interruptedexception suspendexecution int failure submit request asynchronously future string foofuture httpclienttarget http request async get readentity stringclass future string barfuture httpclienttarget http request async get readentity stringclass collect response synchronously string fooresponse null try fooresponse foofutureget catch processingexception e failure string barresponse null try barresponse barfutureget catch processingexception e failure monitoroperation failure return combineresponses fooresponse barresponse notice exactly original example adding future absorb two service latency minor change first adding throw suspendexecution service method designate fiberblocking method alternatively annotate suspendable annotation second use asyncclientbuilder provided comsat provides jaxrs client api implementation fiberaware circuitbreakers critical could add timeouts want quickly respond back failure one service take long mind increased latency sure w might grow l contrained hardware fiber cheap handle hundredsofthousands even million might still want use library like hystrix prevent unbounded number fiber piling even without server recover gracefully shortterm failure cake eat combine awesome performance stability unparalleled tooling monitoring jvm power simplicity lightweight thread make absolute hardware keeping code simple easy understand familiar even need learn new apis clojure scala provide fiberlike functionality scala async clojure wonderful coreasync limited use respective language ie integrate jvm language even based macro restricted single syntactical form explicitely block outermost fiber function call another function block like functional programmingcspactorsrx want learn new apis great like concept believe used make sense computational model make programming easier convoluted way work around o limitation quasar golike channel complete reactive extension rx good measure well full erlanglike actor system built top solid fiber foundation great making businesslogic service endpoint scalable fault tolerant quasar clojure api pulsar even compatible coreasync comsat provides fiberaware implementation standard java apis also offer optional api called web actor web actor let write web application using actor model popularized erlang web actor give excellent scalability faulttolerance particularly fun use interactive web application use websockets serverpush technology comet sse web actor discussed previous blog post conclusion little law determines load request rate server withstand given concurrent request capacity processing latency learned using simple threadperrequest o severly limit server capacity maintain scalability fault tolerance must work around limitation either forgoing simple threadperrequest model adopting functional programming style using language library provides lightweight thread platform developing jvm quasar give lightweight thread fiber comsat give fiberaware implementation standard java apis related article
558,Lobsters,scaling,Scaling and architecture,Writing High-Performance Software,http://blog.bittorrent.com/2014/02/03/tech-talks-writing-high-performance-software/,writing highperformance software,engineering blog,originally posted engineering blog bittorrent tech talk onehour session dedicated stuff keep u busy keep u night keep u coding time time post sharing edition tech talk chief architect arvid norberg describes modern computer work key challenge software performance break modify data structure better take advantage memory cache want write high performance software watch full video follow along slide
560,Lobsters,scaling,Scaling and architecture,Creating a Geospatial database on Amazon RDS,http://www.pheelicks.com/2014/01/creating-a-geospatial-database-on-amazon-rds/,creating geospatial database amazon rds,http wwwpheelickscompostscreatingageospatialdatabaseonamazonrds,http wwwpheelickscompostscreatingageospatialdatabaseonamazonrds
563,Lobsters,scaling,Scaling and architecture,Preventing Unbounded Buffers with RabbitMQ,http://www.rabbitmq.com/blog/2014/01/23/preventing-unbounded-buffers-with-rabbitmq/,preventing unbounded buffer rabbitmq,might problem happen message possible flow control mechanism perqueue message ttl different parameter policy permessage ttl queue ttl queue length limit mixing policy queueing dead lettering deadlettering exchange federation conclusion performance modeling design computer system queueing theory action howto new feature r,different service architecture require certain amount resource operation whether resource cpu ram disk space need make sure enough nt put limit many resource server going use point trouble happens database run file system space medium storage fill image never move somewhere else jvm run ram even back solution problem nt policy expiringdeleting old backup well queue exception make sure application wo nt allow queue grow ever need strategy place deleteevictmigrate old message might problem happen many reason queue might filling message reason number one would data producer outpacing consumer luckily solution easy add consumers happens application still ca nt handle load example consumer take long process message ca nt add consumer since ran server queue start filling message rabbitmq optimized fast message delivery queue message possible rabbitmq come various flow control mechanism course probably want way prevent get situation flowcontrol get activated let see rabbitmq help u perqueue message ttl rabbitmq allows set perqueue message ttls make server deliver message lived queue longer defined perqueue ttl moreover server attempt expire deadletter message soon work great data relevant producer arrives time data ca nt dropped still want queue remain empty possible see section dead lettering two way setup queue ttls one passing extra argument queuedeclare like map string object args new hashmap string object argsput xmessagettl channelqueuedeclare myqueue false false false args previous code tell rabbitmq expire message queue myqueue second could set adding policy queue rabbitmqctl setpolicy ttl messagettl applyto queue policy match queue default virtual host make message expire second note window command bit different course make policy match one queue detail parameter policy want fine grained control message getting expired permessage ttl rabbitmq also support setting permessage ttls set ttl message setting expiration field basicpublish method call previous case value expressed millisecond following code publish message expire second byte messagebodybytes hello world getbytes amqpbasicproperties property new amqpbasicproperties propertiessetexpiration channelbasicpublish myexchange routingkey property messagebodybytes combine permessage ttl perqueue ttl shortest ttl prevail rabbitmq ensure consumer never receive expired message case permessagettl message reach head queue wo nt expired queue ttl rabbitmq also let complete queue expire get deleted rabbitmq unused certain amount time let say set expire queue one hour hour consumer queue basicget command issued queue nt redeclared rabbitmq consider unused delete might want use feature example create queue user online minute inactivity want delete queue think chat application keep queue per connected user could declared autodelete queue go away soon user close channel might useful scenario happens user actually got disconnected mobile network connection quality low certainly nt want delete message soon disconnected feature could let queue live little longer set minute queue expiration using java client map string object args new hashmap string object argsput xexpires channelqueuedeclare myqueue false false false args via policy rabbitmqctl setpolicy expiry expires applyto queue queue length limit want queue nt get message certain threshold configure via xmaxlength argument declare queue rather neat simply way control capacity queue reach threshold new message arrives message front queue older dropped making room newly arrived message one reason behaviour old message probably irrelevant application new one let queue keep mind queue length take account message ready delivered unacked message wo nt add count proper basicqos setting help application since default rabbitmq send many message possible consumer creating situation queue appears empty fact lot unacked message taking resource well setting queue length limit quite easy example java set limit message map string object args new hashmap string object argsput xmaxlength channelqueuedeclare myqueue false false false args via policy rabbitmqctl setpolicy ten maxlength applyto queue mixing policy keep mind one policy applies queue given time run previous setpolicy command succession last one take place trick one policy applying resource lay passing policy together json object example rabbitmqctl setpolicy cappedqueues capped maxlength expires messagettl applyto queue queueing wait read right yes queueing imagine busy day arrive post office see every counter busy since nt time waste waiting line go back continue word request served right without queueing well rabbitmq something similar application message queue trick consists setting perqueuettl zero message ca nt delivered immediately consumer expired right away set deadletter exchange could get message deadlettered separate queue dead lettering mentioning deadlettering couple time already feature could set dead letter exchange dlx one queue message queue expires queue limit exceeded message published dlx bind separate queue exchange later process message sent queuedeclare example setting dlx channelexchangedeclare someexchangename direct map string object args new hashmap string object argsput xdeadletterexchange someexchangename channelqueuedeclare myqueue false false false args java deadlettering message keep queue right size expected amount message wo nt prevent filling node message message queued different queue node point new deadletter queue could present problem could case use exchange federation send message separate node process separately main flow application conclusion one basic question queueing theory regard request arriving system stated follows  mean arrival time mean service rate  happens queue length go infinity time know encounter problem point architecture sooner later application trouble luckily u rabbitmq offer many feature like queue message ttls queue expiration queue length tailored avoid issue interesting nt need lose message use feature deadletter exchange help u reroute message appropriate place time make technique part queueing messaging arsenal performance modeling design computer system queueing theory action entry posted thursday january pm alvaro filed howto new feature follow response entry r feed comment ping currently closed
564,Lobsters,scaling,Scaling and architecture,Brian Hurt: A Pragmatic Case for Static Typing,http://vimeo.com/72870631,brian hurt pragmatic case static typing,,link beginnersfriendly talk explain reason haskell programmer enthusiastic static typing help produce working code sooner consider problem programmer face producing working code language show haskell mitigates solves knowledge haskell assumed subject touched include preventing null pointer exception speeding finding fixing bug problem large scale development nt nt work type documentation type parallel programming type v macro one way skin cat
566,Lobsters,scaling,Scaling and architecture,"simple linear-time sorting algorithm, using a flattened linked-list index",https://gist.github.com/silentbicycle/8389129,simple lineartime sorting algorithm using flattened linkedlist index,,copyright c scott vokes vokess gmailcom permission use copy modify andor distribute software purpose without fee hereby granted provided copyright notice permission notice appear copy software provided author disclaims warranty regard software including implied warranty merchantability fitness event shall author liable special direct indirect consequential damage damage whatsoever resulting loss use data profit whether action contract negligence tortious action arising connection use performance software example code lineartime sorting algorithm variation counting sort note rather sorting input place return array offset used slice input ascending order could used larger value though next need changed either next sparse data structure eg hash table include stdlibh include stdioh include stdinth include stdboolh include stringh include asserth byte buf sz byte generate array index used slice buf get content ascending order linear time space static flattenedindexsort buf sizet sz ao malloc sz sizeof ascending offset idx malloc sz sizeof index ao null idx null goto cleanup next memset next sizeof next const none first pas build flattened linkedlist index idx offset next instance byte buf endoflist offset first instance byte c stored next c int sz c buf idx next c next c second pas fill array ascending offset walking linked list offset index starting next byte point first sizet offset link next link none ao offset link link idx link free idx assert offset sz return ao cleanup ao free ao idx free idx return null int main int argc char argv define bufsz char inbuf bufsz char sorted bufsz printf char fgets inbuf bufsz stdin null break sizet len strlen drop end ao flattenedindexsort len ao null break len sorted inbuf ao sorted len printf offset content ascending ordern len printf n printf ao free ao printf nnsorted n sn sorted void argc void argv return
569,Lobsters,scaling,Scaling and architecture,Scaling Mercurial at Facebook,https://code.facebook.com/posts/218678814984400/scaling-mercurial-at-facebook/,scaling mercurial facebook,choosing source control system mercurial algorithm rewrite tight loop native code speeding file status operation even mercurial watchman working large history improving clone pull clone pull performance gain work open source hgwatchman remotefilelog,thousand commits week across hundred thousand file facebook main source repository time larger even linux kernel checked million line code file given size facebook practice shipping code twice source control one way help engineer move fast choosing source control system two year ago saw repository continue grow staggering rate sat extrapolated growth forward year based projection appeared likely thencurrent technology subversion server git mirror would become productivity bottleneck soon looked available option found none fast easy use scale code base grown organically internal dependency complex could spent lot time making modular way would friendly source control tool number benefit using single repository even current scale often make large change throughout code base single repository useful continuous modernization splitting would make large atomic refactorings difficult top idea scaling constraint source control system dictate code structure sit well u realized solve instead building new system scratch decided take existing one make scale engineer comfortable git preferred stay familiar tool took long hard look improving work scale much deliberation concluded git internals would difficult work ambitious scaling project instead chose improve mercurial mercurial distributed source control system similar git many equivalent feature importantly written mostly clean modular python native code hot path making deeply extensible importantly mercurial developer community actively helping u address scaling problem reviewing patch keeping scale mind designing new feature first started working mercurial found slower git several notable area narrow performance gap contributed patch mercurial last year half range new graph algorithm rewrite tight loop native code helped also wanted make fundamental change address problem scale speeding file status operation repository large major bottleneck simply finding file changed git examines every file naturally becomes slower slower number file increase perforce cheat forcing user tell file going edit git approach scale perforce approach friendly solved monitoring file system change tried even mercurial making work reliably surprisingly challenging decided query build system file monitor watchman see file changed mercurial design made integrating watchman straightforward expected watchman bug developed strategy address safely heavy stress testing internal dogfooding identified fixed many issue race condition common file system monitoring particular ran beta test engineer machine comparing watchman answer real user query actual file system result logging difference couple month monitoring fixing discrepancy usage got rate low enough comfortable enabling watchman default engineer repository enabling watchman integration made mercurial status command faster git status command command look changed diff update became faster working large history rate commits sheer size history also pose challenge thousand commits made every day repository get larger becomes increasingly painful clone pull centralized source control system like subversion avoid checking single commit leaving history server save space client leaf unable work server go recent distributed source control system like git mercurial copy history client take time space allows browse commit entirely locally wanted happy medium speed space centralized system robustness flexibility distributed one improving clone pull normally run pull mercurial figure changed server since last pull downloads new commit metadata file content ten thousand file changing every day downloading history client every day slow solve problem created remotefilelog extension mercurial extension change clone pull command download commit metadata omitting file change account bulk download user performs operation need content file checkout download file content demand using facebook existing memcache infrastructure allows clone pull fast matter much history changed adding slight overhead checkout central mercurial server go big benefit distributed source control ability work without interacting server remotefilelog extension intelligently cache file revision needed local commits checkout rebase commit existing bookmark without needing access server since still download commit metadata operation require file content log completely local well lastly use facebook memcache infrastructure caching layer front central mercurial server even central repository go memcache continue serve many file content request type setup course optimized work environment reliable mercurial server always connected fast lowlatency network work environment fast reliable internet connection extension could result mercurial command slow failing unexpectedly server congested unreachable clone pull performance gain enabling remotefilelog extension employee facebook made mercurial clone pull faster bringing minute second addition way remotefilelog store local data disk large rebases faster compared previous git infrastructure number remain impressive achieving type performance gain extension one big reason chose mercurial finally remotefilelog extension allowed u shift request traffic memcache reduced mercurial server network load make easier mercurial infrastructure keep scaling meet growing demand work mercurial several nice abstraction made extension possible notable filelog class filelog data structure representing every revision particular file version file identified unique hash given hash filelog reconstruct requested version file remotefilelog extension replaces filelog alternative implementation interface accepts hash instead reconstructing version file local data fetch version either local cache remote server need request large number file server large batch avoid overhead many request open source together hgwatchman remotefilelog extension improved source control performance developer allowing spend time getting stuff done instead waiting tool large deployment distributed revision control system encourage take look made difference developer hope prove valuable
570,Lobsters,scaling,Scaling and architecture,The Raft Consensus Algorithm,http://raftconsensus.github.io/,raft consensus algorithm,http raftgithubio,website moved http raftgithubio
571,Lobsters,scaling,Scaling and architecture,SALSA: Scalable and Low Synchronization NUMA-aware Algorithm for Producer-Consumer Pools,http://webee.technion.ac.il/~idish/ftp/SALSA-full.pdf,salsa scalable low synchronization numaaware algorithm producerconsumer pool,,obj length filter flatedecode stream da c jy pgb endstream endobj obj type page content r resource r mediabox parent r endobj obj font r r r procset pdf text endobj obj length filter flatedecode stream nhw c k h dq zz w endstream endobj obj type page content r resource r mediabox parent r endobj obj font r r r r procset pdf text endobj obj length filter flatedecode stream  eal b w q l biuk endstream endobj obj type page content r resource r mediabox parent r endobj obj font r r r procset pdf text endobj obj length filter flatedecode stream h cof r p k qj n endstream endobj obj type page content r resource r mediabox parent r endobj obj font r r r r r r procset pdf text endobj obj length filter flatedecode stream sbm  q u  ei v r  endstream endobj obj type page content r resource r mediabox parent r endobj obj font r r r r r r r r procset pdf text endobj obj length filter flatedecode stream x v wlb b  db endstream endobj obj type page content r resource r mediabox parent r endobj obj type xobject subtype form formtype ptexfilename e ptexpagenumber ptexinfodict r matrix bbox resource colorspace r extgstate r font r procset pdf text length filter flatedecode stream  x u r u b uv eo
572,Lobsters,scaling,Scaling and architecture,jubilee - A rack server with Vert.x awesomeness builtin,https://github.com/isaiah/jubilee,jubilee rack server vertx awesomeness builtin,jubilee server application another rack server get started branch rail event bus example chatapp performance tuning wrk contributing license licensetxt acknowledgment yourkit java profiler yourkit net profiler,jubilee server rack application compatible http server built check demo application another rack server vertx lightweight high performance application platform jvm designed modern mobile web enterprise application vertxio site using vertx jubilee inherent advantage term performance cool feature vertx get started make sure jdk jruby installed bundle bundle exec rake install development branch jubilee us vertx working version please check branch rail default setup jubilee run instance web server jruby runtime find jubilee crash hang outofmemeoryerror please tune jvm opts like export javaopts xx xx o memory quite limited please run jubilee jubilee n event bus event bus pubsub mechanism used server server server client client client api use build living real time web application example assume necessary javascript file loaded page found start jubilee rack application jubilee eventbus eventbus one browser var eb new vertxeventbus eventbus ebregisterhandler test function data consoleinfo data another var eb new vertxeventbus eventbus ebsend test hello world previous tab print greeting sent advanced example please checkout chatapp performance tuning creating lot connection jubilee vertx server short period time eg benchmarking tool like wrk may need tweak setting order avoid tcp accept queue getting full result connection refused packet dropped handshake cause client retry classic symptom see long connection time client tune operating system specific linux need increase couple setting tcp net config arbitrarily large number sudo sysctl w sudo sysctl w operating system please consult operating system documentation contributing kind contribution welcome file issue encounter problem prefer fix fork create feature branch git checkout b mynewfeature commit change git commit add feature push branch git push origin mynewfeature create new pull request license see licensetxt acknowledgment yourkit kindly supporting jubilee server fullfeatured java profiler yourkit llc creator innovative intelligent tool profiling java net application take look yourkit leading software product yourkit java profiler yourkit net profiler
573,Lobsters,scaling,Scaling and architecture,"Dapper, a Large-Scale Distributed Systems Tracing Infrastructure (2010)",http://research.google.com/pubs/pub36356.html,dapper largescale distributed system tracing infrastructure,,modern internet service often implemented complex largescale distributed system application constructed collection software module may developed different team perhaps different programming language could span many thousand machine across multiple physical facili tie tool aid understanding system behavior reasoning performance issue invaluable environment introduce design dapper production distributed system tracing infrastructure describe design goal low overhead applicationlevel transparency ubiquitous deployment large scale system met dapper share conceptual similarity tracing system particularly magpie xtrace certain design choice made key success environment use sampling restricting instrumentation rather small number common library main goal paper report experience building deploying using system two year since foremost measure success usefulness developer operation team dapper began selfcontained tracing tool evolved monitoring platform enabled creation many different tool anticipated designer describe analysis tool built using dapper share statistic usage within google present example use case discus lesson learned far
574,Lobsters,scaling,Scaling and architecture,Framework Benchmarks Round 8,http://www.techempower.com/blog/2013/12/17/framework-benchmarks-round-8/,framework benchmark round,pull request view round result round note observation go hiphop php vm pull request vertx netty undertow grizzly thanks google group github techempower read,merry christmas web framework performance aficionado better way celebrate holiday cheering favorite race variety application fundamental biggest web platform grudge match season certainly ca nt think anything festive framework permutation variation configuration round something everyone nt want join party fruitcake egg nog maybe enjoy pull request almost good egg nog veritable rainbow holiday cheer awaits view round result round note observation go always scrappy competitor flex performance muscle land razorthin victory json serialization hardware aware highestperformance framework network limited json serialization plaintext test anything club sure keep overhead bare minimum leaving maximum headroom custom application logic round missing go framework due configuration problem problem resolved go framework returned round reaffirm go viable performance rival jvm hiphop php vm framework thanks part mysql driver php yield dominion update test hhvm impressive multiple query test well however hhvm trail plain php fortune test presently implementation detail may play interested testing hhvm popular php framework would happy receive pull request vertx netty wrestled plaintext crown undertow rivalry nt yet settled rumor improvement store round meanwhile newcomer named plain may rival go need searchfriendly name though irony go make uncontested champion right behind leader interestingly plain demonstrates highest window performance seen massive margin reaching pipelined plaintext request per second bear mind test networkbound gigabit ethernet netty vertx upgrade paid huge dividend netty breaking pipelined plaintext response per second humble instance grizzly performance json serialization round showing unfortunately yet determined cause plaintext test requirement clarified necessary copy byte small response payload per request using prerendered byte buffer body acceptable long conventional platform framework tested response header composed normally maximum query update performance mongo substantially higher mysql looking chart particular consider filtering preferred data store maintain useful perspective related late change mongo schema intended replace id id caused challenge postponed round rerun mongo test schema provides column allow test complete want normalize implementation round targeting early december round two week still room improvement toward goal monthly cycle target midjanuary round thanks big thankyou contributor added improved existing test implementation round contributor round particular order lhotari methane pseudonom lucassp aualin kpacha nareshv marting bclozel ijl bbrowning sbordet purplefox stuartwdouglas normanmaurer kardianos hamiltont julienschmidt question comment criticism would like contribute new test improvement existing one please join google group visit project github techempower provide web mobile application development service passionate application performance read
575,Lobsters,scaling,Scaling and architecture,The Saddest Moment,https://research.microsoft.com/en-us/people/mickens/thesaddestmoment.pdf,saddest moment,microsoft emeritus researcher page,get know microsoft researcher engineer tackling complex problem across wide range discipline visit microsoft emeritus researcher page learn made significant contribution field computer science year microsoft throughout career
576,Lobsters,scaling,Scaling and architecture,gearman [Gearman Job Server],http://gearman.org/,gearman gearman job server,gearman open source multilanguage flexible fast embeddable single point failure limit message size worried scaling communication gearman work documentation gearman useful involved project example,gearman gearman provides generic application framework farm work machine process better suited work allows work parallel load balance processing call function language used variety application highavailability web site transport database replication event word nervous system distributed processing communicates strong point gearman open source free meaning word gearman active open source community easy get involved need help want contribute worried licensing gearman bsd multilanguage interface number language list growing also option write heterogeneous application client submitting work one language worker performing work another flexible tied specific design pattern quickly put together distributed application using model choose one option mapreduce fast gearman simple protocol interface optimized threaded server written cc minimize application overhead embeddable since gearman fast lightweight great application size also easy introduce existing application minimal overhead single point failure gearman help scale system fault tolerant way limit message size gearman support single message size need something bigger problem gearman chunk message worried scaling worry gearman craig list tumblr yelp etsy discover others known year content updated regularly please check back often may also want check form communication would like learn get involved gearman work gearman powered application consists three part client worker job server client responsible creating job run sending job server job server find suitable worker run job forward job worker performs work requested client sends response client job server gearman provides client worker apis application call talk gearman job server also known gearmand need deal networking mapping job internally gearman client worker apis communicate job server using tcp socket explain gearman work detail let look simple application reverse order character string example given php although apis look quite similar start writing client application responsible sending job waiting result print using gearman client api send data associated function name case function reverse code error handling omitted brevity php reverse client code client new gearmanclient client addserver print client reverse hello world code initializes client class configures use job server addserver argument mean use default port tell client api run reverse function workload hello world function name argument completely arbitrary far gearman concerned could send data structure appropriate application text binary point gearman client api package job gearman protocol packet send job server find appropriate worker run reverse function let look worker code php reverse worker code worker new gearmanworker worker addserver worker addfunction reverse function job return strrev job workload worker work code defines function myreversefunction take string return reverse string used worker object register function named reverse setup connect local job server client job server receives job run look list worker registered function name reverse forward job one free worker gearman worker api take request run function myreversefunction sends result function back job server client see client worker apis along job server deal job management network communication focus application part different way run job gearman including background asynchronous processing prioritized job see documentation available various apis detail gearman useful reverse example seems like lot work run function number way useful simplest answer use gearman interface client worker written different language want php web application call function written c could use php client api c worker api stick job server middle course efficient way like writing php extension c may want php client python worker perhaps mysql client perl worker mix match supported language interface easily need application able understand workload sent favorite language supported yet get involved project probably fairly easy either one existing gearman developer put language wrapper top c library next way gearman useful put worker code separate machine cluster machine better suited work say php web application want image conversion much processing run web server machine could instead ship image separate set worker machine conversion way load impact performance web server php script also get natural form load balancing since job server sends new job idle worker worker running given machine busy need worry new job sent make scaleout multicore server quite simple core worker machine start instance worker perhaps cpu bound also seamless add new machine expand worker pool boot install worker code connect existing job server probably asking job server dy able run multiple job server client worker connect first available job server configured way one job server dy client worker automatically fail another job server probably want run many job server two three good idea redundancy diagram left show simple gearman cluster may look scale client worker needed job server easily handle hundred client worker connected draw physical virtual machine line capacity allows potentially distributing load number machine detail specific us installation see section example update come
577,Lobsters,scaling,Scaling and architecture,How a Small Team Scales Instagram,http://www.infoq.com/presentations/scaling-instagram,small team scale instagram,infoq homepage presentation small team scale instagram summary bio conference related sponsored content,infoq homepage presentation small team scale instagram small team scale instagram summary mike krieger discus instagram best worst infrastructure decision building deploying scalable extensible service bio mike krieger mikeyk graduated stanford university studied symbolic system focus humancomputer interaction undergrad interned microsoft powerpoint team graduating worked meebo year half user experience designer frontend engineer joining instagram team design development conference software changing world qcon empowers software development facilitating spread knowledge innovation developer community practitionerdriven conference qcon designed technical team lead architect engineering director project manager influence innovation team recorded dec related sponsored content
578,Lobsters,scaling,Scaling and architecture,Web Framework Benchmarks Round 7,http://www.techempower.com/blog/2013/10/31/framework-benchmarks-round-7/,web framework benchmark round,view round result round note observation undertow github repository google group thanks google group github techempower read,happy halloween fan web development framework severalmonth hiatus round project measuring performance web application framework platform available round includes many new framework test implementation contributed community falcore grizzly httplistener phpixie plain racketws start stream treefrog whopping framework individual test permutation many preexisting framework test updated include test coverage andor update dependency tune implementation date project processed pull request community thanks much contribution grateful continued interest view round result round note observation round champion undertow web server wildfly continues impress chartdominating showing plaintext request per second meager instance thanks community contribution c test dramatically improved especially querying database also sql server test environment contributor prepared script running benchmark suite window azure unfortunately unable reach author script past week azure expert interested picking work exists please visit github repository google group project highperformance tier become significantly crowded even project relatively short history interesting u many framework easily saturate gigabit ethernet json serialization plaintext test even test intentionally small payload hardware necessary run gigabit ethernet test gbe lab willing run suite love publish result benchmark toolset continues mature gradually lot room improvement still exists great deal sanitychecking remains manual process python programmer interested project let u know several enhancement like make benchmark tool set python script time permitting round used communityreview model wherein project participant able review preliminary result capturing environment submit pull request model perfect need improve round help reduce amount time techempower need allocate round sanity check meaning quicker turnaround round see spun good thing starting aim monthly cycle running official round help reduce perceived severity configuration problem since addressed next run month away also pushed display name test project allowing contributor assign test permutation name choose eg playscalaanorm aspnetmvcmono one particularly interesting anomaly dominance window paired mongo update test performance slightly lower pairing case workstation precise instance differ factor seven possible window instance running newer host linux instance classified speaking database test previous round used ssd host database prior finishing round ssd failed round run ramdiskbacked database excluding sql server project database benchmark believed would fascinating see performance full stack friction database writes reduced bare minimum confirmed previous spot checking round database writes faster across board using ramdisk versus samsung pro ssd using expected read unaffected since test designed allow database engine fit entire data set memory thanks always like say thank contributor added test implementation new framework improved existing implementation round unusually long also thank everyone patience contributor round numerous particular order fernandoacorreia kppullin malcolmevershed methane kevinhoward huntc lucassp dracony kekekeks fwbrasil treefrogframework yogthos oberhamsi purplefox necaris pdonald kepinator davidbadura zznate nightlyone jeapostrophe astaxie troytoman grob torhve trautonen stuartwdouglas xaxaxa sincere apology forgot anyone question comment criticism would like contribute new test improvement existing one please join google group visit project github techempower provide web mobile application development service passionate application performance read
579,Lobsters,scaling,Scaling and architecture,Software Controls Cache Memory to Speed CPUs - IEEE Spectrum,http://spectrum.ieee.org/semiconductors/memory/software-controls-cache-memory-to-speed-cpus,software control cache memory speed cpu ieee spectrum,,
580,Lobsters,scaling,Scaling and architecture,Erlang Style Actors Are All About Shared State (2009),http://james-iry.blogspot.com/2009/04/erlang-style-actors-are-all-about.html,erlang style actor shared state,last week exciting episode situation finally damn code last note postscript,actor become quite popular topic besides erlang famous library implementation scala least java shared state propaganda setting people failure last week exciting episode defined state nt fact something responds differently input time detail represented based show saying erlang style actor nt share state totally wrong headed fact nt want share state actor model clunky way thingsbefore go let emphasize like erlang like actor nifty model stateless model let show ranting problem erlang style actor shared mutable changeable state model shared state nt need share mutable state better alternative actor importantly even using actor well must think problem shared mutable state people believe propaganda rude shocksthe situationlast time described simple sock tube dispensing machine insert two coin press button get tube sock insert many coin get extra coin returned push button inserted enough coin get nothing diagramimagine alice bob work crap corp making high quality crap since day like saunter break room purchase nice warm tube sock crap corp machine nt take regular coin crap corp coin instead ccc weighs pound roughly kilogram naturally alice bob nt want carry pound crappy token around laboriously drag token machine insert walk back cubicle grab another repeat alice bob take sock break different time probably going work fine tend overlap bad thing happen possible alice insert first coin bob insert first coin alice insert second coin get coin back bonus cry happily experiencing greatest joy ever experienced crap corp alice push button get tube sock merrily skip back cube well maybe skip exactly whatever ecstatically happy carrying pound crapthen bob show insert second coin eagerly smash button get well deserved tube sock get nothing feeling cheated pound machine kick shake rock back forth inevitably machine tip fall bob crush ton crap corp coin inserted week tragic ending somebody wanted socksnow outcome nt guaranteed even bob alice start time way inserting first coin alice could waylaid bos looking tps report alice patiently explains tps report never job b discontinued three year ago c eye face chest bob could merrily taken two coin trip safely received tube sock without ever knowing mortal injury narrowly avoidedfinally damn codeany time something unwanted happen result unpredictable delay scheduler priority workload etc race condition could unwanted crushed vending machine could unpredictable pointy haired bos write exactly erlangin file called sockmachineerl first little standard module definition export business module sockmachine export gut machine zerocoins onecoin twocoins state machine one called block waiting message inbox based message get responds nothing nothing happens coin need return coin tubeofsocks win also call appropriate function next state might state private function exported module note clever way write explanatory purpose like thiszerocoins receive coin nothing onecoin button nothing zerocoins end onecoin receive coin nothing twocoins button nothing onecoin end twocoins receive coin coin twocoins button tubeofsocks zerocoins end start spawn new sock machine actor zerocoins statestart spawn fun zerocoins end insertcoin pushbutton rpc style convenience function insert coin push button get backwards well whichever return whatever recieve message back machineinsertcoin machine machine coin self receive x x end pushbutton machine machine button self receive x x endtest spawn many concurrent test loop requested simultaneously pound one machinetest machine process process spawn fun testloop machine end test machine process true io format test process launchedn end testloop repeatedly walk cycle inserting coin pushing button call helper function mirror state sock machine show expects happen step complaining thing nt go well testloop process machine count count testzerocoins process machine count true io format w testing completedn process end testzerocoins process machine count case insertcoin machine nothing testonecoin process machine count coin io format w bonus coin n process testtwocoins process machine count end testonecoin process machine count case insertcoin machine nothing testtwocoins process machine count coin io format w bonus coin n process testtwocoins process machine count end testtwocoins process machine count case pushbutton machine tubeofsocks io format w got socksn process nothing io format w blasted machine ate money give rattle rattle arrrgghghhrhrhghn process end testloop process machine count fire erl compile start machine test running test c sockmachine ok sockmachine machine sockmachine start sockmachine test test process launched ok got sock got sock got sock got sock got sock got sock got sock got sock got sock got sock testing completed ah sweet sweet success run another test concurrent test loop bob alice way around sockmachine test test process launched bonus coin bonus coin ok got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh bonus coin bonus coin got sock blasted machine ate money give rattle rattle arrrgghghhrhrhgh testing completed testing completed litany sock bonus coin crushed intestine machine oddly predictable litany real distributed erlang app would much interesting random litany network delay would emulate pointy haired boss even better erlang schedulersome last noteswith erlang style programming actor central unit statefulness multiple actor share access one stateful actor hence shared state race condition ruptured spleen saying erlang actor bad fact quite like erlang model nicely separate must stateful concurrent pure computation making state much painful write foox foox actor model encourages think consequence sharing also cleanly mirror mechanic distributed computing asynchronous io nice statelessone last note started actor shared state naturally one might ask well stateless actor actor nt change state depend state via io certainly viable us actor longer concurrency parallelism imho future data flow variable haskell data parallelism cleaner way deal parallelism someday soon hope write meantime whole point complexity message passing instead simpler mechanism precisely deal complexity concurrent stateone really last note sadly simple straight actor nt automatically compose well design set actor interact correctly one use case nt interact well plugged different system another aspect actor share traditional manual lock shared mutable memory date best known way deal composable state transaction optimistic pessimistic distributed compensating software hardware database whatever nice transactional capability available erlang yet another area shared state mantra lead people think actor entire answer without needing anything elsetry get crushed good luck sock postscriptit suggested comment people say erlang style actor nt share state mean nt share memory first clearly defined state previous article different representation importantly think saying nt share memory distinction without much relevance mostly implementor point view nt reflect user must think actorshere erlang code simple shared mutable variable nt recommended erlang usage nt break model waymodule variable export malloc spawn fun loop end loop value receive fetch value loop value store newvalue loop newvalue end fetch variable variable fetch self receive x x end store variable value variable store value using c variable ok variable x variable malloc variable store variable fetch x variable store x variable fetch x variable fetch x since variable actor easy share sock machine example
581,Lobsters,scaling,Scaling and architecture,Getting the most out of HAProxy,https://www.twilio.com/engineering/2013/10/16/haproxy,getting haproxy,haproxy logging documentation serviceoriented architecture haproxy logging syslog logrotate http keepalive haproxy keep persistent connection timeout request made haproxy default haproxy retry connect attempt time,haproxy pretty magical piece software use extensively twilio today going walk haproxy configuration explain setting therein first short story history haproxy beginning founder created haproxy config config let one host cluster talk host carried manner traffic config acceptable let founder raise venture capital round founder said make sm service send sm api talk sm service send message good needed config api talk sm service copied config voice service api could talk sm good config adapted sm request config fruitful multiplied across service twilio nobody really understood config option needed new config copied old one way problem spread across cluster state got haproxy setting olden day twilio haproxy configs copy timeout value changed quest five nine reliability tightened haproxy configs made sure appropriate service traffic carried let walk sample configuration twilio http haproxy configuration version line let u know glance config touchedupdated someone recent history company mode http option httplog log info two option turn haproxy power carrying http traffic haproxy definitely setting mode http mean haproxy inspect incoming http request reject request invalid mode also allows log http request generating log line like oct haproxy twiliofrontend get http examplecomacaaaa signaturesignature log give u incredibly rich amount detail request url requested total time spent connecting total time download total number concurrent connection downstream service full explanation log message mean see haproxy logging documentation convert twilio tool serviceoriented architecture log invaluable debugging failure example customer wrote recording downloads failing occasionally recording stack hit multiple different service log like record timing network component helped u nail failure single host third line log info enables haproxy logging syslog separate daemon listens facility log incoming haproxy message log file note enabling info log level mean haproxy log one line per incoming request server stay long period time mean log file overwhelm disk important data purging policy place every night logrotate compress previous day log wipe log older week option httpserverclose let say one haproxy frontend communicating ten different backends moreover let say ten backends nt support http keepalive setting let haproxy keep persistent connection client frontend rotating http request backend server course get full value setting necessary client keepalive aware well timeout client timeout server two setting combine set timeout request made haproxy essentially long haproxy wait server return information client application kind maximum latency want add timeout u api wait response maximum second every internal service timeout set value lower second based sla note server streaming one byte slowly say every second trigger read timeout may want separate thread watching request ensure completes sane amount time also set client server timeouts value desired read timeout socket timeout connect different timeout client server timeouts amount time haproxy spend trying connect host olden day twilio set value server timeout second host died haproxy would try connect dang host second connect usually happens matter millisecond host machine lan nearby az allow second default tcp retransmission window proceed small amount buffer unlike server timeouts connection timeouts imply request safe client retry retries option redispatch said second connect timeout meant haproxy would try bad connection second lied turn default haproxy retry connect attempt time second connect timeout actually second connect timeout blowing sla meaning returning empty response customer common assumption haproxy automatically retry request second host first one go true haproxy marked host healthcheck host go healthchecks take second pull looking possible second failed request host redispatch option make final connect request different downstream host individual request degree protection host going bad two setting combination cut number retries mean attempt connection single host maximum eight second giving trying different host option httpchk get healthcheck default haproxy simply open tcp connection host check whether ping detect host host unhealthy bad disk bad networking connection httpchk option make http request endpoint backend backend query answer whether healthy note healthchecks fairly conservative generally scoped health individual host failed healthcheck lead haproxy send zero traffic host host go bad time left backends hope helpful posted haproxy config update anything reading manual lead significant improvement performance reliability usability stability server hope walkthrough saved time
584,Lobsters,scaling,Scaling and architecture,"Shopify open-sources Sarama, a client for Kafka 0.8 written in Go",http://www.shopify.com/technology/8760937-shopify-open-sources-sarama-a-client-for-kafka-0-8-written-in-go,shopify opensources sarama client kafka written go,,shopify world fastestgrowing commerce platform plan slow working ship quality instead time team continuously deploy new code large scale support hundred thousand online store hundred million request day entrepreneur household brand depending u livelihood tough incredibly rewarding responsibility looking passionate problem solver looking
585,Lobsters,scaling,Scaling and architecture,Rewriting a large production system in Go,http://matt-welsh.blogspot.com/2013/08/rewriting-large-production-system-in-go.html?m=1,rewriting large production system go,go michael piatek goroutines well documented online tutorial,team google wrapping effort rewrite large production system almost entirely go say almost one component system library transcoding image format work perfectly well c decided leave asis rest system go wrapper existing module c another language fun experience thought share lesson learned plus go language cute mascot awwww rewrite first question must answer considered rewrite first place started project adopted existing c based system developed course couple year two sister team google good system job remarkably well however used several different project vastly different goal leading nontrivial accretion cruft time became apparent u continue innovate rapidly would extremely challenging large shared codebase ding original developer fact certain design decision become ossified becomes difficult rethink especially multiple team sharing code rewrite realized needed small subset functionality original system perhaps le project also looking making radical change core logic wanted experiment new feature way would impact velocity team others using code finally cognitive burden associated making change large shared codebase unbearable almost change required touching lot code developer fully understand updating test case unclear consequence user code decided fork fromscratch rewrite bet made taking initial productivity hit initial rewrite would pay drove able add feature time also given u opportunity rethink core design decision system extremely valuable improving understanding working go admit first highly skeptical using go production system sits directly serving path user content fast also handle large query volume cpu memory efficiency key go reliance garbage collection gave pause pun intended har har har given much pain java developer go manage memory footprint also sure well go would supported kind development wanted inside google system lot dependency last thing wanted reinvent lot library go already c finally also simply fear unknown whole attitude changed michael piatek one star engineer group sent initial cut core system rewrite go result le week work unlike original c based system could actually read code even though nt know go yet benefit get go lightweight concurrency provided goroutines instead messy chain dozen asynchronous callback spread ten source file core logic system fit couple hundred line code file read top bottom make sense michael also made observation go language designed writing webbased service standard library provide machinery need serving http processing url dealing socket crypto processing date timestamps compression unlike say python go compiled language therefore fast go modular design make beautiful decomposition code across module clear explicit dependency incremental compilation approach make build lightning fast automatic memory management mean never worry freeing memory although usual caveat gcbased language apply terse syntactically go succinct indeed go style guideline encourage write code tersely possible first drove wall since used using long descriptive variable name spreading expression many line possible appreciate terse coding approach make reading understanding code later much much easier personally really like coding go get point without write bunch boilerplate make compiler happy unlike c nt split logic code across header file cc file unlike java nt write anything compiler infer including type variable go feel lot like coding lean scripting language like python get type safety free gobased rewrite go source file totaling line code including comment compare original system c source file line code remember said new system implementing small subset new system functionality though feel code size reduction disproportionate functionality reduction rampup time learning go easy coming clike language background real surprise language pretty much make sense standard library well documented plenty online tutorial none engineer team taken long come speed language heck even one intern picked couple day overall rewrite taken month already running production also implemented major new feature would taken much longer implement original c based system reason described estimate team productivity improved least factor ten moving new codebase using go go thing go super happy tend bite time time first need know whether variable dealing interface struct structs implement interface course general tend treat thing dealing struct might passing reference type mystruct might passing value type mystruct hand thing dealing interface never pointer interface pointer sense get confusing looking code passing thing around without remember might actually pointer interface rather struct go type inference make lean code requires dig little figure type given variable explicit given code like foo bar somefunc baz really like know foo bar actually case want add new code operate could get use editor vi maybe would get help ide regard staunchly refuse edit code tool requires using mouse finally go liberal use interface allows struct implement interface accident never explicitly declare given struct implement particular interface although good coding style mention comment problem difficult tell reading given segment code whether developer intended struct implement interface appear projecting onto also want refactor interface go find undeclared implementation le hand find coding go really really fun bad thing since know real programming supposed grueling painful exercise fighting compiler tool programming go making soft one day find octagon ring bunch sweaty muscular c programmer bareknuckling death know going mop floor ok keep cuddling stuffed gopher running gofmt autointent code obdisclaimer everything post personal opinion represent view employer
586,Lobsters,scaling,Scaling and architecture,"New Tweets per second record, and how",https://blog.twitter.com/2013/new-tweets-per-second-record-and-how,new tweet per second record,castle sky realtime global conversation starting rearchitect jvm v ruby vm social graph system programming model put together future finagle independent system storage snowflake observability statistic zipkin runtime configuration testing today twittereng jreichhold dhelder aa noradio themattharris,recently something remarkable happened twitter saturday august japan people watched airing castle sky one moment took twitter much hit onesecond peak tweet per second august pdt august jst give context compare typical number normally take million tweet day mean tweet second average particular spike around time greater steady state spike user experience blip twitter one goal make sure twitter always available matter happening around world goal felt unattainable three year ago world cup put twitter squarely center realtime global conversation influx tweet every shot goal penalty kick yellow red card repeatedly took toll made twitter unavailable short period time engineering worked throughout night time desperately trying find implement orderofmagnitudes efficiency gain unfortunately gain quickly swamped rapid growth engineering started run lowhanging fruit fix experience determined needed step back determined needed rearchitect site support continued growth twitter keep running smoothly since worked hard make sure service resilient impulse able withstand event like castle sky viewing super bowl global new eve celebration rearchitecture made service resilient traffic spike record high also provides flexible platform build feature faster including synchronizing direct message across device twitter card allow tweet become richer contain content rich search experience includes story user feature coming detail learned lot changed engineering organization next week publishing additional post go detail topic cover herestarting rearchitect world cup dust settled surveyed state engineering finding running one largest ruby rail installation pushed pretty far time engineer contributing gotten twitter explosive growth term new user well sheer amount traffic handling system also monolithic everything managing raw database memcache connection rendering site presenting public apis one codebase increasingly difficult engineer expert put together also organizationally challenging u manage parallelize engineering team reached limit throughput storage system relying mysql storage system temporally sharded single master system trouble ingesting tweet rate showing operationally create new database ever increasing rate experiencing read write hot spot throughout database machine instead engineering thorough solution frontend ruby machine handling number transaction per second thought reasonable given horsepower previous experience knew machine could lot finally software standpoint found pushed started trade readability flexibility codebase performance efficiency concluded needed start project reenvision system set three goal challenge wanted big infrastructure win performance efficiency reliability wanted improve median latency user experience twitter well bring outlier give uniform experience twitter wanted reduce number machine needed run twitter also wanted isolate failure across infrastructure prevent large outage especially important number machine use go mean chance single machine failing higher failure also inevitable wanted happen much controllable manner wanted cleaner boundary logic one place felt downside running particular monolithic codebase wanted experiment loosely coupled service oriented model goal encourage best practice encapsulation modularity time system level rather class module package level importantly wanted launch feature faster wanted able run small empowered engineering team could make local decision ship userfacing change independent team prototyped building block proof concept rearchitecture everything tried worked everything tried end met goal able settle set principle tool infrastructure gotten u much desirable reliable state today jvm v ruby vmfirst evaluated frontend serving tier across three dimension cpu ram network rubybased machinery pushed limit cpu ram dimension serving many request per machine coming close saturating network bandwidth rail server time effectively single threaded handle one request time rail host running number unicorn process provide hostlevel concurrency duplication translated wasteful resource utilization came rail server capable serving request sec host usage always growing rapidly math would take lot machine keep growth curve time twitter experience deploying fairly large scale jvmbased service search engine written java streaming api infrastructure well flock social graph system written scala enamored level performance jvm gave u going easy get performance reliability efficiency goal ruby vm embarked writing code run jvm instead estimated rewriting codebase could get u performance improvement hardware today push order request sec host level trust jvm lot u come company experience working tuning operating large scale jvm installation confident could pull sea change twitter world jvm decompose architecture figure different service would interact programming modelin ruby system concurrency managed process level single network request queued process handle process completely consumed network request fulfilled adding complexity architecturally taking twitter direction one service compose response service given ruby process singlethreaded would additive extremely sensitive variance backend latency ruby option gave u concurrency however one standard way across different vm option jvm construct primitive supported concurrency would let u build real concurrent programming platform became evident needed single uniform way think concurrency system specifically way think networking know writing concurrent code concurrent networking code hard take many form fact began experience started decompose system service team took slightly different approach example failure semantics client service interact well consistent backpressure mechanism server signal back client experienced client aggressively retrying latent service failure domain informed u importance unified complementary client server library would bundle notion connection pool failover strategy load balancing help u get mindset put together future finagle uniform way thing also baked core library everything system needed could get ground faster rather worry much every system operated could focus application service interface independent systemsthe largest architectural change made move monolithic ruby application one service oriented focused first creating tweet timeline user service move afforded u cleaner abstraction boundary teamlevel ownership independence monolithic world either needed expert understood entire codebase clear owner module class level sadly codebase getting large global expert practice clear owner module class level working codebase becoming harder maintain team constantly spent time going understand certain functionality organize hunting try understand large scale failure occurred end day spend time shipping feature happy theory still service oriented architecture allows u develop system parallel agree networking rpc interface go develop system internals independently also meant logic system selfcontained within needed change something tweet could make change one location tweet service change would flow throughout architecture practice however find team plan change way example change tweet service may require service update tweet representation changed balance though work time system architecture also mirrored way wanted run twitter engineering organization engineering set mostly selfcontained team run independently quickly mean bias toward team spinning running service call back end system huge implication operation however storageeven broke apart monolithic application service huge bottleneck remained storage twitter time storing tweet single master mysql database taken strategy storing data temporally row database single tweet stored tweet order database database filled spun another one reconfigured software start populating next database strategy bought u time still issue ingesting massive tweet spike would serialized single database master experiencing read load concentration small number database machine needed different partitioning strategy tweet storage took gizzard framework create sharded faulttolerant distributed database applied tweet created tbird case gizzard fronting series mysql database every time tweet come system gizzard hash chooses appropriate database course mean lose ability rely mysql unique id generation snowflake born solve problem snowflake allows u create almostguaranteed globally unique identifier rely create new tweet id tradeoff longer identifier identifier rely gizzard store assuming hashing algorithm work tweet close uniformly distributed increase throughput number destination database read also distributed across entire cluster rather pinned database allowing u increase throughput observability traded fragile monolithic application robust encapsulated also complex service oriented application invest tool make managing beast possible given speed creating new service needed make incredibly easy gather data well service default wanted make datadriven decision needed make trivial frictionless get data going spinning service increasingly large system make easier everybody runtime system team created two tool engineering viz zipkin tool exposed integrated finagle service built using finagle get access automatically statstimefuture requestlatencyms dispatch work code block needed service report statistic viz anybody using viz write query generate timeseries graph interesting data like percentile requestlatencyms runtime configuration testingfinally putting together hit two seemingly unrelated snag launch coordinated across series different service place stage service ran could longer rely deployment vehicle get new userfacing code coordination going required across application addition given relative size twitter becoming difficult u run meaningful test fully isolated environment relatively issue testing correctness isolated system needed way test large scale iteration embraced runtime configuration integrated system call decider across service allows u flip single switch multiple system across infrastructure react change nearreal time mean software multiple system go production team ready particular feature need decider also allows u flexibility binary percentage based switching feature available x traffic user deploy code fully safe setting gradually turn confident operating correctly system handle new load alleviates need coordination team level instead runtime todaytwitter performant efficient reliable ever sped site incredibly across percentile distribution number machine involved serving site decreased anywhere last six month twitter flirted four availability twitter engineering set mimic software stack team ready long term ownership expert part twitter infrastructure team interface problem domain every team twitter need worry scaling tweet example team involved running tweet subsystem tweet service team storage team caching team etc scale writes read tweet rest twitter engineering get apis help use two goal drive u work twitter always available user spend time making twitter engaging useful simply better user system engineering team enable u launch new feature faster parallel dedicate different team work improvement simultaneously minimal logjam feature collide service launched deployed independently last week example deploys across twitter service defer putting everything together ready make new build io android keep eye blog twittereng post dive detail topic mentioned thanks go jonathan reichhold jreichhold david helder dhelder arya asemanfar aa marcel molina noradio matt harris themattharris helping contribute blog post
587,Lobsters,scaling,Scaling and architecture,Russ 10 Ingredient Recipe for Making 1 Million TPS on $5KHardware,http://highscalability.com/blog/2012/9/10/russ-10-ingredient-recipe-for-making-1-million-tps-on-5k-har.html,rus ingredient recipe making million tps hardware,russell sullivan tps hardware hardware select right ingredient select right architecture shared nothing architecture inmemory workload data lookup deadsimple dataisolation select right o programming language library modern linux kernel c language epoll tweak taste everything right isolate threadcorepinning irq affinity nic quadcore cpu hexacore greater cpu cpusocketisolation via physicalnicphysicalcpu pairing proof always pudding everything isolated knew right u sy id wa si perfect balance across core achieved optimal performance achieved architectural standpoint still streamline software least flow packet fro aerospike near optimal data served,name russell sullivan author alchemydb highly flexible nosqlsqldocumentstoregraphdbdatastore built top redis spent last several year trying find way sanely house multiple datastoregenres one roof almost paradoxically pushing performance limit recently joined nosql company aerospike formerly citrusleaf goal incrementally grafting alchemydb flexible datamodeling capability onto aerospike highvelocity horizontallyscalable keyvalue datafabric recently completed peakperformance tps optimization project starting tps pushing recent community edition launch tps finally arriving goal tps hardwaregetting one million overthewire clientserver databaserequests persecond single machine costing balance trimming overhead many ax using shared nothing architecture isolate path taken unique request even nt building database server technique described post might interesting database server specific could applied ftp server static web server even dynamic web server personal recipe getting tps per dollar hardware hardware important pretty cheap tps per dollar spent dual socket intel motherboard intel hexacore dram nic port intel quadport nic nic queue select right ingredient architecturesoftwareos ingredient used order get optimal peakperformance rely combination tweaking ingredient hit sweet spot achieve stable databasereadrequests persecond overthewireit difficult quantify importance ingredient general order descending importance select right architecture first imperative start right architecture vertical horizontal scalability essential peakperformance modern hardware flow directly architectural decision shared nothing architecture allows parallelizeisolate without eventually screwed come scaling inmemory workload even think hitting disk request ssds better hdds nothing beat dram dollar type workload data lookup deadsimple ie get packet event loop eventdriven parse action lookup data memory fast enough happen inthread form response packet send packet back via nonblocking call dataisolation previous lookup lockless requires handoff threadtothread sharednothing architecture help determine core machine piece data writtentoservedfrom client map tcpport core lookup go straight data operating system provide multithreading concurrency system select right o programming language library next make sure operating system programming language library one proven perform modern linux kernel anything le centos kernel serious problem w software interrupt also space expect improvement near future linux kernel currently upgraded improve multicore efficiency c language java may fast fast c importantly java le control control path peak performance unknown garbage collection frustrate attempt attain peak performance epoll eventdrivennonblocking io single threaded event loop highspeed code path tweak taste everything right finally use feature system designed tweak hardware o isolate performance critical path threadcorepinning event loop thread reading writing tcp packet pinned core thread allowed core thread critical performance context switching designated core degrade peakperformance significantly irq affinity nic avoid soft interrupt generated tcp packet bottlenecking single core different methodology depending number core quadcore cpu roundrobin spread irq affinity nic queue networkfacingeventloopthreads eg queue map queue core hexacore greater cpu reserve core nothing irqprocessing ie send irq core let thread run core use core networkfacingeventloopthreads similarly running wo competition designated core core receiving irq signal recipient core packet near chance cache transport packet core core near optimal cpusocketisolation via physicalnicphysicalcpu pairing multiple cpu socket holding multiple cpu used like multiple machine avoid intercpu communication dogslow compared communication core cpu die pairing physical nic port physicalcpu simple mean attain goal achieved step use irq affinity physical nic port core designated physicalcpu configure ip routing physical nic port interface packet sent designated cpu back interface instead default interface technique isolates cpunic pair client respect dualcpusocket machine work like singlecpusocket machine much lower tco ingredient fairly straightforward putting together making system really hum turn pretty difficult balancing act practice basic philosophy isolate axis proof always pudding step recipe best illustrated via example client know via multiple hashings datax presently ipy predefined mapping going ipy portz connection client ipy portz previously created request go client ipy portz sends irqs packet get w minimal hardwareos overhead packet creates event trigger dedicated thread run wo competition packet parsed operation look datax local numa memory pool datax retrieved local memory fast enough operation benefit context switching thread reply nonblocking packet go back thru core local sends irqs everything isolated nothing collides eg w software interrupt handled locally cpu irq affinity insures software interrupt bottleneck single core come go fromto designated nic coretocore communication happens within cpu die unnecessary context switch performancecritical code path tcp packet processed event single thread running dedicated core data looked local memory pool isolated path closest software path actually physically happens computer key attaining peak performanceat aerospike knew right watched output top command viewing core near zero idle cpu also uniform balance across core core exactly signature something like u sy id wa si say softwareinterrupts tcp packet using core context switch passing tcppackets back forth operating system taking software taking database transaction perfect balance across core achieved optimal performance achieved architectural standpoint still streamline software least flow packet fro aerospike near optimal data served ingredient got aerospike server one million overthewire database request commodity machine mixed correctly give incredible raw speed give stabilitypredictabilityoverprovisioningforspikes lower speed enjoy related article
588,Lobsters,scaling,Scaling and architecture,Achieving Rapid Response Times in Large Online Services (2012),http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/people/jeff/Berkeley-Latency-Mar2012.pdf,achieving rapid response time large online service,,obj length r filter flatedecode stream zz g endstream endobj obj endobj obj type page parent r resource r content r mediabox annots r endobj obj procset pdf text imageb imagec imagei colorspace r r extgstate r font r r r xobject r endobj obj r r endobj obj length r type xobject subtype image width height interpolate true colorspace r intent perceptual bitspercomponent filter dctdecode stream jfif iccprofile mntrrgb xyz acspappl appl dscm ogxyz l wtpt rxyz bxyz rtrc cprt gtrc btrc mluc enus
589,Lobsters,scaling,Scaling and architecture,Notes on Distributed Systems for Young Bloods,http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/,note distributed system young blood,fallacy distributed computing cap theorem distributed system different fail often writing robust distributed system cost writing robust singlemachine system robust open source distributed system much le common robust singlemachine system coordination hard two general byzantine general hard implement fit problem memory probably trivial slow hardest problem ever debug dapper zipkin implement backpressure throughout system find way partially available metric way get job done use percentile average learn estimate capacity number everyone know feature flag infrastructure rolled choose id space wisely host attack exploit datalocality writing cached data back persistent storage bad computer think crud use cap theorem critique system choose ca extract service bill de hra coda hale jd maturen micaela mcdonald ted nyman update,thinking lesson distributed system engineer learn job great deal instruction scar made mistake made production traffic scar useful reminder sure better engineer full count finger new system engineer find fallacy distributed computing cap theorem part selfeducation abstract piece without direct actionable advice inexperienced engineer need start moving surprising little context new engineer given start list lesson learned distributed system engineer worth told new engineer subtle surprising none controversial list new distributed system engineer guide thinking field taking comprehensive good beginning worst characteristic list focus technical problem little discussion social problem engineer may run since distributed system require machine capital engineer tend work team larger organization social stuff usually hardest part software developer job perhaps especially distributed system development background education experience bias u towards technical solution even social solution would efficient pleasing let try correct people le finicky computer even interface little le standardized alright go distributed system different fail often asked separate distributed system field software engineering new engineer often cite latency believing make distributed computation hard wrong set distributed system engineering apart probability failure worse probability partial failure wellformed mutex unlock fails error assume process unstable crash failure distributed mutex unlock must built lock protocol system engineer worked distributed computation come idea like well send write machine keep retrying write succeeds engineer completely accepted though usually intellectually recognize networked system fail system exist single machine failure tend partial instead total one writes may succeed fails get consistent view data partial failure much harder reason switch go garbage collection pause make leader disappear socket writes seem succeed actually failed machine slow disk drive one machine cause communication protocol whole cluster crawl reading local memory simply stable reading across switch design failure writing robust distributed system cost writing robust singlemachine system creating robust distributed solution requires money singlemachine solution failure occur many machine virtual machine cloud technology make distributed system engineering cheaper cheap able design implement test computer already failure condition difficult replicate single machine whether occur dataset size much larger fit shared machine network condition found datacenters distributed system tend need actual simulated distribution flush bug simulation course useful robust open source distributed system much le common robust singlemachine system cost running many machine long period time burden open source community hobbyist dilettante engine open source software financial resource available explore fix many problem distributed system hobbyist write open source code fun free time machine already much harder find open source developer willing spin maintain pay bunch machine slack taken engineer working corporate entity however priority organization may line priority organization open source community aware problem yet solved hard coordination hard avoid coordinating machine wherever possible often described horizontal scalability real trick horizontal scalability independence able get data machine communication consensus machine kept minimum every time two machine agree something service becomes harder implement information upper limit speed travel networked communication flakier think idea constitutes consensus probably wrong learning two general byzantine general problem useful oh paxos really hard implement grumpy old engineer thinking know better fit problem memory probably trivial distributed system engineer problem local one machine easy figuring process data quickly harder data switch away instead pointer dereferences away distributed system wellworn efficiency trick documented since beginning computer science longer apply plenty literature implementation available algorithm run single machine majority computation done singular uncoordinated machine significantly fewer exist distributed system slow hardest problem ever debug slow might mean one number system involved performing user request slow might mean one part pipeline transformation across many machine slow slow hard part problem statement provide many clue location flaw partial failure one show graph usually look lurking dark corner degradation becomes obvious receive many resource time money tooling solve dapper zipkin built reason implement backpressure throughout system backpressure signaling failure serving system requesting system requesting system handle failure prevent overloading serving system designing backpressure mean bounding resource use time overload time system failure one basic building block creating robust distributed system implementation backpressure usually involve either dropping new message floor shipping error back user incrementing metric case resource becomes limited failure occur timeouts exponential backoffs connection request system also essential without backpressure mechanism place cascading failure unintentional message loss become likely system able handle failure another tends emit failure another system depends find way partially available partial availability able return result even part system failing search ideal case explore search system tradeoff good result long keep user waiting typical search system set time limit long search document time limit expires document searched return whatever result gathered make search easier scale face intermittent slowdown error failure treated able search document system allows partial result returned user resilience increased consider private messaging feature web application point matter enough storage machine private messaging time user notice kind partial failure want system take thought people generally okay private messaging maybe user user message go missing service overloaded one machine failing small fraction userbase preferable missing data larger fraction top choice probably want unrelated feature like public image upload affected private messaging problem much work willing keep failure domain separate able recognize kind tradeoff partial availability good toolbox metric way get job done exposing metric latency percentile increasing counter certain action rate change way cross gap believe system production actually knowing system behavior day different behavior day difference successful engineering failed shamanism course metric necessary understand problem behavior sufficient know next diversion logging log file good tend lie example common logging error class take large proportion space log file actuality occur low proportion request logging success redundant case would blow disk case engineer often guess wrong kind error class useful see log file get filled sort odd bit bob prefer logging someone seen code reading log seen good number outage extended another engineer overemphasizing something odd saw log without first checking metric also seen another engineer sherlockholmes ing entire set failed behavior handful log line note remember success rare b sherlock unless metric experiment back story use percentile average percentile accurate informative average vast majority distributed system using mean assumes metric evaluation follows bell curve practice describes metric engineer care average latency commonly reported metric never seen distributed system whose latency followed bell curve metric follow bell curve average meaningless lead incorrect decision understanding avoid trap talking percentile default percentile better understand user really see system learn estimate capacity learn many second day knowing many machine need perform task difference longlasting system one need replaced month job worse need replaced finish productionizing consider tweet many tweet id fit memory common machine well typical machine end gb memory need overhead gb o another couple least handle request tweet id byte kind back envelope calculation find jeff dean number everyone know slide good expectationsetter feature flag infrastructure rolled feature flag common way product engineer roll new feature system feature flag typically associated frontend ab testing used show new design feature userbase powerful way replacing infrastructure well many project failed went big cutover series big cutovers forced rollback bug found late using feature flag instead gain confidence project mitigate cost failure suppose going single database service hide detail new storage solution using feature flag slowly ramp writes new service parallel writes old database make sure write path correct fast enough write path backfilling service datastore complete use separate feature flag start reading service without using data user response check performance problem another feature flag used perform comparison check read data old system new one one final flag used slowly ramp real read new system breaking deployment multiple step affording quick partial reaction feature flag make easier find bug performance problem occur ramp instead big bang release time issue occurs tamp feature flag setting back lower perhaps zero setting immediately adjusting rate let debug experiment different amount traffic knowing problem hit total disaster feature flag also choose migration strategy like moving request peruser basis provide better insight new system new service still prototyped use flag low setting new system consume fewer resource feature flag sound like terrible mess conditionals classically trained developer new engineer wellintentioned training use feature flag mean accepting multiple version infrastructure data norm rarity deep lesson work well singlemachine system sometimes falter face distributed problem feature flag best understood tradeoff trading local complexity code one system global simplicity resilience choose id space wisely space id choose system shape system id required get piece data option partitioning data fewer id required get piece data easier consume system output consider version twitter api operation get create delete tweet done respect single numeric id tweet tweet id simple number connected piece data number tweet go becomes clear creating user tweet timeline timeline user subscription may efficiently constructed tweet user stored machine public api requires every tweet addressable tweet id partition tweet user lookup service would constructed one know user owns tweet id doable necessary nontrivial cost alternative api could required user id tweet look initially simply used tweet id storage userpartitioned storage came online another alternative would included user id tweet id cost tweet id longer ksortable numeric watch kind information encode id explicitly implicitly client may use structure id deanonymize private data crawl system unexpected way autoincrementing id typical sore point perform host attack exploit datalocality closer processing caching data kept persistent storage efficient processing easier keep caching consistent fast network failure latency pointer dereferences fread course datalocality mean nearby space also mean nearby time multiple user making expensive request nearly time perhaps request joined one multiple instance request kind data made near one another could joined one larger request often affords lower communication overheard easier fault management writing cached data back persistent storage bad happens system think especially one originally designed people le experienced distributed system many system inherit flaw implementers talk russiandoll caching large chance hitting highly visible bug entry could left list special hate heart common presentation flaw user information eg screennames email hashed password mysteriously reverting previous value computer think field today plenty misinformation machine capable practitioner great deal experience end light web server processor gb memory disk space use relatively complex crud application modern language runtime single machine trivially capable thousand request per second within hundred millisecond deep lower bound term operational ability hundred request per second per machine something brag case greater performance hard come especially willing profile application introduce efficiency based measurement use cap theorem critique system cap theorem something build system theorem take first principle derive working system much general purview space possible solution broad however wellsuited critiquing distributed system design understanding tradeoff need made taking system design iterating constraint cap put subsystem leave better design end homework apply cap theorem constraint real world implementation russiandoll caching one last note c p choose ca extract service service mean distributed system incorporates higherlevel logic storage system typically requestresponse style api lookout code change would easier code existed separate service instead system extracted service provides benefit encapsulation typically associated creating library however extracting service improves creating library allowing change deployed faster easier upgrading library client system course extracted service hard deploy client system one become easier deploy ease owed fewer code operational dependency smaller extracted service strict boundary creates make harder take shortcut library allows shortcut almost always make harder migrate internals client system new version coordination cost using service also much lower shared library multiple client system upgrading library even api change needed requires coordinating deploys client system get harder data corruption possible deploys performed order harder predict happen upgrading library also higher social coordination cost deploying service client system different maintainer getting others aware willing upgrade surprisingly difficult priority may align canonical service use case hide storage layer undergoing change extracted service api convenient reduced surface area compared storage layer front extracting service client system know complexity slow migration new storage system format new service evaluated bug certainly found new storage layout great deal operational social issue consider justice another article written much love reviewer bill de hra coda hale jd maturen micaela mcdonald ted nyman insight care invaluable update added permalinks section cleaned text section coordination datalocality feature flag backpressure
591,Lobsters,scaling,Scaling and architecture,Bloom: Big Systems from Small Programs,https://www.youtube.com/watch?v=HqErn9acbto,bloom big system small program,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature
592,Lobsters,scaling,Scaling and architecture,"Designs, Lessons and Advice from Building Large Distributed Systems",http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf,design lesson advice building large distributed system,,obj length r filter flatedecode stream rh bzpf v h endstream endobj obj endobj obj type page parent r resource r content r mediabox annots r endobj obj procset pdf text imageb imagec imagei colorspace r r font r r xobject r endobj obj r r r r endobj obj length r type xobject subtype image width height colorspace r interpolate true bitspercomponent filter flatedecode stream n k yj e qx e  lr p l
593,Lobsters,scaling,Scaling and architecture,Ansible and Salt: A detailed comparison,http://missingm.co/2013/06/ansible-and-salt-a-detailed-comparison/,ansible salt detailed comparison,ansible salt automate various system task chef puppet capistrano fabric func yaml serialization format simple sinatra test application mazerrackham saltrack speed fireball mode security flaw several remote vulnerability keyczar paramiko pretty bad advice update ssh host key checked system impact maintenance security release execution order dependency chain ordering salt state salt documentation requisite lot saltrack clarity recommended directory layout notify action handler watch modulewait conclusion ansible playbook salt state comment powered disqus comment powered,short version ansible salt awesome seriously lucky enough work company let use either one going great time said key difference way attempt solve problem faced modern sysadmins fun would one comparison tell preference end first backstory heard ansible salt framework let automate various system task biggest advantage relative solution like chef puppet capable handling initial setup provisioning server also application deployment command execution mean need augment tool like capistrano fabric func ansible salt capable taking blank server fullyfunctional application maintain code update application time quickly run arbitrary command adhoc basis across hundred thousand different machine oh built around concept using yaml serialization format represent configuration execute command make far pleasant work competition concise syntax allows use resulting configuration form documentation nonprogrammers easily understand experiment decided write collection ansible role salt state perform set task configure brand new ubuntu lts server following also wanted ansible salt handle deploying simple sinatra test application meant also needed create user application file would belong reconfigure openssh allow access via ssh key add ssh public key deploy authorizedkeys file set enable nginx vhost create necessary application directory use git checkout latest revision codebase create required symlinks use bundler install gem dependency restart nginx completely ready go mission accomplished open source ansible playbook achieves open source collection salt state mazerrackham sample ansible playbook rack application saltrack sample salt state rack application make comparison easy best match comment salt state corresponding ansible name talk difference speed salt fast incredibly slick transport layer satisfying send command get instantaneous feedback salt master connected several minion box salt much faster ansible ansible relies ssh transport layer default however ansible also support call fireball mode us ssh bootstrap ephemeral daemon testing see difference whatsoever ansible salt using though initial ansible fireball bootstrap process still take bit happens ssh workflow routinely need send simultaneous command hundred upon hundred machine afford wait ansible set fireball mode ssh salt better fit realistically ansible salt probably fine need actively used supercomputing cluster thousand node default ssh transport also plenty fast easily get job done across hundred server long comfortable rolling update security natively support encryption salt includes aes implementation us protect payload recently flaw discovered code along several remote vulnerability ansible largely immune issue default configuration us standard ssh require daemon running remote server aside openssh think single package entire world trust regarding fireball mode aes encryption done using keyczar instead homegrown solution enabled default connection ephemeral drastically reduces attack surface exactly perfect come security ansible us paramiko ssh connection default paramiko fine library great track record ansible ship overlypermissive configuration warn user key change prompt confirmation first time key seen additionally documentation contains pretty bad advice turn stricthostkeychecking order make remote mercurial git checkout little seamless fortunately issue easy work around using binary ssh option check host key seeding server proper knownhosts file still far likely exploited due fullblown remote vulnerability exploited due mitm attack compromise salt master server equivalent gaining full root access minion connect despite lax policy toward host key verification ansible remains clear winner update july ansible released ssh host key checked secure ever system impact salt brings lot dependency dependency must installed every machine used regardless whether system master minion likely want enable salt stack apt repository make installing salt keeping uptodate simple possible master minion running persistent daemon enable salt perform magic dependency comparatively minimal need installed system running ansible ansibleplaybook command remote dependency python interpreter come almost every linux distribution default ansible leave trace existence remote system finish running playbook daemonless approach allows easily run local git checkout people might find distinction important difference largely academic looking term system impact persistent daemon feature going sending lot command experience salt daemon lightweight wellbehaved running distinction daemon v daemonless important reason however maintenance ansible dramatically easier maintain le month half since first started using salt time five release fair would consider one upgrade mandatory aforementioned security release true easily use salt run upgrade across minion upgrading master simple aptget command away fact remains daemon extra layer moving part simply deal ansible lack daemon also make ansible easier use existing server plus want use latest version upgrade one location done ansible changelog also regularly refreshed even running development branch lot people still easy keep track going comparison think would wonderful salt project better job publishing release note minor update short digging git log way determine happened salt presumably contain bug fix sysadmin like know getting perform upgrade something like salt running root easily become core part infrastructure execution order dependency chain salt ansible take wildly different approach controlling execution order ordering salt state decidedly complicated affair necessitates defining task term requirement several option working requisite use require statement requirein statement mixture two salt documentation requisite describes difference like requisitein statement opposite requisite statement instead saying depend requisiteins say depends well depend confusing given option found intuitive think thing term individual state required therefore eschewed usage requisitein statement use lot requisite statement order get salt execute thing correct order saltrack state contain requisite statement excludes initial line preceded state also contain include statement necessary order state file require state located elsewhere perhaps aggravating example requisite issue upgrading passenger making simple change accommodate ran statehighstate couple new minion failed first time would succeed ran second time right away knew dealing execution order issue likely something task rubyfalcon state file executing turn passenger made subtle change install script caused shell run rake task shell run ruby command two separate point installation process discovered digging passenger source around output indicating install process exiting fix simply add final two line state run nginx install script nginxinstall cmdscript name salt nginxpassengerinstallsh unless optnginxsbinnginx v grep template jinja require file nginxsource cmd passenger file usrlocalbinrake file usrlocalbinruby unfortunately way tell salt execute entire state file completely moving onto another problem first occurred briefly tempted make nginxinstall state require every single state rubyfalcon state file exactly feel professional experience indication spend quite bit time salt frustrated often feel like opaque lottery randomly determines execution order ansible executes playbook sequentially require statement task listed first run first role listed second run second simple satisfying work clarity general find syntax readable come loop also really like ability ansible give describe behavior task using name parameter prefer id declaration always felt need pair additional comment example set log rotation nginx passenger using salt set log rotation nginx passenger rotatetarget nginx passenger etclogrotated rotatetarget filemanaged source salt nginxpassenger rotatetarget logrotate require cmd nginxinstall cmd passenger endfor thing ansible name set log rotation nginx passenger copy src item logrotate destetclogrotated item withitems nginx passenger recommended directory layout ansible also feel organized refer file salt must use absolute path see example salt version explicitly reference location people might justifiably prefer decision le magic happening behind scene think extremely powerful able move ansible role different directory still function properly also find concept notify action handler far elegant watch modulewait method mean look lovely git checkout ansible name check latest revision codebase notify proper handler update git repohttps githubcomjlundimgurdisplaygit dest imgurdisplaylocation current sudo yes sudouser deploy notify symlink log directory shared location install bundle restart imgurdisplay application know exactly handler going notified codebase change read plain english handler given descriptive name conclusion made far probably surprise hear prefer ansible time really meant said beginning though salt ansible pretty great amount embarrassment rich even able writeup like fortunate live world exist aside ansible would still rather use salt anything else plan trying stay current thing moving fast excited see project year encourage check ansible playbook salt state see prefer really go wrong either way please enable javascript view comment powered disqus comment powered
594,Lobsters,scaling,Scaling and architecture,Netflix CTO on Building Resilient Systems,http://www.infoq.com/interviews/Building-Resilient-Systems-Michael-Nygard,netflix cto building resilient system,developer one think kind consideration one problem seen respect thing like low testing downing issue qa thing happen software development process need certain amount hardware available order able would dedicate say server long running testing server qa also tends prohibitive costwise think rise cloud computing platform service ability instantiate bunch machine cloud think help increase adoption scale process pattern developer architect follow make system resilient approach making existing legacy system resilient monitorable resiliency respect existing legacy system got tip recommendation type information referring point monitoring type information verified different mechanism like logging monitoring jmx type thing thinking observation quite often problem happened realize wish bit monitoring logging time happened seems operation application often considered secondary concern development application think mentioned ongoing operation cost system many system seen past operation cost opaque certain cause instance certain number server certain amount load nt really understanding visibility system allows say tweak thing could go server recommendation look system determine see think might something existing goldfishlike system correctly pointed need ops people development people integrated order deal problem nt also need start thinking business people workflow kind assumption make way thing even way unspoken assumption make need relational database opposed formally flat file distributed across network kind assumption importantly business workflow impact performance scaling everything else bring cycle well,developer one think kind consideration tested mechanism load testing run load test hour time get consider longevity test whereas typical qa system run hour get shut reloaded yet another build software like see qa running several day time continuous load give idea whether software exhibit memory leak resource leak sort kind issue though ca nt tested engineered example talk scaling effect lot development qa every system look like pair server web tier pair server app tier pair database server maybe one database server dev system talking system b qa talking scale production may server system calling server system b common kind scaling effect also see scaling effect tier dev qa production may thing work ok ratio nt work well ratio one problem seen respect thing like low testing downing issue qa thing happen software development process need certain amount hardware available order able would dedicate say server long running testing server qa also tends prohibitive costwise think rise cloud computing platform service ability instantiate bunch machine cloud think help increase adoption scale process yes absolutely want go brief tangent first talk continuous performance testing realm functional testing used model functional testing extremely expensive overhead involved whole bunch people reading script clicking around application nt often tried defer release meant accumulating lot technical risk development large number line code unknown quality end one thing unit testing started adopting practice reduced cost additional test point could continuously throughout development cycle possible also load testing open source tool like jmeter work pretty well suited really high volume load test work pretty well testing go checking see introduce performance regression possible continuous basis wo nt prove ready production scale load tell introduced something broke performance one aspect aspect absolutely right cloud computing help arena way one spin new environment software cloud relatively cheaply quickly pay need tear side benefit getting good deployment automated deployment pretty important operation right organization security concern data privacy concern anyone dealing u regulatory system like hipaa industry standard like pci going hard time another alternative spin load farm cloud test software happens reside probably data center another option bring lower cost going large scale vendor pattern developer architect follow make system resilient book release catalog call stability pattern help around resiliency catalog capacity pattern thing help reach higher volume current set resource stability pattern talk way preserve functionality feature user even event partial system failure one thing try get people adopt failure oriented mindset call want everyone understand point every piece system every external system call going break need able survive might mean certain feature ca nt provide user breakage occurring preserve many feature possible pattern call circuit breaker bulkhead key pattern circuit breaker essentially cut external integration point malfunctioning preserve request handling thread calling system often make call external integration point broken tie thread blocking synchronous call indefinite period time get enough thread tied waiting external integration point well mean allowed fault external system propagate failure system never allow circuit breaker allows cut integration point malfunctioning bulkhead another approach preserving functionality many user possible bulkhead mean partition system separate set resource unlikely fail together trying segregate perhaps processor processor pool process go wrong start eating cpu eat cpu instead box come pretty large number core day one kind bulkhead another kind say within application going create multiple thread pool serf different purpose thread pool handle jms queue get overloaded message flood still process front end transaction different thread pool still get admin interface admin thread separate pool another example bulkhead approach making existing legacy system resilient monitorable interesting question embedded tackle monitoring first may need reminder go back resilience monitoring begin log file know sound primitive sound like debug printf log file actually still lot advantage plain text move around lot different place ship vendor consultant need postmortem analysis persist condition causing fault gone nt wait happen try catch act log file still pretty important mechanism also advocate exposing information management interface example anyone java community jmx available jmx let open internals application make transparent status variable counter threshold timer kind thing interesting mentioned circuit breaker little bit earlier open closed state circuit breaker pretty interesting quick indicator overall health system exposing state circuit breaker jmx make immediately monitorable last mile monitoring getting information application operation system jmx connector lot different monitoring system really advocate creating transparency application leaving last mile operation group nt advocate developer directly hooking monitoring system couple reason one thing change different rate separated concern rule monitoring rule threshold change rapidly rule react particular monitoring event change rapidly change control operation group really nt want code deployment cycle accommodate second operation group sometimes buy new product make sweeping change replace netcool openview openview ca something like nt need code change particularly nt need go sweeping code looking call one api replacing call another api monitoring aspect go back address resilience remind question resiliency respect existing legacy system creating resilience legacy system like anything else legacy system challenge matter first aspect identify weak point would always start resource pool external integration point kind thread pool kind database connection pool place know going blocking application likely failure point place calling external system particularly synchronous call going place fault introduced system failure point look got tip recommendation type information referring point monitoring type information verified different mechanism like logging monitoring jmx type thing thinking observation quite often problem happened realize wish bit monitoring logging time happened excellent point absolutely true always want smidgen information actually got particular event first foremost going start resilience think capacity term resilience always want know blocking often something like database connection pool keep track high watermark many connection checked keep track many time thread blocked keep track longest blocking time good statistic collection mechanism built app might even keep stats every time thread begin blocking end blocking record get idea distribution blocking time well instant see thread blocked database connection pool immediately know problem least approximate cause trace backwards figure leaked connection anywhere pool definitely track blocking often high water low water stats number time thing checked kind health indicator place got cache keep track many item cache hit rate eviction rate place got circuit breaker keep track many time circuit breaker flipping open closed state closed open current state course threshold configured useful thing expose monitoring management interface also useful expose control thing instance circuit breaker control reset pool control change high water low water mark think several case ongoing partial failure mode needed go change maximum number connection connection pool dial front end system would stop crushing back end system useful kind control runtime seems operation application often considered secondary concern development application think mostly ask developer talk cio manager happily tell budget go operation development often focus development time pressure soon requirement identified represents current need delay identification current need satisfying need painful people need since people need usually one writing cheque mean time pressure development always severe difficult conversation say need week development handle production readiness issue particularly say thing like going implement feature want need add data purging application layer difficult discussion need look often total cost ownership term get thrown around lot usually people something sell truth often skip thing development create ongoing operational cost need actually balance say extra week development time direct cost development indirect cost delaying revenue need look much cost need compare total ongoing operation cost expected life time system make decision differently case make decision way case mean sometimes choose incur ongoing operational cost sometimes choose spend additional development time avoid ongoing operation cost one example use talk capacity handling say web page request million hit per day million hit per day large day one take extra millisecond first going impact revenue company like google amazon identified clearly secondly extra millisecond million hit per day hour additional computing time mean roughly need additional server handle load additional server draw power every month require administration every month may may require software licensing every month probably support contract get enough administrator need manager administrator keep organization check really millisecond per page seems pretty small development translates pretty substantial ongoing operation cost mentioned ongoing operation cost system many system seen past operation cost opaque certain cause instance certain number server certain amount load nt really understanding visibility system allows say tweak thing could go server recommendation look system determine see think might something existing goldfishlike system think begin thinning barrier tearing wall development operation many company particularly large company development operation completely different management chain different imperative operating may even history conflict antagonism uncommon need start getting visibility work direction need developer get visibility system behave like production need operation get visibility earlier development process influence architecture design system often development happens vacuum developer nt know sort machine going running maybe even know size capacity machine nt know file going live access file local disk versus san versus direct attached storage kind created layer abstraction development world needed create get leverage platform environment also isolated u reality run talked somebody programming year ago probably know many cycle instruction writing going take execute many cycle take invoke method call c vm even know could even find need abstraction deal complexity environment need visibility know causing nt visibility never closed feedback loop without closed feedback loop person creating pain never going stop creating pain think really begin one interesting aspect people cloud computing community starting talk private cloud quite lot private cloud could envision department running cloud developer subscribing consumer service naturally creates closed loop feedback system developer going incur charge business unit based amount resource consume business unit see charge instead seeing charge often case organization developer create application maybe nt perform well consume resource need never pay pay need ask giant budget cio get beaten cfo budget large feedback wrong mechanism correctly pointed need ops people development people integrated order deal problem nt also need start thinking business people workflow kind assumption make way thing even way unspoken assumption make need relational database opposed formally flat file distributed across network kind assumption importantly business workflow impact performance scaling everything else bring cycle well excellent point agree completely think agile development started better job bringing business closer development gaining understanding telling need actually need let conversation instead throwing spreadsheet back forth think need continue integration operation definitely agree starting see inkling coming social network extremely high scale startup space talking thing like architecting take advantage latency instead victimized ask business requirement would normally assume instant piece content get approved price change occurs something like next millisecond web page reflect push road towards complete transactionality checking database time building elaborate cash flush mechanism done seen done many time turn price change system event may taken millisecond execute probably result decision process involved human phone call meeting may taken hour day week length time really necessary web page reflect change one millisecond later probably conversation business user long allowable let something propagate system fast need show people understanding smaller answer larger cost going balloon need conversation think beginning healthy trend encourage
595,Lobsters,scaling,Scaling and architecture,Application Resilience in a Service-Oriented Architecture,http://programming.oreilly.com/2013/06/application-resilience-in-a-service-oriented-architecture.html,application resilience serviceoriented architecture,hystrix bulkheading thread pool semaphore fallback handled pattern operation insight metric stream dashboard latency monkey canary coal mine execution hook open source software velocity conference,failure isolation operation hystrix webscale application netflix serve million customer using thousand server across multiple data center unmitigated system failure impact user experience product image company brand potentially revenue serviceoriented architecture complex completely understand control must treated accordingly relationship node constantly changing actor within system independently evolve failure form error latency emerge relationship resilient system easily drift state vulnerability infrastructure alone relied upon achieve resilience application instance component complex system must isolate failure constantly audit change netflix spent lot time energy engineering resilience system among tool built hystrix specifically focus failure isolation graceful degradation evolved series production incident involving saturated connection andor thread pool cascading failure misconfigurations pool queue timeouts minor mistake led major user impact open source library follows principle protecting system novel failure inevitably occur isolate client network interaction using bulkhead circuit breaker pattern fallback degrade gracefully possible fail fast fallback available rapidly recover monitor alert push configuration change low latency second restricting concurrent access given backend service proven effective form bulkheading limit resource utilization concurrent request limit smaller total resource available application instance using two technique thread pool semaphore provide essential quality restricting concurrent access thread provide added benefit timeouts caller walk away underlying work latent isolating functionality rather transport layer valuable extends bulkhead beyond network failure latency also caused client code example include request validation logic conditional routing different multiple backends request serialization response deserialization response validation decoration network response latent corrupted incompatibly changed time turn result unexpected failure application logic mixed environment also several type client many different type backends different configuration client expose easily auditing modification production environment unfortunately also generally true default configuration optimal despite best effort leak system particularly via transitive dependency take one misconfigured client expose vulnerability result system outage bulkheading around layer network client client reliable protection changing behavior misconfigurations transitive dependency performing unexpected network activity response handling failure overall latency regardless come applying bulkhead functional level also enables addition business logic fallback behavior allow graceful degradation failure occurs failure may come via network client code exception timeouts shortcircuiting concurrent request throttling however handled failure handler provide fallback response functionality may able gracefully degrade consequently fail fast shed load recovery many others return stale data use secondary system use default pattern operation insight going equally important actual isolation technique key aspect low latency metric low latency configuration change common insight service relationship regardless network transport implemented use lowlatency second metric stream aggregate metric application instance cluster alerting dashboard shown video clip annotated image provide visualization traffic performance health bulkhead system near realtime metric improved meantimetodetection mttd meantimetorecovery mttr increased operational effectiveness deployment dealing production incident configuration change visually roll across cluster server second impact seen immediately video auditing production essential maintaining resilient system use latency monkey production inject latency system relationship serviceoriented architecture latency far damaging distributed system difficult address fast failure machine code running latency simulation hystrix realtime monitoring allows u rapidly see impact determine safe need end test time simulation light hystrix bulkhead dashboard show job isolate latency sometimes reveal regression quickly discover end test pursue resolution validated next test run another form auditing applying tracking network traffic leaving jvm finding isolated behind bulkhead use like proverbial canary coal mine permanently run take small percentage production traffic find network traffic spring without isolation occur initial canary deployment new code may occur unexpected code path enabled via transitive dependency ab test turned production configuration changed pushed fleet existing server graceful degradation purely serverside consideration device ui team play equally important role making user experience robust capable degrading gracefully example server use bulkheading isolate failure choose fail silently portion request considered optional uis must behave correctly may cause client fail try render data present response fault injection via hystrix execution hook enables device team target specific uis device account order test failure latency fallback scenario determining whether client code responds engineering resilience application critical achieving fault latency tolerance operational consideration support client application equally important principle applied many different way approach differ language technology stack personal preference hopefully experience perhaps even open source software inspire improved resilience system one series post related upcoming velocity conference santa clara ca june highlighting speaker variety way video email interview post speaker
596,Lobsters,scaling,Scaling and architecture,The network is reliable,http://aphyr.com/posts/288-the-network-is-reliable,network reliable,jepsen peter bailis reliably detect handle subscribe fallacy distributed computing neatly summarizes radically affect rumbling large deployment microsoft datacenter study studied behavior hp enterprise managed network examined google chubby paper google design lesson distributed system design lesson advice building large scale distributed system amazon dynamo dynamo paper yahoo pnutssherpa yahoo pnutssherpa noted applicationlevel failure cpu use service contention discovered long gc pause io searchbox io dozen production user mysql overload pacemaker segfault routine database migration caused unexpectedly high load mysql primary nics driver friend marc donges michael chan followed flow control code elasticsearch split brain report extremely high latency load jumbo frame intel packet death inbound sip packet particular structure would disable nic glusterfs partition caused driver bug citycloud noticed unexpected network failure datacenter network failure power failure redundant switch power distribution unit failed switch splitbrain caused bpdu flood planned network reconfiguration improve reliability bridge loop misconfiguration broken mac cache installed set aggregation switch mystery rabbitmq partition rabbitmq failure drbd splitbrain one user reported netware splitbrain usenet post novellsupportclusterservices mlag spanning tree stonith december hosting provider undetected glusterfs splitbrain alerted freistil anonymous hosting provider pacemakerheartbeat splitbrain detail longrunning partition heartbeat pair cloud network isolated mongodb primary call maybe mongodb mnesia splitbrain mnesia cluster instability causing mongodb elasticsearch unavailability report total partition frontend backend stack voltdb splitbrain regular network failure causing replica divergence elasticsearch discovery failure another splitbrain rabbitmq elasticsearch window azure scattered report window azure partition account elasticsearch splitbrain aws eb outage went hour heroku reported isolated redis primary twilio billing system store account credit redis failed appointment reminder wan link pagerduty aws peering point northern california degraded cenic study quantitatively analyzed global routing failure cloudflare response ddos attack one customer juniper routing bug caused outage global bgp outage briefly rendered unreachable testing go http githubcomaphyrpartitionspost,discussing jepsen partition tolerance peter bailis past week honored present post collaboration two u also like extend sincere appreciation everyone contributed research experience piece network partition contentious subject claim modern network reliable concerned designing theoretical failure mode often accept singlenode failure common argue reliably detect handle conversely others subscribe peter deutsch fallacy distributed computing disagree attest partition occur system james hamilton amazon web service neatly summarizes network partition rare net gear continues cause issue answer debate radically affect design distributed database queue application right key challenge dispute lack evidence normalized base comparing network application even le data track link availability estimate packet loss understanding endtoend effect application difficult scant evidence difficult generalize often deploymentspecific closely tied particular vendor topology application design worse even organization clear picture network behavior rarely share specific finally distributed system designed resist failure mean noticeable outage often depend complex interaction failure mode many application silently degrade network fails resulting problem may understood understood result much know failure mode realwold distributed system founded guesswork rumor sysadmins developer swap story beer detailed public postmortem comprehensive survey network availability far post like bring story together believe first step towards open honest discussion realworld partition behavior ultimately robust distributed system design rumbling large deployment start let consider evidence big player distributed system company running globally distributed infrastructure hundred thousand node data collected report best summarize operation large distilling experience operating likely biggest distributed system ever deployed publication unlike many case study examine later often capture aggregate system behavior largescale statistical trend indicate often obliquely partition significant concern deployment microsoft datacenter study team university toronto microsoft research studied behavior network failure several microsoft datacenters found average failure rate device per day link per day median time repair approximately five minute one week researcher note correlating link failure communication partition challenging estimate median packet loss packet per failure perhaps concerning finding network redundancy improves median traffic network redundancy eliminate many common cause network failure hp enterprise managed network joint study researcher university california san diego hp lab examined cause severity network failure hp managed network analyzing support ticket data connectivity related ticket accounted support ticket highest priority level median incident duration hour minute highest priority ticket median duration hour minute priority google chubby google paper describing design operation chubby distributed lock manager outline root cause outage day operation across several cluster nine outage lasted greater second four caused network maintenance two caused suspected network connectivity problem google design lesson distributed system design lesson advice building large scale distributed system jeff dean suggests typical first year new google cluster involves rack going wonky machine seeing packet loss network maintenance might cause random connectivity loss router failure immediately pull traffic hour google tell u much applicationlevel consequence network partition lesson distributed system suggests significant concern citing challenge e asytouse abstraction resolving conflicting update multiple version piece state useful reconciling replicated state different data center repairing network partition amazon dynamo amazon dynamo paper frequently cite incidence partition driving design consideration specifically author note rejected design traditional replicated relational database system capable handling network partition yahoo pnutssherpa yahoo pnutssherpa designed distributed database operating multiple geographically distinct site originally pnuts supported strongly consistent timeline consistency operation one master per data item however developer noted event network partitioning server failure design decision restrictive many application first deployment sherpa supported timelineconsistency model namely replica record apply update order apilevel feature enable application cope asynchronous replication strict adherence lead difficult situation network partitioning server failure partially addressed override procedure local data replication many circumstance application need relaxed approach applicationlevel failure partition originate physical network sometimes dropped delayed message consequence crash race condition o scheduler latency overloaded process following study highlight fact system delay drop occur layer software stack cpu use service contention bonsaiio discovered high cpu memory use elasticsearch node combined difficulty connecting various cluster component likely consequence excessively high number expensive request allowed cluster restarted cluster restarting cluster partitioned two independent component subsequent cluster restart resolved partition customer complained unable delete create index log revealed server repeatedly trying recover unassigned index poisoned cluster attempt service normal traffic change cluster state failure led minute unavailability six hour degraded service bonsai concludes noting largescale elasticsearch cluster use dedicated node handle routing leader election without serving normal request data prevent partition heavy load also emphasize importance request throttling setting proper quorum value long gc pause io stoptheworld garbage collection blocking disk io cause runtime latency order second minute searchbox io dozen production user found gc pressure elasticsearch cluster cause secondary node declare primary dead attempt new election configuration used low value zenminimummasternodes elasticsearch able elect two simultaneous primary leading inconsistency downtime even minimummasternodes larger majority elasticsearch prevent node taking part multiple network component gc pause high iowait time due io cause split brain write loss index corruption mysql overload pacemaker segfault github relies heavily pacemaker heartbeat program coordinate cluster resource node use percona replication manager resource agent pacemaker replicate mysql database three node september routine database migration caused unexpectedly high load mysql primary percona replication manager unable perform health check busy mysql instance decided primary promoted secondary secondary cold cache performed poorly normal query load node caused slow percona failed back original primary operation team put pacemaker maintenancemode temporarily halting automatic failover site appeared recover next morning operation team discovered standby mysql node longer replicating change primary operation decided disable pacemaker maintenance mode allow replication manager fix problem upon attempting disable maintenancemode pacemaker segfault occurred resulted cluster state partition update two node call b rejected message third node c third node rejected message two despite configured cluster require majority machine agree state cluster taking action two simultaneous master election decision attempted without proper coordination first cluster master election interrupted message second cluster mysql stopped second singlenode cluster node c elected subsequent message twonode cluster discarded luck would c node node operation team previously determined date detected fact powered outofdate node end partition prevent data drift taking production database access thus access githubcom partition caused inconsistency mysql secondary primary mysql data store like redis foreign key relationship consistent github showed private repository wrong user dashboard incorrectly routed newly created repos github thought carefully infrastructure design still surprised complex interaction partial failure software bug note postmortem member operation team asked failover performed answer would resounding distributed system hard nics driver friend unreliable nic hardware driver implicated broad array partition marc donges michael chan bring u thrilling report popular broadcom chipset abruptly dropping inbound outbound packet machine nic dropped inbound packet node unable service request however could still send heartbeat hot spare via keepalived spare considered primary alive refused take service unavailable five hour recover without reboot sven ulland followed reporting symptom chipset linux despite pulling commits mainline supposedly fixed similar set issue driver unable resolve issue version since dell shipped large number server impact firmware bug widely observed instance chip bug flow control code causing spew pause frame chipset crashed buffer filled problem magnified switchonachip device component number dell topofrack switch default spewed pause frame every interface trying communicate offending nic led cascading failure entire switch network driver could also cause transient flapping network failure described elasticsearch split brain report meanwhile broadcom notorious causing extremely high latency load jumbo frame particularly thorny issue esx user iscsibacked storage intel packet death motherboard manufacturer failed flash eeprom correctly intel based system result hard diagnose error inbound sip packet particular structure would disable nic cold restart would bring system back normal glusterfs partition caused driver bug scheduled upgrade citycloud noticed unexpected network failure two distinct glusterfs pair followed third suspecting link aggregation citycloud disabled feature switch allowed selfhealing operation proceed roughly hour later network failure returned one node citycloud identified cause driver issue updated downed node returning service however outage resulted data inconsistency glusterfs pair server lost storage abruptly certain type gluster issue file match two node storage pair also case data corruption vms filesystems due vms going uncontrolled way datacenter network failure individual network interface fail typically appear singlenode outage failure located physical network often nefarious switch subject power failure misconfiguration firmware bug topology change cable damage malicious traffic failure mode accordingly diverse power failure redundant switch microsoft sigcomm paper suggests redundancy always prevent link failure power distribution unit failed took one two redundant topofrack switch fog creek lost service subset customer rack remained consistent available user however switch rack also lost power undetermined reason failure isolated two neighboring rack one another taking demand service switch splitbrain caused bpdu flood planned network reconfiguration improve reliability fog creek suddenly lost access network network loop formed several switch gateway controlling access switch management network isolated generating splitbrain scenario neither accessible due sudden traffic flood flood result multiswitch bpdu bridge protocol data unit flood indicating spanningtree flap likely changing loop domain according bpdu standard flood happened deviation system assumption resulted two hour total service unavailability bridge loop misconfiguration broken mac cache effort address high latency caused daisychained network topology github installed set aggregation switch datacenter despite redundant network installation process resulted bridge loop switch disabled link prevent failure problem quickly resolved later investigation revealed many interface still pegged capacity investigating problem misconfigured switch triggered aberrant automatic fault detection behavior one link disabled fault detector disabled link caused minute hard downtime problem later traced firmware bug preventing switch updating mac address cache correctly forced broadcast packet every interface mystery rabbitmq partition sometimes nobody know system partition rabbitmq failure seems like one case retransmits large gap message clear loss connectivity node upping partition detection timeout minute reduced frequency partition prevent altogether drbd splitbrain twonode cluster partition case node reliably declare primary happens drbd filesystem one user reported node remain online accept writes leading divergent filesystemlevel change realistic option resolving kind conflict discard writes made selected component cluster netware splitbrain shortlived failure lead long outage usenet post novellsupportclusterservices admin report twonode failover cluster running novell netware experienced transient network outage secondary node eventually killed primary though still running longer reachable host network post go detail series network partition event correlated backup job mlag spanning tree stonith github writes great postmortem one exception december planned software update aggregation switch caused mild instability maintenance window order collect diagnostic information instability network vendor killed particular software agent running one aggregation switch github aggregation switch clustered pair using feature called mlag present two physical switch single layer device mlag failure detection protocol relies ethernet link state logical heartbeat message exchanged node switch agent killed unable shut ethernet link unlucky timing confused mlag takeover preventing stillhealthy agg switch handling link aggregation spanningtree protocol normal forced spanningtree leader election reconvergence link blocking traffic access switch second network partition caused fileservers using pacemaker drbd ha failover declare dead issue stonith shoot node head message one another network partition delayed delivery message causing fileserver pair believe active network recovered node shot time node dead file belonging pair unavailable prevent filesystem corruption drbd requires administrator ensure original primary node still primary node resuming replication pair node primary ops team examine log file bring node online isolation determine state recovering downed fileserver pair took five hour github service significantly degraded hosting provider running datacenter cheaper reliable using public cloud infrastructure also mean network server administrator hosting provider rent dedicated virtualized hardware user often take care network hardware setup undetected glusterfs splitbrain freistil host server colocationmanagedhosting provider monitoring system alerted freistil packet loss localized specific datacenter network failure caused router firmware bug returned next day elevated packet loss caused glusterfs distributed filesystem enter splitbrain undetected unfortunately malfunctioning network caused additional problem became aware afternoon customer called support hotline website failed deliver certain image file found caused splitbrain situation storage cluster change made node reflected selfheal algorithm built gluster filesystem able resolve inconsistency two data set repairing inconsistency led brief overload web node short surge network traffic anonymous hosting provider gather informally major managed hosting provider experience regular network failure one company running node major hosting provider reported period provider network went five distinct period partition partition disabled connectivity provider cloud network public internet others separated cloud network provider internal managedhosting network failure caused unavailability company running significant distributed system across partitioned network observed inconsistency data loss pacemakerheartbeat splitbrain post linuxha detail longrunning partition heartbeat pair two linode vms declared dead claimed shared ip successive post suggest network problem email failed dispatch due dns resolution failure node reported network unreachable case impact appears part partitioned application proxy cloud network largescale virtualized environment notorious transient latency dropped packet fullblown network partition often affecting particular software version availability zone sometimes failure occur specific subsection provider datacenter revealing plane cleavage underlying hardware topology isolated mongodb primary comment call maybe mongodb scott bessler observed exactly failure mode kyle demonstrated jepsen post prescient wsafe scenario show including extra fails rollbackreelection happened u today west region network issue caused network partition separated primary secondary node replset hour later old primary rejoined rolled back everything new primary bad using wmajority partition caused two hour write loss conversation largescale mongodb user gather network event causing failover common simultaneous primary accepting writes multiple day unknown mnesia splitbrain outage leave two node connected internet unable see type partition especially dangerous writes side partitioned cluster cause inconsistency lost data exactly happened mnesia cluster diverged overnight state critical operation team simply nuked one side cluster conclude experience convinced u need prioritize network partition recovery strategy instability causing mongodb elasticsearch unavailability network disruption affect certain group node instance report total partition frontend backend stack state web server lose connection backend instance second several time month even though disruption short cluster convergence resulted minute outage corrupted index elasticsearch problem escalated outage occurred time day voltdb splitbrain one voltdb user report regular network failure causing replica divergence also indicates network log included dropped packet cluster enabled splitbrain detection node ran isolated primary causing significant data loss elasticsearch discovery failure another splitbrain twonode cluster failed converge roughly startup discovery message took longer three second exchange result node would start primary cluster name since elasticsearch demote primary automatically splitbrain persisted administrator intervened upping discovery timeout second resolved issue rabbitmq elasticsearch window azure scattered report window azure partition account rabbitmq cluster entered splitbrain weekly basis also report elasticsearch splitbrain since azure relative newcomer compared description network reliability limited aws eb outage april amazon web service went hour causing hundred highprofile web site go offline part normal aws scaling activity amazon engineer shifted traffic away router elastic block store eb network single useast availability zone az traffic shift executed incorrectly rather routing traffic router primary network traffic routed onto lower capacity redundant eb network portion eb cluster affected availability zone meant functioning primary secondary network traffic purposely shifted away primary network secondary network handle traffic level receiving result many eb node affected availability zone completely isolated eb node cluster unlike normal network interruption change disconnected primary secondary network simultaneously leaving affected node completely isolated one another partition coupled aggressive failurerecovery code caused mirroring storm led network congestion triggered previously unknown race condition eb unavailable roughly hour eb unavailable degraded hour eb failure also caused outage amazon relational database service one az fails rds designed fail different az however multiaz database useast failed fail due stuck io primary cause rapid succession network interruption partitioned primary secondary stuck io primary replica triggered previously unencountered bug bug left primary replica isolated state safe monitoring agent automatically fail secondary replica without risking data loss manual intervention required correlated failure caused widespread outage client relying aws example heroku reported hour unavailability user database isolated redis primary july twilio billing system store account credit redis failed network partition isolated redis primary billing secondary twilio promote new secondary writes primary remained consistent however primary became visible secondary secondary initiated full resynchronization primary simultaenously overloaded primary causing service relied redis primary fail ops team restarted redis primary address high load restarting redis primary reloaded incorrect configuration file caused become slave primary entered readonly mode stopped billing system writes account balance zero readonly every twilio call caused billing system automatically recharge customer credit card customer overbilled roughly minute appointment reminder example reported every sm message phone call issued resulted charge credit card stopped accepting charge twilio recovered billing state independent billing relational hiccup restored proper service including credit affected user wan link largely focused failure local area network nearlocal network wide area network wan failure also le frequently documented failure particularly interesting often fewer redundant wan route system guaranteeing high availability disaster recovery often require distribution across multiple datacenters accordingly graceful degradation partition increased latency especially important geographically widespread service pagerduty pagerduty designed system remain available face node datacenter even provider failure service replicated two region datacenter hosted linode april aws peering point northern california degraded causing connectivity issue one pagerduty node latency aws availability zone rose notification dispatch system lost quorum stopped dispatching message entirely even though pagerduty infrastructure designed partition tolerance mind correlated failure due shared peering point two datacenters caused minute unavailability dropping inbound api request delaying queued page quorum reestablished cenic study researcher university california san diego quantitatively analyzed five year operation cenic widearea network contains two hundred router across california crosscorrelating link failure additional external bgp traceroute data discovered isolating network partition caused connectivity problem host average partition duration ranged minute softwarerelated failure hour hardwarerelated failure median minute percentile minute day respectively global routing failure despite high level redundancy internet system network failure take place globally distributed scale cloudflare cloudflare run datacenters redundant network path anycast failover response ddos attack one customer operation team deployed new firewall rule drop packet specific size juniper flowspec protocol propagated rule cloudflare edge happened packet matched rule packet actually large happened instead router encountered rule proceeded consume ram crashed recovering failure complicated router failed reboot automatically inaccessible management port even though data center came back online initially fell back traffic across entire network hit overloaded resource cloudflare monitor network carefully ops team immediate visibility failure however coordinating globally distributed system complex calling onsite engineer find reboot router hand take time recovery began minute complete hour unavailability juniper routing bug firmware bug introduced part upgrade juniper network router caused outage level communication networking backbone subsequently knocked service like time warner cable rim blackberry several uk internet service provider offline global bgp outage several global internet outage related bgp misconfiguration notably pakistan telecom responding government edict block youtubecom incorrectly advertised blocked route provides hijacked traffic site briefly rendered unreachable group duke university researcher achieved similar effect testing experimental flag bgp protocol similar incident occurred knocking site like martha stewart living new york time offline misconfiguration turkey attempted redirect entire internet go post meant reference illustrate according wide range account partition occur many realworld environment process server nics switch local wide area network fail resulting economic consequence real network outage suddenly arise system stable month time routine upgrade result emergency maintenance consequence outage range increased latency temporary unavailability inconsistency corruption data loss splitbrain academic concern happens kind day end partition deserve serious consideration hand network really reliable engineer major financial firm report despite putting serious effort designing system gracefully tolerate partition network rarely ever exhibit partition behavior cautious engineering lot money prevent outage however organization afford cost operational complexity highly reliable network google amazon operate commodity andor lowcost hardware due sheer scale oneman startup built shoestring budget communicationisolating network failure real risk important consider risk partition much easier make decision partition tolerance whiteboard redesign reengineer upgrade complex system production throwing error user application failure characterize explicitly account part design invite contribute experience without network partition open pull request http githubcomaphyrpartitionspost leave comment write blog post release postmortem data inform conversation future design ultimately availability system depend
597,Lobsters,scaling,Scaling and architecture,The SO_REUSEPORT socket option,https://lwn.net/Articles/542629/,soreuseport socket option,buying subscription early discussion pointed noted defect log,know lwnnet subscribersupported publication rely subscriber keep entire operation going please help buying subscription keeping lwn net michael kerriskmarch one feature merged development cycle tcp udp support soreuseport socket option support implemented series patch tom herbert new socket option allows multiple socket host bind port intended improve performance multithreaded network server application running top multicore system basic concept soreuseport simple enough multiple server process thread bind port set option follows int sfd socket domain socktype int optval setsockopt sfd solsocket soreuseport optval sizeof optval bind sfd struct sockaddr addr addrlen long first server set option binding socket number server also bind port also set option beforehand requirement first server must specify option prevents port possibility rogue application bind port already used existing server order capture incoming connection datagrams prevent unwanted process hijacking port already bound server using soreuseport server later bind port must effective user id match effective user id used perform first bind socket soreuseport used tcp udp socket tcp socket allows multiple listening different bound port thread accept incoming connection port calling accept present alternative traditional approach used multithreaded server accept incoming connection single socket first traditional approach single listener thread accepts incoming connection pass thread processing problem approach listening thread become bottleneck extreme case early discussion soreuseport tom noted dealing application accepted connection per second given sort number unsurprising learn tom work google second traditional approach used multithreaded server operating single port thread process perform accept call single listening socket simple event loop form newfd accept processconnection newfd problem technique tom pointed multiple thread waiting accept call wakeups fair high load incoming connection may distributed across thread unbalanced fashion google seen factorofthree difference thread accepting connection thread accepting fewest connection sort imbalance lead underutilization cpu core contrast soreuseport implementation distributes connection evenly across thread process blocked accept port tcp soreuseport allows multiple udp socket bound port facility could example useful dns server operating udp soreuseport thread could use recv socket accept datagrams arriving port traditional approach thread would compete perform recv call single shared socket second traditional tcp scenario described lead unbalanced load across thread contrast soreuseport distributes datagrams evenly across receiving thread tom noted traditional soreuseaddr socket option already allows multiple udp socket bound accept datagrams udp port however contrast soreuseport soreuseaddr prevent port hijacking distribute datagrams evenly across receiving thread two noteworthy point tom patch first useful aspect implementation incoming connection datagrams distributed server socket using hash based peer ip address port plus local ip address port mean example client us socket send series datagrams server port datagrams directed receiving server long continues exist eas task conducting stateful conversation client server noteworthy point defect current implementation tcp soreuseport number listening socket bound port change new server started existing server terminate possible incoming connection dropped threeway handshake problem connection request tied specific listening socket initial syn packet received handshake number server bound port change soreuseport logic might route final ack handshake correct listening socket case client connection reset server left orphaned request structure solution problem still worked may consist implementing connection request table shared among multiple listening socket soreuseport option nonstandard available similar form number unix system notably bsds idea originated seems offer useful alternative squeezing maximum performance network application running multicore system thus likely welcome addition application developer log post comment
598,Lobsters,scaling,Scaling and architecture,The Tumblr Architecture Yahoo Bought for a Cool BillionDollars,http://highscalability.com/blog/2013/5/20/the-tumblr-architecture-yahoo-bought-for-a-cool-billion-doll.html,tumblr architecture yahoo bought cool billion dollar,yahoo bought tumblr billion instagram profiled highscalability,reported yahoo bought tumblr billion may recall instagram profiled highscalability also bought facebook ton money coincidence judge yahoo buying business acumen deal something judge due diligence technology tumblr would probably get big thumb see please keep reading billion page view month tumblr become insanely popular blogging platform user may like tumblr simplicity beauty strong focus user experience friendly engaged community like growing month without challenge reliability problem among help realize tumblr operates surprisingly huge scale million page view day peak rate request per second new data store day running serversone common pattern across successful startup perilous chasm crossing startup wildly successful startup finding people evolving infrastructure servicing old infrastructure handling huge month month increase traffic four engineer mean make difficult choice work tumblr situation twenty engineer enough energy work issue develop interesting solutionstumblr started fairly typical large lamp application direction moving towards distributed service model built around scala hbase redis kafka finagle intriguing cell based architecture powering dashboard effort going fixing short term problem php application pulling thing right using service theme tumblr transition massive scale transition lamp stack somewhat bleeding edge stack transition small startup team fully armed ready development team churning new feature infrastructure help u understand tumblr living theme startup veteran blake matheny distributed system engineer tumblr blake say house tumblr site http wwwtumblrcom stats million page view day page view month engineer peak rate request per second tbday hadoop cluster many tbday mysqlhbaseredismemcache growing month hardware node production billion page visit per month per engineer post day follower list update day dashboard run million writes second read second growing software o x development linux centos scientific production apache php scala ruby redis hbase mysql varnish haproxy nginx memcache gearman kafka kestrel finagle thrift http func secure scriptable remote control framework api git capistrano puppet jenkins hardware web server database server many part spare pool pulled failure memcache server redis server varnish server haproxy node nginx job queue server kestrel gearman architecture tumblr different usage pattern social network million post day average post go many hundred people one two user million follower graph tumblr user hundred follower different social network make tumblr challenging scale social network term time spent user content engaging image video post byte sized long form ability people write indepth content worth reading people stay hour user form connection user go hundred page back dashboard read content social network stream sample implication given number user average reach user high posting activity user huge amount update handle tumblr run one colocation site design keeping geographical distribution mind future two component tumblr platform public tumblelogs dashboard public tumblelog public deal term blog easy cache dynamic dashboard similar twitter timeline user follow realtime update user follow different scaling characteristic blog caching useful every request different especially active follower need realtime consistent show stale data lot data deal post day follower list update day medium stored user leverage tumblr tool consuming content million page view day dashboard dashboard availability quite good tumblelog good legacy infrastructure hard migrate away small team pick choose addressed scaling issue old tumblr company started rackspace gave custom domain blog record outgrew rackspace many user migrate still custom domain rackspace route rackspace back colo space using haproxy varnish lot legacy issue like traditional lamp progression historically developed php nearly every engineer program php started web server database server php application started growing scale started using memcache put frontend caching haproxy front cache mysql sharding mysql sharding hugely helpful use squeeze everything single server approach past year developed couple backend service c id generator staircar using redis power dashboard notification dashboard us scattergather approach event displayed user access dashboard event user follow pulled displayed scale another month since data time ordered sharding scheme work particularly well new tumblr changed jvm centric approach hiring speed development reason goal move everything php app service make app thin layer service request authentication presentation etc scala finagle selection internally lot people ruby php experience scala appealing finagle compelling factor choosing scala library twitter handle distributed issue like distributed tracing service discovery service registration implement stuff come free jvm finagle provided primitive needed thrift zookeeper etc finagle used foursquare twitter scala also used meetup like thrift application interface really good performance liked netty wanted java scala good choice picked finagle cool knew guy worked without lot networking code work needed distributed system nodejs selected easier scale team jvm base nodejs developed enough standard best practice large volume well tested code scala use java code lot knowledge use scalable way target response time ha request per second request per second lot java ecosystem leverage internal service shifted clibevent based scalafinagle based newer nonrelational data store like hbase redis used bulk data currently stored heavily partitioned mysql architecture replacing mysql hbase hbase back url shortner billion url historical data analytics rock solid hbase used situation high write requirement like million writes second dashboard replacement hbase deployed instead mysql bet business hbase people started using smaller le critical path project gain experience problem mysql sharding time series data one shard always really hot also ran read replication lag due insert concurrency slave created common service framework spent lot time upfront solving operation problem manage distributed system built kind rail scaffolding service template used bootstrap service internally service look identical operation perspective checking statistic monitoring starting stopping work way service tooling put around build process sbt scala build tool using plugins helper take care common activity like tagging thing git publishing repository etc developer get gut build system frontend layer us haproxy varnish might hit public blog machine web server running apache php application database server many database server used high availability reason commodity hardware used mtbf surprisingly low much hardware expected lost many spare case failure backend service support php application team dedicated develop backend service new service rolled every week includes dashboard notification dashboard secondary index url shortener memcache proxy handle transparent sharding put lot time effort tooling mysql sharding mongodb used even though popular ny location mysql scale fine gearman job queue system used long running fire forget type work availability measured term reach user reach custom domain dashboard also term error rate historically highest priority item fixed failure mode analyzed addressed systematically intention measure success user perspective application perspective part request fulfilled account initially actor model used finagle dropped fire forget work job queue used addition twitter utility library contains future implementation service implemented term future situation thread pool needed future passed future pool everything submitted future pool asynchronous execution scala encourages shared state finagle assumed correct tested twitter production mutable state avoided using construct scala finagle long running state machine used state pulled database used writte n back database advantage developer need worry thread lock redis server server instance redis instance used production used backend storage dashboard notification notification something like user liked post notification show user dashboard indicate action user taken content high write ratio made mysql poor fit notification ephemeral horrible dropped redis acceptable choice function gave chance learn redis get familiar work redis completely problem free community great scala future based interface redis created functionality moving cell architecture url shortener us redis first level cache hbase permanent storage dashboard secondary index built around redis redis used gearman persistence layer using memcache proxy built using finagle slowly moving memcache redis would like eventually settle one caching service performance par memcache internal firehose internally application need access activity stream activity steam information user creatingdeleting post likingunliking post etc challenge distribute much data realtime wanted something would scale internally application ecosystem could reliably grow around central point distribution needed previously information distributed using scribehadoop service would log scribe begin tailing pipe data app model stopped scaling almost immediately especially peak people creating post second want people tailing file piping grep internal firehose created message bus service application talk firehose via thrift linkedin kafka used store message internally consumer use http stream read firehose mysql used sharding implementation changing frequently hitting huge data stream good idea firehose model flexible like twitter firehose data assumed lost firehose stream rewound time retains week data connection possible specify point time start reading multiple client connect client see duplicate data client client id kafka support consumer group idea consumer consumer group get message see duplicate multiple client created using consumer id client see duplicate data allows data processed independently parallel kafka us zookeeper periodically checkpoint far consumer read cell design dashboard inbox current scattergather model providing dashboard functionality limited runway last much longer solution move inbox model implemented using cell based architecture similar facebook message inbox opposite scattergather user dashboard made post followed user action taken user logically stored together time order solves scatter gather problem inbox ask inbox le expensive going user user follows scale long time rewriting dashboard difficult data distributed nature transactional quality ok user get partial update amount data incredible message must delivered hundred different user average different problem facebook face large date high distribution rate multiple datacenters spec ed million writes second read second data set size data growth replication compression turned million writes second byte row key indicates content inbox already popular application kept running cell cell selfcontained installation data range user data necessary render user dashboard cell user mapped cell many cell exist per data center cell hbase cluster service cluster redis caching cluster user homed cell cell consume post via firehose update cell finagle based populates hbase via firehose service request thrift user come dashboard user home particular cell service node read dashboard via hbase pass data back background task consume firehose populate table process request redis caching layer used post inside cell request flow user publishes post post written firehose cell consume post write post content post database cell lookup see follower post creator cell follower inboxes updated post id advantage cell design massive scale requires parallelization parallelization requires component isolated interaction cell provide unit parallelization adjusted size user base grows cell isolate failure one cell failure impact cell cell enable nice thing like ability test upgrade implement rolling upgrade test different version software key idea easy miss post replicated cell cell store single copy post cell completely satisfy dashboard rendering request application ask post id ask post id return dashboard content user every cell data needed fulfill dashboard request without cross cell communication two hbase table used one store copy post data small compared table store every post id every user within cell second table tell user dashboard look like mean go fetch user user following also mean across client know read post viewing post different device mean read content twice inbox model state kept read post put directly inbox size great id put inbox post content put cell model greatly reduces storage needed making simple return time ordered view user inbox downside cell contains complete copy call post surprisingly post smaller inbox mapping post growth per day per cell inbox grows day user consume produce user dashboard contain text post post id majority growth id follower change design safe post already cell follower post stored cell cell would date follower changed sort back fill process would needed alternative design use separate post cluster store post text downside design cluster go impact entire site using cell design post replication cell creates robust architecture user million follower really active handled selectively materializing user feed access model see feeding frenzy different user different access model distribution model appropriate two different distribution mode one popular user one everyone else data handled differently depending user type post active user actually published post would selectively materialized user follow million user treated similarly user million follower cell size hard determine size cell impact site failure number user homed cell impact tradeoff make willing accept user experience much cost reading firehose biggest network issue within cell network traffic manageable cell added cell placed cell group read firehose replicates cell within group hierarchical replication scheme also aid moving multiple datacenters startup new york ny different environment lot finance advertising hiring challenging much startup experience last year ny focused helping startup nyu columbia program getting student interesting internship startup instead going wall street mayor bloomberg establishing local campus focused technology team structure team infrastructure platform sre product web ops service infrastructure layer ip address dns hardware provisioning platform core app development sql sharding service web operation sre sits service team web ops team focused immediate need term reliability scalability service team focus thing slightly strategic month two month web ops responsible problem detection response tuning software deployment started set rsync script distributed php application everywhere number machine reached system started problem deploys took long time finish machine would various state deploy process next phase built deploy process development staging production service stack using capistrano worked service dozen machine connecting via ssh started failing deploying hundred machine piece coordination software run machine based around func redhat lightweight api issuing command host scaling built func build deployment func saying x set host avoids ssh say want deploy software group master reach set node run deploy command deploy command implemented via capistrano git checkout pull repository easy scale talking http like capistrano support simple directory based versioning work well php app moving towards versioned update directory contains sha easy check version correct func api used report back status say machine software version safe restart service drain connection restart feature run dark mode activation development started philosophy anyone could use tool wanted team grew work onboarding new employee difficult standardized stack get good grow team quickly address production issue quickly build operation around process roughly scrum like lightweight every developer preconfigured development machine get update via puppet dev machine roll change test roll staging roll production developer use vim textmate testing via code review php application service side implemented testing infrastructure commit hook jenkins continuous integration build notification hiring process interview usually avoid math puzzle brain teaser try ask question focused work candidate actually smart get stuff done measuring get thing done difficult ass goal find great people rather keep people focused coding ask sample code phone interview use collabedit write shared code interview confrontational want find best people candidate get use tool like google interview idea developer best tool run interview challenge finding people scaling experience require given tumblr traffic level company world working problem example new id generator needed jvm process generate service response le rate request per second mb ram limit high availability found serial collector gave lowest latency particular work load spent lot time jvm tuning tumblr engineering blog posted memorial giving respect passing dennis ritchie john mccarthy geeky culture lesson learned automation everywhere mysql plus sharding scale apps nt redis amazing scala apps perform fantastically scrap project sure work hire people based survival useless technological gauntlet hire fit team job select stack help hire people need build around skill team read paper blog post key design idea like cell architecture selective materialization taken elsewhere ask peer talked engineer facebook twitter linkedin experience learned may access level reach somebody somewhere wade jump technology took pain learn hbase redis putting production using pilot project role damage would limited like thank blake much interview generous time patient explanation please contact would like talk architecture profiled related article
599,Lobsters,scaling,Scaling and architecture,"Why I left Heroku, and notes on my new AWS setup",http://www.holovaty.com/writing/aws-notes/,left heroku note new aws setup,soundslice heroku amazon web service aws heroku experience sentryraven heroku p scale made reddit homepage broke deployment ugly error message tip jacob heroku lost trust way compile package binary aws setup scott vandenplas ton attention awesome elastic load balancer step bake ami pip step set autoscaling rule check python code step change app use shared cache cookiebased session step migrate mysql frank wile rds update spring step add nice api fabric fabric boto see ongoing update ami needed chef puppet,friday migrated soundslice heroku direct use amazon web service aws happy change want spread word consider similar position heroku experience soundslice heroku since site launched november decided use reason sysadmin thing nt enjoy particularly good soundslice twoman operation developer designer time much better spent working product sysadmin work heroku promise easy setup easy scaling case high traffic getting soundslice running heroku ran problem immediately one automatic detection pythondjango nt work rejigger code four five time settingspy go directory subdirectory subsubdirectory order pick app autodetection stuff kind thing hard debug spent full day half trying get django error email working verified server could send email necessary code worked python shell error nt get emailed app reason never figure problem ended punting using sentryraven highly recommended experience along oddity made weary heroku kept credit heroku handled soundslice launch well issue using heroku p scale command line super cool december soundslice made reddit homepage people visited site period hour heroku handled nicely scaled number dynos next month got burned time first january broke deployment whenever tried deploy got ugly error message ended routing around bug installing different buildpack thanks tip jacob left sour taste mouth one april evening deployed app heroku decided upgrade python version deploy vaguely upsetting nt request upgrade app code worked well new version ok deployment done site completely hard failure ugly heroku error message shown user idea happened raced recent commits looking problem looked heroku log output said stuff soundslice package found ran site locally make sure working working fine deployed successfully earlier day made fundamental change package layout several minute futzing around site completely sent link potential partner know evaluating site moment deployed site worked nothing end clearly something busted heroku deployment process heroku lost trust whenever deployed got little nervous something bad would happen control around time soundslice began using python module compiled c extension various nonpython code deployable heroku standard requirementstxt process heroku offer way compile package binary used successfully work using proprietary process running simple aptget command server root access decided time leave heroku still using heroku blog might use future smallthrowaway project personally would nt recommend using anything substantial especially know easy get powerful aws stack running aws setup lucky friend scott vandenplas director dev ops obama reelection tech team know one got ton attention awesome scott helped set fantastic infrastructure soundslice aws despite used amazon fair amount year idea powerful amazon full suite service really scott showed unsolicited advertisement definitely hire scott need aws work done one best way set soundslice relatively simple made custom ami codedependencies set elastic load balancer autoscaling rule instantiate app server ami based load also converted app use mysql detail step bake ami grabbed existing vanilla ubuntu ami basically frozen image linux box installed various package soundslice need aptget pip also compiled bit code needed nt aptget got app code cloning git repository instance codedependencies created ami create image eb ami dashboard step set autoscaling rule real magic configured load balancer using amazon elb automatically spawn app server based load involves setting thing called launch configuration scaling policy metric alarm check python code see detail basically amazon constantly monitor app server reach certain cpu usage amazon automatically launch x new server associate load balancer running thing applies traffic level go need terminate instance two awesome step change app use shared cache aws migration soundslice used memcache django session data introduces wrinkle autoscaled environment mean server need access common memcache instance rather deal changed app use cookiebased session session data stored signed cooky rather memcache way web app server nt need share state database plus faster end user app nt hit memcache session data step migrate mysql eeeek know diehard postgresql fan since frank wile showed light circa way use postgres aws maintenancescaling distaste sysadmin work greater distate mysql amazon offer rds basically hosted mysql pointandclick replication fell love moment scaled one two availability zone couple click aws admin console simplicity amazing update spring amazon support postgresql rds product step add nice api fabric deployment stupidly simple heroku easy make equally simple using custom aws environment upfront work writing fabric task key nt know many server given moment host name query amazon api using excellent boto library get hostnames dynamically see relevant part fabfile ongoing update ami needed whenever new bit code app need say new aptget package make oneoff instance ami install package freeze new ami associated load balancer new ami new app server use new ami force existing instance use new ami simply terminating amazon console load balancer detect terminated based scaling rule bring new instance new ami another approach would use chef puppet automatically install necessary package new server instantiation time instead baking package ami opted unnecessary complexity app simple enough bakedami approach work nicely put together powerful setup would argue easy use heroku set full power root access box ability install whatever want set scaling rule etc try
600,Lobsters,scaling,Scaling and architecture,S3 is NOT a CDN,http://jdorfman.posthaven.com/medium-bitcoin-660x493-dot-jpg-cdn-vs-s3,cdn,cdn simple delivery service via aws chart summary posthaven maxcdn take away cdn,said last post cdn amazing simple storage service simple delivery service via aws amazon storage internet designed make webscale computing easier developer note throughput higher better numerical representation efficiently system able retrieve element kbps throughput calculated file size header size byte load time chart summary posthaven maxcdn run avg dns m avg connect m avg time first byte m avg response m avg throughput avg webpage response m availability run avg dns m avg connect m avg time first byte m avg response m avg throughput avg webpage response m availability following metric name prefix sd avg gm mdn stand standard deviation average geometric mean median respectivelytake awaywhether posthaven go cdn another provider hope anyone reading take away use cdn designed wasting money user timeoh great weekend everyone
602,Lobsters,scaling,Scaling and architecture,Building Highly Available Systems in Erlang,http://www.infoq.com/presentations/Building-Highly-Available-Systems-in-Erlang#.UUUKT7OwH5g.twitter,building highly available system erlang,infoq homepage presentation building highly available system erlang summary bio conference related sponsored content,infoq homepage presentation building highly available system erlang building highly available system erlang summary joe armstrong discus highly available ha system introducing different type ha system data ha architecture algorithm rule ha ha done erlang bio joe armstrong principal inventor erlang coined term concurrency oriented programming ericsson developed erlang chief architect erlangotp system formed bluetail developed product erlang obtain phd royal institute technology stockholm author book software concurrent world conference software changing world qcon aim empower software development facilitating spread knowledge innovation enterprise software development community achieve qcon organized practitionerdriven conference designed people influencing innovation team team lead architect project manager engineering director recorded apr related sponsored content
603,Lobsters,scaling,Scaling and architecture,Dynamo Sure Works Hard,http://damienkatz.net/2013/05/dynamo_sure_works_hard.html,dynamo sure work hard,dynamo sure work hard amazon dynamo paper couchbase damienkatz,dynamo sure work hard tend think working hard good thing value strong work ethic determination face adversity working harder get result virtue waste time energy business system working harder waste budget dynamo based system work hard simpledbdynamodb riak cassandra voldemort based least part design first described publicly amazon dynamo paper interesting concept ultimately fails provide good balance reliability performance cost pretty neat transaction allows dial level redundancy consistency trade performance efficiency pretty fast efficient nt need consistency ultimately consistency want pay via lot extra work network partition rare server failure well known dealing possibility network failure strong consistency high data availability achieved simultaneously system application need aware property achieved condition system prone server network failure availability increased using optimistic replication technique change allowed propagate replica background concurrent disconnected work tolerated challenge approach lead conflicting change must detected resolved process conflict resolution introduces two problem resolve resolve dynamo designed eventually consistent data store update reach replica eventually amazon dynamo paper dynamo system design treat probability network switch failure probability machine failure pay cost every single read madness expensive madness within datacenter mean time failure mttf network switch one two order magnitude higher server depending quality switch according data google datacenter server failure publish number mtbf cisco switch subtle difference mtbf mttf purpose treat claimed w r n get consistency true without distributed acid transaction never possible achieve w atomically consider network failure likely clientapp tier failure hardware o process crash happens writing data possible replica receive write lag cluster notice syncs another client r two consecutive read getting newer data first node older data next node b key nt even need failure crash first write occurs always lag next server receive write possible fast client read time getting newer version one server older version another true r n get consistency possible read newer value subsequent read get older value vast majority application okay failure leading temporary unavailability amazon belief shopping cart important capture writes worth cost quorum read inconsistency perhaps problem cost multiply extra read achieve high consistency putting extra load machine requiring extra server hardware extra networking infrastructure provide baseline performance increase frequency component failure increase operational cost hardware power rack space personnel maintain better way document master n replica write single master read client know based document key topology map machine serf master would make read far cheaper faster read writes document go master writes replicated replica also serve master document machine master replica might ask achieve strong consistency master go becomes unresponsive happens cluster also notice machine unresponsive slow remove cluster fails new master client try successful read might ask client asks wrong server read machine cluster know role one machine cluster document master time cluster manager regular server node elected paxos consensus make sure remove old master assign new master tell client new topology client update topology map retries new master might ask topology changed client asks read wrong server wrong server let client know client reload topology map rerequest right server right master server nt really right another topology change reload retry many time necessary typically happens might ask network partition client wrong minor side partition read master server nt know master server anymore get stale read little server realizes longer heartbeat contact majority cluster partition like among rarest form cluster failure require network failure client wrong side partition might ask network partition client wrong smaller side partition writes server nt know master server anymore write lost client wanted true multinode durability write would nt succeeded client would timeout waiting replica receive update client would nt unknowingly lose data describing couchbase clustering system let run number given mttf server much hardware quickly must cluster failover new master still meet slas requirement v dynamo based system let start assumption want achieve transactionssec node replication factor load mix writes want consistency nt read newer value older value dynamo r w n couchbase r w n mean dynamo style cluster load read transactionssec read read spread node write transactionssec writes writes spread node mean couchbase style cluster load read transactionssec read document read master node document master evenly spread across node write transactionsec writes writes spread node let assume system equally reliable machine level google research indicates datacenter server mttf hr failure per year google also report rack failure usually power supply year roughly reliable server ignore make analysis simpler google paper studying server failure mtbf cisco network switch published hr low end hr high end purpose ignore switch failure since failure affect availability consistency system order magnitude rarer server failure cisco product spreadsheet assume want meet latency sla time actual latency sla threshold number nt matter dynamo mean node fail sla time query node us value first node chance sla failure across node formula different two must meet sla p p p couchbase master node fails must recognize fail given google mttf failure fail node sec let say take minute warm ram cache given failuresyear minute downtime failover completes query fail time due node failure couchbase meet sla p slafail p nodefail p slafail p nodefail p slafail p slafail note thing omitting analysis dynamo node fails lower latency requirement meeting sla node v would drop also increased work remaining server couchbase server fails temporary go away new server added back initialized cluster nt change number significantly also time add new node rebalance load couchbase work hard make fast efficient possible assume dynamo system cost omit though think leader rebalance performance analysis couchbase node fail sla request dynamo node fail sound good dynamo must throughput per node data total network traffic low latency response time customer often want submillisecond latency typically meeting sla mean dbms must keep large amount relevant data metadata ram huge cost random disk fetch latency disk fetch order magnitude slower ssds order magnitude slower hdds disk access pile faster without enough ram latency dynamo node fail sla higher rate small win still need keep nearly working set ready memory node serving data time read request fail sla slightly often actually necessary ram use network capacity damn dynamo sure work hard couchbase nt perfect either far follow twitter damienkatz posting couchbase shortcoming capability technical roadmap soon posted may
605,Lobsters,scaling,Scaling and architecture,Introducing Heka,https://blog.mozilla.org/services/2013/04/30/introducing-heka/,introducing heka,heka hekad go logstash lua,mozilla service team happy announce first beta release heka tool high performance data gathering analysis monitoring reporting heka main component hekad lightweight daemon program run nearly host machine following gather data reading parsing log file monitoring server health andor accepting client network connection using wide variety protocol syslog statsd http heka etc convert acquired data standardized internal representation consistent metadata envelope support effective handling processing rest heka system evaluates message content metadata set routing rule determines processing filter external endpoint message delivered process message content inflight perform aggregation slidingwindow event processing monitoring extraction structured data unstructured data eg parsing log file output text generate numeric stats data andor processingfriendly data structure generation new message reporting output delivers received internally generated message data external location data might written database time series db file system network service including upstream hekad instance processing andor aggregation heka written go proven wellsuited building data pipeline flexible fast initial testing show single hekad instance capable receiving routing gigabit per second message data also borrowed extended great idea logstash built heka pluginbased system developer build custom input decoder filter ie dataprocessing output plugins extend functionality quickly easily four plugin type implemented go managing plugins requires editing config file restarting introducing new plugins even recompiling hekad binary heka provides another option however allowing sandboxed filter written lua instead go added removed running heka instance without need edit config restart server heka also provides lua apis sandboxed filter use managing circular buffer timeseries data generating adhoc graph report following example show heka reporting dashboard heka new technology running production place inside mozilla still bit rough around edge like everything mozilla produce however open source releasing early often make available interested developer contributor pull request welcome early adopter list resource like learn
606,Lobsters,scaling,Scaling and architecture,"Hey Judy, don't make it bad (on Ruby's GC and Judy Arrays)",https://github.com/blog/1489-hey-judy-don-t-make-it-bad,hey judy nt make bad ruby gc judy array,greatly reduced rendering time web view line code lot pocketsize library babel linguist yaml file lot take slow classifier make better judy array gc jilted generation proposed generational garbage collector inclusion mri rubinius jruby,last week explained greatly reduced rendering time web view switching escaping routine ruby c speedup twofold c code escaping html significantly faster ruby equivalent top c code generating lot fewer object ruby heap meant subsequent garbage collection run would run faster working mark sweep garbage collector like one mri amount object heap given moment time matter lot object longer gc pause take object must traversed mark phase since mri garbage collector also stop world gc running ruby code executing hence web request served ruby objectspace module contains useful metadata regarding current state garbage collector ruby heap probably useful method provided module countobjects return amount object allocated ruby heap separated type offer insightful birdseye view current state heap tried running countobjects fresh instance main rail application soon library dependency loaded githubpreloadall gcstart count objectspacecountobjects put count total count free whelp ruby object allocated boot lotta heap like say country obvious question whether object heap actually necessary whether free simply prevent allocating reduce garbage collection time question however rather hard answer using objectspace module although offer objectspace eachobject method enumerate object allocated enumeration little use tell object allocated fortunately master plan one time line code added sourcefile sourceline method every single object kernel kept track file line object allocated priceless able iterate every single object ruby heap pinpoint aggregate source allocation githubpreloadall gcstart objectspaceeachobjecttoainject hashnew h h osourcefile oclass h sortby k v v first k v printf sn v k string string string activesupport multibyte codepoint string tzinfo timezonetransitioninfo string rubyvm instructionsequence rubyvm instructionsequence string oh boy let take look detail clearly allocation source nothing rail core library example biggest offender look interesting psych yaml parser ship ruby apparently something parsing lot yaml keeping memory time could pocketsize library babel linguist opensource ruby gem developed power language statistic githubcom people push lot code github needed reliable way identify classify text file display web interface actually source code language written need highlighted autogenerated first version linguist took pretty straightforward approach towards solving problem definition language know stored yaml file metadata file extension language type language lexer syntax highlighting however approach fails many important corner case file extension call h extension would take long compile could c could c could objectivec needed reliable way separate case hundred ambiguous situation file extension related one programming language source file even extension decided augment linguist simple classifier armed pocketsize library babel code sample collection source code file different language hosted github attempted perform weighted classification new source code file encounter idea simple faced source code file recognize tokenize use weighted classifier find likehood token file belong given programming language example include token likely belong c c file ruby file class token well belong c file ruby file find include class token file answer definitely c course perform classification need keep memory large list token every programming language hosted github respective probability collection token topping allocation meter ruby garbage collector classifier accurate need trained large dataset bigger although token sample barely enough training classifier lot poor ruby heap take slow classifier make better obvious plan fix issue move massive token dataset ruby heap native cland need garbage collected keep compact possible memory decided store token judy array trielike data structure act associative array keyvalue store interesting performance characteristic opposed traditional trielike data structure storing string branch happen bitlevel ie judy array act trie node highly compressed claim thanks compression judy array packed extremely tightly cache line minimizing amount cache miss per lookup supposed result lookup time compete hash table even though algorithmic complexity judy array log n like trielike structure course realworld silver bullet come algorithmic performance judy array exception despite claim judy original whitepaper cache miss modern cpu architecture fetch data stored prison azkaban fetch cache happens ohnotthatfaraway practice mean constant loss time caused certainly many cache miss hash table lookup enough offset lookup time judy array log n matter tightly packed top hash table linear probing small step size point reduced cache miss becomes moot time collision resolved cache line happened practical result proven realworld test end day properly tuned hash table always faster judy array choose judy array implementation starter goal right related performance classification usually performance critical operation maximizing size training dataset minimizing memory usage judy array thanks remarkable compression technique store key dataset much smaller chunk memory much le redundancy hash table furthermore pitting judy array mri hash table implementation known particularly performant thought way dataset stored memory becomes feasible beat ruby hash table game even performing logarithmic lookup main design constraint problem token dataset need separated language yaml file load memory take straightforward approach creating one hash table per language containing token better using trie structure however store token judy array prefixing unique prefix identifies language creates independent subtrees token inside global data structure different language increase cache locality reduces logarithmic cost lookup average query behavior dataset burst lookup thousand token language row subtrees mean keeping cache permanently warm minimzing amount traversal around array since internal judy cursor never leaf subtree language query result optimization much positive expect benchmarking logarithmic time structure one allegedly performs lookup constant time benchmark disabled mri garbage collector see lookup million token database stay faster hash table even artificially increase dataset random token thanks locality token subtrees per language lookup time remain mostly constant exhibit logarithmic behavior thing get even better judy array enable garbage collector gc cycle start triggered lookup see massive size data structure ruby heap cause garbage collector go banana huge spike lookup time dataset increase gc run triggered judy array stored outside ruby heap remains completely unfazed manages maintain constant lookup time hash table lookup become expensive higher garbage collection time cherry top come graphing r usage ruby process increase size dataset time anticipated judy array throw mri hash table implementation bus growth remains much linear increase extremely slowly appreciate considerable bump fast growth hash table get resized gc jilted generation new storage engine token linguist classifier able dramatically expand sampling dataset bigger dataset mean accurate classification programming language accurate language graph repository make github awesome elephant room still life shape mri garbage collector however without generational gc capable finding marking root heap unlikely freed must keep permanent attention amount object allocate main app object mean higher memory usage also mean higher garbage collection time slower request good news koichi sasada recently proposed generational garbage collector inclusion mri prototype remarkable allows subset generational garbage collection happen maintaining compatibility mri current c extension api current iteration several tradeoff sake simplicity writing extension make memory management internal object extremely difficult compatibility older version course come price object heap need separated shady sunny depending whether write barrier hence whether generationally collected enforces overly complicated implementation gc interface several ruby c apis must drop write barrier object used additional bookkeeping needed separate different kind object creates performance regression lighter gc load top new garbage collector also forced run expensive mark sweep phase young generation opposed eg copying phase design choice make current c api support conservative garbage collection despite best effort koichi contributor ruby core concern backwards compatibility particularly regarding c extension api keep mri lagging decade behind ruby implementation like rubinius jruby already precise generational incremental garbage collector unclear moment whether new gc current state make next version mri whether case little late given many handicap current implementation thing wait like wait c hah amirite guy amirite
607,Lobsters,scaling,Scaling and architecture,Go Circuit - Distributed Go Programming Framework,http://www.gocircuit.org/,go circuit distributed go programming framework,coreos consul mesosphere source circuit github gocircuit issue circuit user group circuit developer group documentation tutorial orchestrating typical web app nodejs using mysql running amazon,circuit minimal distributed operating system enables programmatic reactive control host process connection within compute cluster circuit api provides dynamic hierarchical view compute cluster circuit unique one respect circuit cluster formed circuit system individual host contrast comparable system like coreos consul mesosphere fail hardware hosting system software fails source find source repository circuit github follow u twitter gocircuit submit issue github repo discussion using developing circuit visit circuit user group circuit developer group respectively documentation tutorial orchestrating typical web app nodejs using mysql running amazon
610,Lobsters,scaling,Scaling and architecture,What Does Your Webserver Do When a User Hits Refresh?,http://www.shopify.ca/technology/7535298-what-does-your-webserver-do-when-a-user-hits-refresh,webserver user hit refresh,,shopify world fastestgrowing commerce platform plan slow working ship quality instead time team continuously deploy new code large scale support hundred thousand online store hundred million request day entrepreneur household brand depending u livelihood tough incredibly rewarding responsibility looking passionate problem solver looking
612,Lobsters,scaling,Scaling and architecture,Three Months to Scale NewsBlur,http://blog.newsblur.com/post/45632737156/three-months-to-scale-newsblur,three month scale newsblur,storify history readerocalypse challenge solution writing new fabric script include http header top error template replacereadercom newsblur replacereader next three month newsblur totally open source people spoken instruction github newsblur iphone ipad app android app free user join newsblur,last wednesday got short tothepoint email nilay patel verge link started host googlereaderblogspotcom sudden spike newsblur visitor immediately confirmed google shutting readerlate night officei preparing black swan event like last four year since began newsblur deprecation social feature year ago knew matter time google stopped supporting reader entirely expect come soonas storify history readerocalypse newsblur suffered number hurdle onslaught new subscribersa challenge solutionsi able handle user using service everyday user hit uncachable resource intensive backend unless done homework load tested living crap entire stack going trouble brewing immediate challenge faced past four day hosting provider reliable hosting service neither reliable able host increasing demand service could count switched digital ocean immediately got writing new fabric script could deploy new apptask server issuing single command serve request automatically within minute bootstrappingit take long max amazon simple email service s account quota email day hour melee switched mailgun unfortunately resulted emailing error report tried email get email lost database connection made way ahead lineeventually plain blacklisted s sending many emailsfortunately paypal fraud department called unprecedented spike payment preparedhaproxy would serve error site maintenance timeouts etc ok status code instead proper exception status code ridiculous undocumented requirement include http header top error template webapp us status code determine error get extremely strange behavior load utter crap domthe inevitable file descriptor limit linux mean every database connection make use one file descriptor allocated process default changing limit nontrivial tend stick responsible bringing mongo postgresql realtime node server different time nightthe support queue enormous spend big chunk hour day reassuring paying customer eventually stripe forgive unresponsive server send payment notification responsible automatically upgrading account premiumthe sad extent st patrick dayas onemanshop humbling receive benefit doubt many withheld judgment despite admittedly slow loadtimes downtime newsblur experienced support amazing newsblur community guy could ask tweet encouragement voting newsblur replacereadercom yet please tweet vote newsblur replacereader many positive comment blog post people tried newsblur greatit also dream come true receive accolade many trying newsblur first time loving since announcement newsblur welcomed new premium subscriber new user user originally newsblur user intelligent kind good looking next three monthsover next three month working still trying decide go reader refugee let tell unique thing newsblur offer radical transparency newsblur totally open source remain wayit still feel like r bell whistle newsblur provides actual list post opposed curated magazine format popular replacement clean interface make easy see story want one innovation however four different view option newsblur show original site feed text story viewit training newsblur hide story want read based tag keywords author etc also highlight story want read based criterion allows find story care story hive care best newsblur show story either highlighted hidden showing criterion green rednewsblur rebuilt social community google stripped reader user share story blurblog discover new content following friend blurblogs people spoken blurblog popular storiesbecause newsblur entirely opensource want pay host server instruction github also find source code newsblur iphone ipad app android appmost importantly newsblur entirely free app immediate benefit revenue clear past day newsblur interest aligned user user join newsblur make revenue used directly support new user convinced paid better free read pinboard maciej ceglowski essay free usershiloh better time premium subscription go server cost feeding herwith newsblur native io app android app read news share friend anywhere coming improvement next three month bet newsblur choice google reader refugeesjoin newsblur discover r
614,Lobsters,scaling,Scaling and architecture,Errata Security: Multi-core scaling: its not multi-threaded,http://erratasec.blogspot.com/2013/02/multi-core-scaling-its-not-multi.html,erratum security multicore scaling multithreaded,shmoocon talk,writing series post based shmoocon talk post going discus multicore scaling decade leading intel cpu went thousandfold hundredfold increase speed decade since stuck instead faster clock speed getting logic instead one instruction per clock cycle execute four superscalar instead one computation per instruction eight simd instead single cpu chip put four multicore however desktop processor stuck four core several year software lagging multithreaded software go four core past point fails get benefit additional core worse adding core past four often make software go slower post talk scaling code past fourcore limit instead graph showing performance falling four core technique lead graph like one performance increasing core added reason code fails scale written according outofdate principle based multitasking multitasking problem making single core run multiple task core would switch quickly one task next make appear running time even though particular microsecond one task running time call multithreading thread lighter weight task trying make many task run single core trying split single task across multiple core exact opposite problem look similar case use thread every aspect problem opposite biggest issue synchronization professor pounded two threadscores modify piece data time corrupted even chance modification exactly time rare always happens eventually computer billion computation per second chance one billion mean corruption happens per second prescribed method resolving lock one thread stop wait another thread modifying piece data since rare two thread actually conflict practice rare thread actually wait multiple type lock like spinlocks mutexes critical section semaphore even among class many variation common conflict occurs cause thread stop wait waiting problem core added chance conflict wait increase dramatically moreover wait big problem linux pthreadmutext code stop wait system call return back kernel good idea one cpu core running multiple thread course current thread going able make forward progress anyway whichever thread owns lock allowed proceed release lock multicore becomes insanely bad cost going kernel going thread scheduling system huge software using mutexes get slower add core constant kernel traffic add lot extra overhead short mutexes good many thread share core bad single thread per core want synchronization cause thread stop wait situation lot like traffic intersection multiple flow automobile must share common resource one technique use traffic light force one direction stop wait proceeds another technique freeway overpass used allow direction proceed time without stopping therefore want freeway overpass synchronization technique exist though get complicated basic technique exploit fact modern cpu either reading writing number memory atomic mean combining read write lead corruption either read write alone past reading multibyte number could lead corruption nanosecond reading first byte number another core could write second byte longer happen let exploit fact packet counter linux network stack keep track packetsbytes receivedtransmitted well count error occur multiple core may processing different packet time therefore need synchronize update packet counter stop wait synchronization lead enormous packet loss way solve core maintain packet counter call ifconfig read packet counter display thread sum individual core counter single set counter thread read counter read atomic corruption possible well corruption possible consider program wanted report average packet size calculated total byte divided total packet reading single integer atomic reading integer therefore possible sometimes thread read total byte another core update counter thread read total packet calculation lead slightly le average packet size counter properly synchronized technique perfect depends requirement one example many technique narrow case either traditional synchronization needed mostly avoided term google along line ring buffer read copy update rcu say atomic though mean individual read write combining two single noninteruptable operation processor assembly language instruction called lock really instruction instead modifies following instruction atomic normal add instruction read data memory add writes data back another core could modify memory location meantime causing corruption lock add instruction prevents happening guaranteeing entire addition atomic think hardware mutex rather traditional software mutex cause code stop wait instruction cycle rather way cost done within last level cache current intel cpu clock cycle lock prefix work arithmetic instruction one value time work one value need use instruction first read byte data make change want byte using attempt write change back memory changed meantime instruction fails set flag way know synchronization failed data would corrupted must back try size two pointer allows modify two pointer atomically pointer plus integer feature called compareandswap two number basis lot lockfree stuff described intel new haswell processor shipping extends model region memory next feature called transactional memory make good fast scalable synchronization must easier future want mess around assembly language especially since want code run arm therefore compiler let access instruction builtin function gcc example function syncfetchandadd syncboolcompareandswap work well arm microsoft similar intrinsics compiler atomics act one thing time practice need something complex example might cpu core trying work hash table inserting thing removing thing even resizing entire table time without requiring core stop wait another finish general term go lockfree write hashtables linkedlist data structure instead simply use library created people also link large subsystem incorporate lockfree inside good example heap malloc standard microsoft heap global mutex really sap performance multicore code replace lockfree heap simply linking another library thing tend cross platform afraid unless willing study problem entirety like crypto people tend make nave mistake one example aba problem compareandswap like cmpxchg instruction mentioned sometimes value change change back thus think nothing else changed another example weakstrong memory model problem lockfree code might work fail arm get desire write lockfree algorithm google issue otherwise bite synchronization biggest issue thread scalability concern well go multicore divide application across multiple core two fundamental way pipelining workerthreads pipeline model thread different task hand task next thread pipeline worker model thread carry task course combine two model might equal worker thread stage pipeline tradeoff approach pipeline approach lot synchronization overhead pas job one thread next worker thread anything shared among thread becomes synchronization bottleneck thus shared resource want split stage pipeline thread work independently without sharing something want peer worker thread consider multicore id intrusion detection system like snort example first stage pulling packet network adapter analyzed shared resource among thread hence synchronization bottleneck might therefore want split pipeline stage one thread read packet dispatch packet worker thread likewise another shared resource table tcp control block tcb real world intel network card solves problem network card preprocesses tcp packet hash ipport info based info dispatch packet different queue popular opensource singlethreaded snort application exploit running wholly separate process queue thus entire application multicore even though singlethreaded using pipeline model one thread running inside network adapter split traffic queue worker process process packet find fascinating snort probably stupid idea make classically singlethreaded program multithreaded program need share data need share data create shared memoryregion using pagetables multiple process use take example packet counter example snort process open packet counter sharedmemory region using memorymappingpagetable feature operating system would allow another process read packet counter individual processor sum together report combined packet counter process word redesigned multithreaded snort would put lot structure thread local storage anyway better design multiprocess snort go direction move stuff shared memory mapped region among process fundamentally thing especially linux processesthreads equivalent anyway trying show multicore automatically mean multithreaded snort singlethreaded multicore product actually use memorymapping share data among process therefore lack feature probably future mention snort also good example playing around linux feature theory snort act ip inline network traffic good traffic forwarded bad traffic blocked practice bad idea bad idea linux kernel switch packet processing thread millisecond cause enormous jitter problem snort want happen way fix snort jitter issue change linux boot parameter example set cause linux use first two cpu system sure know cpu core exist never default schedule thread run code call pthreadsetaffinitynp function call put thread one inactive cpu snort configuration option per process long manually put one thread per cpu never interrupted linux kernel schedule two thread cpu interruption happen thus configure snort run dedicates snort lot jitter ip mode go away still get hardware interrupt though interrupt handler really short probably exceed jitter budget tweak well go procirqsmpaffinity turn interrupt snort processing thread point little hazy precisely happens think happen thread interrupted even clock cycle need test using rdtsc counter see exactly thread might interrupted even interrupted good le jitter since cache miss microsecond jitter low practically get course moment use pthreadmutext code synchronization get context switch throw entire jitter budget window even scheduled cpu correctly conclusion overall theme talk impress upon audience order create scalable application need move code operating system kernel need code everything instead letting kernel heavy lifting shown post applies thread synchronization basic design one thread per core lockfree synchronization never cause thread stop wait specifically tried drill idea people call multithreaded coding multicore multithreaded technique like mutexes scale multicore conversely snort demonstrates split problem across multiple process instead thread still multicore code
615,Lobsters,scaling,Scaling and architecture,Netflix Queue: Data migration for a high volume web application,http://techblog.netflix.com/2013/02/netflix-queue-data-migration-for-high.html,netflix queue data migration high volume web application,netflix queue data migration high volume web application netflix queue previous implementation aws cloud simpledb queue rps data size goal data migration forklift b incremental replication c consistency checker shadow writes e shadow writes shadow read validation f end life simpledb life netflix http jobsnetflixcom,netflix queue data migration high volume web applicationby prasanna padmanabhan shashi madappathere come time life system serving data need migrate data reliable scalable high performance data store maintaining improving data consistency latency efficiency document explains data migration technique used netflix migrate user queue data two different distributed nosql storage systemswhat netflix queuethe netflix queue let keep maintain list movie tv show want watch device computersprevious implementationnetflix embrace service oriented architecture soa composed many small fine grained service one thing one thing well vein queue service used fetch maintain user queue every netflix user list ordered video meta data related video added queue persisted aws cloud simpledb source truth data simpledb sharded across multiple domain similar rdbms table performance scalability purpose queue data used display purpose well influence personalization rankingqueue rps data sizefollowing graph show rps served queue service max rps total million record data store total size queue service originally designed simpledb good solution however since kept pace subscriber growth term sla cost effectiveness goal migrate data simpledb following requirement high data consistencyhigh reliability availabilityno downtime read writesno degradation performance existing applicationafter careful consideration running various performance benchmark decided use cassandra new data store queue service suited well high volume low latency writes requirement read primarily accessed keyvalue lookupsdata migrationmigrating data eventually consistent data store cassandra high volume low latency application verifying consistency multi step process involves one time data forklifting applying change incrementally could error case incremental update successfully applied reason timeouts throttling data store temporary node unavailability etc running end end consistency checker validating data shadow read helped u better evaluate consistency migrated data following section elaborate step taken end life simpledb queue serviceour migrator code base configured run one three mode viz forklift incremental replication consistency checkera forkliftthe first step process data migration forklift data source data store target data store mode current snapshot source data store copied entirety target data store simpledb throttle request rps domain greater certain threshold value impose fairness user system hence imperative put much load simpledb domain migration would affect sla requirement existing queue service depending data size throttling source data store latency requirement migration choose number parallel instance number worker thread within instance perform forklifteach migrator thread worked different data set within domain avoid migrating data multiple time based configured number thread migrator automatically chose different data set thread migrator also time aware pause thread execution peak hour production traffic continues forklifting nonpeak hour migrator instance state forklifting related thread persisted periodically thus instance forklift application terminates could resume migration stoppedforklift ran initial step migration process look little hour forklift entire data setb incremental replicationthis phase started forklift completed stage update user queue still sent simpledb migration code run incremental replication mode cassandra sync update happened forklifting mode instead copying data simpledb data changed since previous incremental replication run copied cassandrawe attribute called lastupdatedts simpledb get updated every mutation attribute indexed get better performance fetching source record updated since last run soft deletes delete marker set simpledb mode would able handle hard deletes migration code mode run continuouslyc consistency checkerat stage incremental replication continuously running however could error case incremental update successfully applied cassandra reason timeout throttling simpledb temporary node unavailability etc circumvent case ran end end consistency checker mode similar forklift except instead blindly copying source data compared data source target data store updated target data store record mismatched kept track number mismatch run related meta data record mismatched migration code run continuously even moded shadow writesfollowing step taken chronological order update queue service use cassandra eventually end life simpledbreads simpledb source truth writes simpledb cassandraat stage updated queue service shadow writes cassandra source truth read still simpledb every user request update queue earlier used update simpledb additional asynchronous request update cassandra submitted kept track number successfulunsuccessful update cassandra unsuccessful update would eventually fixed incremental replicator consistency checker like every project netflix make sure cassandra cluster could handle production write traffic rolled feature incrementally starting user eventually user gave u good indication cassandra write latency made source truthe shadow writes shadow read validationreads simpledb source truth cassandrawrites simpledb cassandraonce shadow writes incremental replication consistency checker running next step shadow read source truth read still continued simpledb stage every user request fetch user queue additional asynchronous request fetch queue cassandra submitted asynchronous request completed queue data returned simpledb cassandra compared kept track number request data store mismatched mismatched record would eventually fixed incremental replication consistency checker make sure cassandra cluster could handle production read traffic rolled feature incrementally shadow read traffic also helped u evaluate performance cassandra read latency production traffic patternsf end life simpledbreads cassandra source truth writes cassandrawithin short span time minimal data mismatch found shadow read incremental replication consistency checker stage flipped flag make cassandra source truth request fetch user queue synchronously retrieved cassandra update queue written cassandra simpledb finally laid rest peacelife netflixwhen started project requirement given u remove simpledb dependency u choose right persistence store chose cassandra designed correct data model one thing loved project speed executed way completely determined u made several code push every week production come huge responsibility make sure code well unit integration tested amazing see idea formed implemented pushed production short span timeif kind scalability problem coupled freedom responsibility enthuse looking senior software engineer product infrastructure team netflix working brightest mind industry visit http jobsnetflixcom get started
616,Lobsters,scaling,Scaling and architecture,Hardening node.js for production by using nginx to avoid load,http://blog.argteam.com/coding/hardening-node-js-for-production-part-2-using-nginx-to-avoid-node-js-load/,hardening nodejs production using nginx avoid load,silly face society article update part nginx expressjs gist nginx nodejs link upstream httpproxymodule static file intercept httpheadersmodule caching varnish memcached gzip httpgzipmodule ssl httpsslmodule wrapping draw something silly face society serbocroatian language webhostinggeekscom,part quasiseries hardening nodejs production system eg silly face society previous article covered process supervisor creates multiple nodejs process listening different port load balancing article focus http lighten incoming load nodejs process update also posted part zero downtime deployment setup stack consists nginx serving external traffic proxying upstream nodejs process running expressjs explain nginx used almost everything gzip encoding static file serving http caching ssl handling load balancing spoon feeding client idea use nginx prevent unnecessary traffic hitting nodejs process furthermore remove much overhead possible traffic hit nodejs much talk nginx config http proxycachepath varcachenginx proxytemppath vartmp include mimetypes defaulttype applicationoctetstream sendfile keepalivetimeout gzip gzipcomplevel gzipvary gzipminlength gzipproxied gziptypes textplain texthtml textcss applicationjson applicationxjavascript textxml applicationxml applicationxmlrss textjavascript gzipbuffers upstream sillyfacesocietyupstream server server keepalive server listen listen ssl sslcertificate somelocationsillyfacesocietycombundlecrt sslcertificatekey somelocationsillyfacesocietycomkey sslprotocols sslciphers high anull servername sillyfacesocietycom wwwsillyfacesocietycom host sillyfacesocietycom rewrite http wwwsillyfacesocietycom permanent errorpage location imagesimgjavascriptjscssstylesheetsflashmediastaticrobotstxthumanstxtfaviconico root usrlocalsillyfacesocietynodepublic accesslog expires max location error internal alias usrlocalsillyfacesocietynodepublicerrors location proxyredirect proxysetheader xrealip remoteaddr proxysetheader xforwardedfor proxyaddxforwardedfor proxysetheader xforwardedproto scheme proxysetheader host httphost proxysetheader xnginxproxy true proxysetheader connection proxyhttpversion proxycache one proxycachekey sfs requesturi scheme proxypass http sillyfacesocietyupstream also available gist perhaps code dump particularly enlightening try step config give pointer balance expressjs code nginx nodejs link first thing first get nginx proxy load balance traffic nodejs instance assume running two instance expressjs port take look upstream section http upstream sillyfacesocietyupstream server server keepalive upstream directive specifies two instance work tandem upstream server nginx keepalive directs nginx keep minimum connection proxy server given time true minimum traffic nginx open connection proxy upstream alone sufficient nginx need know route traffic node magic happens within server section scrolling bottom location section like http server location proxyredirect proxysetheader xrealip remoteaddr proxysetheader xforwardedfor proxyaddxforwardedfor proxysetheader host httphost proxysetheader xnginxproxy true proxysetheader connection proxyhttpversion proxypass http sillyfacesocietyupstream section fallthrough traffic matched rule nodejs handle traffic nginx proxy response important part section proxypass tell nginx use upstream server defined higher config next line proxyhttpversion tell nginx use connection proxy server using spare overhead establishing connection nginx nodejs every proxied request significant impact response latency finally couple proxysetheader directive tell expressjs process proxied request direct one full explanation found httpproxymodule doc part config minimum amount needed get nginx serving port proxying nodejs process underneath rest article cover use nginx feature lighten traffic load nodejs static file intercept although expressjs built static file handling connect middleware never use nginx much better job handling static file prevent request nondynamic content clogging node process location directive question http server location imagesimgjavascriptjscssstylesheetsflashmediastaticrobotstxthumanstxtfaviconico root usrlocalsillyfacesocietynodepublic accesslog expires max request uri starting image img cs j matched location expressjs directory structure public directory used store static asset thing like cs javascript like using root instruct nginx serve file without ever talking underlying server expires max section caching hint asset immutable site may appropriate use quicker cache expiry something like expires full information nginx httpheadersmodule caching opinion caching better caching site extremely heavy traffic use kind caching solution including varnish http acceleration memcached fragment caching query caching site hightraffic caching still going save u fortune server cost simplicity configuration decided use nginx builtin caching nginx built caching crude upstream server provides http header hint like cachecontrol enables caching expiry time matching header hint within expiry time next request pull cached file disk instead hitting underlying nodejs process set caching set two directive http section nginx config http proxycachepath varcachenginx proxytemppath vartmp two line instruct nginx going use caching mode proxycachepath specifies root directory cache directorydepth level maxsize cache inactive expire time importantly specifies size inmemory key file keyszone nginx receives request computes hash us key set find corresponding file disk available request hit underlying nodejs process finally make proxied request use cache change location section include caching information http server location proxycache one proxycachekey sfs requesturi scheme instructs nginx use one keysset cache incoming request hash computed using proxycachekey one miss expressjs serving proper http cache hint header wrote quick piece middleware provide functionality cachemiddleware cachemiddleware second req re next ressetheader cachecontrol public maxage second next appropriate apply middleware globally certain request eg post request affect server state never cached use perroute basis expressjs app appget recent cachemiddleware req re next someone hit recent nginx cache minute gzip gzip nobrainer http compressing incoming request client spend le time hogging server everyone save money bandwidth could use expressjs middleware handle gzipping outgoing request nginx better job leave expressjs resource enable gziped request nginx add following line http section http gzip gzipcomplevel gzipvary gzipminlength gzipproxied gziptypes textplain texthtml textcss applicationjson applicationxjavascript textxml applicationxml applicationxmlrss textjavascript gzipbuffers go detail directive like caching gzip better gzip control thousand microoptimizations perform documented well nginx httpgzipmodule ssl continuing theme leaving nodejs handle basic http arrive ssl long upstream server within trusted network make sense encrypt traffic nginx nodejs serve http traffic nginx encrypt setup easy configure server directive tell nginx configure ssl http server listen ssl sslcertificate somelocationsillyfacesocietycombundlecrt sslcertificatekey somelocationsillyfacesocietycomkey sslprotocols sslciphers high anull listen tell nginx enable ssl traffic sslcertificate sslcertificatekey tell nginx find certificate server sslprotocols sslciphers line instruct nginx serve traffic detail full configuration option available nginx httpsslmodule almost configuration get nginx decrypting traffic proxying unecrypted request upstream server however upstream server may need know whether secure context used serve sslenabled asset cdns like cloudfront reject request come unencrypted add following line location section http server location proxysetheader xforwardedproto scheme send http header hint nodejs process whipped bit middleware make ssldetection little bit easier appuse req re next reqforwardedsecure reqheaders xforwardedproto http next within route reqforwardedsecure true iff nginx handling http traffic reference silly face society us ssl facebook authentication token exchange user log using sso single sign phone extension implementing also threw secure version site wrapping phew covered set nodejs upstream server nginx lighten load upstream server letting nginx handle load balancing static file ssl gzip caching caveat emptor silly face society launched yet configuration based personal testing research reached productionlevel traffic yet anyone still reading welcome suggestion improvement comment incidentally use expressjs http traffic accident started using nodejs provide socketserver real time party mode silly face society got close launching decided add draw somethingesque passive mode silly face society could launch inertia successful instead rewriting technology stack reused much old code possible exposed http interface scratch would reevaluate choice nodejs hard beast tame crudapps interest nginx align interest silly face try silly face society iphone article also translated serbocroatian language anja skrba webhostinggeekscom
617,Lobsters,scaling,Scaling and architecture,Handling Growth with Postgres: 5 Tips From Instagram,http://instagram-engineering.tumblr.com/post/40781627982/handling-growth-with-postgres-5-tips-from-instagram,handling growth postgres tip instagram,le year ago sharding id instagram partial index functional index pgreorg compaction mvcc pgreorg wale wal archiving backup wale available github autocommit mode async mode waitcallback psycogreen hacker news,scaled instagram evergrowing number active user postgres continued solid foundation canonical data storage data created user le year ago blogged stored lot data instagram like per second pushing like per second fundamental storage technology changed last two half year picked tip tool scaling postgres wanted wish knew first launched instagram postgresspecific others present database well background horizontally partitioned postgres check sharding id instagram partial indexesif find frequently filtering query particular characteristic characteristic present minority row partial index may big winas example searching tag instagram try surface tag likely many photo use technology like elasticsearch fancier search application one case database good enough let see postgres searching tag name ordering number photo explain analyze select id tag name like snow order mediacount desc limit query plan limit actual sort actual sort key mediacount sort method topn heapsort memory index scan using tagssearch tagstag actual index cond name text snow text name text snox text filter name text snow text total runtime m row notice postgres sort row get right result since tag example exhibit longtail pattern instead first try query tag photo create index concurrently tag name textpatternops mediacount query plan look like explain analyze select tag name like snow mediacount order mediacount desc limit query plan limit actual sort actual sort key mediacount sort method topn heapsort memory index scan using tagstagnameidx tagstag actual index cond name text snow text name text snox text filter name text snow text total runtime m row notice postgres visit row way faster postgres query planner pretty good evaluating constraint later decided wanted query tag photo since subset index still use right partial functional indexeson table need index string example character token quite long creating index string end duplicating lot data postgres functional index feature helpful create index concurrently token substr token multiple row match prefix postgres match prefix filter quick resulting index size would indexed entire pgreorg compactionover time postgres table become fragmented disk due postgres mvcc concurrency model example also time row insertion order match order want row returned example often querying like created one user helpful like contiguous disk minimize disk seeksour solution use pgreorg process compact table acquire exclusive lock tablecreate temporary table accumulate change add trigger original table replicates change temp tabledo create table using select create new table index order disksync change temp table happened select startedcut new tablethere detail around lock acquisition etc general approach vetted tool tried several test run running production run dozen reorgs across hundred machine without wale wal archiving backupswe use contribute code wale heroku toolkit continuous archiving postgres writeahead log file using wale simplified backup newreplica bootstrap process significantlyat core wale program archive every wal file generated pg server amazon using postgres archivecommand wal file used combination base backup restore db point since base backup combination regular base backup wal archiving mean quickly bootstrap new readreplica failover slave toowe made simple wrapper script monitoring repeated failure archive file available autocommit mode async mode time started using advanced feature python driver postgresthe first autocommit mode mode issue begincommit query instead every query run singlestatement transaction particularly useful readonly query transaction semantics needed easy connectionautocommit true lowered chatter application server db significantly lowered system cpu well database box since use pgbouncer connection pooling change allows connection returned pool soonermore detail interacts django db handling hereanother useful feature ability register waitcallback coroutine support using allows concurrent querying across multiple connection useful fanout query hit multiple socket wake notify data read use python select module handling wakeups also play well cooperative multithreading library like eventlet gevent check psycogreen example implementationoverall happy postgres performance reliability interested working one world largest postgres installation small team infrastructure hacker get touch infrajobs instagramcomyou discus post hacker newsmike krieger cofounder
618,Lobsters,scaling,Scaling and architecture,Optimizing the Netflix API,http://techblog.netflix.com/2013/01/optimizing-netflix-api.html,optimizing netflix api,optimizing netflix api embracing difference goal reduce chattiness redesign distribute api development mitigate deployment risk support multiple language distribute operation architecture dynamic endpoint endpoint code repository management dynamic polyglot jvm language runtime asynchronous java api reactive programming model rx observables hystrix fault tolerance previous post recently open sourced dashboard backend service dependency summary always looking september update see also,optimizing netflix apiby ben christensenabout year ago netflix api team began redesigning api improve performance enable ui engineering team within netflix optimize client application specific device philosophy redesign introduced previous post embracing difference different client devicesthis post part one series architecture redesigned apigoalswe multiple goal creating system follows reduce chattinessone key driver pursuing redesign first place reduce chatty nature clientserver communication could hindering overall performance device implementation due generic granular nature original restbased netflix api call return portion functionality given user experience requiring client application make multiple call need assembled order render single user experience interaction model illustrated following diagram reduce chattiness inherent rest api discrete request diagram collapsed single request optimized given client benefit device pay price wan latency leverage low latency powerful hardware serverside side effect also eliminates redundancy occur every incoming requesta single optimized request must embrace serverside parallelism least level previously achieved multiple network request client serverside parallelized request running network one performant executed device must achieved without engineer implementing endpoint needing become expert lowlevel threading synchronization threadsafety concurrent data structure nonblocking io concernsdistribute api developmenta single team become bottleneck need expertise every client application create optimized endpoint rapid innovation fast decoupled development cycle across wide variety device type distributed ownership expertise across team enabled client application team capable implementing operating endpoint corresponding requestsresponsesmitigate deployment risksthe netflix api java application running hundred server processing billion incoming request day million customer around world system must mitigate risk inherent enabling rapid frequent deployment multiple team minimal coordinationsupport multiple languagesengineers implementing endpoint come wide variety background expertise including javascript objectivec java c c ruby python others system able support multiple language timedistribute operationseach client team manage deployment lifecycle web service endpoint operational tool monitoring debugging testing canarying rolling code must exposed distributed set team team operate independentlyarchitectureto achieve goal architecture distilled key point dynamic polyglot runtimefully asynchronous service layerreactive programming modelthe following diagram subsequent annotation explain architecture dynamic endpointsall new web service endpoint dynamically defined runtime new endpoint developed tested canaried deployed client team without coordination unless depend new functionality underlying api service layer shown item case would need wait change deployed pushing endpoint endpoint code repository managementendpoint code published cassandra multiregion cluster globally replicated via restful endpoint management api used client team manage endpoint dynamic polyglot jvm language runtimeany jvm language supported team use language best suited groovy jvm language chosen first supported language existence firstclass function closure listdictionary syntax performance debuggability aspect decision moreover groovy provides syntax comfortable wide range developer help reduce learning curve first language platform asynchronous java api reactive programming modelembracing concurrency key requirement achieve performance gain abstracting away threadsafety parallel execution implementation detail client developer equally important reducing complexity speeding rate innovation making java api fully asynchronous first step allows underlying method implementation control whether something executed concurrently without client code changing chose reactive programming model functional programming style handling composition conditional flow asynchronous callback implementation modeled rx observables hystrix fault toleranceas described previous post service call backend system made via hystrix fault tolerance layer recently open sourced along dashboard isolates dynamic endpoint api service layer inevitable failure occur executing billion network call day api backend systemsthe hystrix layer inherently mutltithreaded due use thread isolating dependency thus leveraged concurrent execution blocking call backend system asynchronous request composed together via reactive framework backend service dependenciesthe api service layer abstract away backend service dependency behind facade result endpoint code access functionality rather system allows u change underlying implementation architecture limited impact code depends api example backend system split different service combined one remote network call optimized inmemory cache none change affect endpoint code thus api service layer ensures object model tightcouplings abstracted allowed leak endpoint codesummarythe new netflix api architecture significant departure previous generic restful api dynamic jvm language combined asynchronous java api reactive programming model proven powerful combination enable safe efficient development highly concurrent codethe end result faulttolerant performant platform put control hand know target application bestfollowing post provide implementation operational detail new architectureif type work interest always looking talented engineersseptember updatethis blog post originally used term functional reactive programming frp term used error rxjava implement continuous time requirement frp previous literaturesee also
620,Lobsters,scaling,Scaling and architecture,Just how big are porn sites? (SFW),http://www.extremetech.com/computing/123929-just-how-big-are-porn-sites,big porn site sfw,term use real world figure youporn scale infrastructure store case redis nginx connectivity realworld number youporn,site may earn affiliate commission link page term use truth universally acknowledged person possession fast internet connection must want pornwhile difficult domain penetrate hard number far know fact porn site trafficked part internet according google doubleclick ad planner track user across web cookie dozen adult destination populate top website xvideos largest porn site web billion page view per month three time size cnn espn twice size reddit livejasmin much smaller youporn pornhub vast vast site dwarf almost everything except google facebooks internetwhile page view fine starting point tell x porn site popular nonporn site four billion page view sure sound like lot factor porn surfer actually size scale adult website truly come focuswe start laying ground work second page real world figure youporn second largest porn site web like take moment try estimate amount traffic youporn handle every second let u know comment guess anywhere nearscalethe main difference porn nonporn site average duration visit news site like engadget extremetech average visit usually three six minute enough time read one two story average time spent porn site however minutesthen need factor website predominantly text image largest porn site push streaming video load extremetech home page talking couple megabyte maybe kilobyte load article stream porn assuming low resolution looking around kilobyte per second minute around megabytesthen need multiply megabyte number monthly visit around million xvideos come around petabyte data transferred every month gigabyte per second put comparison home internet connection probably capable transferring couple megabyte per second time smallerin short porn site cope astronomical amount data site really come close term raw bandwidth youtube hulu even youporn something like six time larger huluinfrastructureserving video requires lot resource plain text image term storage cpu cycle internal io bandwidth obviously varies site site adult site probably store region terabyte porn quite lot website something like google facebook blogger youtube would store data world drive cheap plentiful ultimately large amount last year wrote backblaze storage pod store case cycle io function bitrate streaming video number page view first porn site serve dynamic searchable database thousand video someone click video file need read hard disk streamed internet ever transferred lot big file local network ie stressed hard drive ethernet port know taxing actual hardware requirement almost impossible derive publicized case large porn site probably talking rack quadcpu server gigabit switch load balancer softwarewise large porn site use veryhighthroughput database redis store serve video lightweight http server like nginx serve web pagesfinally bandwidth referring back xvideos example based ad planner estimate large porn site enough connectivity serve gigabyte per second bear mind average data rate peak time xvideos might burst put perspective connectivity london new yorkthere many way coping much traffic set data center rent rack large data center use cloud provider like amazon aws microsoft azurenext page realworld number youporn
622,Lobsters,scaling,Scaling and architecture,Big Data and the US Presidential Campaign - YouTube,https://www.youtube.com/watch?v=X1tJAT7ioEg,big data u presidential campaign youtube,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature,press copyright contact u creator advertise developer term privacy policy safety youtube work test new feature
623,Lobsters,scaling,Scaling and architecture,Speeding up PHP-based development with HipHop VM,https://www.facebook.com/notes/facebook-engineering/speeding-up-php-based-development-with-hiphop-vm/10151170460698920,speeding phpbased development hiphop vm,announced performance wwwfacebookcom development environment open source public github repository getting wordpress running hhvm improving jit hhbc facebookcom wwwfacebookcom itlb road ahead find hhvm open source project hhvmcom related article link,facebook hiphop team constantly strives improve efficiency php execution increase productivity php developer late announced pursuing justintime jit compilation approach called hhvm successor compiler called hphpc goal hhvm project twofold build productionready virtual machine delivers superior performance unify production development environmentsbelow update state hhvm followed deep dive detail hhvm architecture optimization strategiesperformance year pushed make hhvm efficient hphpc massive php code base hphpc tough act follow lot effort put optimizing performance delivered significant efficiency gain year matching hphpc performance difficult requirement satisfy hhvm could used serve production traffic scalewe happy announce hhvm fast hphpc hhvm dynamic translation technique aka jit extremely motivated team key winning race chart show efficiency hhvm relative hphpc past month generating home page wwwfacebookcom hhvm performance overtaking hphpc continuing upward trajectory hiphop team gearing migrate production machine hphpc hhvmthe development environmentprior hhvm development environment call sandbox used custombuilt php interpreter called hphpi shortcut long slow hphpc compilation cycle provide rapid edit save run development workflow hphpi flexible slow slower zend engine hphpc replaced developer around like move fast starting see hphpi strain load complex product feature like timeline tickerwe deployed hhvm sandbox reduced page load time compared hphpi keeping rapid workflow hphpi provided hhvm fully deployed production achieved goal unifying production development environment enable php developer debug tune iterate system run production also allow hiphop team focus improving single system benefit environment simultaneouslyopen source sharing benefit php development stack community important rebooted open source effort july resumed regularly updating public github repository targeted ubuntu precise centos start planning add support platform including mac o x futurewhile efficiency facebook code base priority right started investing making sure able correctly run popular php code base hhvm run latest version wordpress see getting wordpress running hhvm proudly using serve new team blog making sure hhvm able run common php application framework hoping make useful broader php community outside facebook given hhvm support development deployment workflow similar zend php stack think hhvm important step forward reducing friction adoptionimproving jit working make jit efficient faced interesting challenge traditional profiling optimization technique always sufficient improving complex software system virtual machine finding gain hard work required lot experiment many produce positive result time result surprising frustratinghere thing improve jit instruction set coverage hhvm run php program converting php source bytecode hhbc executing bytecode using bytecode interpreter jit compiler seamlessly interoperate jit used possible interpreter used execution engine last resort end jit supported common bytecode instruction input type long tail le common instruction type caused jit transfer control interpreter long tail limiting hhvm performanceduring added jit support many le common instruction input type notably implemented full jit support member instruction instruction accessing array element object property php semantics element access property access complex design member instruction reflects complexity php evaluation order expression involving chain element property access fit nicely hhbc stack model member instruction crafted chain element access property access executed single member instruction furthermore behavior element property expression depends context hhbc several different member instruction accommodate different context example statement x f g j k converted bytecode like fpushfuncd f fcall unboxr fpushfuncd g fcall unboxr fpushfuncd j fcall unboxr fpushfuncd k fcall unboxr cgetm ec ec setm ec ec popcthe first instruction evaluate expression f g j k cgetm instruction offset performs element access righthand side setm instruction offset performs element access lefthand side setm instruction performs assignmentsince hhbc different member instruction one encode arbitrarily long chain element property access adding full jit support instruction essentially required writing minitranslator could handle combination element property access context member instruction broken series simpler operation machine code generated operation passing intermediate result one operation nextall told implementing full jit support member instruction reduced cpu time still room optimization areaside exit hhvm jit us tracebased form compilation limit trace single basic block input type known translation time tracelet approach pushed make jit robust enough use developer sandbox came across tricky bug challenged jit singleentrysingleexit modelconsider following example suppose jit need generate translation line function dosomething val extra global base extra null return base val tracelet approach work observing type local beginning line generating specialized code based type local like tracelet begin guard code shown yellow make sure base val integer entering body shown green one might assume check would sufficient lifetime tracelet many simple operation php capable side effect affect correctness safety example assignment operation line may invoked destruct method turn may changed type base mean safely assume base still integer line dosomething function contrived example encountered realworld instance bug variety form needed way fix preferably without hurting performance decided address issue enhancing jit capable generating side exit like idea behind side exit allow execution bail tracelet part way rare condition arises type aliased local changing conceptually jit still limit trace single basic block main exit usually taken also capable generating side exit handle uncommon casestype prediction return type implementing side exit mechanism sought opportunity could leverage side exit make tracelets longer thereby reducing performance overhead tracelet boundary extending cpu mechanical sympathy effect longer chain continuous code execution basic block contained several php method call noticed jit often generated several short tracelets return type method call known translation time consider following example php code abstract class c function addtwonumbers return one two hhbc fpushobjmethodd one fcall unboxr fpushobjmethodd two fcall unboxr add retcsuppose one two return integer run time return type known translation time translating addtwonumbers method original implementation would generate series tracelets shown figure belowas experiment modified hhvm profile method call return type first web request running bytecode interpreter modified jit make conservative type prediction based profile information using side exit handle case prediction fail using type prediction jit generate single tracelet cover entire body addtwonumbers shown figure type prediction function call method call return type produced reduction cpu time running facebookcom sitechanging parallel tracelets link together one core idea behind jit tracelet approach ability generate multiple tracelets given source location accommodate different input type multiple tracelets generated source location referred parallel tracelets linked chain via conditional branchesin original implementation new parallel tracelet generated linked head chain example suppose simple function foo x called twice passing integer first time passing string second time tracelet generated first invocation tracelet generated second invocation b would linked together chain like experiment tried changing jit link new parallel tracelets end chain instead linking head new scheme tracelets b would chain together like linking new parallel tracelets end chain produced unexpectedly large reduction cpu time approximately time ran experiment surprised result digging hardware counter hhvm internal counter understand win came fromhhvm counter track many time tracelet guard entered many time tracelet body entered dividing latter former compute tracelet success rate generating home page wwwfacebookcom linking new tracelets head chain produce tracelet success rate linking end chain produce tracelet success rate help explain change produce win fully explain large effectthe hardware counter give insight happening processor level many hardware counter showed improvement drop itlb miss really stood lower tracelet success rate mean processor performing jump across different page memory put pressure itlb increase miss rate itlb miss cause processor walk page table called page walk page walk particularly computeintensive large application like facebook site page table big exceeds capacity lastlevel cache llc make sense cutting itlb miss half would large effect cpu timethis surprising win inspired u spend energy looking llc miss code locality hardware resource issue see low hanging fruit areathe road aheaddeploying hhvm production end goal rather mean continue improving performance php code base going forward hhvm enables u pursue broader range optimization strategy php development stack runtime base library apis baremetal machine code generate plan leverage flexibility make facebook web tier even efficient coming yearswe also excited continue improving hhvm development experience making installation development workflow easier flexible supporting wider range popular php application frameworksfind hhvmhhvm open source project developed facebook latest news hhvm find u hhvmcomrelated article linksdrew paroski software engineer hiphop team facebook
625,Lobsters,scaling,Scaling and architecture,Systems Programming at Twitter,http://monkey.org/~marius/talks/twittersystems,system programming twitter,system programming twitter facebook october history lesson twitter evolves late architecture system software stack programming datacenter language tool concurrent system desideratum detour little scala scala value static typing inference function value value object value object method container polymorphic pattern matching composition composition combinators flatmap flatmap gratuitous example concurrent programming future future future applying hollywood principle promise composition motivating example thumbnail extractor flatmap rescue getthumbnail flatmap failure combining many future collect simple web crawler functional style modular decomposition service service simple service filter filter sketch example filter filter stackable filter typesafe finagle rpc system finagle client server proxy putting together observability diagnostics observability diagnostics stats viz tracing githubcomtwitterzipkin zipkin profiling pprof web practice good practice ugly aside thrift githubcomtwitterscrooge open source githubcomtwitter githubcomtwitterutil githubcomtwitterfinagle scala school effective scala,system programming twitter facebook october marius eriksen twitter inc press space enter navigate next slide left arrow go backwards history lesson twitter evolves pure rubyonrails app mysql lot memcache materialized timeline memcaches social graph moved service delayed work queue starting move timeline serving system project godwit started move twitter ruby rail work begin shared serving infrastructure infrastructure matures ton work put porting application tfe go online tfe serf traffic year api traffic served new stack new work happens context work late architecture many open source component memcache redis mysql etc necessarily heterogeneous organized around service distinct responsibility isolated distributed computation data rpc system multiplexing http frontend crucial modularity load balancing system software stack programming datacenter concern include partial failure deep memory hierarchy split heap dynamic topology change variance latency tail heterogeneous component operator error taming resulting complexity central theme work language tool mismatch world view language tool target single computer application demand simultaneous use datacenter language designed serial execution world inherently concurrent concurrent system source concurrency world know client overlapping schedule highcapacity system efficient server must handle request simultaneously fanout fanin coordination server also client desideratum yet magic scaling sprinkle must program locally communicate globally clean concurrent programming model simple safe modular program uniform programming model message passing architecture high concurrency fault tolerant performant protocol support largely agnostic support http thrift memcache redis mysql observability diagnostics profiling optimization detour little scala scala use scala heavily system work hybrid fpoo language expressive statically typed rich type system run jvm interoperates java going introduce enough language grasp following example point language idea technique matter scala happens language value val int val string hello world static typing inference val map one two scala compiler work u using integer string must map int string function value val f int int x x f value object val itostring tostring true true scala pure object oriented language every value object multitude method invocation syntax value object method case class stock ticker string price double def stock price otherprice val goog stock goog val aapl stock aapl aapl goog true goog aapl false container polymorphic val stock seq stock goog stock aapl typed seq stock known c template new trick type class stockssorted seq stock aapl work ordering defined stock pattern matching important tool writing declarative program val newstate state match case idle busy case busy n busy compiler provide exhaustiveness check u guaranteeing function total every time see case partial function composition algebra learned two function g f compose h shorthand h x g f x scala pay attention type val f int string val g string float val h int float g compose f g compose f shorthand like algebra composition took two thing f g combine together make new thing h call widget combinators combinators probably use every day instance collection val l seq val x x val l map l map x x seq flatmap flatmap combinator versatile tool trait seq def flatmap b f seq b seq b name suggest combination map flatten def flatmap b f seq b map f flatten flatmap expanding seq flatmap x seq x x seq conditionals seq flatmap x x seq x else seq seq gratuitous example flatmap sufficiently important syntax sugar eg lazily compute pythagorean triple z streamfrom x streamrange z streamrange x z x x z z yield x z concurrent programming future future placeholder result usually computed concurrently long computation network call reading disk computation fail connection failure timeout div zero future represent concurrent execution future kind container empty full failed wait val f future val result f failure would result exception prefer using try fget match case return re case throw exc applying hollywood principle call call val f future string f onsuccess loginfo onfailure exc logerror exc promise future readonly promise writable future val p promise int val f future int p success failure psetexception new myexc composition motivating example far shown nothing rephrasing callback see future compose trait webpage def imagelinks seq string def link seq string def fetch url string future webpage build style thumbnail extractor def getthumbnail url string future webpage thumbnail extractor def getthumbnail url string future webpage val promise new promise webpage fetch url onsuccess page fetch pageimagelinks onsuccess p promisesetvalue p onfailure exc promisesetexception exc onfailure exc promisesetexception exc promise yuck variant alltoofamiliar callbackhell flatmap rescue want def getthumbnail url string future webpage must first fetch page find first image link fetch image link either operation fail getthumbnail also fails starting smell like flatmap trait future def flatmap b f future b future b getthumbnail flatmap def getthumbnail url string future webpage fetch url flatmap page fetch pageimagelinks failure compose well must recoverable flatmap need dual whereas flatmap operates value rescue operates exception trait future def rescue b f exception future b future b recovering error val f fetch url rescue case connectionfailed fetch url combining many future collect object future def collect f seq future future seq useful fanout operation eg fetch thumbnail def getthumbnails url string future seq webpage fetch url flatmap page futurecollect pageimagelinks map u fetch u simple web crawler def crawl url string future seq webpage fetch url flatmap page futurecollect pagelinks map u crawl u map pps ppsflatten apocryphal functional style emphasize declare meaning computation prescribe computed semantics liberated mechanic enhances modularity simple alter implementation without affecting existing code modular decomposition service service seen use future concurrent programming see network programming fit picture rpc dispatch request wait succeeds fails function type service req rep req future rep server implement client make use simple service server val multiplier futurevalue client multiplier onsuccess re println result r filter many common behavior service agnostic particular service common one retries timeouts exception handling stats filter compose service conceptually want alter behavior service agnostic service filter sketch val timeout filter val service service req rep val servicewithtimeout timeout andthen service filter function type filter reqin service reqout repin future repout example filter given request service dispatch timeout second val timeout req service service req within attempt authenticate request dispatching succeeds val auth req service isauth req service req else futureexception autherr filter stackable val timeout filter val auth filter val service service timeout andthen auth andthen service filter typesafe service requires authenticated request val service service authreq rep bridge filter val filt filter httpreq httprep authhttpreq httprep authenticate serve val authservice service httpreq httprep filt andthen service finagle rpc system finagle crude recap partial failure message passing future service finagle make possible client provide service server consume add behavior largely configurable load balancing connection pooling retrying timeouts rate limiting monitoring stats collection protocol agnostic codecs implement wire protocol manages resource client val client clientbuilder name loadtest codec http host build client service httpreq httprep client httprequest get server val service req futurevalue httpres codeok blah serverbuilder name httpd codec http bindto build service proxy val client clientbuilder serverbuilder client putting together recently wanted add speculative execution easy val backupreq filter req service val reqs seq service req timerdolater delay service req flatten futureselect reqs flatmap case return re seq othercancel futurevalue re case throw seq observability diagnostics observability diagnostics distributed environment standard tool loose efficacy difficult reason measure debugging process interaction vital stats liberally export stats important diagnostics optimization etc statsincr reqs statsaddmetric latencyms available curl host portstatstxt counter reqs metric latencyms viz tracing rpc tracing based dapper sigelman barroso et al tracerecord pc load letter tracetimefuture search rpc searchquery hello world tracing support introduced zero code change required user code zipkin trace aggregation system open source githubcomtwitterzipkin zipkin profiling situ cpu heap contention profilers incredibly useful optimization diagnostics curl host portpprofprofile pprof profile welcome pprof help type help pprof top total sample ljavalangobject lorgapachecommonscodecbinarybasencodec resizebuffer ljavautilarrays copyof ljavautilarrays copyofrange pprof web practice good emphasizing declarativedata flow style programming future combinators result robust modular safe simple system system big really finagle creates promise style programming also encouragesenforces modularity simple build higher level combinators eg generic hashring sharding stats speculative execution custom load balancer everything uniform lot implementor leeway tracing cancellation threadbiasing pool etc zero user code change practice ugly thing nt fit neatly cancellation hairy instance important clean separation sometimes troublesome eg always retry request different host layering never actually clean world messy abstraction result greater garbage collector pressure jvm good aside thrift custom thrift transport tracing soon request multiplexing service multiplexing compression upnegotiated fully compatible clientsservers thrift code generator scrooge githubcomtwitterscrooge produce scalaidiomatic structs service ifaces return future legacy generator future iface support open source true open source project lot external contribution tumblr foursquare stumbleupon serious deployment system software infrastructure githubcomtwittergithubcomtwitterutilgithubcomtwitterfinagle scala school effective scala
627,Lobsters,scaling,Scaling and architecture,What scaling has to do with language design,http://jazzy.id.au/default/2012/11/02/scaling_scala_vs_java.html,scaling language design,previous post online store something go wrong java scaling mean much excellent article java make asynchronous io simple java asynchronous io conclusion github repository spreadsheet comment powered disqus comment powered,previous post showed make sense benchmark scala java concluded saying come performance question asking scala help server falling unanticipated load post seek answer show indeed scala far better language building scalable system java however nt expect journey get easy start easy micro benchmark trying show real world apps nt handle load put hard hard create app small enough demo explain single blog post time big enough actually show real world apps behave load also hard simulate real world load going take one small aspect something might go wrong real world show one way scala help java wo nt explain tip iceberg far situation far feature scala help real world online store exercise implemented online store architecture store diagram see payment service search service store talk store handle three type request one index page nt require going service one making payment us payment service another searching store product list us search service online store part system going benchmarking implement one version java another scala compare search payment service wo nt change actual implementation simple json apis return hard coded value simulate processing time java implementation store going keep simple possible using straight servlets handle request apache common http client making request jackson json parsing formatting deploy application tomcat configure tomcat nio connector using default connection limit thread pool size scala implementation use play framework using play w api backed ning http client make request play json api backed jackson handle json parsing formatting play framework built using netty connection limit us akka thread pooling configured use default thread pool size one thread per cpu machine benchmark performing using jmeter request type index payment search thread spinning loop making request random pause request give average maximum throughput request per second per request type request per second let look result java benchmark graph plotted metric per request type median median request time index page next nothing search payment request also plotted line common metric web application show request give good idea slow request like show almost nothing index page search payment request final metric throughput show number request per second handled far theoretical maximum index showing request per second search payment request coming request per second result good java service handle load throwing without sweat let take look scala benchmark see identical java result surprising since java scala implementation online store absolutely minimal work code wise processing time going making request remote service something go wrong seen two happy implementation thing scala java shrugging load give happens thing nt fine dandy happens one service talking go let say search service start taking second respond point return error unusual failure situation particularly load balancing proxy proxy try connect service fails second giving gateway error let see application handle load throw would expect search request take least second respond others java result well longer happy app search request naturally taking long time payment service taking average second respond line second index page similarly impacted user going waiting long browsed site home page show throughput gone request per second good search service went whole site practically unusable soon start losing customer money scala app fair let find say anything else let point bounded response time search request actually taking second respond graph second next value hardly register line pixel high see search unusable payment index request response time throughput unchanged obviously customer nt going happy able search least still use part site see home page special even still make payment item hey google nt always use google search site might lose business impact limited benchmark see scala win hand thing start go wrong scala application take stride giving best java application likely fall java start bit counter many anticipated criticism people make benchmark first obvious one scala solution used asynchronous io whereas java solution nt ca nt compared true could implemented asynchronous solution java case java result would identical scala result however could done java developer nt ca nt nt written lot webapps java make call system rarely special circumstance ever used asynchronous io let show let say series call series remote service one depending data returned previous good old fashioned synchronous solution java user user getuserbyid id list order order getordersforuser useremail list product product getproductsfororders order list stock stock getstockforproducts product code simple easy read feel completely natural java developer write completeness let look thing scala val user getuserbyid id val order getordersforuser useremail val product getproductsfororders order val stock getstockforproducts product let look code time assuming making asynchronous call returning result promise look like java promise user user getuserbyid id promise list order order userflatmap new function user list order public promise list order apply user user return getordersforuser useremail promise list product product ordersflatmap new function list order list product public promise list product apply list order order return getproductsfororders order promise list stock stock productsflatmap new function list product list stock public promise list stock apply list product product return getstockforproducts product firstly code readable fact much harder follow massively high noise level actual code stuff hence easy make mistake miss thing secondly tedious write developer want write code look like hate developer want write whole app like insane finally nt feel natural way thing java idiomatic nt play well rest java ecosystem third party library nt integrate well style said java developer write code nt see nt good reason let take look asynchronous solution scala user getuserbyid id order getordersforuser useremail product getproductsfororders order stock getstockforproducts product yield stock contrast java asynchronous solution solution completely readable readable scala java synchronous solution nt weird scala feature scala developer never touch typical scala developer writes code every day scala library designed work using idiom feel natural language working fun write code like scala post one language write highly tuned app performance faster app written another language highly tuned performance post scala help write application scalable default using natural readable idiomatic code like ball lawn bowl bias scala bias helping write scalable application java make swim upstream scaling mean much example provided scala scaling well java nt specific example situation app failing high load nt let give example scala much nicer asynchronous io support help write scalable code using akka easily define actor different type request allocate different resource limit certain part single application start struggling receiving unanticipated load part may stop responding rest app stay healthy scala play akka make handling single request using multiple thread running parallel different operation incredibly simple allowing request lot little time klout wrote excellent article api asynchronous io simple offloading processing onto machine safely done without tying thread first machine java make asynchronous io simple java java probably going include support closure sort great news java world especially want asynchronous io however syntax still wo nt anywhere near readable scala code showed java released java released last year took year release java scheduled summer even arrives schedule long take ecosystem catch long take java developer switch synchronous asynchronous mindset opinion java little late asynchronous io far talked shown easy scala make asynchronous io help scale nt stop let pick another feature scala immutability start using multiple thread process single request start sharing state thread thing get messy world shared state computer system crazy world impossible thing happen world deadlock world updating memory one thread another thread seeing change world race condition world performance bottle neck eagerly marked method synchronized however bad simple solution make state immutable state immutable none problem happen scala help big time scala thing immutable default collection apis immutable explicitly ask mutable collection order get mutable collection java make thing immutable library help albeit clumsily work immutable collection easy accidentally forget make something mutable java api language nt make working immutable structure easy using third party library highly likely using immutable structure often requires use mutable structure example jpa requires let look code immutable class scala case class user id long name string email string structure immutable moreover automatically generates accessors property let look corresponding java public class user private final long id private final string name private final string email public user long id string name string email thisid id thisname name thisemail email public long getid return id public string getname return name public string getemail return email enormous amount code add new property add new parameter constructor break existing code define second constructor scala case class user id long name string email string company option company none existing code call constructor still work object grows item constructor constructing becomes nightmare solution java use builder pattern double amount code write object scala name parameter easy see parameter nt right order maybe might want modify one property done scala like case class user id long name string email string company option company none def copy id long id name string name email string email company option company company user id name email company val james user james james jazzyidau val jameswithcompany jamescopy company company typesafe code natural simple readable scala developer write code every day immutable aptly suited concurrent code allows safely write system scale done java tedious joy write big advocate immutable code java written many immutable class java hurt lesser two hurt scala take code use mutable object use immutable scala biased towards helping scale conclusion possibly go way scala help scale java nt hope given taste scala side come writing scalable system shown concrete metric compared java scala solution writing scalable code shown scala system always scale better java system rather scala language side writing scalable system biased towards scaling encourages practice help scale java contrast make difficult implement practice work interested code online store find github repository number performance test found spreadsheet posted november please enable javascript view comment powered disqus comment powered
628,Lobsters,scaling,Scaling and architecture,Google's Tools to Improve Testing and Development Efficiency,http://mike-bland.com/2012/10/01/tools.html,google tool improve testing development efficiency,testing grouplet first post series second post series third post series fourth post series google stream saul alinsky blaze forge srcfs objfs coding testing google v time test excuse blaze bazel srcfs fixit objfs tap sponge chrisjay continuous build test mercenary test automation platform fixit killer rosie fast dynamic loader prelinking google web server gws bloom filter gold google slow produce staticallylinked binary ian lance taylor google released open source gnu binutils new elf linker c gunit gmock saint zhanyong google test google mock googletest doc googlemock doc guice dependency injection framework java guice dependency injection protocol buffer protocol buffer extensibilitybackwardcompatibility efficient reliable parsing efficient transmission storage rpc testing servicemocker remote procedure call rpcs medium size staging datacenter code search code search code review coding standard google code search worked singlemachine implementation code search go buganizer mondrian code review rietveld pyfakefs pyfakefs python testing toilet mox mox training grouplets testing toilet test certified test mercenary fixits software engineer test conclusion,specific tool testing grouplet testing tech build tool others developed improve testing development efficiency google finally whaling series highlevel conceptual cultural challenge testing grouplet ally faced knowledge tool eventually spread throughout google development removed infamous time test excuse post discus specific tool testing grouplet testing tech build tool others developed improve development testing efficiency effectiveness first post series focused highlevel cultural challenge adoption automated developer testing emerged result daytoday development reality second post series focused fundamental objectoriented programming issue formed core google testability solution third post series covered basic automated test written fourth post series described collection process employed google ensuring software limited automated testing previous post discussion tool follow somewhat limited experience memory many testing development tool use within google sure developed since left always aiming provide bigpicture view google development environment pointing detail personally familiar perceived impact left favorite tool discus extent hoped apology advance feel free fill gap left commenting google stream publishing post factchecks embellishment googlers present past welcome via email google comment always tool judged whether solves real problem whether produce value greater proportion coefficient term learning curve speed execution reliability result fond paraphrasing saul alinsky asserted people power solve problem even think trying whereas power right thing willingly tool need improve quality code product system depends problem impacting way team member feel helpless improve quality blaze forge srcfs objfs mentioned many time tried illustrate coding testing google v large part time test excuse due fact existing build system required using tool compile build language enourmous makefile invoking gnu make compile code using distcc cluster running automated test local workstation longer able scale compiling makefile took forever whenever build rule changed rate growth development organization codebase put increasing load distcc checking new project perforce synchronizing existing one often provided opportunity honing one espressomaking foosball smaller office outside mountain view building one project took longer longer longer running test one machine top really threatened put brake one flow given large slow flaky many test time companywide survey revealed people felt time test write careless ignorant bozo looked solution problem certainly better training education regarding testable design powerful testing framework technique part total far training knowledge whatnot worth hill bean tool hand make writing test worth time took build execute sometime build tool team began working blaze aka bazel replacement makefile compiler make would parse build language directly smart recompile using content checksum source file instead timestamps persisting dependency graph checksum input file invocation year development commenced srcfs bigtablebacked cache google enormous perforce depot coupled fuse file system individual developer workstation downloading exact input file necessary build particular target srcfs provided enormous relief perforce server developer otherwise would go day week without sync ing would suffer consequence waiting long integrate change whatever changed since last sync outofband official build tool effort ambrose feinstein began toying idea time execute build linking executing parallel using otherwiseidle machine google production datacenters called system forge got working using modified version make wrapper used throughout google time lobbying build tool allow integrate support directly thenunreleased blaze tool eventually got wish august srcfs verge productionready forge viable proofofconcept blaze still month away release circumstance organized revolution fixit officially rolled three tool january discus revolution indepth future post course organizing revolution u organizing supporting fixit one meeting google tiptop developer including rob pike response concern load shipping compiled object file binary back forth every developer workstation would place internal network tossed idea caching system object file similar objfs called year later objfs released building program executing test got even faster since vast majority compiled object program test continuous build never even sent across network developer workstation slow tool longer excuse writing automated test eventually time test something hear anymore tap sponge idea revolution spawned tried setting chrisjay continuous build using srcfs forge course first test mercenary engagement revolution turn inspired bos time test mercenary testing technology manager mark striebeck conceive test automation platform tap short tap officially rolled march tap mark thenmanager mathieu gagne allowed full advantage power blaze forge srcfs objfs perform continuous integration testing unprecedented scale every single change submitted google perforce depot built tested target affected particular change built tested setting tap build requires filling short web form centrallymanaged service opposed doityourselfstyle chrisjay system frequently required significant chunk developer time maintain long tap launched mark testing technology team delivered sponge centralized repository build test result every build andor test invocation executed every developer every continuous build throughout company yes damn lot data yes integrated blaze relatively early sponge record user machine using command option building target running test lead particular result easily accessible navigable via web interface sponge put end many frustrating online offline conversation executed command exactly command exactly changed machine etc blaze invocation produced unique url pointing result could passed around pored whoever engaged relevant information accessible without anyone ask sponge link happen quoth michael chastain fixit killer something tap sponge think many people realize eliminated need lot fixits certain class problem affected large portion broad largely mechanical refactoring handled small group developer sometimes even single developer reasonably small amount time given tap power build affected target quickly sponge ability provide exhaustive detail every build result single developer rapidly make series change affect large number target sure good without involving many developer process outside code review approval fact worked series change immediately sabbatical june convert core indexing protocol buffer google extensible data description format described newer format order break obstacle older format beginning present current project year many others lamented difficulty legacy format presented ugly hack made around since updating particular protobuf seemed heavyweight risky undertaking however time tackled problem several others done bit work year make necessary change update protocol buffer definition upon big one depended new build tool plus tap sponge could spend day making big push finish conversion yet remain secure taking core business least productivity developer dedicated completed conversion revealed issue senior websearch developer concerned spent couple day working another set change using exact method resolve everything worked one first thing returned sabbatical three month later monday september look see anything rolled back otherwise fixed absence rollback fix rosie rosie another fixitkilling tool developed collin winter automate breakingdown large googlewide mechanical code change distributing code review piece automatically submitting smaller change upon code review approval deprecating older piece code favor another kind tool make updating existing code eventually eliminating deprecated code soooo much easier fact rosie introduction successful deprecation fixit run vincent vanhoucke kevin bourrillion others know ever ever another one protocol buffer example change mechanical one specific relatively limited significant downstream dependency needed submitted particular order needed update user pyfakefs discussed new import interface advance making opensource needed make hundred hundred nearlyidentical change could submitted order wrote script automate change used commandline code search interface find affected file created single perforce client open edit file ran script update produced one giant allinone changelist rosie broke ran test piece mondrian also discussed could display build test result individual brokendown changelist via sponge integration made easy see specific piece automated change needed massaging hand fixed could ping rosie continue sending change review change approved submitted synchronized original client performed code search added master changelist new code made use deprecated interface appeared course initial round review repeat cycle existing deprecated usage remained pulled trigger delete old interface happily ever oneman fixit happier benefit fixitkillers manifold fewer fixits mean le fixit fatigue whereby developer become desensitized outright hostile towards frequent request stop focus problem effort single developer handful developer applying sufficiently powerful tool much efficient use time development resource bruteforce fixit developer involved relative fixit little need advertisement coordination effort make process run much quickly smoothly fixits happen usually involve solving genuinely challenging problem gaining interesting new knowledge adopting powerful new tool rather slogging gruntwork fast dynamic loader mentioned prior forge test executed locally developer workstation compile step handled distcc cluster time linking test execution remained local plus c unit test binary dynamicallylinked saved bit time linking linking explained used another massive pain make lot sense trying iterate rapidly developing however benefit washed away algorithm used dynamic loader resolve symbol hundred dynamic library time binary executed problem get worse dependency added existing component new product integrate feature test binary larger project could take order minute execute even reaching main first project build tool team attempt reduce startup time dynamicallylinked test program using prelinking prelinking would allow dynamicallylinked binary start almost fast staticallylinked binary say almost instantly though managed build prelinked google web server gws proof concept effort eventually failed part nasty manual hacking easily automatable given state build system early around time andrew chatham patched glibc dynamic loader ldlinuxso use bloom filter drop symbollookup process second though silver bullet new fastloader relieved one specific excruciatingly painful symptom time test disease andrew tried submit glibc rejected ground increased memory usage every process couple megabyte unacceptable cost impose linux user benefit single company development model since google maintained speciallypatched dynamic loader development use gold google ld staticallylinked binary naturally much faster startup dynamicallylinked binary however google discovered standard linux linker gnu ld also slow produce staticallylinked binary fact became apparent many case linking dominated overall build time rather compiling performing incremental ie notfromscratch build much drag building running unit test course since almost always dynamicallylinked building debug production build full system really annoying privilege working closely ian lance taylor linker guru extraordinaire one nicest fellow ever grok binary object nicest fellow ever prelinking phase either shortly time ian began writing new linker scratch focused solely elf format consequently le complex gnu ld could staticallylink binary much much faster ian followed written called gold google ld google released open source part gnu binutils ian also published new elf linker detailed technical article describing gold design performance characteristic c gunit gmock saint zhanyong zhanyong wan nearly singlehandedly brought modern unit testing mock object technology c within google via gunit opensourced google test google mock respectively routinely made impossible least made atonetime aggravatingly timeconsuming brittle task easy productive fun top maintained firstrate technical documentation framework largely replicated open source project googletest doc googlemock doc document provide wonderful thorough example explanation profane attempting summarize go read difficult overstate magnitude zhanyong contribution adoption automated testing amongst c developer google hence proposal canonized program c colleague company immeasurable service adopt google test google mock guice dependency injection framework java much written guice extremely popular java project primarily c cat never touched well maybe long ago forgotten much say seemed help lot java project inside google become testable much way google mock inspired greater application dependency injection principle amongst c project fellow extest mercenary christian gruber current maintainer guice google protocol buffer protocol buffer google standard structured data description implementation testing tool per se make writing program test easy far data type concerned invented solve primary problem extensibilitybackwardcompatibility protocol buffer avoid complex code need perform lot version style check client code new field added existing protobuf new code take advantage check foohasbar existing existing server running safely ignore efficient reliable parsing protocol buffer use binary format efficiently parsed client server code generated protocol buffer compiler tool also emit parse textformatted protobufs data selfdescribing la xml protocol buffer definition easy human understand text format quite readable without xmlstyle openclose tag binary data much faster parse inside google tool translating protobufs stored binary format text performing moderately complex query collection stored protobuf data efficient transmission storage token encoded protobuf data far smaller xmlencoded data textbased text binaryencoded protobuf far le network bandwidth disk space introduction gmock mentioned testing code make use protobufs became much much convenient due builtin support protobuf matcher rpc testing servicemocker remote procedure call rpcs message passed different process running different machine make largescale distributed computing system possible nearly thing mentioned blog program talking program client relying server server responding client etc imply use google highlyefficient internal rpc implementation relies upon protocol buffer mentioned direct testing rpchandling code historically awkward test requiring one either launch new server process using canned one test realm least medium size large writing mock stub fake implementation remote service hand piotr kaminski changed servicemocker framework one could set expectation response error condition made direct use underlying rpc framework easily one could use gmock object one could theory use gmock purpose servicemocker proved elegant robust general solution specific domain rpc issue reducing friction testing code directly interacts rpc system smallest possible extreme sure whether test using servicemocker classified small medium make use actual underlying rpc subsystem implying test essentially integration test indicating medium size allows one easily write large number thorough concise stable fastrunning test tickle many dark corner code test without launching process would suggest small end really matter pick one comfortable whatever reason choose case argument could go either way world analog computing digital interpolation reality course tool exist using canned protobuf data perform largerscale rpc testing load testing never used many much team websearch would push binary staging datacenter let process realworld data monitor performance luxury staging area sure become much conversant tool code search google lot code mentioned previous whaling post google tool navigate codebase internally including internal version nowpubliclydefunct code search tool invaluable come discovering part codebase project depends really work finding bug project contributing project material way finding user code depends order make sure change break handful user significant fraction company meriting oneperson fixit described using code search thanks code review coding standard relatively easy dive project completely least familiar google style guide begin make sense going also really handy finding example usage interface intend use example test using code example coding testing general want see jeffandsanjay ken rob pike code look like complete crossreferences code complete change history quick code search away talk company perk could done without cafs microkitchens holiday party emblem corporate excess pull tool like code search cold dead hand though public google code search site officially shut researching section post discovered rus cox wrote excellent public article google code search worked published singlemachine implementation code core member google go team wrote go buganizer buganizer google internal webbased bugtracking tool bugtracking bit misnomer buganizer used track defect yes also feature request feature rollouts binary release production whoknowswhat really fullfledged issuetracker benefit using standard tool nearly everything one might categorize issue worth tracking allows crossreferences issue specific code change via perforce changelist number somehow related issue time buganizer number could embedded perforce changelist description crossreferences course hyperlink one could bounce buganizer mondrian described ease comfort remember much history buganizer know partnerincrime antoine picard manager boht buganizer mondrian team mondrian mondrian guido van rossum google starter project standard google webbased code review tool guido month review done using text diffs email named due guido nationalistic affinity dutch full content changed file observed sidebyside difference highlighted color allowed code review participant make comment specific line code clicking open text window default view dashboard loggedin user showing user change awaiting review review waiting user approval recent code review user completed participated nonapproving commenter imagine mondrian gained little traction quickly became standard tool google grew began strain load along rest build tool time collin winter joined build tool fine job updating mondrian scale performance well extend feature set integration google internal tool particular mondrian integration sponge test result described became invaluable validating author change actually ran test affected passed mentioned code review invaluable practice believe one largest contributing factor google early success adoption automated developer testing across development culture guido mondrian came right time google development organization portfolio product really exploded allowing code review process carried much efficiently given increasinglydistributed nature google development team programmer institute policy frequent mandatory code review already rietveld guido opensource port mondrian webbased opensource code review tool exist well feel like messing formal code review lot folk get ton mileage pair programming could make sure everyone eats lunch together could create environment encourages folk hanging bouncing idea around break eg microkitchen area either way get team regularly talking code spreading knowledge idea expertise good good colleague good code product company pyfakefs pyfakefs started life utility wrote handful test build tool project working written python one python strength ability easily read manipulate file used heavily inside google exactly scripting plus production service though many c java wanted test python module handled number different case involved reading data disk without going route writing disk abstraction layer besides wrote disk abstraction layer test anyway one nice feature python easy create class module implementing interface builtin patch hacked together module replace builtin open call o ospath module using inmemory fake file system could test run faster qualify small test rather medium test data use case associated directly code test rather scattering small test file testdata directory somewhere mingled together allows one test possible corner dataprocessing function class module minimum friction worked like charm wrote one early testing toilet think episode september existence googleatlarge next year kept getting trickle code review requesting feature added actually got little annoyed people making complex never intention shortly realization began appreciate people hacking found genuinely continued work everybody always insisted thorough test time left google used unit several tool supporting google web server several folk requested open source fake file system year finally got started help julien silland busy folk though manage get pushed google code started sabbatical june changed public since presume change still made internally mox mox existed might used instead writing pyfakefs mox generalpurpose mocking framework python based easymock java also widely used within google written steve middlekauff mox used easily mocking basic file system call pyfakefs evolved point pretty standardconforming implementation capable fairly complex file operation depending nature code test degree interaction file system pyfakefs might comfortable fit training tool automated testing among substitute knowledge training clear thinking give people power right thing giving knowledge use power instrumental cultivating desire right thing consequently producing positive outcome grouplets engedu come written much testing toilet test certified test mercenary fixits grouplet program event went long way towards spreading testing tool knowledge good practice software engineer test filling eng prod rather testing grouplet role able carry work testing grouplet test test certified permanent integrated member individual development team engedu google internal training organization grouplets close tie throughout google history recruit google develpers produce training material wikibased technical documentation page called codelabs well tech talk indepth presentation given internal technology benefit google development community atlarge also work develop inhouse training program well hire contractor provide training google yet provide moral invest book training material even need selfproduce imagination important knowledge wager knowledge often fuel imagination especially come raising level code quality across board conclusion wereare certainly many tool mentioned let others chime fill gap moved however course past five whaling article think provided fairly broad yet somewhat detailed overview google development environment culture spoken many biggest challenge facing automated testing movement google regarding aspect personally involved maybe piece puzzle testing grouplet friend started fitting together make little sense given context hopefully future post regarding fixits make bit sense would otherwise one way another hopefully clear lot challenge year systematic effort trying many idea found one stuck able overcome google great benefit
629,Lobsters,scaling,Scaling and architecture,"metamx/druid - Fast, Distributed Column-oriented Datastore Optimized for Analysis",https://github.com/metamx/druid,metamxdruid fast distributed columnoriented datastore optimized analysis,druid license apache license version information http wwwdruidio documentation documentation latest druid release project website getting started quickstart reporting issue github issue community druiduser mailing list druiduser googlegroupscom druiddevelopment list druiddevelopment googlegroupscom contributing,druid druid distributed columnoriented realtime analytics data store commonly used power exploratory dashboard multitenant environment druid excels data warehousing solution fast aggregate query petabyte sized data set druid support variety flexible filter exact calculation approximate algorithm useful calculation druid load streaming batch data integrates samza kafka storm spark hadoop license apache license version information information druid found http wwwdruidio documentation find documentation latest druid release project website would like contribute documentation please docscontent repository submit pull request getting started get started druid quickstart reporting issue find bug please file github issue community community support available druiduser mailing list druiduser googlegroupscom development discussion occur druiddevelopment list druiddevelopment googlegroupscom also couple people hanging irc druiddev ircfreenodenet contributing please follow guideline listed
630,Lobsters,scaling,Scaling and architecture,"Engineer Leaves Google For Cloudera, Rebuilds F1 Query Engine as Impala",http://www.wired.com/wiredenterprise/2012/10/cloudera-impala-hadoop/,engineer leaf google cloudera rebuilds query engine impala,research lab internet spawned hadoop army nosql database includes mike olson revealed past may open source tool hive spanner currently building open source version dremel told web,think google research lab internetevery often company release research paper describing one sweeping software platform help drive online empire year later paper spawn open source software project seek share google creation rest worldpapers describing google file system google mapreduce spawned hadoop open source platform let spread data across thousand dirtcheap computer server crunch something useful google bigtable gave rise army nosql database juggle unusually large amount information google pregel delivered multiple graph database map many online relationship people thing getting point building wanted build started company jeff hammerbacher complained outside world take far long rebuilding groundbreaking google creation includes mike olson ceo cloudera silicon valley startup brought hadoop business world time differenton wednesday cloudera uncloaked software platform known impala development past two year impala mean instantly analyzing massive amount data stored hadoop based sweeping google database known google revealed past may presentation delivered conference arizona yet release full paper describing technology two year ago cloudera hired away one main google engineer behind project database guru named marcel kornackerhadoop widely used across web driving bigname operation facebook yahoo twitter spreading traditional business well according market research outfit idc fuel million software market year originally designed batch processing platform give datacrunching task take several minute several hour complete task build say index entire internet open source tool hive also analyze hadoop data much way would query traditional database using common structured query language sql collected data describing collection digital book instance could run query asking list author take timeimpala let query data realtime ie second according cloudera time faster tool like hivecloudera four year old jeff hammerbacher helped found cloudera overseeing rise hadoop facebook refers impala company version word beginning getting point say building wanted build started company google massive relational database management system rdbms help run company online ad system sits atop spanner much ballyhooed google creation let company store information across worldwide network data center spanner store record data say kornacker give access record run query correlate spanner store record data give access record run query correlate marcel kornacker google marcel kornacker oversaw development query engine system let company instantly analyze information stored database hammerbacher brought cloudera basically rebuilt query engine use hadoop hbase nosql database built work tandem hadoopkornacker left google tell u large part wanted build something everyone could use wanted work something similar say publicly accessible context today announcement wish reality keeping mission cloudera open sourced code behind impala company make money providing service various proprietary tool business use hadoop sister platformsaccording cloudera impala already used various pilot customer including online travel operation expedia expedia immediately respond question toolcloudera nt one bringing realtime query hadoop mapr conspicuous competitor cloudera currently building open source version dremel another sweeping google platform google released paper describing dremel saying could query multiple petabyte data aka million gigabyte matter secondskornacker say dremel two different animal whereas dremel designed primarily instant data analysis also handle online transaction processing oltp meaning shuttle data live application built speedy queriesbut speedy query part spawned impala two year ago google released dremel paper jeff hammerbacher told web hadoop would one day offer realtime query engine along similar line
631,Lobsters,scaling,Scaling and architecture,F1 - The Fault-Tolerant Distributed RDBMS Supporting Google's Ad Business,http://research.google.com/pubs/pub38125.html,faulttolerant distributed rdbms supporting google ad business,,many service critical ad business historically backed mysql recently migrated several service new rdbms developed google implement rich relational database feature including strictly enforced schema powerful parallel sql query engine general transaction change tracking indexing built top highly distributed storage system scale standard hardware google data center store dynamically sharded support transactionallyconsistent replication across data center able handle data center outage without data loss strong consistency property storage system come cost higher write latency compared mysql successfully migrated rich customerfacing application suite heart ad business downtime describe restructured schema application largely hide increased latency external user distributed nature also allows scale easily support higher throughput batch workload traditional rdbms built novel hybrid system combine scalability fault tolerance transparent sharding cost far available system usability familiarity transactional guarantee expected rdbms
632,Lobsters,scaling,Scaling and architecture,Scaling PostgreSQL at Braintree: Four Years of Evolution,https://www.braintreepayments.com/braintrust/scaling-postgresql-at-braintree-four-years-of-evolution,scaling postgresql braintree four year evolution,postgresql riak mongodb redis memcached command prompt beginning ruby rail mysql drbd problem mysql oakonlinealtertable slide pgeast talk initial postgresql tuning postgresql server fall sharding datafabric londiste capistrano spring drbd problem drbd proxy pacemaker summer postgresql http githubcomtmatsuoresourceagentswiki fall today,love postgresql braintree although use many different data store riak mongodb redis memcached core data stored postgresql sexy new nosql database postgresql consistent incredibly reliable two property value storing payment informationwe also love adhoc querying get relational database example traffic look fishy answer question like percentage visa decline coming europe without precompute view write complex mapreduce queriesour postgresql setup changed lot last year post going walk evolution host use postgresql lot help along way knowledgeable people command beginning like ruby rail apps gateway started mysql ran couple app server two database server replicated using drbd drbd us block level replication mirror partition serversthis setup fine first traffic started growing began see problem mysqlthe biggest problem faced schema migration large table took long time mysql dataset grew deploys started taking longer longer iterating quickly schema evolving could nt keep affording take downtime upgraded even added new index large tablewe explored various option mysql oakonlinealtertable decided would rather move database supported directly also starting see deadlock issue mysql operation felt nt deadlock postgresql solved problem wellwe migrated mysql postgresql fall read migration slide pgeast talk postgresql recently released chose go version since longerand well initial postgresqlwe ran postgresql modest hardware kept drbd replication worked fine first traffic continued grow needed upgrade unlike application much heavier writes read forevery credit card charge store lot data customer information raw response theprocessing network table audit next year performed following upgrade tweaked configs around checkpoint shared buffer workmem great start tuning postgresql server moved write ahead log wal partition fsyncs wal nt flush dirty data file moved wal pair disk sequential writes wal slowed random readwrite data file added ram moved better server core disk even ram added ram kept adding keep working set ram fall shardingthese incremental improvement worked great long time database able keep everincreasing volume summer started feel like traffic going outgrow singleserver could keep buying better hardware knew limitwe talked lot different solution end decided horizontally shard database merchanta merchant traffic would live one shard make querying easier different merchant would live ondifferent shardswe used datafabric introduce sharding rail app datafabric let specify model sharded give method activating specific shard conjunction datafabric also wrote fair amount ofcustom code sharding sharded every table except handful global table merchant userssince almost every url merchant id able activate shard applicationcontrollerrb traffic code looked roughly like class applicationcontroller actioncontroller base aroundfilter activateshard def activateshard block merchant merchantfindbypublicid params merchantid datafabricactivateshard shard merchantshard block end end making code work sharding half battle still migrate merchant different shard without downtime londiste statementbased replication tool set new database serversand used londiste mirror entire database current cluster renamed shard newcluster shard paused traffic stopped replication updated shard column global database resumed traffic whole process automated using capistrano point request went new database server tothe old sure everything working removed shard data shard vice versa final cutover completed fall drbd problem sharding took care performance problem spring started running issueswith drbd replication drbd made replicating two server easy two required complex stacked resource harder orchestrate also required moving piece like drbd proxy prevent blocking writes data center drbd block level replication filesystem shared server mean never unmounted checked fsck without taking downtime become increasingly concerned filesystem corruption would go unnoticed corrupt server cluster filesystem mounted primary server standby server sit idle possible run readonly query failover required unmounting remounting filesystems slower desired also since filesystem unmounted target server mounted filesystem cache empty meant backup postgresql slow failover would see slow request sometimes timeouts saw couple issue sandbox environment drbd issue secondary prevented writes primary node thankfully never occurred production lot trouble tracking issue still using manual failover scared horror story pacemaker drbd causing split brain scenario data corruption wanted get automated failover however drbd required kernel module build test new module every time upgraded kernel one upgrade drbd caused huge degradation write performance thankfully discovered issue test environment another reason wary kernel level replicationgiven concern decided leave drbd replication move postgresql streaming replication new postgresql felt like better fit wanted could replicate many server easily standby server queryable letting u offload expensive query failover quickwe made switch summer postgresql updated code support postgresql involved code change along upgrade wanted move fully automated failover decided use pacemaker great open source script managing postgresql streaming replication http githubcomtmatsuoresourceagentswiki script handle promotion moving database ip even switching sync async mode standby server set new database cluster one per shard used two server per datacenter synchronous replication within datacenter asynchronous replication data center configured pacemaker cluster ready go empty performed extensive testing setup fully understand fail scenario exactly pacemaker would react used londiste copy data cluster date similar cutover paused traffic stopped londiste updated databaseyml resumed traffic one shard time entire procedure automated capistrano took downtimefall today today good state postgresql fully automated failover server within datacenter cross datacenter failover still manual since want sure give entire datacenter automated capistrano task orchestrate controlled failover using pacemaker traffic pausing mean perform database maintenance zero downtime one big lesson learned need continually invest postgresql setup always watching postgresql performance making adjustment needed new index restructuring data config tuning etc since traffic continues grow record data know postgresql setup continue evolve coming year
634,Lobsters,scaling,Scaling and architecture,Ricon 2012 Live Stream,http://basho.com/community/ricon2012_live/,ricon live stream,finally nosql database brings tranquility peace mind,finally nosql database brings tranquility peace mind second latency cost thousand dollar outage million call scalable highly available database easy operationalize resoundingly clear riak performs promised keep light
635,Lobsters,scaling,Scaling and architecture,Building Web Platforms Infrastructure,http://blog.webplatform.org/2012/10/building-web-platforms-infrastructure/,building web platform infrastructure,application targeted mediawiki wordpress piwik qwebirc lumberjack scaling target launch statistic approach platform hp cloud openstack automation deployment configuration management salt stack frontend cache fastly docswebplatformorg talkwebplatformorg blogwebplatformorg statswebplatformorg webplatformorg backend cache horizontal scaling shared storage database replication readwrite separation email sendgrid monitoring monitoring server system logging backup future infrastructure change,initial launch web platform decided go alpha release small concrete set platform goal used open source software kept initial set application small focus preparing handle initial launch load application targeted initial set application targeted launched alpha mediawiki doc wordpress blog piwik stats talkforums qwebirc talkchat lumberjack chat logging first four php application qwebirc python lumberjack python php javascript scaling target upperbound target launch day anonymous request per second loggedin request per second use case assumes request read wanted wellprepared whatever internet would throw u launch statistic actual statistic launch day request per second u peak request per second europe peak application server combined cpu load steady memory usage steady database waitio statistic really showed blip storage server showed statistic worth mentioning managed approach going describe backend infrastructure post talk different application later post platform one goal limit amount infrastructure used much possible cloud service way go used hp cloud platform currently using compute block storage service hp cloud us openstack infrastructure stack fit well goal using open source solution hosted solution based open source product possible platform currently following instance application server spec ram disk vcpus database server spec ram disk vcpus storage server spec ram disk volume storage vcpus deployment server spec ram disk vcpus monitoring server spec ram disk vcpus backup server spec ram disk volume storage vcpus bot server spec ram disk vcpu instance run ubuntu linux either lucid precise automation deployment configuration management every infrastructure project start configuration management everything kept repository everything put production automated configuration management deployment remote execution use salt stack configuration management use salt state deployment use combination salt state salt module salt runner remote execution use salt module keep everything number git repository automation done building instance using openstack compute api injecting bootstrapping script user data using cloudinit install salt minion point master instance named role allows u use glob predefine instance role instance app application server db database server etc instance come completely configured ready added pool frontend cache use case application almost totally based read scale use case simply need increase number read handle effectively handled frontend caching frontend caching service use fastly specializes dynamic longtail content like mediawiki fastly top choice purge cache globally short period time mediawiki use case assumes instant purging editing fairly hard requirement cdn chosen fastly also us varnish suit goal using open source solution well use fastly created cname dns entry docswebplatformorg talkwebplatformorg blogwebplatformorg statswebplatformorg fastly service load balance set application server health check infrastructure plan survives contact audience upon launch scaling issue static content webplatformorg webplatformorg must record could nt point fastly pointed wwwwebplatformorg cname webplatformorg fairly long ttl created cname pointed fastly redirected webplatformorg temporary hack deal load balancing dns wwwwebplatformorg expired globally following day pointed request back wwwwebplatformorg point backend cache mediawiki wordpress must generation content expensive mediawiki instance must convert wiki markup html output long article take second using backend cache possible avoid regenerating content additionally php interpreted language must converted bytecode run possible cache bytecode result use memcache backend cache apc php bytecode cache two application server run memcache cache per node combined hold content plus apc installed application server shared memory size horizontal scaling even good frontend caching still fair number request hit backend handle deploy application application server put session memcache used frontend cache load balance application server though deploy application application server still able move traffic application around specific application server needed split application separate subdomains subdomain separate service frontend cache start eating many resource affect performance site could move single application server move service away server handle different service application server using fairly generic virtual host configuration apache generic application server also allows u quickly scale need add application server launch new application server deploy code add fastly configuration shared storage mediawiki wordpress require storage user uploaded file since loadbalanced application server ca nt store uploads locally two storage node running glusterfs replicated volume share volume application server mount share via fstab database replication readwrite separation assuming traffic writes expect loggedin request per second expect low number writes per second database using standard lucid install mysql replication enabled little tuning done since writes per second relatively low mediawiki special configuration using read writes email avoided setting mail service using sendgrid mail delivery accept incoming mail may change future monitoring needed basic amount monitoring launch using ganglion monitor resource usage ganglion configuration single monitoring server server gmond process per cluster gmond every server talk appropriate gmond service monitoring server gmetad subscribes process locally using gangliawebfrontend package ubuntu precise repository system logging using three service logging rsyslog syslogng using rsyslog syslogng apache error log apache configured send log syslog rsyslog application server configured send log deployment server deployment server us syslogng send udp log combined error file also using service called written maintained wikimedia foundation simple service take udp message sends set pipe pipe configured python script called demuxpy demux split message received file name message mediawiki configured send message per log group allows u see exception message page parse slowly memcache error etc like syslogng run deployment system allowing developer operation people see log centralized system backup using simple mechanism backup deployment server run crons backup software repos uploaded file etc database server run crons backup database backup system rsyncs local backup copy deployment server database server onto volume service mounted disk keep backup day use lvm backup disk keep lvm snapshot week future infrastructure change thing nt get implement launch would nice infrastructure point view object storage using glusterfs right ideal mounting shared storage great way really bad outage especially virtualized infrastructure object storage reliable handled cloud provider support object storage mediawiki currently mostly undocumented future switch away using glusterfs using hp cloud object storage based openstack swift would eliminate two server infrastructure database service database usage tiny really wasteful two xlarge instance handling would effective u run hp cloud database service would eliminate two server infrastructure offisite backup object store backup datacenter currently thankfully backup volume storage second copy instance storage different intances would better backup another datacenter object storage lvm snapshot database disk backup nice lvm snapshot database disk much nicer instant recovery fastly api automation setup automatically adding new application server service using fastly api could autoexpand autoshrink application server cluster needed releasing salt configuration management repository repository nt really state currently releasable embedded password sensitive thing also nt reasonable way host currently relying github deployment terrifying since nt proper control repository likely host gerrit instance replicate github
637,Lobsters,scaling,Scaling and architecture,LinkedIn Moves from Rails to Node,http://highscalability.com/blog/2012/10/4/linkedin-moved-from-rails-to-node-27-servers-cut-and-up-to-2.html,linkedin move rail node,update fact left behindthescenes look linkedin mobile engineering related article,update background ikai lan worked mobile server team linkedin say fact left app made cross data center request guy running singlethreaded rail server every request blocked entire process running mongrel leaking memory like sieve explains nonblocking approach would win ikai hope nobody read h somebody else without thinking goal information use make decision ryan paul written excellent behindthescenes look linkedin mobile engineering mobile part story mobile usage focus simplicity ease use reliability using room metaphor native html embedded lightweight http server single clientapp connection could help guide mobile strategy backend effect moving rail nodejs may also prove interesting evaluation advantage nodejs much better performance lower memory overhead tested option running faster scenario programmer could leverage javascript skill frontend backend mobile team could combined single unit server cut enough headroom remains handle current level resource utilization development could focus application development firefighting clearly lot issue mixed together rewrite change stack change logic distribution server client plenty room argue gain really came clear linkedin belief use nodejs big win ymmv comment section well vigorous interesting observation especially liked one comment oluseyi inevitable rearchitecting rewrite want cache content aggressively store template clientside ability invalidate update server keep state purely client side mean application may request server content matching set filter updated since provided timestamp order refresh cache rather opening closing several connection want open single longlived connection stream relevant metadata asset content remember original implementation rendered returned html mean uris image etc pointed server web view created destroyed navigation image effectively cached longlived connection implementation longer view traditional mvc web application sense final result serverside processing controller aggregate necessary data markup written output stream rather large binary blob client unpacks extract relevant data see concern term mvc misused usage correct web application contextspecific sense view markup written output including reference external uris must resolved rendering web view aggregated data stream get unpacked cached rendered clientside view related article
638,Lobsters,scaling,Scaling and architecture,Service Oriented Side Effects,http://rdegges.com/service-oriented-side-effects,service oriented side effect,http,http
640,Lobsters,scaling,Scaling and architecture,"WordPress.com Serves 70,000 req/sec and over 15 Gbit/sec of Traffic usingNGINX",http://highscalability.com/blog/2012/9/26/wordpresscom-serves-70000-reqsec-and-over-15-gbitsec-of-traf.html,wordpresscom serf reqsec gbitsec traffic using nginx,barry abrahamson wordpresscom wordpresscom vip problem wordpressorg solution nginx gravatar reference,guest post barry abrahamson chief system wrangler automattic nginx coufounder andrew alexeev wordpresscom serf million site attracting million people billion page month since april wordpresscom experienced time growth page view wordpresscom vip host many popular site including cnn political ticker nfl time inc page people magazine style watch corporate blog flickr kroq many automattic operates two thousand server twelve globally distributed data center wordpresscom customer data instantly replicated different location provide extremely reliable fast web experience hundred million visitor problem wordpresscom began started shared hosting much like wordpressorg site soon moved single dedicated server two server late wordpresscom opened public early expanded four web server traffic distributed using round robin dns soon thereafter wordpresscom expanded second data center third quickly became apparent round robin dns nt viable longterm solution hardware appliance like bigip offered many feature wordpresscom required automattic system team decided evaluate different option built existing open source software using open source software commodity hardware provides ultimate level flexibility also come cost purchasing pair capable hardware appliance failover configuration single datacenter may little expensive purchasing servicing set data center soon becomes expensive first wordpresscom team chose pound software load balancer ease use builtin ssl support using pound two year wordpresscom required additional functionality scalability namely onthefly reconfiguration capability without interrupting live traffic better health check mechanism allowing smoothly gradually recover backend failure without overloading application infrastructure unexpected load request better request per second number concurrent connection pound threadbased model able reliably handle request per second per load balancing instance solution april automattic converted wordpresscom load balancer pound nginx automattic engineer using nginx gravatar month impressed performance scalability moving wordpresscom natural next step switching wordpresscom nginx automattic evaluated several product including haproxy lvs reason nginx chosen easy flexible logical configuration ability reconfigure upgrade nginx instance onthefly without dropping user request application request routing via fastcgi uwsgi scgi protocol nginx also serve static content directly storage additional performance optimization software tested capable reliably handling request per second live traffic wordpress application single server nginx memory cpu footprint minimal predictable switching nginx cpu usage load balancing server dropped three time overall wordpresscom serving reqsec gbitsec traffic nginx powered load balancer peak plenty room grow hardware configuration dual xeon core cpu hyperthreading ram running debian linux part high availability setup wordpresscom previously used wackamolespread recently started migrate keepalived even distribution inbound request across nginxbased web acceleration load balancing layer based dns roundrobin mechanism reference
641,Lobsters,scaling,Scaling and architecture,Handling Database Failover at Craigslist,http://blog.zawodny.com/2012/09/18/handling-database-failover-at-craigslist/,handling database failover craigslist,mha galera cluster mysql like jeremy zawodny,interesting discussion online recently handle database meaning mysql really applies system failover discussion followed far order rick james yahoo note comment baron posting take approach still advocate use craigslist automated failover get human involved try make easy human two important thing get clear picture state thing put thing motion choice made simple peter posting get heart matter fun scary try build great automated system detect failure right thing also really hard problem solve lot little gotchas get wrong amount pain bring potentially enormous craigslist share similarity yahoo hardware installed space manage try select good hardware take good care thing still fail course failure frequent constantly worried next mysql master going die middle night rick pointed mha comment need look andor point coworkers realize existed spent couple week creating custom tool help event master failure look available slave find suitable candidate present list allows operator choose new master selected script try automate much switching possible though stared code quite bit tried reason way might fail feel pretty good never actually used ok really nicely documented playbook sort situation already served u well said happen often script try automate existing practice turn minute readonly time le minute point start wonder saving worth risk tricky spot bug finding way turning minute many hour late night pain sure stand particular case something like galera cluster mysql interesting kinda feel like pay early adopter lot problem master failure surely feel differently like like loading related jeremy zawodny software engineer pilot work craigslist day hacking various bit backend software data system pilot fly glastar aircraft superstol bonanza high performance glider northern california nevada area also original author high performance mysql published oreilly medium still speak conference user group occasion
643,Lobsters,scaling,Scaling and architecture,The Architecture of Open Source Applications (Volume 2): The Glasgow Haskell Compiler,http://www.aosabook.org/en/ghc.html,architecture open source application volume glasgow haskell compiler,highlevel structure section code metric figure compiler total http llvmorg compiler compiling haskell code figure parsing renaming type checking desugaring core language later optimisation later code generation native code generation llvm code generation c code generation key design choice intermediate language let case figure figure type checking source language symbol table intermodule optimisation extensibility userdefined rewrite rule compiler plugins ghc library ghc api haddock ghcihaskeline leksah hint package system http hackagehaskellorg http hackagehaskellorg developing ghc comment note keep refactoring crime nt pay developing rts coping complexity invariant checking conclusion control system garbage truck,glasgow haskell compiler ghc started part academic research project funded uk government beginning several goal mind ghc year old continuous active development since inception today ghc release downloaded hundred thousand people online repository haskell library package ghc used teach haskell many undergraduate course growing number instance haskell depended upon commercially lifetime ghc generally around two three active developer although number people contributed code ghc hundred ultimate goal u main developer ghc produce research rather code consider developing ghc essential prerequisite artifact research fed back ghc ghc used basis research build previous idea moreover important ghc industrialstrength product since give greater credence research result produced ghc stuffed full cuttingedge research idea great deal effort put ensuring relied production use often tension two seemingly contradictory goal large found path satisfactory research productionuse angle chapter want give overview architecture ghc focus handful key idea successful ghc nt hopefully throughout following page gain insight managed keep large software project active year without collapsing weight generally considered small development team highlevel structure highest level ghc divided three distinct chunk compiler essentially haskell program whose job convert haskell source code executable machine code boot library ghc come set library call boot library constitute library compiler depends library source tree mean ghc bootstrap library tightly coupled ghc implement lowlevel functionality int type term primitive defined compiler runtime system library highlevel compilerindependent datamap library runtime system rts large library c code handle task associated running compiled haskell code including garbage collection thread scheduling profiling exception handling rts linked every compiled haskell program rts represents significant chunk development effort put ghc design decision made responsible haskell key strength efficient support concurrency parallelism describe rts detail section fact three division correspond exactly three subdirectory ghc source tree compiler library rts respectively wo nt spend much time discussing boot library largely uninteresting architecture standpoint key design decision embodied compiler runtime system devote rest chapter discussing two component code metric last time measured number line ghc glasgow haskell compiler technical overview jfit technical conference digest interesting look thing changed since figure give breakdown number line code ghc divided major component comparing current tally modulelines line increase compiler main parser type core stg dataparallel code native code llvm code haskell abstract core stg c abstract c identifier type prelude compiler total runtime system c c figure line code ghc past present notable aspect figure despite nearly year nonstop development compiler increased size factor around around line haskell code obsessively refactor adding new code keeping code base fresh possible several new component although account new line much new component concerned code generation native code generator various processor llvm code generator formerly low level virtual machine llvm project includes generic codegenerator target many different processor information see http llvmorg chapter llvm volume architecture open source application infrastructure interactive interpreter ghci also added line biggest increase single component type checker line added unsurprising given much recent research using ghc new type system extension example gadts type family lot code added main component partly previously perl script called driver rewritten haskell moved ghc proper also support compiling multiple module added runtime system barely grown larger despite accumulated lot new functionality ported platform rewrote completely around ghc complex build system today comprises line gnu make code fourth complete rewrite latest two year ago successive iteration reduced amount code compiler divide compiler three compilation manager responsible compilation multiple haskell source file job compilation manager figure order compile different file decide module need recompiled none dependency changed since last time compiled haskell compiler abbreviate hsc inside ghc handle compilation single haskell source file might imagine action happens output hsc depends backend selected assembly llvm code bytecode pipeline responsible composing together necessary external program hsc compile haskell source file object code example haskell source file may need preprocessing c preprocessor feeding hsc output hsc usually assembly file must fed assembler create object file compiler simply executable performs function library large api used build tool work haskell source code ides analysis tool compiling haskell code compiler compiling haskell source file proceeds sequence phase output phase becoming input subsequent phase overall structure different phase illustrated figure figure compiler phase parsing start traditional way parsing take input haskell source file produce output abstract syntax ghc abstract syntax datatype hssyn parameterised type identifier contains abstract syntax tree type hssyn type identifier enables u add information identifier program pass various stage compiler reusing type abstract syntax tree output parser abstract syntax tree identifier simple string call rdrname hence abstract syntax produced parser type hssyn rdrname ghc us tool alex happy generate lexical analysis parsing code respectively analogous tool lex yacc c ghc parser purely functional fact api ghc library provides pure function called parser take string thing return either parsed abstract syntax error message renaming renaming process resolving identifier haskell source code fully qualified name time identifying outofscope identifier flagging error appropriately haskell possible module reexport identifier imported another module example suppose module defines function called f module b import module reexports f module c import module b refer f name though f originally defined module useful form namespace manipulation mean library use whatever module structure like internally expose nice clean api via interface module reexport identifier internal module compiler however resolve know name source code corresponds make clean distinction entity thing example af name entity referred eg bf given point source code set entity scope may known one different name job renamer replace name compiler internal representation code reference particular entity sometimes name refer several different entity error name actually used renamer flag ambiguity error reject program renaming take haskell abstract syntax hssyn rdrname input also produce abstract syntax output hssyn name name reference particular entity resolving name main job renamer performs plethora task collecting equation function together flagging error differing number argument rearranging infix expression according fixity operator spotting duplicate declaration generating warning unused identifier type checking type checking one might imagine process checking haskell program typecorrect program pass type checker guaranteed crash runtime term crash formal definition includes hard crash like segmentation fault thing like patternmatching failure noncrash guarantee subverted using certain unsafe language feature foreign function interface input type checker hssyn name haskell source qualified name output hssyn id id name extra information notably type fact haskell syntax produced type checker fully decorated type information every identifier type attached enough information reconstruct type subexpression might useful ide example practice type checking renaming may interleaved template haskell feature generates code runtime need renamed type checked desugaring core language haskell rather large language containing many different syntactic form intended easy human read wide range syntactic construct give programmer plenty flexibility choosing appropriate construct situation hand however flexibility mean often several way write code example expression identical meaning case expression true false branch listcomprehension notation translated call map filter concat fact definition haskell language defines construct translation simpler construct construct translated away like called syntactic sugar much simpler compiler syntactic sugar removed subsequent optimisation pass need work haskell program smaller language deal process desugaring therefore remove syntactic sugar translating full haskell syntax much smaller language call core talk core detail later optimisation program core process optimisation begin one ghc great strength optimising away layer abstraction work happens core level core tiny functional language tremendously flexible medium expressing optimisation ranging highlevel strictness analysis lowlevel strength reduction optimisation pass take core produce core main pas called simplifier whose job perform large collection correctnesspreserving transformation goal producing efficient program transformation simple obvious eliminating dead code reducing case expression value scrutinised known involved function inlining applying rewrite rule discussed later simplifier normally run optimisation pass six pass actually run order depends optimisation level selected user code generation core program optimised process code generation begin couple administrative pass code take one two route either turned byte code execution interactive interpreter passed code generator eventual translation machine code code generator first convert core language called stg essentially core annotated information required code generator stg translated cmm lowlevel imperative language explicit stack code take one three route native code generation ghc contains simple native code generator processor architecture route fast generates reasonable code case llvm code generation cmm converted llvm code passed llvm compiler route produce significantly better code case although take longer native code generator c code generation ghc produce ordinary c code route produce significantly slower code two route useful porting ghc new platform key design choice section focus handful design choice particularly effective ghc intermediate language expression e u xvariables kdata constructor kliterals  x e e uvalue abstraction application  e e type abstraction application let x  e u local binding case e expression e casts  p k c  x  pattern figure syntax core typical structure compiler staticallytyped language program type checked transformed untyped intermediate language optimised ghc different staticallytyped intermediate language turn design choice pervasive effect design development ghc ghc intermediate language called core thinking implementation system fc thinking theory syntax given figure exact detail important interested reader consult detail present purpose however following point key one haskell large source language data type representing syntax tree literally hundred constructor contrast core tiny principled lambda calculus extremely syntactic form yet translate haskell core haskell implicitlytyped source language program may type annotation instead type inference algorithm figure type every binder subexpressions type inference algorithm complex occasionally somewhat ad hoc reflecting design compromise every real programming language embodies contrast core explicitlytyped language every binder explicit type term include explicit type abstraction application core enjoys simple fast type checking algorithm check program type correct algorithm entirely straightforward ad hoc compromise ghc analysis optimisation pass work core great core tiny language optimisation case deal although core small extremely f originally developed foundational calculus typed computation new language feature added source language happens time change usually restricted front end core stay unchanged hence compiler core typed type inference engine accepts source program program presumably well typed optimisation pas presumably maintains typecorrectness core may enjoy fast type checking algorithm would ever want run moreover making core typed carry significant cost every transformation optimisation pas must produce welltyped program generating type annotation often nontrivial nevertheless huge win explicitlytyped intermediate language several reason running core type checker call lint powerful consistency check compiler imagine write optimisation accidentally generates code treat integer value function try call chance program segmentation fault fail runtime bizarre way tracing segfault back particular optimisation pas broke program long road imagine instead run lint every optimisation pas use flag dcorelint report precisely located error immediately offending optimisation blessing course type soundness correctness lint signal error optimise x instead x program pass lint guarantee run without segfaults moreover practice found surprisingly hard accidentally write optimisation typecorrect semantically correct type inference algorithm haskell large complex glance figure confirms type checker far largest single component ghc large complex mean errorprone lint serf independent check type inference engine type inference engine accepts program fact typecorrect lint reject lint serf powerful auditor type inference engine existence core also proved tremendous sanity check design source language user constantly suggest new feature would like language sometimes feature manifestly syntactic sugar convenient new syntax something already sometimes deeper hard tell farreaching feature core give u precise way evaluate feature feature readily translated core reassures u nothing fundamentally new going new feature syntacticsugarlike hand would require extension core think much much carefully practice core incredibly stable time period added exactly one new major feature core namely coercion associated cast period source language evolved enormously attribute stability brilliance rather fact core based directly foundational mathematics bravo girard type checking source language one interesting design decision whether type checking done desugaring tradeoff type checking desugaring mean type checker must deal directly haskell large syntax type checker many case consider desugared untyped variant core first one might hope type checker would become much smaller hand type checking desugaring would impose significant new obligation desugaring affect program typecorrect desugaring implies deliberate loss information probably case case problem problem would force compromise design core preserve extra information seriously type checking desugared program would make much harder report error relate original program text sometimes elaborate desugared version compiler type check desugaring ghc made opposite choice type check full original haskell syntax desugar result sound adding new syntactic construct might complicated following french school structured type inference engine way make easy type inference split two part constraint generation walk source syntax tree generating collection type constraint step deal full syntax haskell straightforward code easy add new case constraint solving solve gathered constraint subtlety type inference engine lie independent source language syntax would much smaller much larger language whole typecheckbeforedesugar design choice turned big win yes add line code type checker simple line avoids giving two conflicting role data type make type inference engine le complex easier modify moreover ghc type error message pretty good symbol table compiler usually one data structure known symbol table mapping symbol eg variable information variable type source code defined ghc use symbol table quite sparingly mainly renamer type checker far possible use alternative strategy variable data structure contains information indeed large amount information reachable traversing data structure variable variable see type contains type constructor contain data constructor contain type example data type ghc heavily abbreviated simplified data id mkid name type data type tyconapp tycon type data tycon algtycon name datacon data datacon mkdatacon name type id contains type type might application type constructor argument eg maybe int case contains tycon tycon algebraic data type case includes list data constructor datacon includes type course mention tycon whole structure highly interconnected indeed cyclic example tycon may contain datacon contains type contains tycon started approach advantage disadvantage many query would require lookup symbol table reduced simple field access great efficiency code clarity need carry around extra symbol table abstract syntax tree already contains information space overhead better instance variable share data structure space needed table difficulty arise need change information associated variable symbol table advantage would change entry symbol table ghc traverse abstract syntax tree replace instance old variable new one indeed simplifier regularly need update certain optimisationrelated information variable hard know whether would better worse overall use symbol table aspect design fundamental almost impossible change still avoiding symbol table natural choice purely functional setting seems likely approach good choice haskell intermodule optimisation functional language encourage programmer write small definition example definition standard library bool bool bool true true true false every use function really required function call efficiency would terrible one solution make compiler treat certain function specially another use preprocessor replace call desired inline code solution unsatisfactory one way another especially another solution obvious simply inline function inline function mean replace call copy function body suitably instantiating parameter ghc systematically adopted approach virtually nothing built compiler instead define much possible library use aggressive inlining eliminate overhead mean programmer define library inlined optimised well one come ghc consequence ghc must able crossmodule indeed crosspackage inlining idea simple compiling haskell module libhs ghc produce object code libo interface file libhi interface file contains information function lib export including type sufficiently small function definition compiling module clienths import lib ghc read interface libhi client call function libf defined lib ghc use information libhi inline libf default ghc expose definition function interface file function small flag control size threshold also support inline pragma instruct ghc inline definition aggressively call site regardless size thus foo int int inline foo foo x big expression crossmodule inlining absolutely essential defining superefficient library come cost author upgrade library enough relink cliento new libo cliento contains inlined fragment old libhs may well compatible new one another way say abi application binary interface libo changed way requires recompilation client fact way compilation generate code fixed predictable abi disable crossmodule optimisation typically high price pay abi compatibility user working ghc usually source code entire stack available recompiling normally issue describe later package system designed around mode working however situation recompiling practical distributing bug fix library binary o distribution example future hope may possible find compromise solution allows retaining abi compatibility still allowing crossmodule optimisation take place extensibility often case project life dy according extensible monolithic piece software extensible everything right whereas extensible piece software useful base even nt provide required functionality box open source project course extensible definition anyone take code add feature modifying original source code project maintained someone else highoverhead approach also conducive sharing extension others therefore successful project tend offer form extensibility involve modifying core code ghc exception respect userdefined rewrite rule core ghc long sequence optimisation pass performs semanticspreserving transformation core core author library defines function often nontrivial domainspecific transformation one possibly predicted ghc ghc allows library author define rewrite rule used rewrite program optimisation way programmer effect extend ghc domainspecific optimisation one example foldrbuild rule expressed like rule foldbuild forall k z g forall b b b b b foldr k z build g g k z entire rule pragma introduced rule rule say whenever ghc see expression foldr k z build g rewrite g k z transformation semanticspreserving take research paper argue chance ghc performing automatically together handful rule inline pragmas ghc able fuse together listtransforming function example two loop map f map g x fused one although rewrite rule simple easy use proved powerful extension mechanism first introduced feature ghc ten year ago expected occasionally useful facility practice turned useful many library whose efficiency often depends crucially rewrite rule example ghc base library contains upward rule popular vector library us several dozen compiler plugins one way compiler offer extensibility allow programmer write pas inserted directly compiler pipeline pass often called plugins ghc support plugins following way programmer writes core core pas ordinary haskell function module ph say compiles object code compiling module programmer us commandline flag plugin p alternatively give flag pragma start module ghc search po dynamically link running ghc binary call appropriate point pipeline appropriate point pipeline ghc know allows plugin make decision result matter api plugin must offer bit complicated single core core much plugins sometimes require produce auxiliary pluginspecific data example plugin might perform analysis function module compiled mhs say might want put information interface file mhi plugin access information compiling module import ghc offer annotation mechanism support plugins annotation relatively new ghc higher barrier entry rewrite rule plugin manipulating ghc internal data structure course much remains seen widely used ghc library ghc api one ghc original goal modular foundation others could build wanted code ghc transparent welldocumented possible could used basis research project others imagined people would want make modification ghc add new experimental feature optimisation indeed example example exists version ghc lisp frontend version ghc generates java code developed entirely separately individual little contact ghc team however producing modified version ghc represents small subset way code ghc reused popularity haskell language grown increasing need tool infrastructure understand haskell source code ghc course contains lot functionality necessary building tool haskell parser abstract syntax type checker mind made simple change ghc rather building ghc monolithic program build ghc library linked small main module make ghc executable also shipped library form user call program time built api expose ghc functionality client api provides enough functionality implement ghc batch compiler ghci interactive environment also provides access individual pass parser type checker allows data structure produced pass inspected change given rise wide range tool built using ghc api including documentation tool haddock read haskell source code produce html documentation new version ghci front end additional feature eg ghcihaskeline subsequently merged back ghc ides offer advanced navigation haskell source code eg leksah hint simpler api onthefly evaluation haskell source code package system package system key factor growth use haskell language recent year main purpose enable haskell programmer share code important aspect extensibility package system extends shared codebase beyond ghc package system embodies various piece infrastructure together make sharing code easy package system enabler community built large body shared code rather relying library single source haskell programmer draw library developed whole community model worked well language cpan perl example although haskell predominantly compiled rather interpreted language present somewhat different set challenge basically package system let user manage library haskell code written people use program library installing haskell library simple uttering single command example cabal install zlib downloads code zlib package http hackagehaskellorg compiles using ghc installs compiled code somewhere system eg home directory unix system register installation ghc furthermore zlib depends package yet installed also downloaded compiled installed automatically zlib compiled tremendously smooth way work library haskell code shared others package system made four component first strictly part ghc project tool managing package database simply repository information package installed system ghc read package database start know package available find library called cabal common architecture building application library implement functionality building installing registering individual package website http hackagehaskellorg host package written uploaded user website automatically build documentation package browsed online time writing hackage hosting package covering functionality including database library web framework gui toolkits data structure networking cabal tool tie together hackage website cabal library downloads package hackage resolve dependency build installs package right order new package also uploaded hackage using cabal command line component developed several year member haskell community ghc team together make system fit perfectly open source development model barrier sharing code using code others shared provided respect relevant license course using package someone else written literally within second finding hackage hackage successful remaining problem scale user find difficult choose amongst four different database framework example ongoing development aimed solving problem way leverage community example allowing user comment vote package make easier find best popular package collecting data build success failure user reporting result help user avoid package unmaintained problem developing ghc ghc single project twentyyear life span still ferment innovation development part infrastructure tooling conventional example use bug tracker trac wiki also trac git revision control revisioncontrol mechanism evolved purely manual cv darcs finally moving git point may le universal offer comment note one serious difficulty large longlived project keeping technical documentation date silver bullet offer one lowtech mechanism served u particularly well note writing code often moment careful programmer mentally say something like data type important invariant faced two choice unsatisfactory add invariant comment make data type declaration long hard see constructor alternatively document invariant elsewhere risk going date twenty year everything go date thus motivated developed following simple convention comment significant size interleaved code instead set heading standard form thus note equalityconstrained type type forall ab b blah encoded like forallty forallty b funty tyconapp b blah point comment relevant add short comment referring note data type funty type type see note equalityconstrained type comment highlight something interesting going give precise reference comment explains sound trivial precision vastly better previous habit saying see comment often clear many comment intended year comment even gone altogether possible go code refers note note reverse also possible often useful moreover note may referred multiple point code simple asciionly technique automated support transformed life ghc around note number grows daily keep refactoring code ghc churning quickly ten year ago doubt complexity system increased manyfold time period saw measure amount code ghc earlier yet system remains manageable attribute three main factor crime nt pay looking back change make ghc grown common lesson emerges le purely functional whether purpose efficiency convenience tends negative consequence road couple great example ghc us data structure rely mutation internally one faststring type us single global hash table another global namecache ensures external name assigned unique number tried parallelise ghc make ghc compile multiple module parallel multicore processor data structure based mutation sticking point resorted mutation place ghc would almost trivial parallelise fact although build prototype parallel version ghc ghc currently contain support parallel compilation largely yet invested effort required make mutable data structure threadsafe ghc behaviour governed large extent commandline flag commandline flag definition constant given run ghc early version ghc made value flag available toplevel constant example toplevel value optglasgowexts type bool governed whether certain language extension enabled toplevel constant highly convenient value nt explicitly passed argument code need access course option really constant change run run definition optglasgowexts involves calling unsafeperformio hide side effect nevertheless trick normally considered safe enough value constant within given run nt invalidate compiler optimisation example however ghc later extended singlemodule compiler multimodule compiler point trick using toplevel constant flag broke flag may different value compiling different module refactor large amount code pas around flag explicitly perhaps might argue treating flag state first place would natural imperative language would sidestepped problem extent true although purely functional code number benefit least representing flag immutable data structure mean resulting code already threadsafe run parallel without modification developing rts ghc runtime system present stark contrast compiler many way obvious difference runtime system written c rather haskell also consideration unique rts give rise different design philosophy every haskell program spends lot time executing code rts typical characteristic haskell program vary lot figure greater le range also common every cycle saved optimising rts multiplied many time worth spending lot time effort save cycle runtime system statically linked every haskell program unless dynamic linking used incentive keep small bug runtime system often inscrutable user eg segmentation fault hard work around example bug garbage collector tend tied use particular language feature arise complex combination factor emerges runtime furthermore bug kind tend nondeterministic occurring run highly sensitive tiny change program make bug disappear bug multithreaded version runtime system present even greater challenge therefore worth going extra length prevent bug also build infrastructure make identifying easier symptom rts bug often indistinguishable two kind failure hardware failure common might think misuse unsafe haskell feature like ffi foreign function interface first job diagnosing runtime crash rule two cause rts lowlevel code run several different architecture operating system regularly ported new one portability important every cycle every byte important correctness even moreover task performed runtime system inherently complex correctness hard begin reconciling lead u interesting defensive technique describe following section coping complexity rts complex hostile programming environment contrast compiler rts almost type safety fact even le type safety c program managing data structure whose type live haskell level c level example rts idea object pointed tail con cell either another con information simply present c level moreover process compiling haskell code era type even told rts tail con cell list would still information pointer head con cell rts code lot casting c pointer type get little help term type safety c compiler first weapon battle avoid putting code rts wherever possible put minimum amount functionality rts write rest haskell library rarely turned badly haskell code far robust concise c performance usually perfectly acceptable deciding draw line exact science although many case reasonably clear example might theoretically possible implement garbage collector haskell practice extremely difficult haskell allow programmer precise control memory allocation dropping c kind lowlevel task make practical sense plenty functionality ca nt easily implemented haskell writing code rts pleasant next section focus one aspect managing complexity correctness rts maintaining invariant invariant checking rts full invariant many trivial easy check example pointer head queue null pointer tail also null code rts littered assertion check kind thing assertion goto tool finding bug manifest fact new invariant added often add assertion writing code implement invariant invariant runtime far difficult satisfy check one invariant kind pervades rts following heap dangling pointer dangling pointer easy introduce many place compiler rts violate invariant code generator could generate code creates invalid heap object garbage collector might forget update pointer object scan heap tracking kind bug extremely timeconsuming however one author favourite activity time program eventually crash execution might progressed long way dangling pointer originally introduced good debugging tool available tend good executing program reverse recent version gdb microsoft visual studio debugger support reverse execution however general principle program going crash crash soon noisily often possible quote come ghc coding style guideline originally written alastair reid worked early version rts problem nodanglingpointer invariant something checked constanttime assertion assertion check must full traversal heap clearly run assertion every heap allocation every time gc scan object indeed would even enough dangling pointer nt appear end gc memory freed debug rts optional mode call sanity checking sanity checking enables kind expensive assertion make program run many time slowly particular sanity checking run full scan heap check dangling pointer amongst thing every gc first job investigating runtime crash run program sanity checking turned sometimes catch invariant violation well program actually crash conclusion ghc consumed significant portion author life last year rather proud far come haskell implementation one regular use hundred thousand people get real work done constantly surprised haskell turn used unusual place one recent example haskell used control system garbage truck many haskell ghc synonymous never intended indeed many way counterproductive one implementation standard fact maintaining good implementation programming language lot work hope effort ghc support standard clearly delimit separate language extension make feasible implementation emerge integrate package system infrastructure competition good everyone deeply indebted microsoft particular giving u opportunity develop ghc part research distribute open source
644,Lobsters,scaling,Scaling and architecture,Spanner: Google's Globally-Distributed Database,http://research.google.com/archive/spanner.html,spanner google globallydistributed database,,spanner google scalable multiversion globallydistributed synchronouslyreplicated database first system distribute data global scale support externallyconsistent distributed transaction paper describes spanner structured feature set rationale underlying various design decision novel time api expose clock uncertainty api implementation critical supporting external consistency variety powerful feature nonblocking read past lockfree readonly transaction atomic schema change across spanner
645,Lobsters,scaling,Scaling and architecture,Data Engineering - Special Issue on Big Data War Stories,http://sites.computer.org/debull/A12june/A12JUN-CD.pdf,data engineering special issue big data war story,,obj stream j w x o  h e xod f endobj obj endobj obj stream n oq x fm h mci v v l  ojf z pb u l z p x x
647,Lobsters,scaling,Scaling and architecture,Designing for Concurrency,http://weblog.therealadam.com/2012/09/10/designing-for-concurrency/,designing concurrency,allow object consistent state rely changing state unless encapsulate object state tell ask lean towards immutability value object whenever possible hamster hard share friend,lot made difficult write multithreaded program doubt harder writing crud application testing library hand difficult writing database graphic engine point worth learning skipping hubris knowing program bug require discipline track enabling step learning write multithreaded program seen much written experience writing concurrent program one design class program rule concurrency mind let look learned designing threaded program far headline allow object consistent state rely changing state unless let first look class embody principle class rectangle attraccessor width height def orientation width height wide else tall end end wide wide freeze tall tall freeze end fun mentally review code shortcoming could go wrong would advise writer change purpose first flaw new rectangle object inconsistent state create object immediately call orientation bad thing happen typing along home begin r rectanglenew put rorientation rescue put whoop inconsistent end second flaw object allows bad data able rwidth rheight put rorientation ala third flaw could accidentally share object across thread end messing state one thread logic another thread sort bug really difficult figure designing object happen highly desirable want make sort code safe rheight put rorientation modify width height rectangle get back entirely new object let go fixing flaw encapsulate object state tell ask first flaw rectangle class guaranteed exist consistent state go contortion make sure database consistent ruby object object created ready go possible create new object inconsistent solve second flaw enforcing constraint object use tell ask principle ensure user rectangle change object state get direct access object state instead must pas guard protect object state sound fancy really simpler probably already writing ruby class way class rectangle attrreader width height def initialize width height width height width height end def width w raise negative dimension invalid w width w end def height h raise negative dimension invalid h height h end def orientation width height wide else tall end end end lot little thing changed class constructor requires width height argument know width height create valid rectangle let anyone get confused create rectangle work constructor encodes enforces requirement width height setter enforce validation new value constraint met rather blunt exception raised everything fine setter work like old class written setter use attrreader instead attraccessor bit code little explicitness got rectangle whose failure potential far smaller naive version simply good design want class designed silently blow face crux biscuit article object narrower interface explicit interface need introduce concurrency mechanism like locking serialization ie serial execution straightforward place explicit interface specific message object responds open world good design consequence lean towards immutability value object whenever possible third flaw naive rectangle class could accidentally shared across thread possibly hard detect consequence get around using technique borrowed clojure erlang immutable object class rectangle attrreader width height def initialize width height validatewidth width validateheight height width height width height end def validatewidth w raise negative dimension invalid w end def validateheight h raise negative dimension invalid h end def setwidth w selfclassnew w height end def setheight h selfclassnew width h end def orientation width height wide else tall end end end version rectangle extract validation logic separate method call constructor setter look closely setter something often see ruby code instead changing self setter create entirely new rectangle instance new dimension upside accidentally share object across thread change object result new object owned thread initiated change mean worry locking around rectangle practice sharing worst copying downside side could end proliferation rectangle object memory put pressure ruby gc might cause operational headache line clojure get around using persistent data structure able safely share internal structure reducing memory requirement hamster one attempt bringing persistent data structure ruby let think object design read domaindriven design probably recognize rectangle value object represent particular rectangle bind little bit behavior domain concept program us hard keep trying tell people way writing multithreaded program simple applying common objectoriented design principle build object always sensible state allow twiddling state without going object interface use value object possible consider using immutable value object starting scratch following principle drastically reduces number state think thus make easier reason program run multiple thread protect data whatever form lock appropriate share friend
649,Lobsters,scaling,Scaling and architecture,Introduction to Architecting Systems for Scale,http://lethain.com/introduction-to-architecting-systems-for-scale/,introduction architecting system scale,working pain growing product yahoo digg modwsgi rabbitmq redis load balancing smart client hardware load balancer citrix netscaler software load balancer haproxy caching memcache postgresql cassandra application v database caching least recently used caching algorithm inmemory cache memcached redis order magnitude least recently used content distribution network nginx cache invalidation solr offline processing message queue rabbitmq scobleizer scheduling periodic task cron puppet mapreduce mapreduce hadoop hive hbase platform layer,computer science software development program attempt teach building block scalable system instead system architecture usually picked job working pain growing product working engineer already learned suffering process post attempt document scalability architecture lesson learned working system yahoo digg attempted maintain color convention diagram green external request external client http request browser etc blue code running container django app running modwsgi python script listening rabbitmq etc red piece infrastructure mysql redis rabbitmq etc load balancing ideal system increase capacity linearly adding hardware system one machine add another capacity would double three add another capacity would increase let call horizontal scalability failure side ideal system nt disrupted loss server losing server simply decrease system capacity amount increased overall capacity added let call redundancy horizontal scalability redundancy usually achieved via load balancing article wo nt address vertical scalability usually undesirable property large system inevitably point becomes cheaper add capacity form additional machine rather additional resource one machine redundancy vertical scaling odds oneanother load balancing process spreading request across multiple resource according metric random roundrobin random weighting machine capacity etc current status available request responding elevated error rate etc load need balanced user request web server must also balanced every stage achieve full scalability redundancy system moderately large system may balance load three layer user web server web server internal platform layer internal platform layer database number way implement load balancing smart client adding loadbalancing functionality database cache service etc client usually attractive solution developer attractive simplest solution usually seductive robust sadly alluring easy reuse tragically developer lean towards smart client developer used writing software solve problem smart client software caveat mind smart client client take pool service host balance load across detects downed host avoids sending request way also detect recovered host deal adding new host etc making fun get working decently terror setup hardware load balancer high load balancing buy dedicated hardware load balancer something like citrix netscaler solve remarkable range problem hardware solution remarkably expensive also nontrivial configure generally even large company substantial budget often avoid using dedicated hardware loadbalancing need instead use first point contact user request infrastructure use mechanism smart client hybrid approach discussed next section loadbalancing traffic within network software load balancer want avoid pain creating smart client purchasing dedicated hardware excessive universe kind enough provide hybrid software loadbalancers haproxy great example approach run locally box service want loadbalance locally bound port example might platform machine accessible via database readpool database writepool haproxy manages healthchecks remove return machine pool according configuration well balancing across machine pool well system recommend starting software load balancer moving smart client hardware load balancing deliberate need caching load balancing help scale horizontally across everincreasing number server caching enable make vastly better use resource already well making otherwise unattainable product requirement feasible caching consists precalculating result eg number visit referring domain previous day pregenerating expensive index eg suggested story based user click history storing copy frequently accessed data faster backend eg memcache instead postgresql practice caching important earlier development process loadbalancing starting consistent caching strategy save time later also ensures nt optimize access pattern ca nt replicated caching mechanism access pattern performance becomes unimportant addition caching found many heavily optimized cassandra application challenge cleanly add caching ifwhen database caching strategy ca nt applied access pattern datamodel generally inconsistent cassandra cache application v database caching two primary approach caching application caching database caching system rely heavily application caching requires explicit integration application code usually check value cache retrieve value database write value cache value especially common using cache observes least recently used caching algorithm code typically look like specifically readthrough cache read value database cache missing cache key user userid userblob memcacheget key userblob none user mysqlquery select user userid userid user memcacheset key jsondumps user return user else return jsonloads userblob side coin database caching flip database going get level default configuration provide degree caching performance initial setting optimized generic usecase tweaking system access pattern generally squeeze great deal performance improvement beauty database caching application code get faster free talented dba operational engineer uncover quite bit performance without code changing whit colleague rob coli spent time recently optimizing configuration cassandra row cache succcessful extent spent week harassing u graph showing io load dropping dramatically request latency improving substantially well inmemory cache term raw encounter store entire set data memory memcached redis example inmemory cache caveat redis configured store data disk access ram order magnitude faster disk hand generally far le ram available disk space need strategy keeping hot subset data memory cache straightforward strategy least recently used employed memcache redis configured employ well lru work evicting le commonly used data preference frequently used data almost always appropriate caching strategy content distribution network particular kind cache might argue usage term find fitting come play site serving large amount static medium content distribution network cdns take burden serving static medium application server typically optimzed serving dynamic page rather static medium provide geographic distribution overall static asset load quickly le strain server new strain business expense typical cdn setup request first ask cdn piece static medium cdn serve content locally available http header used configuring cdn cache given piece content nt available cdn query server file cache locally serve requesting user configuration acting readthrough cache site nt yet large enough merit cdn ease future transition serving static medium separate subdomain eg staticexamplecom using lightweight http server like nginx cutover dns server cdn later date cache invalidation caching fantastic require maintain consistency cache source truth ie database risk truly bizarre applicaiton behavior solving problem known cache invalidation dealing single datacenter tends straightforward problem easy introduce error multiple codepaths writing database cache almost always going happen nt go writing application caching strategy already mind high level solution time value change write new value cache called writethrough cache simply delete current value cache allow readthrough cache populate later choosing read write cache depends application detail generally prefer writethrough cache reduce likelihood stampede backend database invalidation becomes meaningfully challenging scenario involving fuzzy query eg trying add application level caching infront fulltext search engine like solr modification unknown number element eg deleting object created week ago scenario consider relying fully database caching adding aggressive expiration cached data reworking application logic avoid issue eg instead delete retrieve item match criterion invalidate corresponding cache row delete row primary key explicitly offline processing system grows complex almost always necessary perform processing ca nt performed inline client request either creates unacceptable latency eg want want propagate user action across social graph need occur periodically eg want create daily rollups analytics message queue processing like perform inline request slow easiest solution create message queue example rabbitmq message queue allow web application quickly publish message queue consumer process perform processing outside scope timeline client request dividing work offline work handled consumer inline work done web application depends entirely interface exposing user generally either perform almost work consumer merely scheduling task inform user task occur offline usually polling mechanism update interface task complete example provisioning new vm slicehost follows pattern perform enough work inline make appear user task completed tie hanging end afterwards posting message twitter facebook likely follow pattern updating tweetmessage timeline updating follower timeline band simple nt feasible update follower scobleizer realtime message queue another benefit allow create separate machine pool performing offline processing rather burdening web application server allows target increase resource current performance throughput bottleneck rather uniformly increasing resource across bottleneck nonbottleneck system scheduling periodic task almost large system require daily hourly task unfortunately seems still problem waiting widely accepted solution easily support redundancy meantime probably still stuck cron could use cronjobs publish message consumer would mean cron machine responsible scheduling rather needing perform processing anyone know recognized tool solve problem seen many homebrew system nothing clean reusable sure store cronjobs puppet config machine make recovering losing machine easy would still require manual recovery likely acceptable perfect mapreduce large scale application dealing large quantity data point likely add support mapreduce probably using hadoop maybe hive hbase adding mapreduce layer make possible perform data andor processing intensive operation reasonable amount time might use calculating suggested user social graph generating analytics report sufficiently small system often get away adhoc query sql database approach may scale trivially quantity data stored writeload requires sharding database usually require dedicated slave purpose performing query point maybe rather use system designed analyzing large quantity data rather fighting database platform layer application start web application communicating directly database approach tends sufficient application compelling reason adding platform layer web application communicate platform layer turn communicates database first separating platform web application allow scale piece independently add new api add platform server without adding unnecessary capacity web application tier generally specializing server role open additional level configuration optimization nt available general purpose machine database machine usually high io load benefit solidstate drive wellconfigured application server probably nt reading disk normal operation might benefit cpu second adding platform layer way reuse infrastructure multiple product interface web application api iphone app etc without writing much redundant boilerplate code dealing cache database etc third sometimes underappreciated aspect platform layer make easier scale organization best platform expose crisp productagnostic interface mask implementation detail done well allows multiple independent team develop utilizing platform capability well another team implementingoptimizing platform intended go moderate detail handling multiple datacenters topic truly deserves post mention cache invalidation data replicationconsistency become rather interesting problem stage sure made controversial statement post hope dear reader argue learn bit thanks reading
650,Lobsters,scaling,Scaling and architecture,Netflix Shares Cloud Load Balancing And Failover Tool: Eureka!,http://techblog.netflix.com/2012/09/eureka.html,netflix share cloud load balancing failover tool eureka,netflix share cloud load balancing failover tool eureka eureka eureka need eureka different eureka aws elb different eureka route eureka used netflix asgard cassandra use eureka application client application server communicate high level architecture one region one zone register heartbeat registry nonjava service client configurability archaius resilience multiple region monitoring servo stay tuned asgard reference,netflix share cloud load balancing failover tool eureka karthikeyan ranganathanwe proud announce eureka service registry critical component netflix infrastructure aws cloud underpins midtier load balancing deployment automation data storage caching various serviceswhat eureka eureka rest based service primarily used aws cloud locating service purpose load balancing failover middletier server call service eureka server eureka also come javabased client component eureka client make interaction service much easier client also builtin load balancer basic roundrobin load balancing netflix much sophisticated load balancer wrap eureka provide weighted load balancing based several factor like traffic resource usage error condition etc provide superior resiliency previously referred eureka netflix discovery servicewhat need eureka aws cloud inherent nature server come go unlike traditional load balancer work server well known ip address host name aws load balancing requires much sophistication registering deregistering server load balancer fly since aws yet provide middle tier load balancer eureka fill big gap areahow different eureka aws elb aws elastic load balancer load balancing solution edge service exposed enduser web traffic eureka fill need midtier load balancing theoretically put midtier service behind aws elb expose outside world thereby losing usefulness aws security groupsone solution proxy based load balancer including aws elb offer eureka offer box sticky session based load balancing fact netflix majority midtier service stateless prefer nonsticky based load balancing work well aws autoscaling modelaws elb traditional proxybased load balancing solution whereas eureka load balancing happens instanceserverhost level client instance know information server need talk contact directlyanother important aspect differentiates proxybased load balancing load balancing using eureka application resilient outage load balancer since information regarding available server cached client require small amount memory buy better resiliency also small decrease latency since contacting server directly avoids two network hop proxyhow different eureka route route naming service eureka provides naming midtier server similarity stop route dns service host dns record even nonaws data center route also latency based routing across aws region eureka analogous internal dns nothing dns server across world eureka also regionisolated sense know server aws region primary purpose holding information load balancing within regionwhile register midtier server route rely aws security group protect server public access midtier server identity still exposed external world also come drawback traditional dns based load balancing solution traffic still routed server may healthy may even exist case aws cloud server disappear anytime eureka used netflix netflix eureka used following purpose apart midtier load balancingfor aiding netflix asgard open source tool managing cloud deploymentsfast rollback version case problem avoiding relaunch instancesin rolling push avoiding propagation new version instancesfor cassandra deployment take instance traffic maintenancefor memcached based evcache service identify list node ringfor carrying additional application specific metadata serviceswhen use eureka typically run aws cloud host middle tier service want register aws elb expose traffic outside world either looking simple roundrobin load balancing solution willing write wrapper around eureka based load balancing need need sticky session load session data external cache memcached importantly architecture fit model client based load balancer favored eureka well positioned fit usagehow application client application server communicate communication technology could anything like eureka help find information service would want communicate impose restriction protocol method communication instance use eureka obtain target server address use protocol thrift http rpc mechanismshigh level architecturethe architecture depicts eureka deployed netflix would typically run one eureka cluster per region know instance region least one eureka server per zone handle zone failuresservices register eureka send heartbeat renew lease every secondsif client renew lease time taken server registry secondsthe registration information renewal replicated eureka node cluster client zone look registry information happens every second locate service could zone make remote callsnonjava service clientsfor service nonjava based choice implementing client part eureka language service run sidekick essentially java application embedded eureka client handle registration heartbeat rest based endpoint also exposed operation supported eureka client nonjava client use rest endpoint query information servicesconfigurabilitywith eureka add remove cluster node fly tune internal configuration timeouts thread pool eureka us archaius configuration tuned dynamicallyresilienceeureka benefit experience gained several year operating aws resiliency built client serverseureka client built handle failure one eureka server since eureka client registry cache information operate reasonably well even eureka server go downeureka server resilient eureka peer going even network partition client server server builtin resiliency prevent large scale outagemultiple regionsdeploying eureka multiple aws region trivial task eureka cluster region communicate one anothermonitoringeureka us servo track lot information client server performance monitoring alertingthe data typically available jmx registry exported amazon cloud watchstay tuned forasgard eureka integrationmore sophisticated netflix midtier load balancing solutionsif building critical infrastructure component like service million people use world wide excites take look jobsnetflixcomreferences
651,Lobsters,scaling,Scaling and architecture,"Free online book on libuv, evented IO for node",http://nikhilm.github.com/uvbook/introduction.html,free online book libuv evented io node,introduction libuv official libuv documentation book background nodejs libev libev removed rust variety code clone download,introduction book small set tutorial using libuv high performance evented io library offer api window unix meant cover main area libuv comprehensive reference discussing every function data structure official libuv documentation may consulted full detail book still work progress section may incomplete hope enjoy grows book reading book either system programmer creating lowlevel program daemon network service client found event loop approach well suited application decided use libuv nodejs module writer want wrap platform apis written c c set synchronous apis exposed javascript use libuv purely context nodejs require resource book cover part specific book assumes comfortable c programming language background nodejs project began javascript environment decoupled browser using google marc lehmann libev nodejs combined model io evented language well suited style programming due way shaped browser nodejs grew popularity important make work window libev ran unix window equivalent kernel event notification mechanism like kqueue e poll iocp libuv abstraction around libev iocp depending platform providing user api based libev version libuv libev removed since libuv continued mature become high quality standalone library system programming user outside nodejs include mozilla rust programming language variety language binding book code based libuv version code code book included part source book github clonedownload book build libuv cd libuv autogensh configure make need make install build example run make code directory
652,Lobsters,scaling,Scaling and architecture,How we keep GitHub fast,https://github.com/blog/1252-how-we-keep-github-fast,keep github fast,responsiveness performance dashboard browser public mission control bar render cache sql git grit job rackbug queryreviewer many new relic graphite fully shipped fast,important factor web application design responsiveness first step toward responsiveness speed speed within web application complicated strategy keeping github fast begin powerful internal tool expose explain performance metric data easily understand complex production environment remove bottleneck keep github fast responsive performance dashboard response time simple average useful complex application number useful performance dashboard attempt give answer question powered data graphite display overview response time throughout githubcom split response time kind request serving ambiguous item browser page loaded browser logged user public page loaded browser logged user clicking one row allows dive see mean percentile percentile response time performance dashboard show performance information explain needed something finegrained detailed mission control bar github staff browse site staff mode mode activated via keyboard shortcut provides access staffonly feature including mission control bar showing see staffonly feature ability moderate site hidden regular user spoiler alert might notice thing screenshot fully shipped yet lefthand side show branch currently deployed total time took serve render page browser like chrome show detailed breakdown various time period make rendered page massively useful understanding slowness come network browser application righthand side collection various application metric given page show current compressed javascript cs size background job queue various data source time ambiguous item render long take render page server cache memcached call sql mysql call git grit call job current background job queue ready make page fast dive number clicking hijacked many feature rackbug queryreviewer produce breakdown many go without saying use many tool like new relic graphite plain old unixfoo aid performance investigation well lot number post much slower like hoping better transparency able deliver fastest web application ever existed tnm say fully shipped fast
654,Lobsters,scaling,Scaling and architecture,Why Tarsnap doesn't use Glacier,http://www.daemonology.net/blog/2012-09-04-why-tarsnap-doesnt-use-glacier.html,tarsnap nt use glacier,tarsnap nt use glacier amazon announced new glacier storage service online backup service view forum thread blog comment powered,tarsnap nt use glacier two week ago amazon announced new glacier storage service providing archival storage little per gb per month since run online backup service naturally interest day following amazon announcement two dozen tweet email asking tarsnap would using glacier answer yet maybe day future people want use glacierized tarsnap would end surface tarsnap sound like perfect use case glacier every tb data stored tarsnap given month approximately gb data deleted gb data downloaded word tarsnap much write read maybe storage system tarsnap largest operational expense storage roughly ten time expensive glacier rest instance run tarsnap server code large majority tarsnap revenue pergb storage pricing could switch tarsnap cheaper storage backend could correspondingly reduce price charge tarsnap user spite friend advising tarsnap cheap already definitely something like downside glacier reason much cheaper retrieval slow request data come back four hour later many tarsnap user would willing wait hour extra retrieve backup meant could store ten time much data price usual devil detail case one detail make thing particularly devilish deduplication taking personal laptop example every hour tarsnap generates archive gb file currently archive stored instead uploading entire gb would require mbps uplink far beyond canadian residential isps provide tarsnap split gb somewhere around block block tarsnap check data uploaded part earlier archive typically around new block need uploaded rest simply handled storing pointer previous block incrementing reference counter reference counter needed archive deleted tarsnap know block still used archive result extracting archive nt simply matter downloading single gb blob involves making separate block read request retrieving data glacier nt cheap like glacier perrequest fee perget fee per million request glacier perretrieval fee per million request would pay extract archive tarsnap right stored glacier would cost tarsnap glacier retrieval alone tarsnap pricing downloads would increase dramatically result get worse tarsnap nt merely deduplicate block data also deduplicates block containing list block block containing list block list block important reducing tarsnap bandwidth storage usage amount data tarsnap uploads laptop hour le would take list block make hourly gb archive make glacier four hour round trip requesting data able read much worse since would need read block wait four hour knowing block need read clearly reading tarsnap archive directly glacier feasible maybe reading archive stored glacier something avoid need restore backup usually want recent backup sure case realize need important file deleted two month ago important keep older backup well could tarsnap save money offloading older rarelyneeded backup glacier tarsnap deduplication get way aforementioned block comprising latest hourly backup thousand uploaded earlier today vast majority uploaded week month ago tarsnap server ca nt simply offload old data glacier since many oldest block data still included newest archive recent archive likely retrieved recent block backup system work full plus incrementals approach advantage since extracting recent archive never going need data prior last complete backup older archive placed cold storage course counterbalanced fact system end performing many full backup lifetime dramatically increasing amount bandwidth storage space used nt feasible tarsnap server move old archive fast storage slow glacier storage tarsnap client could potentially tell server list block nt expect need time soon turn problem arises archive retrieval instead archive creation consider happens block data glacier tarsnap deduplication code decides block needed new archive block referenced location glacier would situation immediately uploading archive wait four hour download could tarsnap client reupload block stored without waiting fetched glacier yes expense using extra bandwidth tarsnap server code accepted would allowing block overwritten new data would violate tarsnap security requirement archive immutable deleted one feasible way see tarsnap use glacier sense simplest obvious one rather operating finegrained level archive cold storage others warm tarsnap could support glaciation permachine level machine glaciated would possible create archive glacier grow snow fall top storage would cost significantly le currently probably around cent per gb per month would able read delete data either would need deglaciate machine would take several hour cost somewhere around cent per gb stored data stored data would back accessible normal random access would charged tarsnap normal storage pricing glaciated machine would likely free charge useful model sure model going happen near future since migrating data glacier way would involve significant amount careful design coding enough people interested goal move towards tell dear reader tarsnap allowed glaciate machine temporarily losing ability read delete archive significantly reducing monthly storage bill would view forum thread blog comment powered
656,Lobsters,scaling,Scaling and architecture,"Observations on Errors, Corrections, & Trust of Dependent Systems",http://perspectives.mvdirona.com/2012/02/26/ObservationsOnErrorsCorrectionsTrustOfDependentSystems.aspx,observation error correction trust dependent system,baikonur cosmodrome main conclusion interdepartmental commission analysis cause abnormal situation arising course flight testing spacecraft phobosgrunt google translate ieee spectrum magazine bad memory chip russia mar probe static random access memory phobos grunt design extreme latchup susceptibility modern commercialofftheshelf cot monolithic cmos static randomaccess memory sram device bad memory chip russia mar probe designing deploying internetscale service http wwwmvdironacom http blogmvdironacom http perspectivesmvdironacom,every couple week get question along line checksum application file given disk already error correction given tcpip error correction every communication packet need application level network error detection another frequent question nonecc mother board much cheaper really need ecc memory answer always yes scale error detection correction lower level fails correct even detect problem software stack introduce error hardware introduces error firmware introduces error error creep everywhere absolutely nobody nothing trusted year time opportunity see impact adding new layer error detection result fire fast fire frequently case predicted would find issue scale even starting perspective time amazed frequency error correction code fired one high scale onpremise server product worked upon page checksum temporarily added detect issue limited beta release code fired constantly customer complaining new beta version buggy use upon deep investigation customer site found software fine customer one sometimes several latent data corruption disk perhaps introduced hardware perhaps firmware possibly software could even corruption introduced one previous release page last written page may written year amazed amount corruption found started reflecting often seen index corruption reported product problem probably corruption introduced software hardware stack u disk complex hardware hundred thousand line code storage area network complex data path million line code device driver ten thousand line code operating system million line code application million line code u screwup opportunity corrupt highly likely entire aggregated million line code never tested precisely combination hardware specific customer actually currently running another example case fleet ten thousand server instrumented monitor frequently dram ecc correcting course several month result somewhere amazing frightening ecc firing constantly immediate lesson absolutely need ecc server application crazy even contemplate running valuable application without extension learning ask really different client server mostly ecc client client correction would instead corruption client dram better fact often worse dimension data corruption happening client system every day day client data silently corrupted day application crash without obvious explanation scale additional cost ecc asymptotically approach cost additional memory store ecc argued year microsoft require ecc window hardware certification system including client would good ecosystem remove substantial source customer frustration fact observation lead embedded system part support ecc nobody want car camera tv crashing given cost scale low ecc memory part client system interesting example space flight world caught attention ended digging ever deeper detail last week learning step russian space mission phobosgrunt also written fobosgrunt roughly translate phobos ground space mission designed amongst objective return soil sample martian moon phobos mission launched atop launch vehicle taking baikonur cosmodrome november november officially reported mission failed vehicle stuck low earth orbit orbital decay subsequently sent satellite plunging earth fiery end expensive mission went wrong aboard phobosgrunt february official accident report released main conclusion interdepartmental commission analysis cause abnormal situation arising course flight testing spacecraft phobosgrunt course document released russian google translate actually good job ieee spectrum magazine reported failing well ieee article bad memory chip russia mar probe good summary translated russian article offer detail interested digging deeper conclusion report double memory fault board phobosgrunt essentially computer dualredundant set failed similar time static random access memory failure computer part newlydeveloped flight control system focused dropping mass flight control system kg lb kg lb le weight flight control weight payload gain important however new flight control system blamed delay mission year eventual demise mission two flight control computer identical computer system supplied techcom spinoff argon design bureau phobos grunt design official postmortem report computer suffered sram failure sram srams manufactured white electronic design model number decoded w white electronic design sram memory bit wide access v improvement mark memory access time package type indicates military grade part paper extreme latchup susceptibility modern commercialofftheshelf cot monolithic cmos static randomaccess memory sram device joe benedetto report sram package susceptible latchup condition requires power recycling return operation permanent case steven mcclure nasa jet propulsion laboratory leader radiation effect group report sram part would unlikely approved use jpl bad memory chip russia mar probe rare even two failure lead disaster case exception upon double failure flight control system spacecraft autonomously go safe mode vehicle attempt stay stable lowearth orbit orient solar cell towards sun continues sufficient power common design pattern system able stabilize extreme condition allow flight control personal back earth figure step take mitigate problem case mitigation likely fairly simple restarting computer probably happened automatically restarting mission would likely sufficient unfortunately still one failure one design fault spacecraft go safe mode incapable communicating earth station probably due spacecraft orientation essentially system need go safe mode still earth orbit mission lost ground control never able command safe mode find last fault fascinating smart people could never make obviously incorrect mistake yet sort design flaw show time large system expert vertical area component good work interaction across vertical area complex sufficiently deep crossverticalarea technical expertise design flaw may get seen good people design good component yet often exist obvious fault mode across component get missed system sufficiently complex enough require deep vertical technical specialization risk complexity blindness vertical team know component well nobody understands interaction component two solution welldefined welldocumented interface component hardware software experienced highlyskilled engineer team focusing understanding intercomponent interaction overall system operation especially fault mode assigning responsibility senior manager often sufficiently effective fault follow complexity blindness often serious depressingly easy see retrospect case example summarizing lesson loss sram chip probably poor choice computer system restart scrub memory fault able detect load corrupt code secondary location going safemode safemode actually allow mitigating action taken ground station useless software system constantly scrubbing memory fault checksumming running software corruption tiny amount processor power spent continuous redundant checking line code implement simple recovery path fault encountered may saved mission finally remember old adage nothing work tested every major fault tested error path common one tested particularly important focus general rule keep error path simple use fewest possible test frequently back wrote set best practice software design testing operation high scale system designing deploying internetscale service paper target largescale service surprising perhaps many suggestion could applied successfully complex space flight system common theme across two partlyrelated domain biggest enemy complexity exploding number failure mode follow complexity incident reminds u importance never trusting anything component multicomponent system checksum every data block welldesigned welltested failure mode even unlikely event rather complex recovery logic near infinite number fault possible simple bruteforce recovery path use broadly test frequently remember hardware firmware software fault introduce error trust anyone anything test system bit flip corrupts ensure production system operate fault scale rare event amazingly common dig deeper phobosgrunt loss james hamilton e jrh mvdironacom w http wwwmvdironacom b http blogmvdironacom http perspectivesmvdironacom
657,Lobsters,scaling,Scaling and architecture,Plugins in C,http://eli.thegreenplace.net/2012/08/24/plugins-in-c/,plugins c,basic plugins c fundamental concept fundamental concept plugin infrastructure discovery registration application hook exposing application capability back plugins detail plugin implementation c exporting symbol application plugins positionindependent code symbol visibility plugins crossdso memory allocation explicitly request static linking,second article series plugin infrastructure c perfect extreme python low level mainstream programming language almost universally serf glue language system understanding plugins may work c help u understand implement crosslanguage plugins future basic plugins c plugins c almost always implemented dsos dynamic shared object aka shared library dlls window c relatively rigid language dsos provide degree dynamism help lot developing plugins namely dynamic loading mechanism provided o allows u add execute new code program runtime basic idea main application load additional dsos represent plugins plugin wellknown symbol function andor global variable application know thus load dynamically dso like shared libary plugin call application code application call plugin code rest article explain topic detail fundamental concept fundamental concept plugin infrastructure help explain c implementation htmlize work quick reminder concept discovery registration application hook plugins attach exposing application capability back plugins follows detailed examination concept implemented example discovery registration main application known directory look plugin dsos implementation directory location relative working directory could anywhere really also specified kind configuration file many application follow route know directory application go file look file appear plugins file ending extension convention dsos linux try load file dlopen relevant portion code make sure path dlopen slash consider actual filesystem path lookup name dstring slashedpath dstringformat dstringcstr fullpath attempt open plugin dso void libhandle dlopen dstringcstr slashedpath rtldnow dstringfree slashedpath libhandle printf error loading dso sn dlerror return null story nt end however register application valid plugin expected initialization function application call function name must init pluginname pluginname name plugin file without extension take ttso plugin example nonstatic initialization function must named inittt code look init function dso attempt find init function call dstring initfuncname dstringformat init dstringcstr name dlsym return void obviously need cast function pointer able call since void function pointer mutually inconvertible eye pedantic complains plain cast cast pointersized integer plugininitfunc initfunc plugininitfunc intptrt dlsym libhandle dstringcstr initfuncname dstringfree initfuncname initfunc printf error loading init function sn dlerror dlclose libhandle return null type plugininitfunc typedef int plugininitfunc pluginmanager pluginmanager central piece infrastructure discus detail later suffices say interface application plugins anyhow init function successfully found plugin dso application call passing pointer pluginmanager init function expected return nonnegative value everything ok int rc initfunc pm rc printf error plugin init function returned dn rc dlclose libhandle return null point plugin discovered registered application loaded shared library initialization function found executed successfully implemented plugindiscovery module pair h c file application hook place discus pluginmanager object c sense word interface expose opaque data type function operate pluginmanagerhc pluginmanager used application plugins plugins use register hook application us find registered hook execute similarly python version htmlize two kind hook hook specific role hook whole content relevant callback function prototype role hook called role content db post object typedef dstring pluginrolehook dstring db post content hook called post content db post object typedef dstring plugincontentshook dstring db post note db post argument discus later registration function plugins use add hook register hook specific role note rolename copied internal data structure void pluginmanagerregisterrolehook pluginmanager pm dstring rolename pluginrolehook hook register hook content void pluginmanagerregistercontentshook pluginmanager pm plugincontentshook hook right time show full code ttso plugin register tt role wrapping content tt tt tag static dstring ttrolehook dstring str db db post post return dstringformat tt tt dstringcstr str int inittt pluginmanager pm dstring rolename dstringnew tt pluginmanagerregisterrolehook pm rolename ttrolehook dstringfree rolename return initialization function plugin recall must called inittt found register role hook tt role plugin manager return success hook simple function performs required transformation completeness application side plugin manager api apply registered role hook given rolenamerolecontents returning string replace role first plugin agrees handle role used plugin found null returned dstring pluginmanagerapplyrolehooks pluginmanager pm dstring rolename dstring rolecontents db db post post apply registered content hook given content returning transformed content registered hook composed hasplugins content applynextplugin content content plugin exists null returned dstring pluginmanagerapplycontentshooks pluginmanager pm dstring content db db post post look pluginmanagerc see implementation function pretty simple pluginmanager hold list registered hook pluginmanagerapply function simply walk list applying hook application request exposing application capability back plugins already seen example pluginmanager api pluginfacing component registering hook technically application capability exposed plugins want reimplement mock database api used python example since provides realistic example applicable many situation point interesting highlight important difference python c python due duck typing one module pas object another module nt type information object call method c thing easy therefore use db post object plugins need include application header file defining dbh note due nature dynamic linking linux plugins nt actually link dbo object later demonstrate code part narcissistso plugin turn occurrence b username b include dbh static dstring narcissistcontentshook dstring str db db post post dstring replacement dstringformat b b dstringcstr postgetauthor post int initnarcissist pluginmanager pm pluginmanagerregistercontentshook pm narcissistcontentshook return hook get passed pointer db post object plugin us dbh api access post object case postgetauthor function extract username post detail plugin implementation c concludes description htmlize application plugins implemented c want complete lowlevel implementation detail may interesting reader thing make plugin implementation c trickier python since c manually deal much detail exporting symbol application plugins compiler invocation required build ttso plugin gcc c pluginsttc pluginstto pedantic g wall fpic gcc pluginsttso pluginstto shared standard linux dso build source compiled fpic generate positionindependent code dso built shared tell linker create shared library creating dso shared nt link object file found application load dso plugin us symbol number object file dstringo dbo pluginmanagero let see look symbol table readelf dynsyms pluginsnarcissistso symbol table dynsym contains entry num value size type bind vi ndx name notype local default und section local default notype global default und dstringlen notype global default und dstringnewlen snip notype global default und postgetauthor snip dynamic symbol table section used dynamic linker linux symbol management say symbol dstringlen postgetauthor others undefined dynamic linker expect find application loading dso otherwise get symbol resolution error runtime important gotcha linker export symbol application plugins default explicitly told mean exportdynamic linker flag portion ld manual page describes flag well exportdynamic noexportdynamic creating dynamically linked executable using e option exportdynamic option cause linker add symbol dynamic symbol table dynamic symbol table set symbol visible dynamic object run time use either option use noexportdynamic option restore default behavior dynamic symbol table normally contain symbol referenced dynamic object mentioned link use dlopen load dynamic object need refer back symbol defined program rather dynamic object probably need use option linking program also use dynamic list control symbol added dynamic symbol table output format support see description dynamiclist behavior easy observe example interested main application htmlizemain currently compiled exportdynamic flag look dynamic symbol table readelf dynsyms see global symbol exported recompile without flag check dynamic symbol table wo nt contain symbol dlopen call plugindiscoveryc fail undefined symbol error symbol visibility plugins seen special provision required application symbol visible inside pligins true symbol visibility plugins though mechanism different application load plugin dlopen plugin symbol found calling dlsym application however plugins need use symbol well default wo nt work make work possible pas rtldglobal flag dlopen opening plugin want expose symbol symbol plugin dso made available resolve reference subsequently loaded dsos crossdso memory allocation hard see htmlize example memory allocated one dso main application released another especially come window background may raise eyebrow crossdso memory allocation likely wrong c library linked statically dso get version c library bookkeeping malloc et al memory allocated one dso ca nt released another however linux customary link c library dynamically happens default unless explicitly request static linking linked dynamically single version c library symbol exists process address space execution crossdso memory allocation release safe
658,Lobsters,scaling,Scaling and architecture,Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web,http://highscalability.com/blog/2012/7/30/prismatic-architecture-using-machine-learning-on-social-netw.html,using machine learning social network figure read web,prismatic jason wolfe tech crunch article stats platform data storage io service data ingest backend onboarding backend api clientfacing service client facing batch service graph library machine learning document user ml document fast running story clustering component ml user lesson learned find story find embarrassingly parallel opportunity exploit work parallel user waiting use functional programming avoid large monolithic framework right people vital making work code put production early often keep simple invest building great tool library think carefully type data related article,post prismatic architecture adapted email conversation prismatic programmer jason wolfewhat read web today thoroughly modern person must solve dilemma every day usually using occult process divine important many feed twitter r facebook pinterest g email techmeme uncountable number information sourcesjason wolfe prismatic generously agreed describe thoroughly modern solution answering read question using lot sexy word like machine learning social graph bigdata functional programming inmemory realtime feed processing result possibly even occult something much like meet challenge finding interesting topic story hidden inside infinitely deep pool informationa couple thing stand prismatic want know prismatic built small team four computer scientist three strong young phd stanford berkeley halfhazard method bringing brain power solving information overload problem phd also programmer working everything website io programming well sexy bigdataml backend programming one thing excited prismatic architecture problem need solved future applying machine learning great stream socially mediated information realtime secrecy prevents saying much machine learning tech get peak behind curtainas might expect thing little differently chosen clojure modern lisp compiles java bytecode programming language choice idea use functional programming build finegrained flexible abstraction composed express problemspecific logic one example functional power graph library use place example graph computation described create equivalent lowlatency pipelined set mapreduce job user another example use subgraphs compactly describe service configuration modular waygiven focus functional elaboration avoid large framework like hadoop going smaller reliable easier debug easier extend easier understand codebase criticism prismatic approach long training period needed get result first say take long start getting good content second would add start thinking type system using long sight ml based recommenders start trained childhood stay entire life scalable digital analog mind act information gatekeeper wingman tech crunch article prismatic founder bradford cross pithily describes prismatic built around complex system provides large scale realtime dynamic personalized reranking information well classifying grouping topic ontology let see system look like stats every day million million new news article blog post shared content fetched analyzed ten million social signal twitter facebook google reader around web within second user signing prismatic enough historical activity fetched analyzed make pretty solid topic publisher suggestion social network digested find friend seem share relevant content every interaction user make prismatic also learning opportunity within tenth second visiting home feed feed produced interesting story matching model user interest content million article impression served user every week platform hosted backend pipeline api server written clojure modern lisp compiles java bytecode heavy lifting happens inside jvm mongodb mysql dynamo focus building custom code efficiently solve particular problem data storage io rather typical architecture service read writes live data database distributed filesystem system designed around data data flow backend pipeline one service next without roundtrips disk end pipeline api machine rely heavily custom inmemory data structure periodically snapshotted disk keeping data close cpu possible api request served little io allows serving feed low latency keep api machine cpu bound make gracefully scaling handle user much easier number offtheshelf storage solution used mongodb mysql amazon dynamo chosen carefully match size access pattern characteristic data stored service high level architecture split distinct service type roughly different category data ingest backend onboarding api client facing service client facing batch service service designed one kind thing horizontally scalable particular way often limited particular resource type two io cpu ram economics favor quite large machine service singleton expectation major bottleneck prevent horizontally scaling data ingest backend ton great content news article blog post web page etc created day prismatic want know much possible piece content must known sharing saying relevant commentary shown alongside article user shown content shared friend people similar taste first step process ingesting analyzing content social data top ingest pipeline pollers r poller loop feed looking new article twitter facebook pollers connect corresponding apis fetch commentstweets user friend service pretty simple mostly stateless real state interesting part figuring seen already intelligently prioritizing poll next one difficult problem actually figuring piece content social interaction flow rest pipeline hard since people may share many version article shortened link mobile regular version etc r entry commentssharestweets properly canonicalized url flow barrier decided actually fetch analyze spam junk eliminated later pipeline best viagra spam article one cycle wasted url make cut passed fetchinganalysis pipeline url phase go fetchinganalysis pipeline lot magic start happening queue url kept run graph successively elaborates url fetching html applying machine learning algorithm extract text article identify best image extract publisher label applicable topic whole lot engineering gone making algorithm run fast fit memory perform well problem handling url course embarrassingly parallel end ingest phase doc master whose job receive elaborated article social context continues trickle time match cluster article online fashion story cluster decide current active set doc manage index api machine onboarding backend onboarding ingesting data new user provided great personalized experience within second signing app two major component figuring suggested topic publisher user based activity twitter facebook google reader analyzing social graph user try deduce friend share powerful signal like article service embarrassingly parallel time across user aside intricacy efficient social graph analysis prismatic want say much key service tuned quite low latency reasonable throughput example user active twitter facebook google reader possible compute hundred accurate topic publisher suggestion second le fast enough suggestion start computed user oauth usually ready personalized suggestion time user finished creating account selecting handle password reached walkthrough second quite lot happens user recent post fetched twitter facebook article liked google reader may take second finish identification collection thousand unique url shared user friend etc flow version fetchinganalysis pipeline url fetched elaborated ml stack result analysis aggregated post processed saved dynamodb latency process really critical process executed serially pipelined parallelized much possible throughput also important process quite heavyweight lot signups take lot effort keep latency prismatic stream processing aggregation library really pull weight allowing performance equivalent lowlatency pipelined set mapreduce job user multiple user parallel using close full capacity machine api clientfacing onboarding data ingest service come together api machine much said say actual feed generation process complex highly optimized process lot stage match user fingerprint query index retrieves rank page resulting feed hundred millisecond major design goalschallenges system perspective recent article must indexed available lowlatency feed generation index quite large many gigabyte must kept uptodate real time user find breaking story user load balanced across machine relatively easy fast spin new machine shut response load generating good user feed requires index also need user fingerprint interest social connection article recently seen fingerprint quite large constantly change user view interacts content site solution first problem involve docmaster docmaster machine organizes current document set preprocess doc every minute writes set prepared index file new api machine spin first read file dump memory giving nearlyuptodate copy index docmaster also publishes command index change documentcomment addition deletion story clustering change etc realtime live api machine new machine come replay history last minute change bring index present general nonuserspecific state needed machine also read memory periodically refreshed stored dynamo data larger access le frequent remaining problem efficiently serve feed user without incurring io cost latency fetching updating fingerprint request approach use sticky session bind user api machine user first sign data loaded memory expiring flushing writeback cache throughout user session data stay memory used generate feed user action pas api machine throughout session update le crucial part fingerprint batched flushed redundant storage every minute session expires machine shuts crucial update done synchronously least direct writethrough cache high io hit paid reading user fingerprint throughout course session amortized across typically relatively large number feed click page article click share user take limit number writes back data machine come data flushed user moved another machine worst case minute noncritical data lost user consider worth benefit simplicity scalability loss always recoverable via later batch job raw event log service client facing separate clientfacing service publicfeeds smart caching topic publisher feed nonlogged user fetching regular api demand allowing multiple age feed paged auth handle account creation signin mostly thin layer front sql database store critical user data need periodically snapshotted backed urlshortener batch service service machine language training data archival event trackinganalytics mongodb used store server metric user analytics largely support nice lowhassle story sending raw event different shape maintaining right index keeping online rollups count graph library really nice way declaratively describe graph computation play clojure strength graph clojure map key keywords value keyword function take value computed function map well external parameter used place instance docanalysis pipeline graph elaboration document may depend previous eg identifying topic document depends already extracted text newsfeed generation process graph composed many query ranking step production service graph resource eg data store memory index http handler node depend others make easy thing like compactly describe service configuration modular way eg using subgraphs produce chart diagramming dependency process service measure computation time error happen node complex computation like document analysis activity resource complex service like api smartly schedule different node computation different thread theory even machine easily write test entire production service replacing node graph mock eg faking storage machine learning document user document user two area prismatic applies ml machine learning ml document given html document learn extract main text page rather sidebar footer comment etc title author best image etc determine feature relevance eg article topic etc setup task pretty typical model trained using big batch job machine read data save learned parameter file read periodically refresh model ingest pipeline data flow system fed back pipeline help learn interesting learn mistake time software engineering perspective one interesting framework prismatic written flop library implement stateoftheart ml training inference code look similar nice ordinary clojure code compiles using magic macro lowlevel array maniuplation loop close metal get java without resorting jni code order magnitude compact easy read corresponding java execute basically speed lot effort gone creating fast running story clustering component ml user guess user interested social network data refine guess using explicit signal within app remove problem using explicit singnals interesting user input reflected feed quickly user remove article given publisher row stop showing article publisher right tomorrow mean nt time run another batch job user solution online learning immediately update model user observation provide u raw stream user interaction event saved allows rerruning later user interest ml raw event case data lost slightly loose writeback cache data machine go something like drift online learning corrected accurate model computed lesson learned find story think carefully entire pipeline data flow work around scalability challenge problemspecific solution service scaling story built service communicate way make easy scale component without putting much pressure others starting instead system built around hadoop job data stored raw form distributed databasefilesystem prismatic would telling different story find embarrassingly parallel opportunity exploit work parallel user waiting onboarding process example ingests data new user provided great personalized experience within second signing app use functional programming build finegrained flexible abstraction abstraction composed express problemspecific logic avoid large monolithic framework like hadoop yield smaller code base thing equal le error prone simpler understand easier extend built library simply much functionality opensource code locked monolithic framework easily reusable extensible debuggable something break fails scale right people vital making work current backend team consists three c phd working everything ml machine language algorithm lowlevel system engineering web iphone client code code put production early often though investment tool make building debugging production service easy fun keep simple nt take burden complex library framework simple stuff nt get fancy much simpler solution good enough example use simple httpbased messaging protocol rather one popular framework make sense happy pay bit managed solution dynamo work invest building great tool library instance prismatic flop library allows write numbercrunching machine learning algorithm fast java code store abstract away many unimportant detail keyvalue storage allowing writing higherlevel abstraction caching batching flushing work variety circumstance across variety storage engine graph make writing testing monitoring distributed stream processing service breeze think carefully type data nt expect find onesizefitsall solution io storage related article
660,Lobsters,scaling,Scaling and architecture,Amazon Web Services Blog: Amazon Glacier: Archival Storage for One Penny Per GB Per Month,http://aws.typepad.com/aws/2012/08/amazon-glacier-offsite-archival-storage-for-one-penny-per-gb-per-month.html,amazon web service blog amazon glacier archival storage one penny per gb per month,amazon glacier tell introduced one trillion object vault archive aws importexport job amazon sn pricing page enterprise department digital medium scientific research data glacier aws management console aws sdks glacier documentation september glacierjobs amazoncom,need glacier im going bet organization spend lot time lot money archiving missioncritical data matter whether youre currently using disk optical medium tapebased storage probably complicated expensive process youd like spending time maintaining hardware planning capacity negotiating vendor managing facility true going find newest service amazon glacier interesting glacier store amount data high durability cost allow get rid tape library robot operational complexity overhead part parcel data archiving decade glacier provides cost low one u penny one onehundredth dollar per gigabyte per month extremely low cost archive storage store little bit store lot terabyte petabyte beyond upfront fee pay storage use worry capacity planning never run storage space glacier remove problem associated overprovisioning archival storage maintaining geographically distinct facility verifying hardware data integrity irrespective length retention period tell introduced amazon march growth past year strong steady store one trillion object glacier build reputation durability dependability new access model designed able allow u offer archival storage extremely low cost store data glacier start creating named vault vault per region aws account created vault simply upload data archive glacier terminology archive contain terabyte data use multipart uploading aws importexport optimize upload process glacier encrypt data using store durably immutable form glacier acknowledge storage request soon data stored multiple facility creating vault amazon glacier glacier store data high durability service designed provide average annual durability per archive behind scene glacier performs systematic data integrity check heals necessary intervention part plenty redundancy glacier sustain concurrent loss data two facility point may thinking sound like amazon amazon glacier differs two crucial way first optimized rapid retrieval generally ten hundred millisecond per request glacier call glacier nothing glacier retrieval request queued honored somewhat leisurely pace archive available downloading hour retrieval request make glacier called job poll glacier see data available ask send notification amazon sn topic choice data available access data via http get request including byte range request data remain available hour retrieval request priced differently retrieve average monthly storage prorated daily free month beyond charged retrieval fee starting per gigabyte see pricing page detail data youll need retrieve greater volume frequently may costeffective service notification retrieval job secondly allows assign name choice object order keep cost low possible glacier assign unique id archive upload time glacier action sure already us mind glacier get started part enterprise department store email corporate file share legal record business document kind stuff need keep around year decade little reason access work digital medium archive book movie image music news footage forth asset easily grow ten petabyte generally accessed infrequently generate collect scientific research data store glacier case need get back later get started glacier available use today useast n virginia uswest n california uswest oregon asia pacific tokyo euwest ireland region watch new video see get started access glacier aws management console glacier apis added glacier support aws sdks also plenty glacier documentation like know even glacier please join u online seminar september think jeff p engineer engineering manager interest massive scale distributed storage system wed love hear please send resume glacierjobs amazoncom
661,Lobsters,scaling,Scaling and architecture,Amazon Glacier lauches: $.01/G reliable storage with multi-hour retrieval queue,http://aws.amazon.com/glacier/,amazon glacier lauches reliable storage multihour retrieval queue,aws cloudtrail learn security learn compliance,amazon glacier glacier deep archive storage class offer sophisticated integration aws cloudtrail log monitor retain storage api call activity auditing support three different form encryption storage class also support security standard compliance certification including sec rule pcidss hipaahitech fedramp eu gdpr fisma amazon object lock enables worm storage capability helping satisfy compliance requirement virtually every regulatory agency around globe learn security learn compliance
664,Lobsters,scaling,Scaling and architecture,Getting 2.5 Megalines of code to behave,http://jlouisramblings.blogspot.com/2012/08/getting-25-megalines-of-code-to-behave.html,getting megalines code behave,,
665,Lobsters,scaling,Scaling and architecture,Debunking the Node.js Gish Gallop,http://www.unlimitednovelty.com/2012/08/debunking-nodejs-gish-gallop.html,debunking nodejs gish gallop,programmer ruby rail enthusiast switch nodejs think awesome proceeds write blog post node bee knee rail crap stop think heard one keep hearing gish gallop worry keep hearing well rail api activemodel serializers handled middleware layer ip spoofing attack timing attack handled actionpack layer github gist api lightweight stack backbone spine ember may heard ember throne j framework choice activemodel serializers ember data libuv phrase flow control specific definition relating rate data transmitted system producerconsumer problem cramp reel support websockets managed catch tenderlove recent blog post streaming live data dcell celluloid zmq cost ram done better ruby,programmer ruby rail enthusiast switch nodejs think awesome proceeds write blog post node bee knee rail crap attention drawn changing nature web design web page servergenerated html singlepage jsheavy apps written using backbone ember etc stop think heard one argument keep hearing far concerned nothing gish gallop completely specious argument really worry worry keep hearing fact keep hearing make worry people actually believing nt know keep hearing sure people running problem reading prevailing wisdom coming conclusion really make sad whenever read post like feel previous passion idea half lifetime ago opinion changed road mountain blazed trail realized stupid defeat gish gallop nt really enjoy far tell way must go argument one one show completely ludicrous go case confused rail awesome json apis single page application love clientheavy apps nt want every page web one many application benefit ton keeping state browser general something without go across network provide better user experience bar none primary thing application crave awesome json apis websockets stay tuned use rail json api nt rail designed htmljs page benefit rail give building json apis nt rail really slow well building apionly application singlepage frontend definitely check rail api rail api completely eliminates actionviewcentrism may worried rail give awesome tool building json apis like activemodel serializers alone ca nt express rail brings table list feature rail provides useful json apis courtesy rail api readme handled middleware layer reloading rail application support transparent reloading work even application get big restarting server every request becomes nonviable development mode rail application come smart default development making development pleasant without compromising productiontime performance test mode ditto test mode logging rail application log every request level verbosity appropriate current mode rail log development include information request environment database query basic performance information security rail detects thwart ip spoofing attack handle cryptographic signature timing attack aware way nt know ip spoofing attack timing attack exactly parameter parsing want specify parameter json instead urlencoded string problem rail decode json make available params want use nested urlencoded params work conditional get rail handle conditional get etag lastmodified processing request header returning correct response header status code need use stale check controller rail handle http detail caching use dirty public cache control rail automatically cache response easily configure cache store head request rail transparently convert head request get request return header way make head work reliably rail apis handled actionpack layer resourceful routing building restful json api want using rail router clean conventional mapping http controller mean spend time thinking model api term http url generation flip side routing url generation good api based http includes url see github gist apifor example header redirection response head nocontent redirectto userurl currentuser come handy sure could manually add response header caching rail provides page action fragment caching fragment caching especially helpful building nested json object basic digest token authentication rail come outofthebox support three kind http authentication instrumentation rail added instrumentation api trigger registered handler variety event action processing sending file data redirection database query payload event come relevant information action processing event payload includes controller action params request format request method request full path generator may pass advanced rail user nice generate resource get model controller test stub route created single command plugins many thirdparty library come support rail reduces eliminates cost setting gluing together library web framework includes thing like overriding default generator adding rake task honoring rail choice like logger cache backend rail unquestionably awesome feature set even applied exclusively json apis guy taking completely granted rail server becomes api web site like io app client clean separation responsibilies given rail designed like horse rider climb top elephant design rail rail provided clean abstraction using code provide servergenerated html view rest apis multiple serialization format big deal time time year node even existed fast forward year rail rewritten emphasis modularization allowing strip component nt use build lightweight stack thing need rail api provides convention configuration lightweight jsonoriented stack let back little bit view mvc html cs presentation logic presentation logic need structure need clientside framework like backbone spine ember come picture hear guy yehuda katz worked ember rail may heard ember throne j framework choice backbone library category appeal authority aside using ember rail combination actually get problem certain run manual nature serializing json exactly translate domain object json representation client want avoid repeat request eagerly loading domain object associated one want retrieve including json result would nt great single canonical representation standardized domain object abstraction running browser could automatically consume u nt manually write bunch json serialization deserialization logic everything system put json rail yes called activemodel serializers ember data glue code writing serializing unserializing json stop seriously better thing deal idiosyncrasy whether wrap particular array object return literal string number opposed object future proofing wasting time minutia chance activemodel serializers representation better one using let take look defining characteristic activemodel serializers json representation explicitly avoids nesting object within object instead preferring keep resulting structure flat using id correlate relationship data structure example post object includes comment tag taken activemodel serializers readme post id title new post body body comment comment id body dumb post tag id body liked tag tag id name short id name whiny id name happy multiple nested relationship document post many comment comment many tag yet nt see duplication comment tag object nt worry version repeated object canonical repeated object object within resulting document deduplicated referred symbolically id using json structure represent arbitrarily nested relationship object efficient manner possible completely avoid problem inconsistency duplicated version object present document representation json make sense perhaps standardized upon better yet use representation little effort part ember data automatically consume use ember rail abstract away json save headache writing custom serialization code going say score one rail single page application maybe node thing nt know seriously think rail bad json apis nt know rail moving right along let continue slogging gish gallop node nonblocking async io rail nt rail slow start one hmm let start think ruby rail performance think ilya grigorik let start saying ilya awesome guy done thorough nuanced survey many facet ruby performance time taking single thing said context treating like gospel probably disservice ilya said let see thing ilya said guy chose single present context quoth ilya nothing node ca nt reproduced ruby python eventmachine twisted fact framework force think use right component place fully async nonblocking exactly currently grabbing mindshare early adopter rubyists pythonistas others ignore trend peril moving forward endtoend performance scalability framework become important line hear ryan dahl lot line used believe folk stuff awhile first discovered synchronous io multiplexing half lifetime ago since building network server using approach built abstraction layer across selectpollepollkqueue wrapped libev ruby revcoolio latter crossplatform abstraction java nio jruby express much work invested thing evented nonblocking way nt think nonblocking io good fit web application talk http although think good fit websocket application get reason later first let continue digging gish gallop ilya mentioned frameworkecosystem consider threat rail nodejs biggest thing noticed difference performance consumed le memory ruby served request per second sinatra even rack huge pet peeve people talk performance without number tried faster tried slower really want make point performance particular thing least pretend using science hate think destroy god let see ilya software stack mine crappy hello world web server benchmark first number web server reel httperf ruby version throughput latency jruby head reqss msreq ruby reqss msreq jruby reqss msreq rbx head reqss msreq let compare ilya web server goliath well thin nodejs web server throughput latency goliath reqss msreq thin reqss msreq nodejs reqss msreq server including mine using nonblocking evented io remotely relevant coincidence web server faster ilya gish gallop logic ilya must wrong everything must reason use ilya web server let write everything node since benchmark huge problem goliath thing reel thin node http server nt reason slower nt ilya suck clueless performance reason goliath feature web server nt make apple orange comparison guess scumbag putting big list reel web page said rail probably nt ever going better latency entire stack nodejs framework latency rail stack probably going lot le application logic still going drop bucket compared network latency given user celluloid solves every single problem whining better node node lot problem talking audience attracts let start saying many thing built celluloid based technology originally developed node web server reel us node http parser quite likely next iteration develop based libuv said let start node fundamental problem callbackdriven io celluloid io one many system including erlang go demonstrate nonblocking evented io orthogonal callback celluloid us ruby coroutine mechanism provide synchronous io api top underlying nonblocking system however system like node force use nonblocking io everything celluloid let mix match blocking nonblocking io need demand ever worked language like c java probably know amazing property socket mix match blocking nonblocking io even lifecycle single socket perhaps handle incoming socket nonblocking manner first make complex request might change socket blocking mode hand worker thread celluloid io make handoff completely transparent simply giving socket another ruby thread nt celluloid io actor automatically switch nonblocking blocking mode completely transparently let talk node real fundamental problem one extremely difficult solve callbackdriven system flow control unfortunately nodejs community adopted phrase flow control mean building abstraction around managing callback however phrase flow control specific definition relating rate data transmitted system general callbackdriven system ca nt manage flow control effectively notable pathological case producerconsumer problem whereby slow consumer might force system like node unboundedly buffer data unchecked producer clear simple solution problem make io synchronous using coroutines provide blockingstyle apis easily compose producerconsumer problem manner nt result unbounded writes buffer simply virtue virtual blocking api rate data transfered producer consumer kept check websockets ruby pretty awesome albeit overlooked therefore stagnant solution websockets awhile like cramp working webbased push technology half decade explored multitude solution including comet xmppbosh rabbitmq long polling xhr long polling system originally built around gasp thread nearly year ago point well quite happy say reel support websockets certainly nt want say recent spike anywhere mature websockets node surrounding ecosystem instead think api reel provides websocks simply better design managed catch tenderlove recent blog post streaming live data may understand previous apis may encountered system like rail node streaming data really obscuring one api truly make sense use case socket websockets many way similar socket used dcell via celluloid zmq websockets provide framing mechanism provides messagebased transport instead typical streambased transport provided tcp said processing message sequence callback become extremely problematic must reconstruct state current request point incoming message callback work well eg chat protocol state relationship message soon effectively stuck building finite state machine manage processing incoming message madness much better much straightforward solution problem use goddamn stack order need provide blocking api nt orthogonal using nonblocking io celluloid io go erlang let build concurrent multithreaded potentially multicore system top coroutines spread across multiple native thread said native thread cheap nowadays getting cheaper ruby vms native thread cost ram want build blocking io system completely native thread without using sort evented io system scale ten thousand connection nt believe hype node provides limited subset ruby done better ruby node web framework caliber rail node nt thread ruby spare node callback soup finally elephant room javascript terrible terrible programming language compared ruby forced use javascript browser server choose best language job ruby rail remains bestinclass web framework argument made one hear coming confused nodejs detractor hold water
666,Lobsters,scaling,Scaling and architecture,How to crawl a quarter billion webpages in 40 hours,http://www.michaelnielsen.org/ddi/how-to-crawl-a-quarter-billion-webpages-in-40-hours/,crawl quarter billion webpage hour,presentation jeff dean number url cuil billion page code common crawl architecture alexa top million domain fabric set python script boto extra large amazon machine image single instance architecture domain allocated across thread problem author url frontier work redis python binding use bloom filter bloom filter mike axiak pybloomfiltermmap point comment anticipated versus unanticipated error domain subdomain handling john kurkowski tldextract library representation url frontier choice number thread independent post use python lxml politeness robotparser library robot exclusion protocol useragent webpage problem author truncation report storage instance storage price spot instance improvement crawler architecture presentation jeff dean pubsubhubbub evented architecture chef puppet chapter survey paper heritrix nutch,precisely crawled page dollar hour minute using amazon machine instance carried project among several reason wanted understand resource required crawl small nontrivial fraction web post describe detail course nothing especially new wrote vanilla distributed crawler mostly teach something crawling distributed computing still learned lesson may interest others post describe post also mix personal working note future reference mean crawl nontrivial fraction web fact notion nontrivial fraction web well defined many website generate page dynamically response user input example google search result page dynamically generated response user search query make much sense say soandso many billion trillion page web turn make difficult say precisely meant nontrivial fraction web however reasonable proxy size web use number webpage indexed large search engine according presentation googler jeff dean november google indexing ten billion page note number url trillion apparently duplicated page content multiple url pointing content nowdefunct search engine cuil claimed index billion page comparison quarter billion obviously small still seemed like encouraging start code originally intended make crawler code available open source license github however better understood cost crawler impose website began reservation crawler designed polite impose relatively little burden single website could like many crawler easily modified thoughtless malicious people impose heavy burden site decided postpone possibly indefinitely releasing code general issue get crawl web relatively site exclude crawler company google microsoft lot crawler many without much respect need individual siteowners quite reasonably many siteowners take aggressive approach shutting activity le wellknown crawler possible side effect becomes common point future may impede development useful new service need crawl web possible longterm solution may service like common crawl provide access common corpus crawl data interested hear people thought issue later update get regular email asking send people code let preemptively say decline request architecture basic architecture master machine laptop begin downloading alexa list top million domain used domain whitelist crawler generate starting list seed url domain whitelist partitioned across machine instance crawler done numbering instance allocating domain domain instance number hash domain hash standard python hash function deployment management cluster handled using fabric welldocumented nicely designed python library streamlines use ssh cluster machine managed connection amazon using set python script wrote wrap boto library used amazon extra large instance running ubuntu natty narwhal amazon machine image provided canonical used extra large instance testing several instance type extra large instance provided marginally page downloaded per dollar spent used u east north virginia region least expensive amazon region along u west oregon region single instance architecture instance partitioned domain whitelist separate block domain launched python thread thread responsible crawling domain one block worked detail reason using thread python standard library us blocking io handle http network connection mean singlethreaded crawler would spend time idling usually waiting network connection remote machine crawled much better use multithreaded crawler make fuller use resource available instance chose number crawler thread empirically kept increasing number thread speed crawler started saturate number thread crawler using considerable fraction cpu capacity available instance informal testing suggested cpu limiting factor far away network disk speed becoming bottleneck sense extra large instance good compromise memory useage never issue possible reason highcpu extra large instance type would better choice experimented instance type early version crawler memorylimited domain allocated across thread thread numbered domain allocated basis python hash function thread number hash domain similar allocation across machine cluster whitelisted domain seed url allocated thread crawl done simple breadthfirst fashion ie seed url download corresponding web page extract linked url check url see whether extracted url fresh url already seen added url frontier b whether extracted url seed domain page crawled condition met url added url frontier current thread otherwise url discarded architecture essentially carrying large number independent crawl whitelisted domain obtained alexa note architecture also ensures example crawling page techcrunch extract page link huffington post latter link discarded even though huffington post domain whitelist link added url frontier point back techcrunch reason avoid adding dealing whitelisted external link may require communication different instance would substantially complicate crawler importantly b practice site lot internal link unlikely policy mean crawler missing much one advantage allocating url domain crawler thread make much easier crawl politely since one connection site open given time particular ensures hammering given domain many simultaneous connection different thread different machine problem author large rapidly changing website may necessary open multiple simultaneous connection order crawl keep change site decide appropriate url frontier work separate url frontier file maintained domain simply text file line containing single url crawled initially file contains single line seed url domain spoke url frontier thread frontier thought combination url frontier file domain crawled thread thread maintained connection redis server domain crawled thread redis keyvalue pair used keep track current position url frontier file domain used redis python binding store information fashion persistent fast look persistence important meant crawler could stopped started without losing track url frontier thread also maintained dictionary whose key hashed domain thread corresponding value next time would polite crawl domain value set second last time domain crawled ensure domain getting hit often crawler thread simply iterated key dictionary looking next domain polite crawl found domain extracted next url url frontier domain went downloading page url frontier exhausted domain run page crawl domain key removed dictionary one limitation design restarting crawler thread identify domain already exhausted deleted dictionary slowed restart little something modify work crawler use bloom filter used bloom filter keep track url already seen added url frontier enabled fast check whether new candidate url added url frontier low probability erroneously adding url already added done using mike axiak nice cbased pybloomfiltermmap update jeremy mclain point comment got backward bloom filter low probability never crawl certain url bloom filter telling already crawled fact better albeit slightly slower solution would simply store url check directly anticipated versus unanticipated error crawler ingests input external source need deal many potential error design two broad class error anticipated error unanticipated error anticipated error thing like page failing download timing containing unparseable input robotstxt file disallowing crawling page anticipated error arise crawler writes error perthread informational log info log diagram continues whatever way appropriate example robotstxt file disallows crawling simply continue next url url frontier unanticipated error error anticipated designed rather crawler falling crawler simply log error critical log diagram move next url url frontier time crawler track many unanticipated error occurred close succession many unanticipated error occur close succession usually indicates key piece infrastructure failed many unanticipated error close succession crawler shuts entirely developing testing crawler closely followed unanticipated error logged critical log enabled understand many problem faced crawler example early development found sometimes html page would badly formed html parser would little choice raise exception came understand error would rewrite crawler code error become anticipated error handled gracefully possible thus natural tendency development unanticipated error become anticipated error domain subdomain handling mentioned crawler work lot parallel intradomain crawl work well problem arises widespread use subdomains example start seed url http barclayscom crawl url within barclayscom domain quickly run url crawl reason internal link barclayscom site actually groupbarclayscom barclayscom crawler also add url latter domain url frontier barclayscom resolve stripping subdomains working stripped domain deciding whether add url url frontier removing subdomains turn surprisingly hard problem variation way domain name formed fortunately problem seems well solved using john kurkowski tldextract library representation url frontier noted separate url frontier file maintained domain early version code crawler thread url frontier maintained single flat text file crawler thread read line file would crawl url append new url found end file approach seemed natural organizing url frontier file perthread rather perdomain basis caused surprising number problem crawler thread moved file find next url crawl crawler thread would encounter url belonging domain yet polite crawl crawled recently initial strategy simply append url end file would found later unfortunately often lot url row consecutive url often came domain since extracted page strategy caused file url frontier grow rapidly eventually consuming disk space exacerbating problem approach url frontier caused unforseen domain clumping problem understand problem imagine crawler thread encountered say consecutive url single domain might crawl first extracting say extra url append end url frontier next url would skipped since yet polite crawl also appended end url frontier url domain end url frontier crawler thread get may well process repeat leading clump url domain end file leading long run url domain consumes lot disk space also slows crawler since crawler thread may need examine large number url find new url okay crawl problem could solved various way moving perdomain url frontier file chose address problem seemed work well choice number thread mentioned number crawler thread chosen empirically however important constraint number particular relationship number instance used suppose instead thread used say thread would create problem see note domain allocated instance number say would necessarily satisfy hash domain would imply hash domain consequence domain would allocated one three crawler thread thread number crawler thread would lie idle defeating purpose using multiple thread one way solve problem would use two independent hash function allocate domain instance crawler thread however even simpler way solving problem choose number crawler thread coprime number instance coprimality ensures domain allocated reasonably evenly across instance thread prove proved little effort easily checked coprime note incidentally python hash true hash function sense guarantee domain spread evenly across instance turn python hash take similar key string similar hash value talk point example fifth paragraph post however found empirically hash seems spread domain evenly enough across instance worry using better slower hash function like available python hashlib library use python code written python initially wondered python might slow create bottleneck crawling however profiling crawler showed time spent either managing network connection downloading data b parsing resulting webpage parsing webpage done using lxml python binding fast underlying c library seem likely easy speed concluded python likely particular bottleneck crawling politeness crawler used python robotparser library order observe robot exclusion protocol noted also imposed absolute minimum time interval access given domain practice mean time access like minute initial test run crawler got occasional email webmaster asking explanation crawling site crawler useragent included link webpage explaining purpose crawler exclude site step taking crawl politely presume helpful webmaster also helpful reduced number inquiry handful people asked exclude site crawl complied quickly problem author crawl take long robotstxt file downloaded domain beginning crawl longer crawl decide long wait downloads robotstxt truncation crawler truncates large webpage rather downloading full page part necessary really surprise someone terabyte html file sitting server somewhere part many application interest focus earlier part page reasonable threshold truncation according report google may average network size webpage top site kb however includes image script stylesheets crawler ignores ignore image average network size drop kb however number kb content may served compressed network truncation based uncompressed size unfortunately google report tell u average size uncompressed content however get estimate since google report average uncompressed size total page including image kb average network size kb assuming compression ratio typical estimate average uncompressed size content crawler downloads kb event experimented several truncation setting found truncation threshold kilobyte enabled download great majority webpage entirety addressing problem large html file mentioned unfortunately think check actual average uncompressed size mistake storage stored data using builtin instance storage terabyte extralarge instance using storage nonpersistent data stored instance vanish instance terminated many kind streaming shortterm analysis data would adequate indeed might even necessary store data course many application crawl approach appropriate instance storage supplemented something permanent purpose using instance storage seemed fine price price broke two component dollar use extralarge instance hour dollar little gigabyte outgoing bandwidth used make http request note amazon charge incoming bandwidth good thing would interesting compare cost appropriately amortized cost using cloud provider selfhosting something experiment use amazon spot instance bid use amazon unused capacity think launch crawl went look spot instance pricing history discovered surprise spot instance price often factor lower price ondemand instance factoring charge outgoing bandwidth mean may possible use spot instance similar crawl dollar factor five saving considered switching ultimately decided thinking might take day work properly understand implication switching get thing working exactly wanted admittedly possible would taken much le time case missed opportunity trade money little extra time improvement crawler architecture let finish noting way interesting improve current crawler many longrunning application crawler would need smart crawl policy know recrawl page according presentation jeff dean google mean time index new page minute know work imagine notification protocol pubsubhubbub play important role good change crawler pubsubhubbub aware crawler currently us threaded architecture another quite different approach use evented architecture pro con multithreaded versus evented architecture instance cluster configured using fabric shell script install program redis pybloomfilter slow completely reliable better way creating ami configuration management software chef puppet considered using one latter deferred upfront cost learning system logging currently done using python logging module unfortunately finding welladapted python threading better solution crawler initially designed crawling batch environment run terminates since modified stopped modification made restarted good add instrumentation modified dynamically real time many interesting research paper published crawling read skimmed quite writing crawler ultimately used idea getting basic right proved challenging enough future iteration useful look work incorporate best idea good starting point include chapter book manning raghavan sch utze survey paper olston najork existing open source crawler heritrix nutch would also interesting look depth
667,Lobsters,scaling,Scaling and architecture,How to Build a GitHub,http://zachholman.com/talk/how-to-build-a-github,build github,slide slide audio video,github lot growth past five year expanding two million user almost seven million repository push limit interact tool like git designed work scale like talk system built handle growth starting local git architecture migrating networked backend setup optimizing disk usage netshard finally talk newest addition gitrpc talked publicly talk also cover human side thing gained employee two year never anyone quit last forever think good indication type company built certain thing retain employee worrying employee happiness ultimately create much better product slide slide audio video coming soon
669,Lobsters,scaling,Scaling and architecture,Netflix Tech Blog: Benchmarking High Performance I/O with SSD for Cassandra on AWS,http://techblog.netflix.com/2012/07/benchmarking-high-performance-io-with.html,netflix tech blog benchmarking high performance io ssd cassandra aws,benchmarking high performance io ssd cassandra aws aws launched new solid state disk ssd based instance instance memory capacity cpu performance along network benchmarking published apache cassandra performance benchmark priam summary aws instance io option ssd based instance benchmark result netflix application benchmark described previous techblog post cost comparison benefit moving cassandra workload ssd summary tl dr ssd filesystem test iozone,benchmarking high performance io ssd cassandra awsby adrian cockcrofttoday aws launched new solid state disk ssd based instance address need high performance io run initial benchmark see shape announcement aws make easy provision extremely high io capacity consistently low latency aws competitive instance memory capacity long time leading industry cpu performance along network extremely io intensive application deployed commonly cited obstacle running cloud removedbenchmarkinglast year published apache cassandra performance benchmark achieved million client writes per second using hundred fairly small instance testing scalability priam tooling used create manage cassandra proved large scale cassandra cluster scale linearly ten time number instance get ten time throughput today publishing benchmark result include comparison cassandra running existing instance type new ssd based instance typesummary aws instance io optionsthere several existing storage option based internal disk ephemeral go away instance terminates three option previously tested cassandra found instance joined new ssd based aws specifies relative total cpu performance instance type using compute unit ecu primarily use run cassandra netflix today best balance cpu io ram capacity workload although careful overload io maintenance operation scheduling compaction repair sequence across nodesthe ssd based instancethis new instance type provides high performance internal ephemeral ssd based storage cpu reported proccpuinfo intel westmere core hyper threading appears cpu thread fall cpu performance similar ram capacity network interface like disk configuration appears two large ssd volume around terabyte instance capable around low latency iop gigabyte per second throughput provides hundred time higher throughput achieved storage option extremely low latency variance since instance local access ssd network traffic storage pathbenchmark resultsthe first thing new storage subsystem basic filesystem level performance testing used iozone benchmark verify could get iop gbytes throughput disk level low service time per request microsecondsthe second benchmark use standard cassandra stress test run simple data access pattern small dataset similar benchmark published last year found test mostly cpu bound could get close gigabyte per second throughput disk short startup data loaded memory increased cpu performance ecus ecus gave useful speedup test generating enough iopsthe third complex took biggest cassandra data store restored two copy backup one one could evaluate realworld workload figure best configure ssd instance replacement existing configuration concentrate application level benchmark next interesting comparisonnetflix application benchmarkour architecture fine grain development team owning set service data store result ten distinct cassandra cluster production serving different data source one picked storing data rest based data provider application currently us memcached tier cache result read workload well cassandra persistent writes goal see could use smaller number ssd based cassandra instance without memcached tier without impacting response time memcached tier wrapped service call evcache described previous techblog post two configuration compared existing system cassandra evcache based system cassandra application one complex demanding workload run requires ten thousand read thousand writes per second query column family layout far complex simple stress benchmark evcache tier absorbs read existing system cassandra instance using available cpu use lot memory reduce io workload sustainable levelthe ssd based system running workload plenty iop left could also run compaction operation full load without affecting response time overall throughput ssd based system cpu limited le existing system much lower mean percentile latency sizing exercise indicated could replace get throughput much lower latencycost comparisonwe already found running cassandra using ephemeral disk triple replicated instance scalable reliable cost effective storage mechanism despite overconfigure ram cpu capacity compensate relative lack iop instance ssd instance bottleneck move iop cpu able reduce instance count substantiallythe relative cost two configuration show overall cost saving using ssd instance perinstance software licensing cost using apache cassandra user commercial data store could also see licensing cost saving reducing instance countbenefits moving cassandra workload ssdthe configuration half system cost throughputthe mean read request latency reduced percentile request latency reduced able validate claimed raw performance number real world benchmark give u simpler lower cost solution running cassandra workloadstl drwhat follows detailed explanation benchmark configuration result tl dr short long read get way end understand get filesystem test iozonethe cassandra disk access workload consists large sequential writes sstable flush small random read stored version key checked get operation file written number read increase compaction replaces smaller file one large one iozone benchmark used create similar workload one instance standard data size recommendation iozone twice memory capacity case neededusing sixty thread write file using writes result service time
670,Lobsters,scaling,Scaling and architecture,"Scaling lessons learned at Dropbox, part 1",http://eranki.tumblr.com/post/27076431887/scaling-lessons-learned-at-dropbox-part-1,scaling lesson learned dropbox part,scaling lesson learned dropbox part rajiveranki gmailcom run extra load appspecific metric poor man analytics bash one one log spam really helpful something fail make sure run infrequent stuff often general try keep thing homogeneous keeping downtime log utc technology used simulateanalyze thing trying securityconvenience tradeoff show note,scaling lesson learned dropbox part rajiv eranki rajiveranki gmailcom charge scaling dropbox roughly user time one three people working backend suggestion scaling particularly resourceconstrained fastgrowing environment always afford thing right way ie realworld engineering project people find useful try come tip write part extra loadone technique repeatedly used creating artificial extra load live site example would lot memcached read necessary memcached broke could quickly switch duplicate query time come solution plan ahead time abrupt failure detect monitoring canary coal mine could buy time plan well take note apparent limitsnote perfect extra read likely high write load cause problem writes hard simulate risk endangering data getting realistic amount lock contention experience extra read alone actually good enough buy timeas idea could server sits play back readonly log frontend switched necessary better switching feature appspecific metricsanother thing became increasingly useful thousand custom stats aggregated thousand server graphed outofthebox monitoring solution meant handle sort load wanted oneline way adding stats think whether costed anything fuss around configuration file add stat decreasing friction testing monitoring big priority chose implement solution combination memcached cron ganglion every time something happened wanted graphed would store threadlocal memory buffer every second post stat timekeyed bucket timestamp mod something memcached central machine every minute bucket memcached would get cleared aggregated posted ganglion scalable made possible u track thousand stats near real time even stats finegrained average time read memcached key happened dozen time per request performed fineof course thousand stats becomes tough look graph find anomolies one summary graph found useful top line represents average response time site one graph web traffic one client traffic segment represents partition work see spike response time around caused something mysql commit phase actual graph bunch segment imagine much screen real estate save trying figure shit cpu cheating actually average response time minus everything else factored outif way would also cool marking graph point event annotate code push aws outagepoor man analytics bashif used shell much eye opening much faster certain task help demystify language like perl structured way check let say trying debug something webserver want know maybe spike activity recently log graphing webserver would great minute interval like system might finegrained enough maybe want look certain kind request whatever apr post apr get apr get apr post could use shell like cut logtxt xargs date uniq c echo plot using line cat gnuplotboom quickly nice graph going tailor easily select one url certain time frame change histogram etc almost command line tool take line separated space delimited array input implicit numeric conversion necessary usually need backflips pipe program together also throw exception bad data think good thing trying something quickly care dropped data point unfamiliar command line tool shortlist recommend becoming acquainted sed awk grep cut head tail sort uniq tr date xargsfor sed awk richer command kept couple cheatsheets bookmarked case forgot something one really good sed one awk command super quick learnand output would summary file graphed gnuplot want make traffic diagram dotlog spam really helpfullog spam bad used many random print statement scattered around code would end webserver log count number time turned unintentionally useful almost way randomly tracing code example debugging particularly nasty race condition noticed particular fuuucckkkkkasdjkfnff getting printed made clear problem happening best keep clean log file dirty one unstructured spam make sure duplicate clean stuff dirty one load match timestampsit tempting remove superverbose logging every often almost always regretted didif something fail make sure doesif something know fail point think failover graceful actually test every often randomly take server network make sure failover work couple thing happen since last failover increased load mean failover process cause cascadein last failover bazillion code push database schema change internal dns renames etc script run since might depend old assumptionsthese thing better figure peacetime best make stuff happen intentionally restart process cron clear memcached maybe sound stupid run fire drill live site testing environment sufficient really good insurancerun infrequent stuff often generalthe point also go stuff run often codebase afford push code infrequent code path often save headache like cron run every month maybe run dryrun every day week make sure least assumption consistent debug month worth commits even checking module import would help anything crossing finger running every month go script run manually eg script fix user state run diagnostics good put cron catch error even useful work testing environment unlikely sufficient stuff try keep thing homogeneous long ago two shard user data started getting full added third shard put new user damn headache two shard growing almost exactly pace new one growing much faster meaning reshard different time much better obviously trickier split shard two keep looking samehomogeneity good hardware capacity planning becomes simpler problem best small number machine type example breakdown could highcpu highmemory highdisk beast maxed everything analytics heavy db type machine might cost little extra force everything one category think worth simplicity also switch machine around easily need keeping downtime log every time site go degrades even short blip write start end time outage label applicable cause bad code review insufficient monitoring log overflow look list objectively answer question minimize minute downtime right figuring cover minute solution might span multiple problem problem might solvable many way help write much instance proper monitoring might alert impending disk full problem limit amount stuff written disk utc keep everything utc internally server time stuff database etc save lot headache daylight saving time software even handle nonutc time properly kept clock wall set utc want display time user make timezone conversion happen last second technology used curious chose software used python virtually everything couple thousand line cmysqlpasterpylonscheetah web framework minimal use beyond templating handling form input storing serving file blocksmemcached front database handling interserver coordinationganglia graphing drraw custom graph like stack graph mentioned abovenginx frontend serverhaproxy load balancing app server nginx better configurability nginx balancing module nagios internal health checkspingdom external service monitoring paginggeoip mapping ip locationspretty standard reason chose thing reliability even memcached conceptually simplest technology used many company really nasty memory corruption bug deal shudder think using stuff newer complicated mysql v postgres postgres builtin replication time mysql huge network support pretty sure problem google yahoo facebook would deal patch used sqlalchemy orm really hate orm giant nuisance deal eventually ended converting everything sqlalchemy lowestlevel language constructing sql one step away raw sql orm apologist like say ok use right know use understand etc actually sometimes sqlalchemy plain wrong delivered incorrect result need use one knock least nine mysql usually rocksolid performancemy suggestion choosing technology would pick lightweight thing known work see lot use outside company else prepared become primary contributor projectsimulateanalyze thing trying themunlike product harder reason backend engineering fairly objective optimize page load time uptime etc use advantage think something produce result might consider simulating effect easier way committing implementing like think faster procs going help maybe run machine one worker see make much difference really io bound toying around moving database server place latency add m latency lowlevel database glue see happens going cache something database simulate effect tally hit justify spending time worrying invalidation policy etc securityconvenience tradeoffsecurity really important dropbox people personal file service different many security decision inconvenience someone whether programmer userfor instance almost every website thing enter wrong username wrong password tell got one wrong tell one good security use information figure usernames giant pain as people like remember username registered actually care exposing usernames maybe something like forum pinterest public anyway consider revealing information make convenient usershaving internal firewall server need talk good idea service actually need necessarily matter separate thirdparty forum software core website sure thing buggy likely get hacked website maybe security something people like lipservice armchair philosophize reality think lot service even bank serious security problem seem able weather small pr storm figure really important worth hacking actually care hacked worth engineering product cost go lock everything show notesloading
671,Lobsters,scaling,Scaling and architecture,Twitter Engineering: Caching with Twemcache,http://engineering.twitter.com/2012/07/caching-with-twemcache.html,twitter engineering caching twemcache,cooky use,using service agree cooky use use cooky purpose including analytics personalisation ad
672,Lobsters,scaling,Scaling and architecture,"What Powers Instagram: Hundreds of Instances, Dozens of Technologies",http://instagram-engineering.tumblr.com/post/13649370142/what-powers-instagram-hundreds-of-instances-dozens-of,power instagram hundred instance dozen technology,o hosting load balancing nginx application server django http gunicornorg fabric data storage previously written vmtouch script repmgr pgbouncer christophe pettus blog redis django session backend related system geosearch api apache solr task queue push notification realtime subscriber gearman http githubcomsamuraisampyapns monitoring munin pythonmunin pingdom pagerduty sentry looking devops person join u help u tame instance herd,one question always get asked meetups conversation engineer stack thought would fun give sense system power instagram highlevel look forward indepth description system future system evolved live part always reworking glimpse startup small engineering team scale million user little year core principle choosing system keep simpledon reinvent wheelgo proven solid technology canwe go top bottom o hostingwe run ubuntu linux natty narwhal amazon found previous version ubuntu sort unpredictable freezing episode high traffic natty solid got engineer need still evolving selfhosting option explored deeply yet though something may revisit future given unparalleled growth usageload balancingevery request instagram server go load balancing machine used run nginx machine dns roundrobin downside approach time take dns update case one machine need get decomissioned recently moved using amazon elastic load balancer nginx instance behind swapped automatically taken rotation fail health check also terminate ssl elb level lessens cpu load nginx use amazon dns recently added pretty good gui tool aws consoleapplication serversnext come application server handle request run django amazon highcpu extralarge machine usage grows gone machine luckily one area easy horizontally scale stateless found particular workload cpubound rather memorybound highcpu extralarge instance type provides right balance memory cpuwe use http gunicornorg wsgi server used use modwsgi apache found gunicorn much easier configure le cpuintensive run command many instance like deploying code use fabric recently added useful parallel mode deploys take matter secondsdata storagemost data user photo metadata tag etc life postgresql previously written shard across different postgres instance main shard cluster involves quadruple extralarge memory instance twelve replica different zone found amazon network disk system eb support enough disk seek per second working set memory extremely important get reasonable io performance set eb drive software raid using mdadmas quick tip found vmtouch fantastic tool managing data memory especially failing one machine another active memory profile already script use parse output vmtouch run one machine print corresponding vmtouch command run another system match current memory statusall postgresql instance run masterreplica setup using streaming replication use eb snapshotting take frequent backup system use xfs file system let u freeze unfreeze raid array snapshotting order guarantee consistent snapshot original inspiration came get streaming replication started favorite tool repmgr folk connect database app server made early huge impact performance using pgbouncer pool connection postgresql found christophe pettus blog great resource django postgresql pgbouncer tipsthe photo go straight amazon currently store several terabyte photo data u use amazon cloudfront cdn help image load time user around world like japan second mostpopular country also use redis extensively power main feed activity feed session system django session backend related system redis data need fit memory end running several quadruple extralarge memory instance redis occasionally shard across redis instance given subsystem run redis masterreplica setup replica constantly saving db disk finally use eb snapshot backup db dump found dumping db master taxing since redis allows writes replica make easy online failover new redis machine without requiring downtimefor geosearch api used postgresql many month medium entry sharded moved using apache solr simple json interface far application concerned another api consumefinally like modern web service use memcached caching currently memcached instance connect using pylibmc libmemcached amazon elastic cache service recently launched cheaper running instance pushed switch quite yettask queue push notificationswhen user decides share instagram photo twitter facebook need notify one realtime subscriber new photo posted push task gearman task queue system originally written danga asynchronously task queue mean medium uploads finish quickly heavy lifting run background worker written python consuming task queue given time split service share also feed fanout gearman posting responsive new user user many followersfor push notification costeffective solution found http githubcomsamuraisampyapns opensource twisted service handled billion push notification u rocksolidmonitoringwith instance important keep top going across board use munin graph metric across system also alert u anything outside normal range write lot custom munin plugins building top pythonmunin graph metric systemlevel example signups per minute photo posted per second etc use pingdom external monitoring service pagerduty handling notification incidentsfor python error reporting use sentry awesome opensource django app written folk disqus given time signon see error happening across system real timeyou description system interest hopping ready tell u thing change system love hear looking devops person join u help u tame instance herd
674,Lobsters,scaling,Scaling and architecture,The Architecture of nginx,http://www.aosabook.org/en/nginx.html,architecture nginx,high concurrency important nt apache suitable proclaimed advantage using nginx overview nginx architecture code structure section figure worker model nginx process role brief overview nginx caching nginx configuration directive nginx internals spdy experimental protocol faster web,nginx pronounced engine x free open source web server written igor sysoev russian software engineer since public launch nginx focused high performance high concurrency low memory usage additional feature top web server functionality like load balancing caching access bandwidth control ability integrate efficiently variety application helped make nginx good choice modern website architecture currently nginx second popular open source web server internet high concurrency important day internet widespread ubiquitous hard imagine nt exactly know decade ago greatly evolved simple html producing clickable text based ncsa apache web server alwayson communication medium used billion user worldwide proliferation permanently connected pc mobile device recently tablet internet landscape rapidly changing entire economy become digitally wired online service become much elaborate clear bias towards instantly available live information entertainment security aspect running online business also significantly changed accordingly website much complex generally require lot engineering effort robust scalable one biggest challenge website architect always concurrency since beginning web service level concurrency continuously growing uncommon popular website serve hundred thousand even million simultaneous user decade ago major cause concurrency slow adsl dialup connection nowadays concurrency caused combination mobile client newer application architecture typically based maintaining persistent connection allows client updated news tweet friend feed another important factor contributing increased concurrency changed behavior modern browser open four six simultaneous connection website improve page load speed illustrate problem slow client imagine simple apachebased web server produce relatively short kb web page text image merely fraction second generate retrieve page take second transmit client bandwidth kbps kb essentially web server would relatively quickly pull kb content would busy second slowly sending content client freeing connection imagine simultaneously connected client requested similar content mb additional memory allocated per client would result mb gb extra memory devoted serving client kb content reality typical web server based apache commonly allocates mb additional memory per connection regrettably ten kbps still often effective speed mobile communication although situation sending content slow client might extent improved increasing size operating system kernel socket buffer general solution problem undesirable side effect persistent connection problem handling concurrency even pronounced avoid latency associated establishing new http connection client would stay connected connected client certain amount memory allocated web server consequently handle increased workload associated growing audience hence higher level able continuously website based number efficient building block part equation hardware cpu memory disk network capacity application data storage architecture obviously important web server software client connection accepted processed thus web server able scale nonlinearly growing number simultaneous connection request per second nt apache suitable apache web server software still largely dominates internet today root beginning originally architecture matched thenexisting operating system hardware also state internet website typically standalone physical server running single instance apache beginning obvious standalone web server model could easily replicated satisfy need growing web service although apache provided solid foundation future development architected spawn copy new connection suitable nonlinear scalability website eventually apache became general purpose web server focusing many different feature variety thirdparty extension universal applicability practically kind web application development however nothing come without price downside rich universal combination tool single piece software le scalability increased cpu memory usage per connection thus server hardware operating system network resource ceased major constraint website growth web developer worldwide started look around efficient mean running web server around ten year ago daniel kegel prominent software engineer proclaimed time web server handle ten thousand client simultaneously predicted call internet cloud service kegel manifest spurred number attempt solve problem web server optimization handle large number client time nginx turned one successful one aimed solving problem simultaneous connection nginx written different architecture much suitable nonlinear scalability number simultaneous connection request per second nginx eventbased follow apache style spawning new process thread web page request end result even load increase memory cpu usage remain manageable nginx deliver ten thousand concurrent connection server typical hardware first version nginx released meant deployed alongside apache static content like html cs javascript image handled nginx offload concurrency latency processing apachebased application server course development nginx added integration application use fastcgi uswgi scgi protocol distributed memory object caching system like memcached useful functionality like reverse proxy load balancing caching added well additional feature shaped nginx efficient combination tool build scalable web infrastructure upon february apache branch released public although latest release apache added new multiprocessing core module new proxy module aimed enhancing scalability performance soon tell performance concurrency resource utilization par better pure eventdriven web server would nice see apache application server scale better new version though could potentially alleviate bottleneck backend side still often remain unsolved typical nginxplusapache web configuration advantage using nginx handling high concurrency high performance efficiency always key benefit deploying nginx however even interesting benefit last year web architect embraced idea decoupling separating application infrastructure web server however would previously exist form lamp linux apache mysql php python perl based website might become merely lempbased one e standing engine x often exercise pushing web server edge infrastructure integrating revamped set application database tool around different way nginx well suited provides key feature necessary conveniently offload concurrency latency processing ssl secure socket layer static content compression caching connection request throttling even http medium streaming application layer much efficient edge web server layer also allows integrating directly memcachedredis nosql solution boost performance serving large number concurrent user recent flavor development kit programming language gaining wide use company changing application development deployment habit nginx become one important component changing paradigm already helped many company start develop web service quickly within budget first line nginx written released public twoclause bsd license number nginx user growing ever since contributing idea submitting bug report suggestion observation immensely helpful beneficial entire community nginx codebase original written entirely scratch c programming language nginx ported many architecture operating system including linux freebsd solaris mac o x aix microsoft window nginx library standard module use much beyond system c library except zlib pcre openssl optionally excluded build needed potential license conflict word window version nginx nginx work window environment window version nginx like proofofconcept rather fully functional port certain limitation nginx window kernel architecture interact well time known issue nginx version window include much lower number concurrent connection decreased performance caching bandwidth policing future version nginx window match mainstream functionality closely overview nginx architecture traditional process threadbased model handling concurrent connection involve handling connection separate process thread blocking network inputoutput operation depending application inefficient term memory cpu consumption spawning separate process thread requires preparation new runtime environment including allocation heap stack memory creation new execution context additional cpu time also spent creating item eventually lead poor performance due thread thrashing excessive context switching complication manifest older web server architecture like apache tradeoff offering rich set generally applicable feature optimized usage server resource beginning nginx meant specialized tool achieve performance density economical use server resource enabling dynamic growth website followed different model actually inspired ongoing development advanced eventbased mechanism variety operating system resulted modular eventdriven asynchronous singlethreaded nonblocking architecture became foundation nginx code nginx us multiplexing event notification heavily dedicates specific task separate process connection processed highly efficient runloop limited number singlethreaded process called worker within worker nginx handle many thousand concurrent connection request per second code structure nginx worker code includes core functional module core nginx responsible maintaining tight runloop executing appropriate section module code stage request processing module constitute presentation application layer functionality module read write network storage transform content outbound filtering apply serverside include action pas request upstream server proxying activated nginx modular architecture generally allows developer extend set web server feature without modifying nginx core nginx module come slightly different incarnation namely core module event module phase handler protocol variable handler filter upstreams load balancer time nginx nt support dynamically loaded module ie module compiled along core build stage however support loadable module abi planned future major release detailed information role different module found section handling variety action associated accepting processing managing network connection content retrieval nginx us event notification mechanism number disk io performance enhancement linux solaris bsdbased operating system like kqueue epoll event port goal provide many hint operating system possible regard obtaining timely asynchronous feedback inbound outbound traffic disk operation reading writing socket timeouts usage different method multiplexing advanced io operation heavily optimized every unixbased operating system nginx run highlevel overview nginx architecture presented figure figure diagram nginx architecture worker model previously mentioned nginx nt spawn process thread every connection instead worker process accept new request shared listen socket execute highly efficient runloop inside worker process thousand connection per worker specialized arbitration distribution connection worker nginx work done o kernel mechanism upon startup initial set listening socket created worker continuously accept read write socket processing http request response runloop complicated part nginx worker code includes comprehensive inner call relies heavily idea asynchronous task handling asynchronous operation implemented modularity event notification extensive use callback function finetuned timer overall key principle nonblocking possible situation nginx still block enough disk storage performance worker process nginx fork process thread per connection memory usage conservative extremely efficient vast majority case nginx conserve cpu cycle well ongoing createdestroy pattern process thread nginx check state network storage initialize new connection add runloop process asynchronously completion point connection deallocated removed runloop combined careful use syscalls accurate implementation supporting interface like pool slab memory allocator nginx typically achieves moderatetolow cpu usage even extreme workload nginx spawn several worker handle connection scale well across multiple core generally separate worker per core allows full utilization multicore architecture prevents thread thrashing lockup resource starvation resource controlling mechanism isolated within singlethreaded worker process model also allows scalability across physical storage device facilitates disk utilization avoids blocking disk io result server resource utilized efficiently workload shared across several worker disk use cpu load pattern number nginx worker adjusted rule somewhat basic system administrator try couple configuration workload general recommendation might following load pattern cpu instance handling lot tcpip ssl number nginx worker match number cpu core load mostly disk io instance serving different set content storage heavy number worker might one half two time number core engineer choose number worker based number individual storage unit instead though efficiency approach depends type configuration disk storage one major problem developer nginx solving upcoming version avoid blocking disk io moment enough storage performance serve disk operation generated particular worker worker may still block readingwriting disk number mechanism configuration file directive exist mitigate disk io blocking scenario notably combination option like sendfile aio typically produce lot headroom disk performance nginx installation planned based data set amount memory available nginx underlying storage architecture another problem existing worker model related limited support embedded scripting one standard nginx distribution embedding perl script supported simple explanation key problem possibility embedded script block operation exit unexpectedly type behavior would immediately lead situation worker hung affecting many thousand connection work planned make embedded scripting nginx simpler reliable suitable broader range application nginx process role nginx run several process memory single master process several worker process also couple special purpose process specifically cache loader cache manager process singlethreaded version nginx process primarily use sharedmemory mechanism interprocess communication master process run root user cache loader cache manager worker run unprivileged user master process responsible following task reading validating configuration creating binding closing socket starting terminating maintaining configured number worker process reconfiguring without service interruption controlling nonstop binary upgrade starting new binary rolling back necessary reopening log file compiling embedded perl script worker process accept handle process connection client provide reverse proxying filtering functionality almost everything else nginx capable regard monitoring behavior nginx instance system administrator keep eye worker process reflecting actual daytoday operation web server cache loader process responsible checking ondisk cache item populating nginx inmemory database cache metadata essentially cache loader prepares nginx instance work file already stored disk specially allocated directory structure traverse directory check cache content metadata update relevant entry shared memory exit everything clean ready use cache manager mostly responsible cache expiration invalidation stay memory normal nginx operation restarted master process case failure brief overview nginx caching caching nginx implemented form hierarchical data storage filesystem cache key configurable different requestspecific parameter used control get cache cache key cache metadata stored shared memory segment cache loader cache manager worker access currently inmemory caching file optimization implied operating system virtual filesystem mechanism cached response placed different file filesystem hierarchy level naming detail controlled nginx configuration directive response written cache directory structure path name file derived hash proxy url process placing content cache follows nginx read response upstream server content first written temporary file outside cache directory structure nginx finish processing request renames temporary file move cache directory temporary file directory proxying another file system file copied thus recommended keep temporary cache directory file system also quite safe delete file cache directory structure need explicitly purged thirdparty extension nginx make possible control cached content remotely work planned integrate functionality main distribution nginx configuration nginx configuration system inspired igor sysoev experience apache main insight scalable configuration system essential web server main scaling problem encountered maintaining large complicated configuration lot virtual server directory location datasets relatively big web setup nightmare done properly application level system engineer result nginx configuration designed simplify daytoday operation provide easy mean expansion web server configuration nginx configuration kept number plain text file typically reside usrlocaletcnginx etcnginx main configuration file usually called nginxconf keep uncluttered part configuration put separate file automatically included main one however noted nginx currently support apachestyle distributed configuration ie htaccess file configuration relevant nginx web server behavior reside centralized set configuration file configuration file initially read verified master process compiled readonly form nginx configuration available worker process forked master process configuration structure automatically shared usual virtual memory management mechanism nginx configuration several different context main http server upstream location also mail mail proxy block directive context never overlap instance thing putting location block main block directive also avoid unnecessary ambiguity nt anything like global web server configuration nginx configuration meant clean logical allowing user maintain complicated configuration file comprise thousand directive private conversation sysoev said location directory block global server configuration feature never liked apache reason never implemented nginx configuration syntax formatting definition follow socalled cstyle convention particular approach making configuration file already used variety open source commercial software application design cstyle configuration wellsuited nested description logical easy create read maintain liked many engineer cstyle configuration nginx also easily automated nginx directive resemble certain part apache configuration setting nginx instance quite different experience instance rewrite rule supported nginx though would require administrator manually adapt legacy apache rewrite configuration match nginx style implementation rewrite engine differs general nginx setting also provide support several original mechanism useful part lean web server configuration make sense briefly mention variable tryfiles directive somewhat unique nginx variable nginx developed provide additional evenmorepowerful mechanism control runtime configuration web server variable optimized quick evaluation internally precompiled index evaluation done demand ie value variable typically calculated cached lifetime particular request variable used different configuration directive providing additional flexibility describing conditional request processing behavior tryfiles directive initially meant gradually replace conditional configuration statement proper way designed quickly efficiently trymatch different uritocontent mapping overall tryfiles directive work well extremely efficient useful recommended reader thoroughly check tryfiles directive adopt use whenever applicable nginx internals mentioned nginx codebase consists core number module core nginx responsible providing foundation web server web mail reverse proxy functionality enables use underlying network protocol build necessary runtime environment ensures seamless interaction different module however protocol applicationspecific feature done nginx module core internally nginx process connection pipeline chain module word every operation module relevant work eg compression modifying content executing serverside includes communicating upstream application server fastcgi uwsgi protocol talking memcached couple nginx module sit somewhere core real functional module module http mail two module provide additional level abstraction core lowerlevel component module handling sequence event associated respective application layer protocol like http smtp imap implemented combination nginx core upperlevel module responsible maintaining right order call respective functional module http protocol currently implemented part http module plan separate functional module future due need support protocol like spdy see spdy experimental protocol faster web functional module divided event module phase handler output filter variable handler protocol upstreams load balancer module complement http functionality nginx though event module protocol also used mail event module provide particular osdependent event notification mechanism like kqueue epoll event module nginx us depends operating system capability build configuration protocol module allow nginx communicate http tlsssl smtp imap typical http request processing cycle look like following client sends http request nginx core chooses appropriate phase handler based configured location matching request configured load balancer pick upstream server proxying phase handler job pass output buffer first filter first filter pass output second filter second filter pass output third final response sent client nginx module invocation extremely customizable performed series callback using pointer executable function however downside may place big burden programmer would like write module must define exactly module run nginx api developer documentation improved made available alleviate example module attach configuration file read processed configuration directive location server appears main configuration initialized server ie hostport initialized server configuration merged main configuration location configuration initialized merged parent server configuration master process start exit new worker process start exit handling request filtering response header body picking initiating reinitiating request upstream server processing response upstream server finishing interaction upstream server inside worker sequence action leading runloop response generated look like following begin ngxworkerprocesscycle process event o specific mechanism epoll kqueue accept event dispatch relevant action processproxy request header body generate response content header body stream client finalize request reinitialize timer event runloop step ensures incremental generation response streaming client detailed view processing http request might look like initialize request processing process header process body call associated handler run processing phase brings u phase nginx handle http request pass number processing phase phase handler call general phase handler process request produce relevant output phase handler attached location defined configuration file phase handler typically four thing get location configuration generate appropriate response send header send body handler one argument specific structure describing request request structure lot useful information client request request method uri header http request header read nginx lookup associated virtual server configuration virtual server found request go six phase server rewrite phase location phase location rewrite phase bring request back previous phase access control phase tryfiles phase log phase attempt generate necessary content response request nginx pass request suitable content handler depending exact location configuration nginx may try socalled unconditional handler first like perl proxypass flv etc request match content handler picked one following handler exact order random index index autoindex gzipstatic static indexing module detail found nginx documentation module handle request trailing slash specialized module like autoindex nt appropriate content considered file directory disk static served static content handler directory would automatically rewrite uri trailing slash always issue http redirect content handler content passed filter filter also attached location several filter configured location filter task manipulating output produced handler order filter execution determined compile time outofthebox filter predefined thirdparty filter configured build stage existing nginx implementation filter outbound change currently mechanism write attach filter input content transformation input filtering appear future version nginx filter follow particular design pattern filter get called start working call next filter final filter chain called nginx finalizes response filter nt wait previous filter finish next filter chain start work soon input previous one available functionally much like unix pipeline turn output response generated passed client entire response upstream server received header filter body filter nginx feed header body response associated filter separately header filter consists three basic step decide whether operate response operate response call next filter body filter transform generated content example body filter include serverside includes xslt filtering image filtering instance resizing image fly charset modification gzip compression chunked encoding filter chain response passed writer along writer couple additional special purpose filter namely copy filter postpone filter copy filter responsible filling memory buffer relevant response content might stored proxy temporary directory postpone filter used subrequests subrequests important mechanism requestresponse processing subrequests also one powerful aspect nginx subrequests nginx return result different url one client originally requested web framework call internal redirect however nginx go filter perform multiple subrequests combine output single response subrequests also nested hierarchical subrequest perform subsubrequest subsubrequest initiate subsubsubrequests subrequests map file hard disk handler upstream server subrequests useful inserting additional content based data original response example ssi serverside include module us filter parse content returned document replaces include directive content specified url example making filter treat entire content document url retrieved appends new document url upstream load balancer also worth describing briefly upstreams used implement identified content handler reverse proxy proxypass handler upstream module mostly prepare request sent upstream server backend receive response upstream server call output filter upstream module exactly set callback invoked upstream server ready written read callback implementing following functionality exist crafting request buffer chain sent upstream server reinitializingresetting connection upstream server happens right creating request processing first bit upstream response saving pointer payload received upstream server aborting request happens client terminates prematurely finalizing request nginx finish reading upstream server trimming response body eg removing trailer load balancer module attach proxypass handler provide ability choose upstream server one upstream server eligible load balancer register enabling configuration file directive provides additional upstream initialization function resolve upstream name dns etc initializes connection structure decides route request update stats information currently nginx support two standard discipline load balancing upstream server roundrobin iphash upstream load balancing handling mechanism include algorithm detect failed upstream server reroute new request remaining lot additional work planned enhance functionality general work load balancer planned next version nginx mechanism distributing load across different upstream server well health check greatly improved also couple interesting module provide additional set variable use configuration file variable nginx created updated across different module two module entirely dedicated variable geo map geo module used facilitate tracking client based ip address module create arbitrary variable depend client ip address module map allows creation variable variable essentially providing ability flexible mapping hostnames runtime variable kind module may called variable handler memory allocation mechanism implemented inside single nginx worker extent inspired apache highlevel description nginx memory management would following connection necessary memory buffer dynamically allocated linked used storing manipulating header body request response freed upon connection release important note nginx try avoid copying data memory much possible data passed along pointer value calling memcpy going bit deeper response generated module retrieved content put memory buffer added buffer chain link subsequent processing work buffer chain link well buffer chain quite complicated nginx several processing scenario differ depending module type instance quite tricky manage buffer precisely implementing body filter module module operate one buffer chain link time must decide whether overwrite input buffer replace buffer newly allocated buffer insert new buffer buffer question complicate thing sometimes module receive several buffer incomplete buffer chain must operate however time nginx provides lowlevel api manipulating buffer chain actual implementation thirdparty module developer become really fluent arcane part nginx note approach memory buffer allocated entire life connection thus longlived connection extra memory kept time idle keepalive connection nginx spends byte memory possible optimization future release nginx would reuse share memory buffer longlived connection task managing memory allocation done nginx pool allocator shared memory area used accept mutex cache metadata ssl session cache information associated bandwidth policing management limit slab allocator implemented nginx manage shared memory allocation allow simultaneous safe use shared memory number locking mechanism available mutexes semaphore order organize complex data structure nginx also provides redblack tree implementation redblack tree used keep cache metadata shared memory track nonregex location definition couple task unfortunately never described consistent simple manner making job developing thirdparty extension nginx quite complicated although good document nginx internals instance produced evan document required huge reverse engineering effort implementation nginx module still black art many despite certain difficulty associated thirdparty module development nginx user community recently saw lot useful thirdparty module instance embedded lua interpreter module nginx additional module load balancing full webdav support advanced cache control interesting thirdparty work author chapter encourage support future
675,Lobsters,scaling,Scaling and architecture,Tarsnap outage post-mortem,http://www.daemonology.net/blog/2012-07-04-tarsnap-outage.html,tarsnap outage postmortem,tarsnap outage amazon postmortem incident tarsnap store user data amazon keep metadata cached timeline event status page worstcase performance behaviour lesson learned according amazon final word derecho view forum thread blog comment powered,tarsnap outage approximately utc central tarsnap server hosted amazon useast region went offline due power outage according amazon postmortem incident caused two generator bank independently failing provide stable voltage utility grid power lost severe electrical storm tarsnap instance came back online hour later found abrupt loss power resulted filesystem corruption explained blog post december tarsnap store user data amazon keep metadata cached could see evidence power loss resulted cached metadata corrupted could absolutely rule possibility keeping mind first responsibility backup system avoid possibility data loss corruption decided err side caution treating state untrustworthy described december blog post losing state failure mode tarsnap designed survive reading data back replaying operation log entry said losing local state needing recover completely offsite backup amazon count since replicated multiple datacenters much worstcase scenario also quite literally nightmare scenario day outage nightmare happening probably provoked power outage affected instance different availability zone restored tarsnap fully operational state utc slightly le hour outage started obviously outage bad outage length unacceptable course recovery process learned several lesson make recovery future complete metadata loss incident faster see tarsnap formal sla rather illdefined policy issuing credit tarsnap user affected outage bug tarsnap code based personal sense fairness original cause outage control outage much shorter result credited tarsnap user account month storage cost timeline event time approximate worrying keeping note process underway reconstruct timeline based memory log file file timestamps utc power lost tarsnap server observe ceasing respond network traffic due two recent outage useast region power outage june amazon wrote postmortem also completed audit backup power distribution circuit network outage earlier june note two earlier outage far limited scope neither affected tarsnap initial presumption outage caused network issue utc presumption outage networkrelated reinforced amazon posting status page investigating connectivity issue utc amazon post status page large number instance lost power start attempting launch replacement instance case becomes necessary perform full state recovery eventually proved case attempt fail due apis offline result power outage utc power restored tarsnap server ssh find suffered filesystem corruption result power outage since rule possibility local state corrupted continue plan perform full state recovery utc succeed launching replacement tarsnap server start configuring installing tarsnap server code process includes creating attaching elastic block store disk using aws management console slowed repeated timeouts error utc finish configuring replacement tarsnap server start process regenerating local state first phase process involves reading million stored object unfortunately read performed sequential order triggering worstcase performance behaviour result phase recovery took much longer anticipated unfortunately design code meant changing order object read something could fly utc inspecting outaged tarsnap server concluded state corruption almost certainly limited archive committed last second power loss consequently brought server back online readonly mode anyone needed data urgently could retrieve full recovery process complete utc first phase tarsnap recovery retrieving bit completes second phase reconstructing map database identifies location block data within start utc notice map database reconstruction running anomalously slowly turn caused io ratelimiting code put place prevent backend process starving frontend tarsnap daemon much stricter scheduling requirement io obviously limit necessary reconstruction stage frontend daemon running utc restart second phase tarsnap recovery without io rate limit place utc third final phase tarsnap recovery replaying log reconstruct serverside cached state machine start utc third phase tarsnap recovery fails due request timing normally tarsnap code retries failed request particular code path functionality missing increase timeouts factor restart third recovery phase utc third phase tarsnap recovery completes look reconstructed state compare mostlycorrect state outaged server confirm reconstruction worked properly start tarsnap server code run test utc satisfied everything working properly switch tarsnap server elastic ip address point new server instance lesson learned test disaster recovery process scale always tested process recovering state stored every time prepare roll new code part make test always done small data set test six order magnitude le data may help confirm recovery process work certainly nt test process work quickly success speed important disaster recovery process rely aws management console according amazon eb control plane restored utc experiencing timeouts error commonly many request console hour beyond point obviously disaster occurs large influx user management console evidently need work improve scalability disaster recovery process start first sign outage particular case would nt made difference since first hour outage began impossible launch replacement instance failure mode amazon say addressing future starting recovery immediately rather waiting find outage transient network issue severe could save time sequential access amazon bad nt much lesson learned lesson remembered applies aware nt realized would slow recovery process much appropriate behaviour state recovery always appropriate behaviour normal operation disk io rate limiting backend process filesystem syncing normally useful much important recover state get back online quickly possible code shared normal recovery operation fact tarsnap server code aware mode running control behaviour appropriately tarsnap user amazing people ok knew already still episode reinforced nt see single irate email tweet comment irc entire outage people politely asked outage related outage aside communication positive mostly thanking effort status report final word writing much tarsnap like take moment provide wider context outage power outage knocked tarsnap offline big judging amazon statement instance number ip address useast region somewhere around instance went offline sure everybody reading aware also affected big name netflix pinterest instagram heroku people stop point story power outage caused derecho thunderstorm system believed one severe nonhurricane storm system north american history people believed died direct result storm million home business lost power approximately million people still without power five day later even worse storm system caused heat wave set record temperature hundred location across eastern u many area exceeding c f several day row elderly young individual chronic disease prolonged exposure temperature lifethreatening without electricity air conditioning available time heat wave could easily responsible hundred even thousand death many u datacenter losing power effect see storm people directly affected storm least worry view forum thread blog comment powered
676,Lobsters,scaling,Scaling and architecture,Visualizing Device Utilization,http://dtrace.org/blogs/brendan/2011/12/18/visualizing-device-utilization/,visualizing device utilization,utilization heat map command line interface tool tabulated data highlighted data surface plot animated data instantaneous value bar graph vector graph line graph ternary plot quantized heat map definition solaris performance tool problem statement command line interface one server second metric one server second data center second tabulated data one server second data center second highlighted data one server second hsv data center second affinity limitation one server hour surface plot one server second data center second animated data instantaneous value explains heat map bar graph vector graph product server cpu line graph one server second data center second random color average one server ternary plot ternary plot data center second cpuplayer tool limitation quantized heat map sun storage analytics visualizing latency cloud analytics data center second one server second idle server light load busy server saturation hue value heatmap coloring background summary design implementation realtime cloud analytics platform acknowledgment jeff bonwick r lattice visualizing performance imagemagick screenshot gnuplot cpuplayer bryan cantrill dave pacheco robert mustacchi gimp deirdr straughan jason hoffman main page,utilization heat map device utilization key metric performance analysis capacity planning page illustrate different way visualize device utilization across multiple device utilization changing time system study examine production cloud environment contains virtual cpu physical processor show well different visualization work environment scale including cpu utilization heat map command line interface tool tabulated data highlighted data surface plot animated data instantaneous value bar graph vector graph line graph ternary plot quantized heat map originally published http definition device utilization defined time device busy processing work interval device utilization active work time device may accept work becoming system bottleneck may accept work higher latency causing poor performance utilization may imperfect metric depending defined measured listed reason page solaris performance tool still tremendously useful quickly identifying eliminating device source performance issue problem statement given device type cpu disk network interface number device single device cloud server like identify following single multiple device utilization average minimum maximum device utilization device utilization balance tight loose distribution timebased characteristic including time domain identify whether utilization steady changing various finer detail may include short burst high utilization useful know length burst interval longer pattern time may also observed load change hour weekly cycle timebased pattern may also compared metric correlation observed illuminate complex system interaction may possible studying utilization varies across timeseries example use x second value finally like observe realtime command line interface device utilization usually available via command line tool may show perdevice utilization numerically interval summary printed realtime output scroll change time identified reading comparing previous summary tool usually nt handle scale illustrate cpu utilization one server second cpu utilization unixlinux system examined mpstat tool print single line output virtual cpu various useful metric column vary different system one second summary show virtual cpu cpu utilization calculated inverting last column idle evidence software scaleability issue single hot thread one server second also like see change time x second summary mpstat sense scale highlighted second summary shown earlier amount output already difficult digest terminal would many page scroll one server data center second data center server showing second across give impression amount data involved term mpstat output output small whitespace row column creates effect appears like fabric rectangle represents amount data single server contributes server data image actually placed horizontal line one line darker others middle top darkness caused high multidigit value many mpstat column replacing whitespace number prompted investigate server issue turned misconfigured sendmail calling exec per second caused high value minf xcal migr smtx sysctl column tabulated data visualize percpu utilization value persecond table value strip mpstat output inverted idl column provides better sense volume utilization data trying understand one server second cpu x second summary utilization unlike mpstat server summary time font size almost large enough read place single cpu hit utilization visible unbroken line digit data center second server image interesting would guessed click highres faint darker pattern caused area double tripledigit utilization digit give darkness effect differs mpstat dark pattern highlight utilization highlighted data utilization value could highlighted deliberately coloring background relative value one server second distinct pattern emerge burst cpu load across many cpu cpu seems busy entire time perhaps mapped device interrupt cpu visualization thought three dimension pictured right third utilization value represented color saturation using hsv definition saturation data center second server time necessary highlight single server busier server clearly visible red rectangle observation server single hot thread appear depending well thread stay one cpu affinity skip around server like multiple hot cpu also idle cpu may sign load balanced either due thread scalability cpu resource cap server show consistent cpu load time like showing high variance one shown previous one server example idle server clearly seen often contain one two short burst single cpu usage monitoring software entire image speckled short burst also apparent server data center idle time day peak limitation server image answer tricker question problem statement identify single multiple hot device unbalanced utilization bonus utilization shift device nt good expressing exact utilization value rely well eye differentiate color average utilization across device also hard determine data center image provides great impression across server used tool number could dropped highlighting sufficient could made interactive mouse server expanded detail however scaling much difficult example little pixel per data element server virtual cpu instead number element would increase factor image look similar heat map covered later nt one reason heat map usually scalar dimension within server image xaxis scalar time yaxis set cpu id may relative meaning operating system enumerate virtual cpu odd way data center image x yaxes span repeated range server nearby image also nearby physically due way data collected way reliable scalar dimension one server hour server example full hour horizontal strip represents minute included show could happen scaling time interesting image nt cpu utilization lack cpu utilization idle time shown white patch surface plot threedimensional plot created dimension cpu id time utilization given two dimension provided data set regular step cpu id time surface plot may suitable map regular latitude longitude point utilization value becomes surface elevation one problem may already expected shown right utilization value change steeply one point next making surface difficult follow improved reducing elevation utilization dimension one server second cpu x second average time xaxis left right cpu id yaxis zaxis elevation utilization value also colorized based utilization value utilization represented elevation color saturation issue scaling plot type grid line polygon edge wireframe visualization become dense resulting black surface removed data center second server similar visualization highlighted data section server ordering aligned differently click high re zooming returning grid line line width creates extra effect highlighting subtle change elevation click full version line fine visualization approach previous version line thick appears black animated data server time utilization data second shown one frame consists cpu highlighted pixel similar digit dropped animation sped normal time show second loop click full second advantage similar highlighted data example additional disadvantage included printed text identifying timebased pattern relies memory patience memory identify difference sequence frame patience consume information rate animation may become irritating one frame sixsecond loop interesting difficult study visible every loop additional control could added slow pause animation instantaneous value highlighted table current utilization value simple visualization answer question without density including historic data current utilization across cpu actually single frame previous animation utilization digit returned click highres server appear vertical column two row server microsoft window include type visualization task manager show instantaneous utilization value system virtual cpu logical processor screenshot right msdn blog post explains move show cpu scrollbar reveal switched screenshot blue red fit better visuals click original also think red better suggests hot cpu nt include historic data worth including consideration window type visualization device utilization may become widely understood also note microsoft call heat map show different type heat map quantized heat map section bar graph bar graph used show single utilization value scale length bar mac o x activity montior provide floating cpu window bar graph placed anywhere screen example pictured laptop two cpu core bar graph click see green original advantage utilization understood glance instead reading utilization digit examining color visualization could also enhanced placing watermark recent minimum maximum value using bar graph device become difficult scaling cpu scale bar graph may better suited average utilization across device vector graph angle used visualization device example gauge similar car tachometer show server cpu utilization average cpu commercial monitoring product intended cloud environment bar graph show single utilization value value may indicated around edge example green red spectrum sure coloring mean context cpu utilization least show end range could also decorative along whitespace around round shape make visualization type one least dense may problem scaling see could scale cloud environment created couple montage example image server cpu pixel wide image include single cpu utilization another find line graph showing time xaxis allows passing time visualized intuitively left right level utilization shown yaxis understood glance compared quickly accurately one second next comparison difficult color requires reading digit one server second cpu drawn separate line server previously visualized separately shown single cpu hitting clearly seen although line multiple cpu remaining overlap comparison busier server previously visualized cpu utilization loosely grouped around cpu hitting every eight second also noticeable longer flat bottom edge idle cpu showing usually cpu work server mostly idle previously visualized look like activity two period seen large spike singlecpu utilization every second smaller burst every eight second data center second scaling cpu visualization nt really worked using full range random color nt help either pattern horizontal line utilization visible cpu drawn last top previous line see bottom data center visualization highlighted data section cpu ordering shuffled drawn line vanish average one server taking first server showing average utilization across cpu work well average change time often used capacity planning average hide presence device utilization cpu system cpu contributes average even cpu change utilized line move much smaller harder see across multiple system adding maximum line show hottest device work practice often like know many device hot disk zfs filesystem example often hit period second transaction group flush perfectly normal one two disk utilization however normal particularly difficult pathology unreported drive failure seen many time like know knowing something hit many device close ternary plot barycentric coordinate system used create ternary plot showing three component cpu utilization user system idle usertime time spent application code systemtime time spent kernel processing system call interrupt routine asynchronous kernel thread breakdown cpu utilization commonly used unix linux operating system useful better understanding cpu workload mpstat tool print default usr sys idl data center second plot right includes cpu also animated frame second first second included keep gif small advantage visualization one three dimension picked point considered based dimension plot rotated aid also three dimension read directly point another potential advantage pattern user system cpu time could identified could also identified xy plot usr sys device type breakdown utilization three component could also visualized using created using cpuplayer tool awk program reprocess previously collected mpstat data modified cpuplayer handle high cpu count made cosmetic change simplify look original includes gridlines aid reading dimension limitation one issue visualization cpu overlap especially corner making difficult know many cpu state made worse data set using integer value utilization due limitation mpstat underlying operating system statistic case high resolution cpu microstates also issue making animation memory patience discussed previously triangle shape leaf much room unused top corner may become noticeable multiple ternary plot drawn instead animation note rectangular xy plot usr sys would similar degree unused space triangular area usr sys greater quantized heat map finally device utilization heat map aka heatmap us column quantization visualize utilization three dimension time xaxis percent utilization yaxis number cpu zaxis color saturation within timelatency range perhaps useful visualization created date bryan cantrill first developed sun storage analytics worked advanced product team sun microsystems mentioned towards end acmq article visualizing latency heading application summarizes concept utilization component also visualized heat map showing percent utilization individual component instead displaying average percent utilization across component utilization shown yaxis number component utilization shown color heatmap pixel particularly useful examining disk cpu utilization check load balanced across component tight grouping darker color show load balanced evenly cloud lighter pixel show nt outlier also interesting single cpu percent utilization may shown light line top heat map typically result software scalability issue single thread execution single disk percent utilization also interesting result disk failure identified using average maximum alone maximum differentiate single disk percent utilization multiple disk percent utilization happen normal burst load developed joyent cloud analytics product used analyze performance device across multiple system cloud utilization heat map may become standard tool visualizing device utilization especially light cpu scaling cloud computing environment data center second server cpu recapping xaxis time yaxis cpu utilization percent zaxis saturation show many cpu time utilization level shown right different previous visualization color longer represents utilization used cpu count rectangle make heat map bucket spanning time utilization range colored based cpu count darker mean darker color bottom heat map show constant concentration idle cpu red line top show constant presence cpu utilization dark color line show multiple cpu one explain exact color algorithm saturation section apart identifying multiple cpu show generally cpu idle near bottom plot subtle band also seen around one server second seeing look single server selected previously cpu cpu hitting seen top plot period cpu idle also clearly visible bottom number quantize bucket yaxis high cpu appears scatter plot reducing number bucket ten well reducing height quantize range likely span multiple cpu shade chosen help create pattern server example follow include much whitespace simple border added idle server idle server shown line graph earlier light load server light load single cpu sometimes hitting busy server busier server tighter distribution cpu utilization grouped around saturation color saturation bucket reflects relative number cpu quantized timeutilization range cpu darker color actual algorithm used nonlinear help identify subtle pattern linear algorithm could used make bucket highest cpu count darkest shade available bucket lowest cpu count probably zero lightest bucket inbetween scaled linearly practice wash detail bucket case cpu utilization idle bucket representing utilization would high cpu count others use much lighter shade appear washed example linear application saturation based value right heat map bucket first sorted least cpu full spectrum shade applied sorted list ensures full spectrum shade used making best use dimension allowing subtle pattern seen would otherwise washed approach devised bryan cantrill heat map used sun storage product named rankbased coloring hue value hue used red merely stay consistent image used highlighting data section red meant hotter cpu seems intuitive red mean concentration cpu even line looking represents idle cpu probably nt good choice color easily changed sun microsystems analytics chose blue joyent cloud analytics chose orange heat map could adjusted retain intuitive nature red mean hot first example right allowed top utilization range red lower utilization range grayscale beneath different example value red hue scaled based utilization may referred saturation depending color model used another use hue reflect fourth dimension joyent cloud analytics makeup heat map investigated highlighting component different hue collected fourth dimension data data center heat map individual server could highlighted hue david pacheco wrote heatmap coloring explain also provides example rankbased v linear coloring background thought using heat map device utilization burned performance issue development sun storage appliance including sloth disk disk mysteriously begin returning slow io second yet return error count hard soft percent utilization reported operating system percent busy would stay second time disk raid stripe idle sloth disk would kill performance needed way field engineer customer could easily identify constraint could nt look max utilization zfs file system often drove disk utilization transaction group flush identify presence one two disk hot thread usually software designed scale across available cpu cpu idle others utilization could simple codepath multithreaded nt one particular issue ran zfs pipeline originally stage could processed eight thread hot stage compression could limit zfs performance eight cpu could used since fixed type issue workload become bounded performance busy device majority device idle seen type problem across device type cpu disk network interface storage controller etc device utilization heat map quickly proved excellent way identify type issue well show many useful characteristic summary visualization created illustrate different way observe device utilization large scale environment frequently need analyze urgent performance issue environment using variety tool varying degree success sometimes customer unable resolve crippling issue visualization hide important detail common problem line graph showing average device utilization making impossible identify single multiple device condensed year pain frustration problem statement top page showed various visualization satisfy need suggest using quantized heat map identify single multiple device utilization minimum maximum device utilization device utilization balance time performance analysis line graph observe average utilization across multiple device time capacity planning visualization realtime change environment analyzed immediately repaired sooner dave pacheco showed joyent cloud analytics oscon presentation design implementation realtime cloud analytics platform visualization interactive example user could click device quantized heat map shown information explain many device server also consider including visualization plus text time important verbalize state performance quickly emergency concall text could include average utilization across device different time interval previous minute hour day week maximum utilization percentile could also included convey detail upper distribution love see quantized heat map show place currently bar graph line graph used acknowledgment many tool used create image post type command line interface tool mpstat solaris originally jeff bonwick visualized data using shell scripting awk firefox screengrab plugin tabulated data tool highlighted data tool surface plot made using r lattice package inspired dominic kay visualizing performance work animated data tool imagemagick assemble animation instantaneous value includes screenshot microsoft window bar graph includes mac o x activity monitor line graph created gnuplot trying tool could nt handle line ternary plot cpu visualization type created dr neil j gunther cpuplayer written stefan parvu quantized heat map type device utilization created sun microsystems working bryan cantrill developed sun storage zfs appliance developed dave pacheco robert mustacchi others cloud analytics team joyent gimp used post processing image style inspired edward tufte including clearing chart junk line graph use high definition graphic text micro heat map like tufte recommend reading visual explanation beautiful evidence envisioning information visual display quantitative information william cleveland element graphing data visualizing data tempted reassemble blog post categorize visualization univariate bivariate multivariate type thanks deirdr straughan editing page suggestion improve content people particularly jason hoffman referring book article link read visualization heat map see main page
677,Lobsters,scaling,Scaling and architecture,Summary of the AWS Service Event in the US East Region,http://aws.amazon.com/message/67457/,summary aws service event u east region,,july like share service disruption occurred last friday night june one availability zone u region event triggered large scale electrical storm swept northern virginia area regret problem experienced customer affected disruption addition giving detail also wanted provide information action taking mitigate issue future u region consists datacenters structured multiple availability zone availability zone distinct physical location engineered isolate failure last friday due weather warning approaching storm change activity u region cancelled extra personnel called datacenters evening friday night storm progressed several u datacenters availability zone would remain unaffected event evening saw utility power fluctuation backup system datacenters responded designed resulting loss power customer impact pdt large voltage spike experienced electrical switching equipment two u datacenters supporting single availability zone utility electrical switch datacenters initiated transfer generator power one datacenters transfer completed without incident generator started successfully generator independently failed provide stable voltage brought service result generator pick load server operated without interruption period uninterruptable power supply ups unit shortly thereafter utility power restored datacenter personnel transferred datacenter back utility power utility power region failed second time pdt room one facility failed successfully transfer generator power datacenters region continued operate without customer impact single datacenter successfully transfer generator backup server continued operate normally uninterruptable power supply ups power onsite personnel worked stabilize primary backup power generator ups system depleting server began losing power pdt ten minute later backup generator power stabilized upss restarted power started restored pdt pdt full facility power rack generator electrical switching equipment datacenter experienced failure brand installed late early prior installation facility generator rigorously tested manufacturer datacenter commissioning time passed load test approximately hour testing without issue may year conducted full load test entire datacenter switched ran successfully generator system operated correctly generator electrical equipment datacenter le two year old maintained manufacturer representative manufacturer standard tested weekly addition generator operated flawlessly brought online friday night hour utility power restored datacenter equipment repaired recertified manufacturer retested full load onsite replaced entirely interim generator ran successfully hour manually brought online confident perform properly load transferred therefore prior completing engineering work mentioned lengthen amount time electrical switching equipment give generator reach stable power switch board ass whether generator ready accept full power load additionally expand power quality tolerance allowed evaluating whether switch load generator power expand size onsite engineering staff ensure repeat event switch generator completed manually necessary upss discharge customer impact though resource datacenter including elastic compute cloud instance elastic block store eb storage volume relational database service rds instance elastic load balancer elb instance represent singledigit percentage total resource u region significant impact many customer impact manifested two form first unavailability instance volume running affected datacenter kind impact limited affected availability zone availability zone u region continued functioning normally second form impact degradation service control plane allow customer take action create remove change resource across region control plane required ongoing use resource particularly useful outage customer trying react loss resource one availability zone moving another eb approximately instance region impacted availability zone impacted power loss instance offline power restored system restarted instance operating availability zone within u region continued function prior event internet connectivity region unaffected vast majority instance came back online pdt midnight time completion recovery extended bottleneck server booting process removing bottleneck one action take improve recovery time face power failure eb comparable percentage relative volume region impacted event majority eb server brought pdt saturday however eb data volume inflight writes time power loss volume potential inconsistent state rather return volume potentially inconsistent state eb server back available eb brings customer volume back online impaired state io volume paused customer verify volume consistent resume using though time recover eb volume reduced dramatically last month number volume requiring processing large enough still took several hour complete backlog pdt outstanding volume turned customer identified several area recovery process optimize improve speed processing recovered volume control plane eb significantly impacted power failure call create new resource change existing resource failed pdt pdt customer able launch new instance create eb volume attach volume availability zone region pdt control plane functionality restored region customer trying attach detach impacted eb volume would continued experienced error impacted eb volume recovered duration recovery time eb control plane result inability rapidly fail new primary datastore eb apis implemented multiavailability zone replicated datastores datastores used store metadata resource instance volume snapshot protect datastore corruption currently primary copy loses power system automatically flip readonly mode availability zone power restored affected availability zone determine safe promote another copy primary addressing source blockage forced manual assessment required handmanaged failover control plane work already underway flip happen automatically elastic load balancing elastic load balancer elbs allow web traffic directed single ip address spread across many instance tool high availability traffic single endpoint handled many redundant server elbs live individual availability zone front instance zone availability zone singleavailability zone elbs elb service maintains one elb specified availability zone elb fails elb control plane assigns configuration ip address another elb server availability zone normally requires short period time large scale issue availability zone may insufficient capacity immediately provide new elb replacement wait capacity made available elbs also deployed multiple availability zone configuration availability zone endpoint separate ip address single domain name point endpoint ip address client web browser query dns domain name receives ip address record elbs random order client process single ip address many newer version webbrowsers retry subsequent ip address fail connect first large number nonbrowser client operate single ip address multiavailability zone elbs elb service maintains elbs redundantly availability zone customer request failure single machine datacenter take endpoint elb service avoids impact even client process single ip address detecting failure eliminating problematic elb instance ip address list returned dns elb control plane process management event elbs including traffic shift due failure size scaling elb due traffic growth addition removal instance association given elb disruption past friday night control plane encompasses call add new elb scale elb add instance elb remove traffic elbs began performing traffic shift account loss load balancer affected availability zone power system returned large number elbs came state triggered bug seen bug caused elb control plane attempt scale elbs larger elb instance size resulted sudden flood request began backlog control plane time customer began launching new instance replace capacity lost impacted availability zone requesting instance added existing load balancer zone request increased elb control plane backlog elb control plane currently manages request u region shared queue fell increasingly behind processing request pretty soon request started taking long time complete direct impact limited elbs failed poweraffected datacenter yet traffic shifted elb service inability quickly process new request delayed recovery many customer replacing lost capacity launching new instance availability zone multiavailability zone elbs client attempted connect elb healthy availability zone succeeded client attempted connect elb impacted availability zone retry using one alternate ip address returned would fail connect backlogged traffic shift occurred issued new dns query mentioned many modern web browser perform multiple attempt given multiple ip address many client especially game console consumer electronics use one ip address returned dns query result impact learning breaking elb processing multiple queue improve overall throughput allow rapid processing timesensitive action traffic shift also going immediately develop backup dns reweighting quickly shift elb traffic away impacted availability zone without contacting control plane relational database service rds rds provides two mode operation single availability zone singleaz single database instance operates one availability zone multi availability zone multiaz two database instance synchronously operated two different availability zone multiaz rds one two database instance primary standby primary handle database request replicates standby case primary fails standby promoted new primary singleaz rds instance default backup turned singleaz rds instance fails two kind recovery possible eb volume require recovery database instance simply restarted recovery required backup used restore database case backup turned customer recovery instance lost unless manual backup taken multiaz rds instance detect failure primary standby immediately take action primary fails dns cname record updated point standby standby fails new instance launched instantiated primary new standby failure confirmed failover take place le minute server lost power impacted datacenter many singleaz rds instance availability zone became unavailable way recover instance server powered booted brought online pdt large number affected singleaz rds instance brought online many remaining instance required eb recover storage volume followed timeline described eb impact volume recovered customer could apply backup restore singleaz rds instance addition action noted eb rds working improve speed volume available recovery processed point power loss multiaz instance almost instantly promoted standby healthy az primary expected however small number multiaz rds instance complete failover due software bug bug introduced april made change way handle storage failure manifested certain sequence communication failure experienced situation saw event variety server shutdown sequence occurred triggered failsafe required manual intervention complete failover case manual work could completed without eb recovery taking place majority remaining multiaz failovers completed pdt remaining multiaz instance processed eb volume recovery completed storage volume address issue multiaz rds instance failovers mitigation bug test rolling production coming week final thought apologize inconvenience trouble caused affected customer know critical service customer business followed history aws customer focus pace iterate think know everything learn event use drive improvement across service spend many hour coming day week improving understanding detail various part event determining make change improve service process sincerely aws team
678,Lobsters,scaling,Scaling and architecture,The Netflix Tech Blog: Asgard: Web-based Cloud Management and Deployment,http://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html,netflix tech blog asgard webbased cloud management deployment,asgard webbased cloud management deployment asgard open sourced github visual language cloud tango cloud model application cluster obiwan deployment method fast rollback rolling push task automation auto scaling servo aws management console aws management console hide amazon key auto scaling group enforce convention logging integrate system automate workflow simplify rest api cost http http feature film thor thor tale asgard conclusion netflix tech blog netflixoss netflix job page related resource asgard netflix cloud platform amazon web service,asgard webbased cloud management deploymentby joe sondow engineering toolsfor past several year netflix developer using selfservice tool build deploy hundred application service amazon cloud one tool asgard web interface application deployment cloud managementasgard named home norse god thunder lightning asgard netflix developer go control cloud happy announce asgard open sourced github available download use anyone need amazon web service account like open source netflix project asgard released apache license version please feel free fork project make improvement itsome information blog post also published following presentation note asgard originally named netflix application console nacvisual language cloudto help people identify various type cloud entity asgard us tango open source icon set addition icon help establish visual language help people understand looking navigate tango icon look familiar also used jenkins ubuntu mediawiki filezilla gimp sampling asgard cloud iconscloud modelthe netflix cloud model includes concept aws support directly application clustersapplicationbelow diagram amazon object required run single frontend application netflix autocomplete servicehere quick summary relationship cloud objectsan auto scaling group asg attach zero elastic load balancer elbs new instancesan elb send user traffic instancesan asg launch terminate instancesfor instance launch asg us launch configurationthe launch configuration specifies amazon machine image ami security group use launching instancethe ami contains bit instance including operating system common infrastructure apache tomcat specific version specific applicationsecurity group restrict traffic source port instancesthat lot stuff keep track one applicationwhen large number cloud object serviceoriented architecture like netflix important user able find relevant object particular application asgard us application registry simpledb naming convention associate multiple cloud object single application application owner email address establish responsible existence state application associated cloud objectsasgard limit set permitted character application name name cloud object parsed determine association applicationhere screenshot asgard showing filtered subset application running production account amazon cloud region screenshot detail screen single application link related cloud object clusteron top auto scaling group construct supplied amazon asgard infers object called cluster contains one asgs asgs associated naming convention new asg created within cluster incremented version number appended cluster base name form name new asg cluster provides asgard user ability perform deployment rolled back quicklyexample deployment cluster obiwan contains asgs screenshot cluster middeploymentthe old asg disabled meaning taking traffic remains available case problem occurs new asg traffic come elbs andor discovery internal netflix service yet open sourceddeployment methodsfast rollbackone primary feature asgard ability use cluster screen shown deploy new version application way reversed first sign trouble method requires instance use deployment greatly reduce duration service outage caused bad deploymentsthis animated diagram show simplified process using cluster interface try deployment roll back quickly problem animation illustrates following deployment use case create new asg traffic traffic result notice thing going badlyreenable traffic traffic log bad server diagnose problemsdelete pushasgard also provides alternative deployment system called rolling push similar conventional data center deployment cluster application server one asg needed old instance get gracefully deleted replaced new instance one two time instance asg replaced rolling push useful asg instance sharded instance distinct purpose duplicated another instanceif clustering mechanism application cassandra support sudden increase instance count clusterdownsides rolling push replacing instance small batch take long timereversing bad deployment take long timetask automationseveral common task built asgard automate deployment process animation showing timecompressed view automated rolling push action auto scalingnetflix focus asg primary unit deployment asgard also provides variety graphical control modifying asg setting metricsdriven auto scaling desiredcloudwatch metric selected default provided amazon cpuutilization custom metric published application using library like servo javawhy aws management console aws management console us someone amazon account password need configure something asgard provide however everyday largescale operation aws management console yet met need netflix cloud usage model built asgard instead reasonshide amazon keysnetflix grant employee lot freedom responsibility including right duty enhancing repairing production system system run amazon cloud although want enable hundred engineer manage cloud apps prefer give secret key access company amazon account directly providing internal console allows u grant asgard user access amazon account without telling many employee shared cloud password strategy also save u needing assign revoke hundred identity access management iam cloud account employeesauto scaling groupsas writing aws management console lack support auto scaling group asgs netflix relies asgs basic unit deployment management instance application one goal open sourcing asgard help amazon customer make greater use amazon sophisticated auto scaling feature asgs big part netflix formula provide reliability redundancy cost saving clustering discoverability ease deployment ability roll back bad deployment quicklyenforce conventionslike growing collection thing user allowed create cloud easily become confusing place full expensive unlabeled clutter part netflix cloud architecture use registered service associated cloud object naming convention asgard enforces naming convention order keep cloud saner place possible audit clean regularly thing get stale messy forgottenloggingso far aws console expose log recent user action account make difficult determine call problem start recent change might relate problem lack logging also nonstarter sensitive subsystem legally require auditabilityintegrate systemshaving console empowers u decide want add integration point engineering system jenkins internal discovery serviceautomate workflowmultiple step go safe intelligent deployment process knowing certain use case advance asgard perform necessary step deployment based one form submissionsimplify rest apifor common operation system need perform expose publish rest api exactly want way hide complex step usercostswhen using cloud service important keep lid cost june amazon provides way track account charge frequently data exposed asgard writing someone company keep track cloud cost regularly see http asgard initially cause incur amazon charge amazon free tier simpledb usage charge creating security group launch configuration empty auto scaling group however soon increase size asg zero amazon begin charging instance usage depending status amazon free usage tier creating elbs rds instance cloud object also cause incur charge become familiar cost creating many thing cloud remember delete experiment soon longer need amazon cost responsibility run cloud operation wiselycost reference http filmsby extraordinary coincidence thor thor tale asgard available watch netflix streamingconclusionasgard one primary tool application deployment cloud management netflix year releasing asgard open source community hope people find amazon cloud auto scaling easier work even large scale like netflix asgard feature released regularly welcome participation user githubfollow netflix tech blog netflixoss twitter feed open source component netflix cloud platformif interested working u solve interesting problem look netflix job page see something might suit hiring related resourcesasgardnetflix cloud platformamazon web service
